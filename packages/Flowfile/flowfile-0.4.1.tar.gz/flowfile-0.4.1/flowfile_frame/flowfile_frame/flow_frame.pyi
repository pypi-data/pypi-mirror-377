# Standard library imports
import collections
import inspect
import os
import sys
import typing
from io import IOBase
from typing import List, Optional, ForwardRef, TypeVar, Any, Iterable, Sequence, Mapping, Collection, Callable, Literal, IO, Union
from datetime import timedelta
from pathlib import Path
from collections.abc import Awaitable

# Third-party imports
import polars as pl
from polars._typing import *
from polars._typing import ParquetMetadata, PlanStage
from polars._utils.async_ import _GeventDataFrameResult
from polars.dependencies import polars_cloud as pc
from polars.io.cloud import CredentialProviderFunction
from polars.lazyframe.frame import LazyGroupBy
from polars import LazyFrame, DataFrame, QueryOptFlags
from polars.io.parquet import ParquetFieldOverwrites
from polars.lazyframe.opt_flags import DEFAULT_QUERY_OPT_FLAGS
from polars.type_aliases import (Schema, IntoExpr, ClosedInterval, Label, StartBy, RollingInterpolationMethod, IpcCompression, CompatLevel, SyncOnCloseMethod, ExplainFormat, EngineType, SerializationFormat, AsofJoinStrategy)

# Local application/library specific imports
import flowfile_frame
from flowfile_core.flowfile.flow_graph import FlowGraph
from flowfile_core.flowfile.flow_node.flow_node import FlowNode
from flowfile_frame import group_frame
from flowfile_frame.expr import Expr
from flowfile_core.schemas import transform_schema

# Conditional imports
if sys.version_info >= (3, 10):
    from typing import Concatenate
else:
    from typing_extensions import Concatenate

T = TypeVar('T')
P = typing.ParamSpec('P')
LazyFrameT = TypeVar('LazyFrameT', bound='LazyFrame')
FlowFrameT = TypeVar('FlowFrameT', bound='FlowFrame')
Self = TypeVar('Self', bound='FlowFrame')
NoneType = type(None)

# Module-level functions (example from your input)
def can_be_expr(param: inspect.Parameter) -> bool: ...
def generate_node_id() -> int: ...
def get_method_name_from_code(code: str) -> str | None: ...
def _contains_lambda_pattern(text: str) -> bool: ...
def _to_string_val(v) -> str: ...
def _extract_expr_parts(expr_obj) -> tuple[str, str]: ...
def _check_ok_for_serialization(method_name: str = None, polars_expr: pl.Expr | None = None, group_expr: pl.Expr | None = None) -> None: ...

class FlowFrame:
    data: LazyFrame
    flow_graph: FlowGraph
    node_id: int
    parent_node_id: Optional[int]

    # This special method determines how the object behaves in boolean contexts.
    def __bool__(self) -> Any: ...

    # This special method enables the 'in' operator to work with FlowFrame objects.
    def __contains__(self, key) -> Any: ...

    def __eq__(self, other: object) -> typing.NoReturn: ...

    def __ge__(self, other: Any) -> typing.NoReturn: ...

    def __gt__(self, other: Any) -> typing.NoReturn: ...

    # The __init__ method is intentionally left empty.
    def __init__(self, *args, **kwargs) -> None: ...

    def __le__(self, other: Any) -> typing.NoReturn: ...

    def __lt__(self, other: Any) -> typing.NoReturn: ...

    def __ne__(self, other: object) -> typing.NoReturn: ...

    # Unified constructor for FlowFrame.
    def __new__(cls, data: typing.Union[LazyFrame, collections.abc.Mapping[str, typing.Union[collections.abc.Sequence[object], collections.abc.Mapping[str, collections.abc.Sequence[object]], ForwardRef('Series')]], collections.abc.Sequence[typing.Any], ForwardRef('np.ndarray[Any, Any]'), ForwardRef('pa.Table'), ForwardRef('pd.DataFrame'), ForwardRef('ArrowArrayExportable'), ForwardRef('ArrowStreamExportable'), ForwardRef('torch.Tensor')] = None, schema: typing.Union[collections.abc.Mapping[str, typing.Union[ForwardRef('DataTypeClass'), ForwardRef('DataType'), type[int], type[float], type[bool], type[str], type['date'], type['time'], type['datetime'], type['timedelta'], type[list[typing.Any]], type[tuple[typing.Any, ...]], type[bytes], type[object], type['Decimal'], type[None], NoneType]], collections.abc.Sequence[typing.Union[str, tuple[str, typing.Union[ForwardRef('DataTypeClass'), ForwardRef('DataType'), type[int], type[float], type[bool], type[str], type['date'], type['time'], type['datetime'], type['timedelta'], type[list[typing.Any]], type[tuple[typing.Any, ...]], type[bytes], type[object], type['Decimal'], type[None], NoneType]]]], NoneType] = None, schema_overrides: collections.abc.Mapping[str, typing.Union[ForwardRef('DataTypeClass'), ForwardRef('DataType')]] | None = None, strict: bool = True, orient: typing.Optional[typing.Literal['col', 'row']] = None, infer_schema_length: int | None = 100, nan_to_null: bool = False, flow_graph: typing.Optional[flowfile_core.flowfile.flow_graph.FlowGraph] = None, node_id: typing.Optional[int] = None, parent_node_id: typing.Optional[int] = None, **kwargs) -> Self: ...

    def __repr__(self) -> Any: ...

    # Helper method to add a connection between nodes
    def _add_connection(self, from_id, to_id, input_type: typing.Literal['main', 'left', 'right'] = 'main') -> Any: ...

    # Add a cross join node to the graph.
    def _add_cross_join_node(self, new_node_id: int, join_input: transform_schema.CrossJoinInput, description: str, other: FlowFrame) -> None: ...

    def _add_number_of_records(self, new_node_id: int, description: str = None) -> 'FlowFrame': ...

    def _add_polars_code(self, new_node_id: int, code: str, depending_on_ids: typing.Optional[typing.List[str]] = None, convertable_to_code: bool = True, method_name: str = None, polars_expr: typing.Union[flowfile_frame.expr.Expr, typing.List[flowfile_frame.expr.Expr], NoneType] = None, group_expr: typing.Union[flowfile_frame.expr.Expr, typing.List[flowfile_frame.expr.Expr], NoneType] = None, kwargs_expr: typing.Optional[typing.Dict] = None, group_kwargs: typing.Optional[typing.Dict] = None, description: str = None) -> Any: ...

    # Add a regular join node to the graph.
    def _add_regular_join_node(self, new_node_id: int, join_input: transform_schema.JoinInput, description: str, other: FlowFrame) -> None: ...

    # Build kwargs dictionary for Polars join code.
    def _build_polars_join_kwargs(self, on: typing.Union[typing.List[str | flowfile_frame.expr.Column], str, flowfile_frame.expr.Column], left_on: typing.Union[typing.List[str | flowfile_frame.expr.Column], str, flowfile_frame.expr.Column], right_on: typing.Union[typing.List[str | flowfile_frame.expr.Column], str, flowfile_frame.expr.Column], left_columns: typing.Optional[typing.List[str]], right_columns: typing.Optional[typing.List[str]], how: str, suffix: str, validate: str, nulls_equal: bool, coalesce: bool, maintain_order: typing.Literal[None, 'left', 'right', 'left_right', 'right_left']) -> dict: ...

    def _comparison_error(self, operator: str) -> typing.NoReturn: ...

    # Helper method to create a new FlowFrame that's a child of this one
    def _create_child_frame(self, new_node_id) -> 'FlowFrame': ...

    # Detect if the expression is a cum_count operation and use record_id if possible.
    def _detect_cum_count_record_id(self, expr: Any, new_node_id: int, description: typing.Optional[str] = None) -> 'FlowFrame': ...

    # Ensure both FlowFrames are in the same graph, combining if necessary.
    def _ensure_same_graph(self, other: FlowFrame) -> None: ...

    # Execute join using native FlowFile join nodes.
    def _execute_native_join(self, other: FlowFrame, new_node_id: int, join_mappings: typing.Optional[typing.List], how: str, description: str) -> 'FlowFrame': ...

    # Execute join using Polars code approach.
    def _execute_polars_code_join(self, other: FlowFrame, new_node_id: int, on: typing.Union[typing.List[str | flowfile_frame.expr.Column], str, flowfile_frame.expr.Column], left_on: typing.Union[typing.List[str | flowfile_frame.expr.Column], str, flowfile_frame.expr.Column], right_on: typing.Union[typing.List[str | flowfile_frame.expr.Column], str, flowfile_frame.expr.Column], left_columns: typing.Optional[typing.List[str]], right_columns: typing.Optional[typing.List[str]], how: str, suffix: str, validate: str, nulls_equal: bool, coalesce: bool, maintain_order: typing.Literal[None, 'left', 'right', 'left_right', 'right_left'], description: str) -> 'FlowFrame': ...

    # Generates the `input_df.sort(...)` Polars code string using pure expression strings.
    def _generate_sort_polars_code(self, pure_sort_expr_strs: typing.List[str], descending_values: typing.List[bool], nulls_last_values: typing.List[bool], multithreaded: bool, maintain_order: bool) -> str: ...

    # Parse and validate join column specifications.
    def _parse_join_columns(self, on: typing.Union[typing.List[str | flowfile_frame.expr.Column], str, flowfile_frame.expr.Column], left_on: typing.Union[typing.List[str | flowfile_frame.expr.Column], str, flowfile_frame.expr.Column], right_on: typing.Union[typing.List[str | flowfile_frame.expr.Column], str, flowfile_frame.expr.Column], how: str) -> tuple[typing.Optional[typing.List[str]], typing.Optional[typing.List[str]]]: ...

    # Determine if we should use Polars code instead of native join.
    def _should_use_polars_code_for_join(self, maintain_order, coalesce, nulls_equal, validate, suffix) -> bool: ...

    def _with_flowfile_formula(self, flowfile_formula: str, output_column_name, description: str = None) -> 'FlowFrame': ...

    # Approximate count of unique values.
    def approx_n_unique(self, description: Optional[str] = None) -> 'FlowFrame': ...

    # Return the `k` smallest rows.
    def bottom_k(self, k: int, by: IntoExpr | Iterable[IntoExpr], reverse: bool | Sequence[bool] = False, description: Optional[str] = None) -> 'FlowFrame': ...

    def cache(self, description: Optional[str] = None) -> 'FlowFrame': ...

    # Cast LazyFrame column(s) to the specified dtype(s).
    def cast(self, dtypes: Mapping[ColumnNameOrSelector | PolarsDataType, PolarsDataType | PythonDataType] | PolarsDataType | pl.DataTypeExpr, strict: bool = True, description: Optional[str] = None) -> 'FlowFrame': ...

    # Create an empty copy of the current LazyFrame, with zero to 'n' rows.
    def clear(self, n: int = 0, description: Optional[str] = None) -> 'FlowFrame': ...

    # Create a copy of this LazyFrame.
    def clone(self, description: Optional[str] = None) -> 'FlowFrame': ...

    # Collect lazy data into memory.
    def collect(self, *args, **kwargs) -> DataFrame: ...

    # Collect DataFrame asynchronously in thread pool.
    def collect_async(self, gevent: bool = False, engine: EngineType = 'auto', optimizations: QueryOptFlags = DEFAULT_QUERY_OPT_FLAGS) -> Awaitable[DataFrame] | _GeventDataFrameResult[DataFrame]: ...

    # Resolve the schema of this LazyFrame.
    def collect_schema(self) -> Schema: ...

    # Get the column names.
    @property
    def columns(self) -> typing.List[str]: ...

    # Combine multiple FlowFrames into a single FlowFrame.
    def concat(self, other: typing.Union[ForwardRef('FlowFrame'), typing.List[ForwardRef('FlowFrame')]], how: str = 'vertical', rechunk: bool = False, parallel: bool = True, description: str = None) -> 'FlowFrame': ...

    # Return the number of non-null elements for each column.
    def count(self, description: Optional[str] = None) -> 'FlowFrame': ...

    # Simple naive implementation of creating the frame from any type. It converts the data to a polars frame,
    def create_from_any_type(self, data: typing.Union[collections.abc.Mapping[str, typing.Union[collections.abc.Sequence[object], collections.abc.Mapping[str, collections.abc.Sequence[object]], ForwardRef('Series')]], collections.abc.Sequence[typing.Any], ForwardRef('np.ndarray[Any, Any]'), ForwardRef('pa.Table'), ForwardRef('pd.DataFrame'), ForwardRef('ArrowArrayExportable'), ForwardRef('ArrowStreamExportable'), ForwardRef('torch.Tensor')] = None, schema: typing.Union[collections.abc.Mapping[str, typing.Union[ForwardRef('DataTypeClass'), ForwardRef('DataType'), type[int], type[float], type[bool], type[str], type['date'], type['time'], type['datetime'], type['timedelta'], type[list[typing.Any]], type[tuple[typing.Any, ...]], type[bytes], type[object], type['Decimal'], type[None], NoneType]], collections.abc.Sequence[typing.Union[str, tuple[str, typing.Union[ForwardRef('DataTypeClass'), ForwardRef('DataType'), type[int], type[float], type[bool], type[str], type['date'], type['time'], type['datetime'], type['timedelta'], type[list[typing.Any]], type[tuple[typing.Any, ...]], type[bytes], type[object], type['Decimal'], type[None], NoneType]]]], NoneType] = None, schema_overrides: collections.abc.Mapping[str, typing.Union[ForwardRef('DataTypeClass'), ForwardRef('DataType')]] | None = None, strict: bool = True, orient: typing.Optional[typing.Literal['col', 'row']] = None, infer_schema_length: int | None = 100, nan_to_null: bool = False, flow_graph = None, node_id = None, parent_node_id = None, description: Optional[str] = None) -> Any: ...

    # Creates a summary of statistics for a LazyFrame, returning a DataFrame.
    def describe(self, percentiles: Sequence[float] | float | None = ..., interpolation: QuantileMethod = 'nearest') -> DataFrame: ...

    # Read a logical plan from a file to construct a LazyFrame.
    def deserialize(self, source: str | Path | IOBase, format: SerializationFormat = 'binary', description: Optional[str] = None) -> 'FlowFrame': ...

    # Remove columns from the DataFrame.
    def drop(self, *columns, strict: bool = True, description: Optional[str] = None) -> 'FlowFrame': ...

    # Drop all rows that contain one or more NaN values.
    def drop_nans(self, subset: ColumnNameOrSelector | Collection[ColumnNameOrSelector] | None = None, description: Optional[str] = None) -> 'FlowFrame': ...

    # Drop all rows that contain one or more null values.
    def drop_nulls(self, subset: ColumnNameOrSelector | Collection[ColumnNameOrSelector] | None = None, description: Optional[str] = None) -> 'FlowFrame': ...

    # Get the column data types.
    @property
    def dtypes(self) -> typing.List[pl.classes.DataType]: ...

    # Create a string representation of the query plan.
    def explain(self, format: ExplainFormat = 'plain', optimized: bool = True, type_coercion: bool = True, predicate_pushdown: bool = True, projection_pushdown: bool = True, simplify_expression: bool = True, slice_pushdown: bool = True, comm_subplan_elim: bool = True, comm_subexpr_elim: bool = True, cluster_with_columns: bool = True, collapse_joins: bool = True, streaming: bool = False, engine: EngineType = 'auto', tree_format: bool | None = None, optimizations: QueryOptFlags = DEFAULT_QUERY_OPT_FLAGS) -> str: ...

    # Explode the dataframe to long format by exploding the given columns.
    def explode(self, columns: typing.Union[str, flowfile_frame.expr.Column, typing.Iterable[str | flowfile_frame.expr.Column]], *more_columns, description: str = None) -> 'FlowFrame': ...

    # Collect a small number of rows for debugging purposes.
    def fetch(self, n_rows: int = 500, type_coercion: bool = True, _type_check: bool = True, predicate_pushdown: bool = True, projection_pushdown: bool = True, simplify_expression: bool = True, no_optimization: bool = False, slice_pushdown: bool = True, comm_subplan_elim: bool = True, comm_subexpr_elim: bool = True, cluster_with_columns: bool = True, collapse_joins: bool = True) -> DataFrame: ...

    # Fill floating point NaN values.
    def fill_nan(self, value: int | float | Expr | None, description: Optional[str] = None) -> 'FlowFrame': ...

    # Fill null values using the specified value or strategy.
    def fill_null(self, value: Any | Expr | None = None, strategy: FillNullStrategy | None = None, limit: int | None = None, matches_supertype: bool = True, description: Optional[str] = None) -> 'FlowFrame': ...

    # Filter rows based on a predicate.
    def filter(self, *predicates, flowfile_formula: typing.Optional[str] = None, description: typing.Optional[str] = None, **constraints) -> 'FlowFrame': ...

    # Get the first row of the DataFrame.
    def first(self, description: Optional[str] = None) -> 'FlowFrame': ...

    def fuzzy_match(self, other: FlowFrame, fuzzy_mappings: typing.List[flowfile_core.schemas.transform_schema.FuzzyMap], description: str = None) -> 'FlowFrame': ...

    # Take every nth row in the LazyFrame and return as a new LazyFrame.
    def gather_every(self, n: int, offset: int = 0, description: Optional[str] = None) -> 'FlowFrame': ...

    def get_node_settings(self, description: Optional[str] = None) -> FlowNode: ...

    # Start a group by operation.
    def group_by(self, *by, description: Optional[str] = None, maintain_order: bool = False, **named_by) -> group_frame.GroupByFrame: ...

    # Group based on a time value (or index value of type Int32, Int64).
    def group_by_dynamic(self, index_column: IntoExpr, every: str | timedelta, period: str | timedelta | None = None, offset: str | timedelta | None = None, include_boundaries: bool = False, closed: ClosedInterval = 'left', label: Label = 'left', group_by: IntoExpr | Iterable[IntoExpr] | None = None, start_by: StartBy = 'window', description: Optional[str] = None) -> LazyGroupBy: ...

    def head(self, n: int, description: str = None) -> Any: ...

    # Inspect a node in the computation graph.
    def inspect(self, fmt: str = '{}', description: Optional[str] = None) -> 'FlowFrame': ...

    # Interpolate intermediate values. The interpolation method is linear.
    def interpolate(self, description: Optional[str] = None) -> 'FlowFrame': ...

    # Add a join operation to the Logical Plan.
    def join(self, other, on: typing.Union[typing.List[str | flowfile_frame.expr.Column], str, flowfile_frame.expr.Column] = None, how: str = 'inner', left_on: typing.Union[typing.List[str | flowfile_frame.expr.Column], str, flowfile_frame.expr.Column] = None, right_on: typing.Union[typing.List[str | flowfile_frame.expr.Column], str, flowfile_frame.expr.Column] = None, suffix: str = '_right', validate: str = None, nulls_equal: bool = False, coalesce: bool = None, maintain_order: typing.Literal[None, 'left', 'right', 'left_right', 'right_left'] = None, description: str = None) -> 'FlowFrame': ...

    # Perform an asof join.
    def join_asof(self, other: FlowFrame, left_on: str | None | Expr = None, right_on: str | None | Expr = None, on: str | None | Expr = None, by_left: str | Sequence[str] | None = None, by_right: str | Sequence[str] | None = None, by: str | Sequence[str] | None = None, strategy: AsofJoinStrategy = 'backward', suffix: str = '_right', tolerance: str | int | float | timedelta | None = None, allow_parallel: bool = True, force_parallel: bool = False, coalesce: bool = True, allow_exact_matches: bool = True, check_sortedness: bool = True, description: Optional[str] = None) -> 'FlowFrame': ...

    # Perform a join based on one or multiple (in)equality predicates.
    def join_where(self, other: FlowFrame, *predicates, suffix: str = '_right', description: Optional[str] = None) -> 'FlowFrame': ...

    # Get the last row of the DataFrame.
    def last(self, description: Optional[str] = None) -> 'FlowFrame': ...

    # Return lazy representation, i.e. itself.
    def lazy(self, description: Optional[str] = None) -> 'FlowFrame': ...

    def limit(self, n: int, description: str = None) -> Any: ...

    # Apply a custom function.
    def map_batches(self, function: Callable[[DataFrame], DataFrame], predicate_pushdown: bool = True, projection_pushdown: bool = True, slice_pushdown: bool = True, no_optimizations: bool = False, schema: None | SchemaDict = None, validate_output_schema: bool = True, streamable: bool = False, description: Optional[str] = None) -> 'FlowFrame': ...

    # Match or evolve the schema of a LazyFrame into a specific schema.
    def match_to_schema(self, schema: SchemaDict | Schema, missing_columns: Literal['insert', 'raise'] | Mapping[str, Literal['insert', 'raise'] | Expr] = 'raise', missing_struct_fields: Literal['insert', 'raise'] | Mapping[str, Literal['insert', 'raise']] = 'raise', extra_columns: Literal['ignore', 'raise'] = 'raise', extra_struct_fields: Literal['ignore', 'raise'] | Mapping[str, Literal['ignore', 'raise']] = 'raise', integer_cast: Literal['upcast', 'forbid'] | Mapping[str, Literal['upcast', 'forbid']] = 'forbid', float_cast: Literal['upcast', 'forbid'] | Mapping[str, Literal['upcast', 'forbid']] = 'forbid', description: Optional[str] = None) -> 'FlowFrame': ...

    # Aggregate the columns in the LazyFrame to their maximum value.
    def max(self, description: Optional[str] = None) -> 'FlowFrame': ...

    # Aggregate the columns in the LazyFrame to their mean value.
    def mean(self, description: Optional[str] = None) -> 'FlowFrame': ...

    # Aggregate the columns in the LazyFrame to their median value.
    def median(self, description: Optional[str] = None) -> 'FlowFrame': ...

    # Unpivot a DataFrame from wide to long format.
    def melt(self, id_vars: ColumnNameOrSelector | Sequence[ColumnNameOrSelector] | None = None, value_vars: ColumnNameOrSelector | Sequence[ColumnNameOrSelector] | None = None, variable_name: str | None = None, value_name: str | None = None, streamable: bool = True, description: Optional[str] = None) -> 'FlowFrame': ...

    # Take two sorted DataFrames and merge them by the sorted key.
    def merge_sorted(self, other: FlowFrame, key: str, description: Optional[str] = None) -> 'FlowFrame': ...

    # Aggregate the columns in the LazyFrame to their minimum value.
    def min(self, description: Optional[str] = None) -> 'FlowFrame': ...

    # Aggregate the columns in the LazyFrame as the sum of their null value count.
    def null_count(self, description: Optional[str] = None) -> 'FlowFrame': ...

    # Offers a structured way to apply a sequence of user-defined functions (UDFs).
    def pipe(self, function: Callable[Concatenate[LazyFrame, P], T], *args, description: Optional[str] = None, **kwargs) -> T: ...

    # Pivot a DataFrame from long to wide format.
    def pivot(self, on: str | list[str], index: str | list[str] | None = None, values: str | list[str] | None = None, aggregate_function: str | None = 'first', maintain_order: bool = True, sort_columns: bool = False, separator: str = '_', description: str = None) -> 'FlowFrame': ...

    # Profile a LazyFrame.
    def profile(self, type_coercion: bool = True, predicate_pushdown: bool = True, projection_pushdown: bool = True, simplify_expression: bool = True, no_optimization: bool = False, slice_pushdown: bool = True, comm_subplan_elim: bool = True, comm_subexpr_elim: bool = True, cluster_with_columns: bool = True, collapse_joins: bool = True, show_plot: bool = False, truncate_nodes: int = 0, figsize: tuple[int, int] = ..., engine: EngineType = 'auto', optimizations: QueryOptFlags = DEFAULT_QUERY_OPT_FLAGS, **_kwargs) -> tuple[DataFrame, DataFrame]: ...

    # Aggregate the columns in the LazyFrame to their quantile value.
    def quantile(self, quantile: float | Expr, interpolation: QuantileMethod = 'nearest', description: Optional[str] = None) -> 'FlowFrame': ...

    # Run a query remotely on Polars Cloud.
    def remote(self, context: pc.ComputeContext | None = None, plan_type: pc._typing.PlanTypePreference = 'dot', description: Optional[str] = None) -> pc.LazyFrameExt: ...

    # Remove rows, dropping those that match the given predicate expression(s).
    def remove(self, *predicates, description: Optional[str] = None, **constraints) -> 'FlowFrame': ...

    # Rename column names.
    def rename(self, mapping: Mapping[str, str] | Callable[[str], str], strict: bool = True, description: Optional[str] = None) -> 'FlowFrame': ...

    # Reverse the DataFrame.
    def reverse(self, description: Optional[str] = None) -> 'FlowFrame': ...

    # Create rolling groups based on a temporal or integer column.
    def rolling(self, index_column: IntoExpr, period: str | timedelta, offset: str | timedelta | None = None, closed: ClosedInterval = 'right', group_by: IntoExpr | Iterable[IntoExpr] | None = None, description: Optional[str] = None) -> LazyGroupBy: ...

    # Save the graph
    def save_graph(self, file_path: str, auto_arrange: bool = True, description: Optional[str] = None) -> Any: ...

    # Get an ordered mapping of column names to their data type.
    @property
    def schema(self) -> pl.Schema: ...

    # Select columns from the frame.
    def select(self, *columns, description: typing.Optional[str] = None) -> 'FlowFrame': ...

    # Select columns from this LazyFrame.
    def select_seq(self, *exprs, description: Optional[str] = None, **named_exprs) -> 'FlowFrame': ...

    # Serialize the logical plan of this LazyFrame to a file or string in JSON format.
    def serialize(self, file: IOBase | str | Path | None = None, format: SerializationFormat = 'binary', description: Optional[str] = None) -> bytes | str | None: ...

    # Flag a column as sorted.
    def set_sorted(self, column: str, descending: bool = False, description: Optional[str] = None) -> 'FlowFrame': ...

    # Shift values by the given number of indices.
    def shift(self, n: int | IntoExprColumn = 1, fill_value: IntoExpr | None = None, description: Optional[str] = None) -> 'FlowFrame': ...

    # Show a plot of the query plan.
    def show_graph(self, optimized: bool = True, show: bool = True, output_path: str | Path | None = None, raw_output: bool = False, figsize: tuple[float, float] = ..., type_coercion: bool = True, _type_check: bool = True, predicate_pushdown: bool = True, projection_pushdown: bool = True, simplify_expression: bool = True, slice_pushdown: bool = True, comm_subplan_elim: bool = True, comm_subexpr_elim: bool = True, cluster_with_columns: bool = True, collapse_joins: bool = True, engine: EngineType = 'auto', plan_stage: PlanStage = 'ir', _check_order: bool = True, optimizations: QueryOptFlags = DEFAULT_QUERY_OPT_FLAGS) -> str | None: ...

    # Write the data to a CSV file.
    def sink_csv(self, file: str, *args, separator: str = ',', encoding: str = 'utf-8', description: str = None) -> 'FlowFrame': ...

    # Evaluate the query in streaming mode and write to an IPC file.
    def sink_ipc(self, path: str | Path | IO[bytes] | PartitioningScheme, compression: IpcCompression | None = 'uncompressed', compat_level: CompatLevel | None = None, maintain_order: bool = True, storage_options: dict[str, Any] | None = None, credential_provider: CredentialProviderFunction | Literal['auto'] | None = 'auto', retries: int = 2, sync_on_close: SyncOnCloseMethod | None = None, mkdir: bool = False, lazy: bool = False, engine: EngineType = 'auto', optimizations: QueryOptFlags = DEFAULT_QUERY_OPT_FLAGS, description: Optional[str] = None) -> LazyFrame | None: ...

    # Evaluate the query in streaming mode and write to an NDJSON file.
    def sink_ndjson(self, path: str | Path | IO[bytes] | IO[str] | PartitioningScheme, maintain_order: bool = True, storage_options: dict[str, Any] | None = None, credential_provider: CredentialProviderFunction | Literal['auto'] | None = 'auto', retries: int = 2, sync_on_close: SyncOnCloseMethod | None = None, mkdir: bool = False, lazy: bool = False, engine: EngineType = 'auto', optimizations: QueryOptFlags = DEFAULT_QUERY_OPT_FLAGS, description: Optional[str] = None) -> LazyFrame | None: ...

    # Evaluate the query in streaming mode and write to a Parquet file.
    def sink_parquet(self, path: str | Path | IO[bytes] | PartitioningScheme, compression: str = 'zstd', compression_level: int | None = None, statistics: bool | str | dict[str, bool] = True, row_group_size: int | None = None, data_page_size: int | None = None, maintain_order: bool = True, storage_options: dict[str, Any] | None = None, credential_provider: CredentialProviderFunction | Literal['auto'] | None = 'auto', retries: int = 2, sync_on_close: SyncOnCloseMethod | None = None, metadata: ParquetMetadata | None = None, mkdir: bool = False, lazy: bool = False, field_overwrites: ParquetFieldOverwrites | Sequence[ParquetFieldOverwrites] | Mapping[str, ParquetFieldOverwrites] | None = None, engine: EngineType = 'auto', optimizations: QueryOptFlags = DEFAULT_QUERY_OPT_FLAGS, description: Optional[str] = None) -> LazyFrame | None: ...

    # Get a slice of this DataFrame.
    def slice(self, offset: int, length: int | None = None, description: Optional[str] = None) -> 'FlowFrame': ...

    # Sort the dataframe by the given columns.
    def sort(self, by: typing.Union[typing.List[typing.Union[flowfile_frame.expr.Expr, str]], flowfile_frame.expr.Expr, str], *more_by, descending: typing.Union[bool, typing.List[bool]] = False, nulls_last: typing.Union[bool, typing.List[bool]] = False, multithreaded: bool = True, maintain_order: bool = False, description: typing.Optional[str] = None) -> 'FlowFrame': ...

    # Execute a SQL query against the LazyFrame.
    def sql(self, query: str, table_name: str = 'self', description: Optional[str] = None) -> 'FlowFrame': ...

    # Aggregate the columns in the LazyFrame to their standard deviation value.
    def std(self, ddof: int = 1, description: Optional[str] = None) -> 'FlowFrame': ...

    # Aggregate the columns in the LazyFrame to their sum value.
    def sum(self, description: Optional[str] = None) -> 'FlowFrame': ...

    # Get the last `n` rows.
    def tail(self, n: int = 5, description: Optional[str] = None) -> 'FlowFrame': ...

    # Split text in a column into multiple rows.
    def text_to_rows(self, column: str | flowfile_frame.expr.Column, output_column: str = None, delimiter: str = None, split_by_column: str = None, description: str = None) -> 'FlowFrame': ...

    # Get the underlying ETL graph.
    def to_graph(self, description: Optional[str] = None) -> Any: ...

    # Return the `k` largest rows.
    def top_k(self, k: int, by: IntoExpr | Iterable[IntoExpr], reverse: bool | Sequence[bool] = False, description: Optional[str] = None) -> 'FlowFrame': ...

    # Drop duplicate rows from this dataframe.
    def unique(self, subset: typing.Union[str, ForwardRef('Expr'), typing.List[typing.Union[ForwardRef('Expr'), str]]] = None, keep: typing.Literal['first', 'last', 'any', 'none'] = 'any', maintain_order: bool = False, description: str = None) -> 'FlowFrame': ...

    # Decompose struct columns into separate columns for each of their fields.
    def unnest(self, columns: ColumnNameOrSelector | Collection[ColumnNameOrSelector], *more_columns, description: Optional[str] = None) -> 'FlowFrame': ...

    # Unpivot a DataFrame from wide to long format.
    def unpivot(self, on: list[str | flowfile_frame.selectors.Selector] | str | None | flowfile_frame.selectors.Selector = None, index: list[str] | str | None = None, variable_name: str = 'variable', value_name: str = 'value', description: str = None) -> 'FlowFrame': ...

    # Update the values in this `LazyFrame` with the values in `other`.
    def update(self, other: FlowFrame, on: str | Sequence[str] | None = None, how: Literal['left', 'inner', 'full'] = 'left', left_on: str | Sequence[str] | None = None, right_on: str | Sequence[str] | None = None, include_nulls: bool = False, maintain_order: MaintainOrderJoin | None = 'left', description: Optional[str] = None) -> 'FlowFrame': ...

    # Aggregate the columns in the LazyFrame to their variance value.
    def var(self, ddof: int = 1, description: Optional[str] = None) -> 'FlowFrame': ...

    # Get the number of columns.
    @property
    def width(self) -> int: ...

    # Add or replace columns in the DataFrame.
    def with_columns(self, *exprs: Union[Expr, Iterable[Expr], Any], flowfile_formulas: Optional[List[str]] = None, output_column_names: Optional[List[str]] = None, description: Optional[str] = None, **named_exprs: Union[Expr, Any]) -> 'FlowFrame': ...

    # Add columns to this LazyFrame.
    def with_columns_seq(self, *exprs, description: Optional[str] = None, **named_exprs) -> 'FlowFrame': ...

    # Add an external context to the computation graph.
    def with_context(self, other: Self | list[Self], description: Optional[str] = None) -> 'FlowFrame': ...

    # Add a column at index 0 that counts the rows.
    def with_row_count(self, name: str = 'row_nr', offset: int = 0, description: Optional[str] = None) -> 'FlowFrame': ...

    # Add a row index as the first column in the DataFrame.
    def with_row_index(self, name: str = 'index', offset: int = 0, description: str = None) -> 'FlowFrame': ...

    def write_csv(self, file: str | os.PathLike, separator: str = ',', encoding: str = 'utf-8', convert_to_absolute_path: bool = True, description: str = None, **kwargs) -> 'FlowFrame': ...

    # Write the data frame to cloud storage in CSV format.
    def write_csv_to_cloud_storage(self, path: str, connection_name: typing.Optional[str] = None, delimiter: str = ';', encoding: typing.Literal['utf8', 'utf8-lossy'] = 'utf8', description: typing.Optional[str] = None) -> 'FlowFrame': ...

    # Write the data frame to cloud storage in Delta Lake format.
    def write_delta(self, path: str, connection_name: typing.Optional[str] = None, write_mode: typing.Literal['overwrite', 'append'] = 'overwrite', description: typing.Optional[str] = None) -> 'FlowFrame': ...

    # Write the data frame to cloud storage in JSON format.
    def write_json_to_cloud_storage(self, path: str, connection_name: typing.Optional[str] = None, description: typing.Optional[str] = None) -> 'FlowFrame': ...

    # Write the data to a Parquet file. Creates a standard Output node if only
    def write_parquet(self, path: str | os.PathLike, convert_to_absolute_path: bool = True, description: str = None, **kwargs) -> 'FlowFrame': ...

    # Write the data frame to cloud storage in Parquet format.
    def write_parquet_to_cloud_storage(self, path: str, connection_name: typing.Optional[str] = None, compression: typing.Literal['snappy', 'gzip', 'brotli', 'lz4', 'zstd'] = 'snappy', description: typing.Optional[str] = None) -> 'FlowFrame': ...

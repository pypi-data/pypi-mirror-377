paidiverpy.utils.benchmark.benchmark_test
=========================================

.. py:module:: paidiverpy.utils.benchmark.benchmark_test

.. autoapi-nested-parse::

   This module contains functions to run the benchmark test.

   ..
       !! processed by numpydoc !!


Functions
---------

.. autoapisummary::

   paidiverpy.utils.benchmark.benchmark_test.benchmark_task
   paidiverpy.utils.benchmark.benchmark_test.plot_results
   paidiverpy.utils.benchmark.benchmark_test.update_yaml
   paidiverpy.utils.benchmark.benchmark_test.benchmark_threads
   paidiverpy.utils.benchmark.benchmark_test.benchmark_local
   paidiverpy.utils.benchmark.benchmark_test.benchmark_slurm
   paidiverpy.utils.benchmark.benchmark_test.benchmark_handler


Module Contents
---------------

.. py:function:: benchmark_task(configuration_file: str | pathlib.Path, logger: logging.Logger) -> tuple[float, float]

   
   Run the benchmark task.

   :param configuration_file: The path to the configuration file.
   :type configuration_file: str
   :param logger: The logger to log messages.
   :type logger: logging.Logger















   ..
       !! processed by numpydoc !!

.. py:function:: plot_results(results: list[dict[str, Any]], cluster_type: str, filename: str) -> None

   
   Plot the benchmark results.

   :param results: The list of benchmark results.
   :type results: list
   :param cluster_type: The cluster type.
   :type cluster_type: str
   :param filename: The filename to save the plot.
   :type filename: str















   ..
       !! processed by numpydoc !!

.. py:function:: update_yaml(file_path: str | pathlib.Path, cluster_type: str | None, output_file: str | pathlib.Path, n_jobs: int, **kwargs: dict[str, Any]) -> str | pathlib.Path

   
   Update the YAML file with new benchmarking parameters and save it.

   :param file_path: The path to the configuration file.
   :type file_path: str
   :param cluster_type: The cluster type.
   :type cluster_type: str
   :param output_file: The output file path.
   :type output_file: str
   :param n_jobs: The number of jobs.
   :type n_jobs: int
   :param \*\*kwargs: The benchmarking parameters. It should be a dictionary with the following:
                      For LocalCluster:
                      - workers (int): The number of workers.
                      - threads (int): The number of threads.
                      - memory (int): The memory limit.
                      For SLURM:
                      - cores (int): The number of cores.
                      - processes (int): The number of processes.
                      - memory (int): The memory limit.
                      - walltime (str): The walltime.
                      - queue (str): The queue name.

   :returns: The output file path.
   :rtype: str















   ..
       !! processed by numpydoc !!

.. py:function:: benchmark_threads(benchmark_params: dict[str, Any], configuration_file: str | pathlib.Path, logger: logging.Logger) -> list[dict[str, Any]]

   
   Handle the benchmark test for LocalCluster.

   :param benchmark_params: The benchmark parameters.
   :type benchmark_params: dict
   :param configuration_file: The path to the configuration files.
   :type configuration_file: str | Path
   :param logger: The logger to log messages.
   :type logger: logging.Logger

   :returns: The benchmark results.
   :rtype: list















   ..
       !! processed by numpydoc !!

.. py:function:: benchmark_local(benchmark_params: dict[str, Any], configuration_file: str | pathlib.Path, logger: logging.Logger) -> list[dict[str, Any]]

   
   Handle the benchmark test for LocalCluster.

   :param benchmark_params: The benchmark parameters.
   :type benchmark_params: dict
   :param configuration_file: The path to the configuration files.
   :type configuration_file: str | Path
   :param logger: The logger to log messages.
   :type logger: logging.Logger

   :returns: The benchmark results.
   :rtype: list















   ..
       !! processed by numpydoc !!

.. py:function:: benchmark_slurm(benchmark_params: dict[str, Any], configuration_file: str | pathlib.Path, logger: logging.Logger) -> list[dict[str, Any]]

   
   Handle the benchmark test for SLURM.

   :param benchmark_params: The benchmark parameters.
   :type benchmark_params: dict
   :param configuration_file: The path to the configuration files.
   :type configuration_file: str | Path
   :param logger: The logger to log messages.
   :type logger: logging.Logger

   :returns: The benchmark results.
   :rtype: list















   ..
       !! processed by numpydoc !!

.. py:function:: benchmark_handler(benchmark_params: dict[str, Any], configuration_file: str | pathlib.Path, logger: logging.Logger) -> None

   
   Handle the benchmark test.

   :param benchmark_params: The benchmark parameters.
   :type benchmark_params: dict
   :param configuration_file: The path to the configuration files.
   :type configuration_file: str
   :param logger: The logger to log messages.
   :type logger: logging.Logger















   ..
       !! processed by numpydoc !!


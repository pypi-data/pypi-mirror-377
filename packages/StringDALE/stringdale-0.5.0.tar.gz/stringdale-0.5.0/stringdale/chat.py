# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/024_llms.ipynb.

# %% auto 0
__all__ = ['logger', 'tools_client', 'json_client', 'raw_client', 'complete_raw', 'complete', 'answer_question', 'choose',
           'choose_many', 'clean_model', 'structured_output', 'User', 'function_to_input_description',
           'description_to_model', 'function_to_input_model', 'call_tools', 'Chat', 'image_to_text', 'speech_to_text']

# %% ../nbs/024_llms.ipynb 4
from .core import get_git_root, load_env, checkLogs,  json_render,json_undeclared_vars,disk_cache
import openai 
from pydantic import BaseModel, create_model
from typing import Optional, Dict, Any, List, Union
import json
import re
from parse import parse
from pathlib import Path
from enum import Enum
from pydantic import BaseModel
import logging
import os
from .core import semaphore_decorator


# %% ../nbs/024_llms.ipynb 5
load_env()
logger = logging.getLogger(__name__)


# %% ../nbs/024_llms.ipynb 6
from typing import (
    Callable, 
    Type, 
    Optional, 
    List, 
    Dict, 
    Any, 
    Union,
    Literal,
    get_type_hints
)
from pydantic import (
    BaseModel, 
    Field, 
    create_model,
    ConfigDict
)
import inspect
import sys
import sqlmodel
from typing import Optional
# TODO make chat outputschema also get typing (like bool)

# %% ../nbs/024_llms.ipynb 10
import instructor
from openai import OpenAI,AsyncOpenAI
from pydantic import BaseModel
from singleton_decorator import singleton


# %% ../nbs/024_llms.ipynb 11
# these singletons ensure we dont require an OPENAI_API_KEY env var before actually trying to get completions from the LLM
@singleton
def tools_client():
    return instructor.from_openai(AsyncOpenAI(),mode=instructor.Mode.TOOLS)
@singleton
def json_client():
    return instructor.from_openai(AsyncOpenAI(),mode=instructor.Mode.JSON)
@singleton
def raw_client():
    return AsyncOpenAI()

# %% ../nbs/024_llms.ipynb 12
@disk_cache.cache(ignore=['response_model'])
async def complete_raw(model, messages, response_model=None, response_schema=None, mode = 'json' , seed=42,**kwargs):
    """
    This function is used to complete a chat completion with instructor without having basemodels as input or output.
    used for disk caching of results.
    """
    if mode == 'json':
        client = json_client()
    elif mode == 'tools':
        client = tools_client()
    elif mode == 'raw':
        client = raw_client()
        # For raw mode, we use the standard OpenAI client API
        completion = await client.chat.completions.create(
            model=model,
            messages=messages,
            seed=seed,
            **kwargs
        )
        usage = {
            "input_tokens": completion.usage.prompt_tokens,
            "output_tokens": completion.usage.completion_tokens
        }
        # Return the raw response content and usage
        return completion.choices[0].message.content, usage
    else:
        raise ValueError(f"Invalid mode: {mode}")
    
    response, completion = await client.chat.completions.create_with_completion(
        model=model,
        messages=messages,
        response_model=response_model,
        seed=seed,
        **kwargs
    )
    usage = {
        "input_tokens": completion.usage.prompt_tokens,
        "output_tokens": completion.usage.completion_tokens
    }
    return response.model_dump_json(), usage

async def complete(model, messages, response_model,mode='json',print_prompt=False,**kwargs):
    # Compute schema if response model provided
    if isinstance(response_model, type) and issubclass(response_model, BaseModel):
        response_schema = response_model.model_json_schema()
    else:
        response_schema = str(response_model)
    if print_prompt:
        print(messages)
    response, usage = await complete_raw(
        model=model,
        messages=messages,
        response_model=response_model,
        response_schema=response_schema,
        mode=mode,
        **kwargs
    )
    if mode == 'raw':
        return response, usage
    else:
        return response_model.model_validate_json(response), usage

# %% ../nbs/024_llms.ipynb 16
async def answer_question(model,messages,**api_kwargs):
    res,usage = await complete(model,messages,response_model=None,mode='raw',**api_kwargs)
    return res,usage

# %% ../nbs/024_llms.ipynb 19
async def choose(model,messages,choices,**api_kwargs):
    class Choice(BaseModel):
        choice: Literal[tuple(choices)]
    res,usage = await complete(model,messages,Choice,**api_kwargs)
    return res.choice,usage


# %% ../nbs/024_llms.ipynb 22
async def choose_many(model,messages,choices,**api_kwargs):
    class Choice(BaseModel):
        choice: Literal[tuple(choices)]

    class Choices(BaseModel):
        choices: List[Choice]   
    res,usage = await complete(model,messages,Choices,**api_kwargs)
    return [c.choice for c in res.choices],usage


# %% ../nbs/024_llms.ipynb 25
def clean_model(model: Type[sqlmodel.SQLModel], name: Optional[str] = None) -> Type[BaseModel]:
    """Convert an SQLModel to a Pydantic BaseModel.
    used to clean up the output for the LLM
    Args:
        model: SQLModel class to convert
        name: Optional name for the new model class
        
    Returns:
        A Pydantic BaseModel class with the same fields
    """
    # Get field definitions from the SQLModel
    fields = {}
    for field_name, field in model.model_fields.items():
        fields[field_name] = (field.annotation, field)
    
    # Create new model name if not provided
    model_name = name or f"{model.__name__}Schema"
    
    # Create and return new Pydantic model
    return create_model(model_name, **fields)

# %% ../nbs/024_llms.ipynb 26
async def structured_output(model,messages,output_schema,as_json=False,**api_kwargs):

    is_sqlmodel = isinstance(output_schema,type) and issubclass(output_schema,sqlmodel.SQLModel)
    if is_sqlmodel:
        clean_schema = clean_model(output_schema)
    else:
        clean_schema = output_schema

    res,usage = await complete(model,messages,clean_schema,**api_kwargs)

    if is_sqlmodel:
        res = output_schema(**res.model_dump())
    if as_json:
        res = res.model_dump()
    return res,usage


# %% ../nbs/024_llms.ipynb 30
class User(sqlmodel.SQLModel, table=False):
    id: Optional[int] = sqlmodel.Field(default=None, primary_key=True)
    name: Optional[str] = sqlmodel.Field(default=None)
    age: Optional[int] = sqlmodel.Field(default=None)
    email: Optional[str] = sqlmodel.Field(default=None)



# %% ../nbs/024_llms.ipynb 33
import docstring_parser 


# %% ../nbs/024_llms.ipynb 34
def function_to_input_description(func: Callable) -> Dict[str, Any]:
    """Extract parameter information from a function's signature and docstring.
    
    Args:
        func: Function to analyze
        
    Returns:
        Dictionary containing:
            - name: Function name
            - params: Dict of parameter info with:
                - type: Parameter type annotation
                - description: Parameter description from docstring
                - default: Default value if any
    """
    # Get type hints and signature
    hints = get_type_hints(func)
    sig = inspect.signature(func)
    
    # Remove return annotation if present
    hints.pop('return', None)
    
    # Parse docstring for parameter descriptions
    docstring = inspect.getdoc(func)
    arg_descriptions = {}
    if docstring:
        parsed = docstring_parser.parse(docstring)
        arg_descriptions = {
            p.arg_name: p.description 
            for p in parsed.params
        }
    
    # Build parameter info
    params = {}
    for name, param in sig.parameters.items():
        params[name] = {
            'type': hints.get(name, Any),
            'description': arg_descriptions.get(name, f"Parameter {name}"),
            'default': None if param.default == inspect.Parameter.empty else param.default
        }
    
    return {
        'name': func.__name__,
        'params': params
    }

def description_to_model(desc: Dict[str, Any], model_name: Optional[str] = None) -> Type[BaseModel]:
    """Create a Pydantic model from a function description.
    
    Args:
        desc: Function description from function_to_input_description
        model_name: Optional name for the model class
        
    Returns:
        Pydantic model class
    """
    # Create model fields
    fields = {}
    
    for name, info in desc['params'].items():
        field = Field(
            description=info['description'],
            default=info['default'] if info['default'] is not None else ...
        )
        fields[name] = (info['type'], field)
    
    # Create model class
    model_name = model_name or f"{desc['name']}Input"
    return create_model(
        model_name, 
        __config__=ConfigDict(extra='forbid'),
        **fields
    )



# %% ../nbs/024_llms.ipynb 37
def function_to_input_model(func: Callable,name:str,descriminator_field:str="tool_name") -> Type[BaseModel]:
    """Convert a function to a Pydantic input model.
    
    Args:
        func: Function to analyze
        
    Returns:
        Pydantic model class for function inputs
    """
    desc = function_to_input_description(func)
    desc['params'][descriminator_field] = {
        'type': Literal[name],
        'description': 'The name of the function to call',
        'default': None
    }
    return description_to_model(desc)


# %% ../nbs/024_llms.ipynb 38
async def call_tools(
    model: str,
    messages: List[Dict[str, str]], 
    tools: Dict[str, Callable],
    call_function: bool=False,
    descriminator_field:str="tool_name",
    **api_kwargs
) -> Dict[str, Any]:
    """Call OpenAI chat completion with tool selection and input parsing.
    
    Args:
        model: OpenAI model name
        messages: List of message dicts with role and content
        tools: Dictionary mapping tool names to functions
        descriminator_field: The name of the field to use as the discriminator
    Returns:
        Dict with:
            - tool_name: Selected tool name
            - tool_input: Parsed input for the tool
    """
    # Create input models for each tool
    tool_models = {
        name: function_to_input_model(func,name,descriminator_field)
        for name, func in tools.items()
    }
    
    # Create discriminated union model for tool inputs
    class ToolsInput(BaseModel):
        tool_input: Union[tuple(tool_models.values())] = Field(discriminator=descriminator_field)
    
    tool_description = '\n'.join([f'{name}:{inspect.getdoc(func)}' for name,func in tools.items()])
    system_message = [
        {'role':'system',
        'content':f'choose an appropriate tool to use to answer the following thought based on the following tools:\n'
                  f'{tool_description}'
        }
    ]
    messages = system_message + messages

    # Get tool input from LLM
    response, usage = await complete(
        model=model,
        messages=messages,
        response_model=ToolsInput,
        mode='tools',
        **api_kwargs
    )
    
    kwargs = response.tool_input.model_dump()
    func_name = kwargs.pop(descriminator_field)

    result = {
        'name': func_name,
        'input': kwargs,
    }
    if call_function:
        try:
            result['output'] = tools[func_name](**kwargs)
        except Exception as e:
            result['output'] = f'Error: {str(e)}'
    return result,usage
    

# %% ../nbs/024_llms.ipynb 44
from copy import deepcopy,copy
from pprint import pformat,pprint


# %% ../nbs/024_llms.ipynb 45
class Chat:
    """A Chat objects the renders a prompt and calls an LLM. Currently supporting openai models.
    
    Args:
        model: OpenAI model name
        messages: List of message dicts, must have at least a role and content field
        output_schema: Optional schema for structured output
        as_json: Optional boolean to return the response as a json object
        tools: Optional dictionary of tool names and functions that the LLM can decide to call. Causes the content of the response
            to be a dict of the form {'name':tool_name,'input':tool_input_dict}
        call_function: if tools are provided, whether to call the function and save the output in the output field of the response's content
        choices: Optional List of choices for multi-choice questions
        multi_choice: if choices are provided, whether to choose multiple items from the list
        seed: Optional seed for random number generation
        stop: Optional string or list of strings where the model should stop generating
        save_history: Optional boolean to save the history of the chat between calls
        append_output: Optional, whether to append the output of the chat to history automatically, default False
        init_messages: Optional list of messages that are always prepended to messages.
            Useful for supplying additional messages during calls.
            Can have template variables that are fed during initialization only.
            If save_history is True, the init messages are added to the history.
        **kwargs: Keyword arguments to interpolate into the messages
    """
    def __init__(self,
        model: Optional[str] = None,
        messages: Optional[List[Dict[str, str]]] = None, 
        output_schema: Optional[BaseModel] = None,
        as_json: Optional[bool] = False,
        tools: Optional[Dict[str,Callable]] = None,
        call_function: Optional[bool] = False,
        choices: Optional[Enum] = None,
        multi_choice: Optional[bool] = False,
        seed: Optional[int] = 42,
        stop: Optional[Union[str, List[str]]] = None,
        log_prompt: bool = False,
        save_history: bool = False,
        append_output: bool = False,
        init_messages: Optional[List[Dict[str, str]]] = None,
        **kwargs):

        self.model = model
        self.messages = deepcopy(messages)
        self.output_schema = output_schema
        self.as_json = as_json
        self.tools = tools
        self.call_function = call_function
        self.choices = choices
        self.multi_choice = multi_choice
        self.seed = seed
        self.stop = stop
        self.log_prompt = log_prompt
        self.baked_kwargs = kwargs
        self.save_history = save_history
        self.append_output = append_output
        
        if init_messages is None:
            init_messages = []
        self.init_messages = json_render(init_messages,context=kwargs)
    
        self.reset()

    def reset(self):
        """Resets state of Chat"""
        if self.save_history:
            self.history = []
            self.history.extend(self.init_messages)
        else:
            self.history = None
    
    def dump_state(self):
        """dumps the node state"""
        return self.history

    def load_state(self,state_object):
        """loads node state"""
        self.history = state_object

    def __copy__(self):
        chat_copy = Chat(
            model=self.model,
            messages=self.messages,
            output_schema=self.output_schema,
            as_json=self.as_json,
            tools=self.tools,
            call_function=self.call_function,
            choices=self.choices,
            multi_choice=self.multi_choice,
            seed=self.seed,
            stop=self.stop,
            log_prompt=self.log_prompt,
            save_history=self.save_history,
            append_output=self.append_output,
            init_messages=self.init_messages,
            **self.baked_kwargs
        )
        return chat_copy

    async def __call__(self, **kwargs) -> Dict[str, Any]:
        """Format prompt with kwargs and call OpenAI chat.
        Init parameters such as output_schema, tools, choices, seed, stop, as well as template variables
        can be set or overridden by kwargs
        
        Args:
            **kwargs: Values for format string placeholders
            
        Returns:
            a dictionary with the following keys:
            - role (str): Always "assistant"
            - content: the llm response.
            - meta (dict): Usage statistics including input and output tokens
        """
        model = kwargs.get("model", self.model)
        messages = kwargs.get("messages", self.messages)
        output_schema = kwargs.get("output_schema", self.output_schema)
        as_json = kwargs.get("as_json", self.as_json)
        tools = kwargs.get("tools", self.tools)
        call_function = kwargs.get("call_function", self.call_function)
        choices = kwargs.get("choices", self.choices)
        multi_choice = kwargs.get("multi_choice", self.multi_choice)
        seed = kwargs.get("seed", self.seed)
        stop = kwargs.get("stop", self.stop)

        if model is None:
            raise ValueError("model is required but not provided")
        if messages is None:
            raise ValueError("messages is required but not provided")

        prompt_kwargs = {**self.baked_kwargs, **kwargs}

        required_kwargs = json_undeclared_vars(messages)
        if not required_kwargs <= set(prompt_kwargs):
            missing = required_kwargs - set(prompt_kwargs)
            raise ValueError(f"Missing required kwargs: {missing}")

        formatted_messages = json_render(messages, context=prompt_kwargs)

        if self.save_history:
            self.history.extend(formatted_messages)
            formatted_messages = self.history
        else:
            formatted_messages = self.init_messages + formatted_messages

        if self.log_prompt:
            logger.warning(f'calling llm with model={model} and prompt:\n'
                        f'messages={pformat(formatted_messages)}\n'
                        )

        completion_kwargs = {
            'model':model,
            'messages':formatted_messages,
            'seed':seed,
            'stop':stop,
            'print_prompt':prompt_kwargs.get('print_prompt',False),
        }

        if choices:
            if multi_choice:
                res,usage = await choose_many(choices=choices,**completion_kwargs)
            else:
                res,usage = await choose(choices=choices,**completion_kwargs)
        elif output_schema:
            res,usage = await structured_output(output_schema=output_schema,as_json=as_json,**completion_kwargs)
        elif tools:
            res,usage = await call_tools(tools=tools,call_function=call_function,**completion_kwargs)
        else:
            res,usage = await answer_question(**completion_kwargs)

        response = {
            'role':'assistant',
            'content':res,
            'meta':usage
        }
        if self.save_history and self.append_output:
            self.history.append(response)
        return response
    
    def __str__(self) -> str:
        """String representation showing required keys, model, and output schema."""
        parts = [f"Chat(model='{self.model}'"]
        
        if self.messages:
            required_keys = json_undeclared_vars(self.messages) - set(self.baked_kwargs.keys())
            parts.append(f"required_keys={required_keys}")
            
        if self.output_schema:
            parts.append(f"output_schema={self.output_schema.__name__}")
        
        if self.tools:
            parts.append(f"""tools={",".join(self.tools.keys())}""")
        if self.call_function:
            parts.append(f"call_function={self.call_function}")
            
        # if self.choices:
        #     parts.append(f"choices={self.choices}")
        # if self.multi_choice:
        #     parts.append(f"multi_choice={self.multi_choice}")
            
        if self.seed:
            parts.append(f"seed={self.seed}")
            
        if self.stop:
            parts.append(f"stop={self.stop}")

        if self.save_history:
            parts.append(f"save_history={self.save_history}")
            
        return ", ".join(parts) + ")"
    
    def metadata(self) -> Dict[str, Any]:
        """Return metadata about the chat."""
        meta =  {
            'model':self.model,
            'messages':self.messages,
            'output_schema':self.output_schema,
            'tools':self.tools,
            'call_function':self.call_function,
            'choices':self.choices,
            'multi_choice':self.multi_choice,
            'seed':self.seed,
            'stop':self.stop,
            
        }
        if self.save_history:
            meta['history'] = self.history
        meta = {k:v for k,v in meta.items() if v is not None and v is not False}
        return meta

    def __repr__(self) -> str:
        """Same as string representation."""
        return self.__str__()

# %% ../nbs/024_llms.ipynb 69
@disk_cache.cache
async def image_to_text(path:str,model:str="gpt-4o-mini",url=False):
    """
    This function takes an image (either from a local file path or URL) and uses OpenAI's
    vision model to generate a detailed description of the image contents. The results are
    cached using disk_cache to avoid redundant API calls.
        
    Args:
        path (str): Path to the image file or URL of the image
        model (str, optional): OpenAI model to use for image analysis. Defaults to "gpt-4o-mini".
        url (bool, optional): Whether the path is a URL. Defaults to False.
        
    Returns:
        dict: A dictionary containing:
            - role (str): Always "assistant"
            - content (str): Detailed description of the image
            - meta (dict): Usage statistics including input and output tokens
    
    """
    if url:
        image = instructor.Image.from_url(path)
    else:
        image = instructor.Image.from_path(path)

    class ImageAnalyzer(BaseModel):
        description:str

    res,usage = await complete(
        model=model,
        messages=[{"role":"user","content":[
            "What is in this image, please describe it in detail\n",
            image,
            "\n"
        ]}],
        response_model=ImageAnalyzer,
    )
    return {
        'role':'assistant',
        'content':res.description,
        'meta':usage
    }


# %% ../nbs/024_llms.ipynb 75
from instructor.multimodal import Audio
import openai

# %% ../nbs/024_llms.ipynb 76
@disk_cache.cache
async def speech_to_text(audio_path: str, model: str = "whisper-1") -> Dict[str,str]:
    """Extract text from an audio file using OpenAI's Whisper model.
    
    Args:
        audio_path (str): Path to the audio file
        model (str, optional): OpenAI model to use. Defaults to "whisper-1".
    
    Returns:
        dict: A dictionary containing:  
            - role (str): Always "assistant"
            - content (str): Transcribed text from the audio
    """
    client = raw_client()

    with open(audio_path, "rb") as audio_file:
        response = await client.audio.transcriptions.create(
            model=model,
            file=audio_file
        )
    
    res =  {
        'role':'assistant',
        'content':response.text,
    }
    
    return res

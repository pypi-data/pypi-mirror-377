{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import random\n",
    "import gc\n",
    "import copy\n",
    "\n",
    "# Third-party library imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# PyTorch and related libraries\n",
    "import torch.nn as nn\n",
    "\n",
    "# Custom TINTO library imports\n",
    "from TINTOlib.tinto import TINTO\n",
    "from TINTOlib.supertml import SuperTML\n",
    "from TINTOlib.igtd import IGTD\n",
    "from TINTOlib.refined import REFINED\n",
    "from TINTOlib.barGraph import BarGraph\n",
    "from TINTOlib.distanceMatrix import DistanceMatrix\n",
    "from TINTOlib.combination import Combination\n",
    "from TINTOlib.featureWrap import FeatureWrap\n",
    "from TINTOlib.bie import BIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Version: 11.8\n",
      "cuDNN Version: 90100\n",
      "PyTorch Version: 2.6.0+cu118\n",
      "CUDA is available. PyTorch can use GPU.\n",
      "Current GPU: NVIDIA GeForce RTX 3080\n",
      "Is this tensor on GPU? True\n",
      "Is CUDA initialized? True\n",
      "Number of available GPUs: 1\n",
      "Current device index: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Get CUDA version\n",
    "cuda_version = torch.version.cuda\n",
    "print(f\"CUDA Version: {cuda_version}\")\n",
    "\n",
    "# Get cuDNN version\n",
    "cudnn_version = torch.backends.cudnn.version()\n",
    "print(f\"cuDNN Version: {cudnn_version}\")\n",
    "\n",
    "# Get PyTorch version\n",
    "pytorch_version = torch.__version__\n",
    "print(f\"PyTorch Version: {pytorch_version}\")\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. PyTorch can use GPU.\")\n",
    "    \n",
    "    # Get the name of the current GPU\n",
    "    print(f\"Current GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    # Create a random tensor and move it to GPU to verify\n",
    "    x = torch.rand(5, 3)\n",
    "    print(f\"Is this tensor on GPU? {x.cuda().is_cuda}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. PyTorch will use CPU.\")\n",
    "\n",
    "# Additional check: is CUDA initialized?\n",
    "print(f\"Is CUDA initialized? {torch.cuda.is_initialized()}\")\n",
    "\n",
    "# Number of available GPUs\n",
    "print(f\"Number of available GPUs: {torch.cuda.device_count()}\")\n",
    "\n",
    "# Current device index\n",
    "print(f\"Current device index: {torch.cuda.current_device()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 64\n",
    "# SET RANDOM SEED FOR REPRODUCIBILITY\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variable to store dataset name\n",
    "dataset_name = 'boston'\n",
    "results_path = f'./logs/Regression/{dataset_name}/ViT_Regression'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"../Dataset/Regression/{dataset_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 14)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296.0   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242.0   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242.0   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222.0   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD AND PREPROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "# Function to load and preprocess data\n",
    "def load_and_preprocess_data(images_folder, image_model, problem_type, batch_size=32):\n",
    "    \n",
    "    X_train, X_val = train_test_split(df, test_size=0.20, random_state=SEED)\n",
    "    X_val, X_test = train_test_split(X_val, test_size=0.50, random_state=SEED)\n",
    "    X_train = X_train.reset_index(drop=True)\n",
    "    X_val = X_val.reset_index(drop=True)\n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "    \n",
    "    ### X_train\n",
    "    # Generate the images if the folder does not exist\n",
    "    if not os.path.exists(f'{images_folder}/train'):\n",
    "        #Generate thet images\n",
    "        image_model.fit_transform(X_train, f'{images_folder}/train')\n",
    "        image_model.saveHyperparameters(f'{images_folder}'+\"/model.pkl\")\n",
    "    else:\n",
    "        print(\"The images are already generated\")\n",
    "\n",
    "    img_paths = os.path.join(f'{images_folder}/train',problem_type+\".csv\")\n",
    "\n",
    "    print(img_paths)\n",
    "    \n",
    "    imgs = pd.read_csv(img_paths)\n",
    "    \n",
    "    # Update image paths\n",
    "    imgs[\"images\"] = images_folder + \"/train/\" + imgs[\"images\"]\n",
    "\n",
    "    # Combine datasets\n",
    "    combined_dataset = pd.concat([imgs, X_train], axis=1)\n",
    "\n",
    "    # Split data\n",
    "    X_train = combined_dataset.drop(df.columns[-1], axis=1).drop(\"values\", axis=1)\n",
    "    y_train = combined_dataset[\"values\"]\n",
    "        \n",
    "    ### X_val\n",
    "    # Generate the images if the folder does not exist\n",
    "    if not os.path.exists(f'{images_folder}/val'):\n",
    "        #Generate thet images\n",
    "        image_model.transform(X_val, f'{images_folder}/val')\n",
    "    else:\n",
    "        print(\"The images are already generated\")\n",
    "\n",
    "    img_paths = os.path.join(f'{images_folder}/val',problem_type+\".csv\")\n",
    "\n",
    "    print(img_paths)\n",
    "    \n",
    "    imgs = pd.read_csv(img_paths)\n",
    "\n",
    "    # Update image paths\n",
    "    imgs[\"images\"] = images_folder + \"/val/\" + imgs[\"images\"]\n",
    "\n",
    "    # Combine datasets\n",
    "    combined_dataset = pd.concat([imgs, X_val], axis=1)\n",
    "\n",
    "    # Split data\n",
    "    X_val = combined_dataset.drop(df.columns[-1], axis=1).drop(\"values\", axis=1)\n",
    "    y_val = combined_dataset[\"values\"]\n",
    "    \n",
    "    ### X_test\n",
    "    # Generate the images if the folder does not exist\n",
    "    if not os.path.exists(f'{images_folder}/test'):\n",
    "        #Generate thet images\n",
    "        image_model.transform(X_test, f'{images_folder}/test')\n",
    "    else:\n",
    "        print(\"The images are already generated\")\n",
    "\n",
    "    img_paths = os.path.join(f'{images_folder}/test',problem_type+\".csv\")\n",
    "\n",
    "    print(img_paths)\n",
    "    \n",
    "    imgs = pd.read_csv(img_paths)\n",
    "\n",
    "    # Update image paths\n",
    "    imgs[\"images\"] = images_folder + \"/test/\" + imgs[\"images\"]\n",
    "\n",
    "    # Combine datasets\n",
    "    combined_dataset = pd.concat([imgs, X_test], axis=1)\n",
    "\n",
    "    # Split data\n",
    "    X_test = combined_dataset.drop(df.columns[-1], axis=1).drop(\"values\", axis=1)\n",
    "    y_test = combined_dataset[\"values\"]\n",
    "    \n",
    "    # Numerical data\n",
    "    X_train_num = X_train.drop(\"images\", axis=1)\n",
    "    X_val_num = X_val.drop(\"images\", axis=1)\n",
    "    X_test_num = X_test.drop(\"images\", axis=1)\n",
    "\n",
    "    # Image data\n",
    "    X_train_img = np.array([cv2.imread(img) for img in X_train[\"images\"]])\n",
    "    X_val_img = np.array([cv2.imread(img) for img in X_val[\"images\"]])\n",
    "    X_test_img = np.array([cv2.imread(img) for img in X_test[\"images\"]])\n",
    "\n",
    "    ## Create a MinMaxScaler object\n",
    "    scaler = MinMaxScaler()\n",
    "#\n",
    "    ## Scale numerical data\n",
    "    X_train_num = pd.DataFrame(scaler.fit_transform(X_train_num), columns=X_train_num.columns)\n",
    "    X_val_num = pd.DataFrame(scaler.transform(X_val_num), columns=X_val_num.columns)\n",
    "    X_test_num = pd.DataFrame(scaler.transform(X_test_num), columns=X_test_num.columns)\n",
    "\n",
    "    attributes = len(X_train_num.columns)\n",
    "    height, width, channels = X_train_img[0].shape\n",
    "    imgs_shape = (channels, height, width)\n",
    "\n",
    "    print(\"Images shape: \", imgs_shape)\n",
    "    print(\"Attributes: \", attributes)\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train_num_tensor = torch.as_tensor(X_train_num.values, dtype=torch.float32)\n",
    "    X_val_num_tensor = torch.as_tensor(X_val_num.values, dtype=torch.float32)\n",
    "    X_test_num_tensor = torch.as_tensor(X_test_num.values, dtype=torch.float32)\n",
    "    X_train_img_tensor = torch.as_tensor(X_train_img, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "    X_val_img_tensor = torch.as_tensor(X_val_img, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "    X_test_img_tensor = torch.as_tensor(X_test_img, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "    y_train_tensor = torch.as_tensor(y_train.values, dtype=torch.float32).reshape(-1, 1)\n",
    "    y_val_tensor = torch.as_tensor(y_val.values, dtype=torch.float32).reshape(-1, 1)\n",
    "    y_test_tensor = torch.as_tensor(y_test.values, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "    # Normalize images to [0, 1]\n",
    "    #X_train_img_tensor = X_train_img_tensor / 255.0\n",
    "    #X_val_img_tensor = X_val_img_tensor / 255.0\n",
    "    #X_test_img_tensor = X_test_img_tensor / 255.0\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_dataset = TensorDataset(X_train_img_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_img_tensor, y_val_tensor)\n",
    "    test_dataset = TensorDataset(X_test_img_tensor, y_test_tensor)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, attributes, imgs_shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## MODEL ARCHITECTURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_divisors(n):\n",
    "    divisors = []\n",
    "    for i in range(1, int(n**0.5) + 1):\n",
    "        if n % i == 0:\n",
    "            divisors.append(i)\n",
    "            if i != n // i:  # Check to include both divisors if they are not the same\n",
    "                divisors.append(n // i)\n",
    "    divisors.sort()\n",
    "    return divisors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vit_pytorch.vit import ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model1(nn.Module):\n",
    "    def __init__(self, imgs_shape, patch_size):\n",
    "        super(Model1, self).__init__()\n",
    "        \n",
    "        self.vit = ViT(\n",
    "            image_size = imgs_shape,\n",
    "            patch_size = patch_size,\n",
    "            dim = 128,  \n",
    "            depth = 4,  \n",
    "            heads = 8,  \n",
    "            mlp_dim = 256,\n",
    "            dropout = 0.0,\n",
    "            emb_dropout = 0.0\n",
    "        )\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, vit_input):\n",
    "        vit_output = self.vit(vit_input)\n",
    "        mlp_output = self.mlp(vit_output)\n",
    "        return mlp_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model2(nn.Module):\n",
    "    def __init__(self, imgs_shape, patch_size):\n",
    "        super(Model2, self).__init__()\n",
    "        \n",
    "        # Enhanced ViT branch with increased depth and heads\n",
    "        self.vit = ViT(\n",
    "            image_size = imgs_shape,\n",
    "            patch_size = patch_size,\n",
    "            dim = 512,  # Increased dimensionality\n",
    "            depth = 4,  \n",
    "            heads = 8, \n",
    "            mlp_dim = 512,\n",
    "            dropout = 0.0,\n",
    "            emb_dropout = 0.0\n",
    "        )\n",
    "        \n",
    "        # More complex MLP\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, vit_input):\n",
    "        vit_output = self.vit(vit_input)\n",
    "        mlp_output = self.mlp(vit_output)\n",
    "        return mlp_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMPILE AND FIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "def compile_and_fit(model, train_loader, val_loader, test_loader, dataset_name, model_name, batch_size=32, epochs=100, min_lr=1e-3, max_lr=1, device='cuda', weight_decay=1e-2):\n",
    "    model = model.to(device)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=min_lr, weight_decay=weight_decay)\n",
    "    \n",
    "    total_steps = epochs * len(train_loader)\n",
    "    scheduler = OneCycleLR(optimizer, max_lr=max_lr, div_factor=max_lr/min_lr, total_steps=total_steps, pct_start=0.3, final_div_factor=1)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "    early_stopping_patience = 20\n",
    "    best_model = None\n",
    "    best_epoch = 0\n",
    "    warm_up_epochs = epochs*0.3\n",
    "\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_mse': [], 'val_mse': [], 'train_rmse': [], 'val_rmse': [], 'learning_rate': [], 'epoch_time': []}\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_predictions = []\n",
    "        train_targets = []\n",
    "        for img_data, targets in train_loader:\n",
    "            img_data, targets = img_data.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(img_data)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_predictions.extend(outputs.cpu().detach().numpy())\n",
    "            train_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_predictions = []\n",
    "        val_targets = []\n",
    "        with torch.no_grad():\n",
    "            for img_data, targets in val_loader:\n",
    "                img_data, targets = img_data.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "                outputs = model(img_data)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_predictions.extend(outputs.cpu().numpy())\n",
    "                val_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        # Get the current learning rate\n",
    "        current_lr = scheduler.get_last_lr()\n",
    "        \n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            best_epoch = epoch + 1\n",
    "            #early_stopping_counter = 0\n",
    "        #else:\n",
    "            #if epoch > warm_up_epochs:\n",
    "                #early_stopping_counter += 1\n",
    "                #if early_stopping_counter >= early_stopping_patience:\n",
    "                    #print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                    #break\n",
    "\n",
    "        train_mse = mean_squared_error(train_targets, train_predictions)\n",
    "        train_rmse = np.sqrt(train_mse)\n",
    "        val_mse = mean_squared_error(val_targets, val_predictions)\n",
    "        val_rmse = np.sqrt(val_mse)\n",
    "        train_r2 = r2_score(train_targets, train_predictions)\n",
    "        val_r2 = r2_score(val_targets, val_predictions)\n",
    "\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_mse'].append(train_mse)\n",
    "        history['val_mse'].append(val_mse)\n",
    "        history['train_rmse'].append(train_rmse)\n",
    "        history['val_rmse'].append(val_rmse)\n",
    "        history['learning_rate'].append(current_lr)\n",
    "        history['epoch_time'].append(epoch_time)\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    model.load_state_dict(best_model)\n",
    "\n",
    "    # Calculate and save metrics\n",
    "    train_metrics = calculate_metrics(model, train_loader, device)\n",
    "    val_metrics = calculate_metrics(model, val_loader, device)\n",
    "    test_metrics = calculate_metrics(model, test_loader, device)\n",
    "\n",
    "    metrics = {\n",
    "        'train_loss': train_metrics['loss'],\n",
    "        'train_mse': train_metrics['mse'],\n",
    "        'train_mae': train_metrics['mae'],\n",
    "        'train_rmse': train_metrics['rmse'],\n",
    "        'train_r2': train_metrics['r2'],\n",
    "        'val_loss': val_metrics['loss'],\n",
    "        'val_mse': val_metrics['mse'],\n",
    "        'val_mae': val_metrics['mae'],\n",
    "        'val_rmse': val_metrics['rmse'],\n",
    "        'val_r2': val_metrics['r2'],\n",
    "        'test_loss': test_metrics['loss'],\n",
    "        'test_mse': test_metrics['mse'],\n",
    "        'test_mae': test_metrics['mae'],\n",
    "        'test_rmse': test_metrics['rmse'],\n",
    "        'test_r2': test_metrics['r2'],\n",
    "        'min_lr': min_lr,\n",
    "        'max_lr': max_lr,\n",
    "        'total_time': total_time,\n",
    "        'average_epoch_time': sum(history['epoch_time']) / len(history['epoch_time'])\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nTraining completed in {total_time:.2f} seconds\")\n",
    "    print(f\"Best model found at epoch {best_epoch}/{epochs}\")\n",
    "    print(f\"Best Train Loss: {history['train_loss'][best_epoch-1]:.4f}, Best Val Loss: {history['val_loss'][best_epoch-1]:.4f}\")\n",
    "    print(f\"Best Train MSE: {history['train_mse'][best_epoch-1]:.4f}, Best Val MSE: {history['val_mse'][best_epoch-1]:.4f}\")\n",
    "    print(f\"Best Train RMSE: {history['train_rmse'][best_epoch-1]:.4f}, Best Val RMSE: {history['val_rmse'][best_epoch-1]:.4f}\")\n",
    "\n",
    "    # Save figures for this fold\n",
    "    os.makedirs(f\"models/Regression/{dataset_name}/ViT/{model_name}\", exist_ok=True)\n",
    "    plot_metric(history['train_loss'], history['val_loss'], 'Loss', dataset_name, model_name)\n",
    "    plot_metric(history['train_mse'], history['val_mse'], 'MSE', dataset_name, model_name)\n",
    "    plot_metric(history['train_rmse'], history['val_rmse'], 'RMSE', dataset_name, model_name)\n",
    "    plot_learning_rate(history['learning_rate'], dataset_name, model_name)\n",
    "\n",
    "    # Save metrics to a file\n",
    "    os.makedirs(f'logs/Regression/{dataset_name}/ViT/{model_name}', exist_ok=True)\n",
    "    with open(f'logs/Regression/{dataset_name}/ViT/{model_name}/metrics.txt', 'w') as f:\n",
    "        for key, value in metrics.items():\n",
    "            f.write(f'{key}: {value}\\n')\n",
    "\n",
    "    # Save best model\n",
    "    model_save_path = f\"models/Regression/{dataset_name}/ViT/{model_name}/best_model.pth\"\n",
    "    os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
    "    torch.save(best_model, model_save_path)\n",
    "    print(f\"Best model saved to {model_save_path}\")\n",
    "            \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def calculate_metrics(model, data_loader, device):\n",
    "    model.eval()\n",
    "    loss_fn = nn.MSELoss(reduction='sum')\n",
    "    total_loss = 0\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img_data, targets in data_loader:\n",
    "            img_data, targets = img_data.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "            outputs = model(img_data)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "            all_predictions.append(outputs.cpu().numpy())\n",
    "\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    all_predictions = np.concatenate(all_predictions)\n",
    "\n",
    "    mse = mean_squared_error(all_targets, all_predictions, multioutput='raw_values')\n",
    "    mae = mean_absolute_error(all_targets, all_predictions, multioutput='raw_values')\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(all_targets, all_predictions, multioutput='raw_values')\n",
    "\n",
    "    metrics = {\n",
    "        'loss': total_loss / len(data_loader.dataset),\n",
    "        'mse': mse,\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'r2': r2\n",
    "    }\n",
    "    return metrics \n",
    "\n",
    "def plot_metric(train_metric, val_metric, metric_name, dataset_name, model_name):\n",
    "    plt.figure()\n",
    "    plt.plot(train_metric, label=f'Train {metric_name}')\n",
    "    plt.plot(val_metric, label=f'Validation {metric_name}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.legend()\n",
    "    plt.title(f'{metric_name} vs. Epoch')\n",
    "    plt.savefig(f\"models/Regression/{dataset_name}/ViT/{model_name}/{metric_name.lower()}_plot.png\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_learning_rate(learning_rates, dataset_name, model_name):\n",
    "    plt.figure()\n",
    "    plt.plot(learning_rates)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.title('Learning Rate vs. Epoch')\n",
    "    plt.savefig(f\"models/Regression/{dataset_name}/ViT/{model_name}/learning_rate_plot.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_compile_and_fit(model, train_loader, val_loader, test_loader, dataset_name, model_name, batch_size=64, epochs=100, min_lr=1e-3, max_lr=1 , device='cuda', weight_decay=1e-2):\n",
    "    try:\n",
    "        if model is None:\n",
    "            print(f\"Model {model_name} is None\")\n",
    "            return None\n",
    "        else:\n",
    "            # Compile and fit the model\n",
    "            metrics = compile_and_fit(model, train_loader, val_loader, test_loader, dataset_name, model_name, epochs=epochs, min_lr=min_lr, max_lr=max_lr, device=device, weight_decay=weight_decay)\n",
    "            return metrics\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to compile and fit {model_name}: {str(e)}\")\n",
    "        return None\n",
    "    finally:\n",
    "        # Clear CUDA cache and force garbage collection\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "def try_create_model(model_class, patch_size, imgs_shape):\n",
    "    try:\n",
    "        model = model_class(imgs_shape[1:], patch_size)\n",
    "        \n",
    "        # Test the model with a sample input\n",
    "        sample_input = torch.randn(4, *imgs_shape)\n",
    "        output = model(sample_input)\n",
    "        \n",
    "        print(f\"Successfully created and tested {model_class.__name__}\")\n",
    "        \n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating or testing {model_class.__name__}: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jiayu\\anaconda3\\envs\\Testing1\\lib\\site-packages\\torch_lr_finder\\lr_finder.py:5: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_lr_finder import LRFinder\n",
    "\n",
    "def run_lr_finder(model_class, patch_size, attributes, imgs_shape, dataset_name, name, train_loader, val_loader, num_iter):\n",
    "\n",
    "    # Define the path where the plot will be saved\n",
    "    save_dir = os.path.join(f\"logs/Regression/{dataset_name}/ViT/{name}\")\n",
    "    save_path = os.path.join(save_dir, 'lr_finder_plot.png')\n",
    "\n",
    "    # Check if the file already exists\n",
    "    if not os.path.exists(save_path):\n",
    "        # Create and train Model\n",
    "        model = try_create_model(model_class, patch_size, imgs_shape)\n",
    "        \n",
    "        if model is None:\n",
    "            return None\n",
    "        \n",
    "        # Move model to device\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model = model.to(device)\n",
    "        \n",
    "        optimizer = optim.AdamW(model.parameters(), lr=1e-7, weight_decay=0.0001)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        lr_finder = LRFinder(model, optimizer, criterion, device=device)\n",
    "        lr_finder.range_test(train_loader, val_loader=val_loader, end_lr=1, num_iter=num_iter, step_mode=\"exp\")\n",
    "        \n",
    "        axis, lr = lr_finder.plot()\n",
    "        \n",
    "        # Create the directory if it doesn't exist\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        # Get the figure from the axis and save it\n",
    "        fig = axis.figure\n",
    "        fig.savefig(save_path)\n",
    "        print(f\"Plot saved to: {save_path}\")\n",
    "        \n",
    "        # Close the figure to ensure it's saved properly\n",
    "        plt.close(fig)\n",
    "        \n",
    "        lr_finder.reset()\n",
    "        print(f\"Suggested learning rate: {lr}\")\n",
    "        \n",
    "        return lr\n",
    "    else:\n",
    "        print(f\"LR finder plot already exists at {save_path}. Skipping LR finder process.\")\n",
    "        # Load and display the existing image\n",
    "        img = plt.imread(save_path)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')  # Turn off axis numbers and ticks\n",
    "        plt.title(\"Learning Rate Finder Plot\")\n",
    "        plt.show()\n",
    "        \n",
    "        return None  # Or you could return a default learning rate here\n",
    "\n",
    "# Usage example:\n",
    "# lr = run_lr_finder(Model1, attributes, imgs_shape, dataset_name, name, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPERIMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = TINTO(problem= problem_type, blur=True, random_seed=SEED)\n",
    "#image_model = REFINED(problem= problem_type,hcIterations=5)\n",
    "#image_model = IGTD(problem= problem_type)\n",
    "#image_model = BarGraph(problem= problem_type)\n",
    "#image_model = DistanceMatrix(problem= problem_type)\n",
    "#image_model = Combination(problem= problem_type)\n",
    "#image_model = SuperTML(problem= problem_type)\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"./Synthetic_images/Regression/{dataset_name}/images_{dataset_name}_TINTO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iterations_per_epoch(dataset_size, batch_size):\n",
    "    iterations = dataset_size // batch_size\n",
    "    if dataset_size % batch_size != 0:\n",
    "        iterations += 1\n",
    "    return iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = calculate_iterations_per_epoch(df.shape[0], batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### EXPERIMENT 1: TINTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = TINTO(problem= problem_type, blur=True, random_seed=SEED)\n",
    "name = f\"TINTO_blur\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"./Synthetic_images/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Synthetic_images/Regression/boston/images_boston_TINTO_blur/train\\regression.csv\n",
      "./Synthetic_images/Regression/boston/images_boston_TINTO_blur/val\\regression.csv\n",
      "./Synthetic_images/Regression/boston/images_boston_TINTO_blur/test\\regression.csv\n",
      "Images shape:  (3, 20, 20)\n",
      "Attributes:  13\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape  = load_and_preprocess_data(images_folder, image_model, problem_type, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 4, 5, 10, 20]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "find_divisors(imgs_shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created and tested Model1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 31/32 [00:01<00:00, 30.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping early, the loss has diverged\n",
      "Learning rate search finished. See the graph with {finder_name}.plot()\n",
      "LR suggestion: steepest gradient\n",
      "Suggested LR: 1.56E-02\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved to: logs/Regression/boston/ViT/TINTO_blur_Model1_patch4\\lr_finder_plot.png\n",
      "Suggested learning rate: 0.015615230060004972\n"
     ]
    }
   ],
   "source": [
    "lr = run_lr_finder(Model1, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model1_patch4\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created and tested Model1\n",
      "\n",
      "Training completed in 28.37 seconds\n",
      "Best model found at epoch 88/100\n",
      "Best Train Loss: 11.9268, Best Val Loss: 14.2158\n",
      "Best Train MSE: 12.0711, Best Val MSE: 16.6895\n",
      "Best Train RMSE: 3.4744, Best Val RMSE: 4.0853\n",
      "Best model saved to models/Regression/boston/ViT/TINTO_blur_Model1_patch4/best_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Create and train Model1\n",
    "model1 = try_create_model(Model1, patch_size, imgs_shape)  # Attempt to create Model1\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1_patch4\", min_lr=1e-4, max_lr=1e-2)  # Train and evaluate Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created and tested Model2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 28/32 [00:00<00:00, 33.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping early, the loss has diverged\n",
      "Learning rate search finished. See the graph with {finder_name}.plot()\n",
      "LR suggestion: steepest gradient\n",
      "Suggested LR: 5.52E-03\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved to: logs/Regression/boston/ViT/TINTO_blur_Model2_patch4\\lr_finder_plot.png\n",
      "Suggested learning rate: 0.005519954321281566\n"
     ]
    }
   ],
   "source": [
    "lr = run_lr_finder(Model2, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model2_patch4\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created and tested Model2\n",
      "\n",
      "Training completed in 30.35 seconds\n",
      "Best model found at epoch 46/100\n",
      "Best Train Loss: 40.4394, Best Val Loss: 24.9796\n",
      "Best Train MSE: 41.3525, Best Val MSE: 28.9047\n",
      "Best Train RMSE: 6.4306, Best Val RMSE: 5.3763\n",
      "Best model saved to models/Regression/boston/ViT/TINTO_blur_Model2_patch4/best_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Create and train Model2\n",
    "model2 = try_create_model(Model2, patch_size, imgs_shape)  # Attempt to create Model2\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2_patch4\", min_lr=1e-4, max_lr=1e-2)  # Train and evaluate Model2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 Metrics: {'train_loss': 11.866475549074682, 'train_mse': array([11.866476], dtype=float32), 'train_mae': array([2.4801226], dtype=float32), 'train_rmse': array([3.444775], dtype=float32), 'train_r2': array([0.85895795], dtype=float32), 'val_loss': 16.68945914623784, 'val_mse': array([16.68946], dtype=float32), 'val_mae': array([3.0319545], dtype=float32), 'val_rmse': array([4.0852737], dtype=float32), 'val_r2': array([0.7243502], dtype=float32), 'test_loss': 26.61487429749732, 'test_mse': array([26.614876], dtype=float32), 'test_mae': array([3.3980372], dtype=float32), 'test_rmse': array([5.158961], dtype=float32), 'test_r2': array([0.7493001], dtype=float32), 'min_lr': 0.0001, 'max_lr': 0.01, 'total_time': 28.37159037590027, 'average_epoch_time': 0.2837159037590027}\n",
      "Model 2 Metrics: {'train_loss': 38.67400265684222, 'train_mse': array([38.674004], dtype=float32), 'train_mae': array([4.238204], dtype=float32), 'train_rmse': array([6.2188425], dtype=float32), 'train_r2': array([0.5403303], dtype=float32), 'val_loss': 28.904743194580078, 'val_mse': array([28.904741], dtype=float32), 'val_mae': array([4.04177], dtype=float32), 'val_rmse': array([5.376313], dtype=float32), 'val_r2': array([0.5225978], dtype=float32), 'test_loss': 56.77113611557905, 'test_mse': array([56.771133], dtype=float32), 'test_mae': array([4.8737936], dtype=float32), 'test_rmse': array([7.5346622], dtype=float32), 'test_r2': array([0.46524197], dtype=float32), 'min_lr': 0.0001, 'max_lr': 0.01, 'total_time': 30.345070362091064, 'average_epoch_time': 0.30345070362091064}\n"
     ]
    }
   ],
   "source": [
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)  # Print metrics for Model1 if available\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)  # Print metrics for Model2 if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = TINTO(problem= problem_type, blur=True, option=\"maximum\", random_seed=SEED)\n",
    "name = f\"TINTO_blur_maximum\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"./Synthetic_images/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape  = load_and_preprocess_data(images_folder, image_model, problem_type, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "find_divisors(imgs_shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model1, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model1_patch4\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model1\n",
    "model1 = try_create_model(Model1, patch_size, imgs_shape)  # Attempt to create Model1\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1_patch4\", min_lr=5e-4, max_lr=2e-2)  # Train and evaluate Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model2, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model2_patch4\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model2\n",
    "model2 = try_create_model(Model2, patch_size, imgs_shape)  # Attempt to create Model2\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2_patch4\", min_lr=2e-4, max_lr=2e-2)  # Train and evaluate Model2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)  # Print metrics for Model1 if available\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)  # Print metrics for Model2 if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = TINTO(problem= problem_type, random_seed=SEED)\n",
    "name = f\"TINTO\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"./Synthetic_images/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape  = load_and_preprocess_data(images_folder, image_model, problem_type, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "find_divisors(imgs_shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model1, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model1_patch4\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model1\n",
    "model1 = try_create_model(Model1, patch_size, imgs_shape)  # Attempt to create Model1\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1_patch4\", min_lr=5e-4, max_lr=3e-2)  # Train and evaluate Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model2, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model2_patch4\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model2\n",
    "model2 = try_create_model(Model2, patch_size, imgs_shape)  # Attempt to create Model2\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2_patch4\", min_lr=2e-4, max_lr=1e-2)  # Train and evaluate Model2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)  # Print metrics for Model1 if available\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)  # Print metrics for Model2 if available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### EXPERIMENT 2: IGTD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the shape of the dataframe\n",
    "num_columns = df.shape[1]\n",
    "\n",
    "# Calculate number of columns - 1\n",
    "columns_minus_one = num_columns - 1\n",
    "\n",
    "# Calculate number of columns - 2 if multi objective...\n",
    "# columns_minus_one = num_columns - 2\n",
    "\n",
    "# Calculate the square root for image size\n",
    "import math\n",
    "image_size = math.ceil(math.sqrt(columns_minus_one))\n",
    "print(image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = IGTD(problem= problem_type, scale=[image_size,image_size], fea_dist_method='Euclidean', image_dist_method='Euclidean', error='abs', max_step=30000, val_step=500, random_seed=SEED)\n",
    "name = f\"IGTD_{image_size}x{image_size}_fEuclidean_iEuclidean_abs\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"./Synthetic_images/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape  = load_and_preprocess_data(images_folder, image_model, problem_type, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "find_divisors(imgs_shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model1, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model1\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model1\n",
    "model1 = try_create_model(Model1, patch_size, imgs_shape)  # Attempt to create Model1\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\", min_lr=2e-4, max_lr=1e-2)  # Train and evaluate Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model2, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model2\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model2\n",
    "model2 = try_create_model(Model2, patch_size, imgs_shape)  # Attempt to create Model2\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\", min_lr=1e-4, max_lr=1e-2)  # Train and evaluate Model2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)  # Print metrics for Model1 if available\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)  # Print metrics for Model2 if available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### EXPERIMENT 3: REFINED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = REFINED(problem= problem_type, random_seed=SEED)\n",
    "name = f\"REFINED\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"./Synthetic_images/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape  = load_and_preprocess_data(images_folder, image_model, problem_type, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "find_divisors(imgs_shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model1, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model1_patch5\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model1\n",
    "model1 = try_create_model(Model1, patch_size, imgs_shape)  # Attempt to create Model1\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1_patch5\", min_lr=5e-4, max_lr=2e-2)  # Train and evaluate Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model2, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model2_patch5\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model2\n",
    "model2 = try_create_model(Model2, patch_size, imgs_shape)  # Attempt to create Model2\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2_patch5\", min_lr=1e-4, max_lr=1e-2)  # Train and evaluate Model2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)  # Print metrics for Model1 if available\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)  # Print metrics for Model2 if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = REFINED(problem= problem_type, zoom=2, random_seed=SEED)\n",
    "name = f\"REFINED_zoom2\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"./Synthetic_images/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape  = load_and_preprocess_data(images_folder, image_model, problem_type, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "find_divisors(imgs_shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model1, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model1\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model1\n",
    "model1 = try_create_model(Model1, patch_size, imgs_shape)  # Attempt to create Model1\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\", min_lr=1e-5, max_lr=8e-4)  # Train and evaluate Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model2, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model2\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model2\n",
    "model2 = try_create_model(Model2, patch_size, imgs_shape)  # Attempt to create Model2\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\", min_lr=5e-6, max_lr=1e-3)  # Train and evaluate Model2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)  # Print metrics for Model1 if available\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)  # Print metrics for Model2 if available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### EXPERIMENT 4: BAR GRAPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = BarGraph(problem= problem_type)\n",
    "name = f\"BarGraph\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"./Synthetic_images/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape  = load_and_preprocess_data(images_folder, image_model, problem_type, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "find_divisors(imgs_shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model1, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model1\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model1\n",
    "model1 = try_create_model(Model1, patch_size, imgs_shape)  # Attempt to create Model1\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\", min_lr=1e-7, max_lr=1e-4)  # Train and evaluate Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model2, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model2\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model2\n",
    "model2 = try_create_model(Model2, patch_size, imgs_shape)  # Attempt to create Model2\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\", min_lr=5e-6, max_lr=5e-4)  # Train and evaluate Model2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)  # Print metrics for Model1 if available\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)  # Print metrics for Model2 if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = BarGraph(problem= problem_type, zoom=2)\n",
    "name = f\"BarGraph_zoom2\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"./Synthetic_images/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape  = load_and_preprocess_data(images_folder, image_model, problem_type, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "find_divisors(imgs_shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model1, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model1\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model1\n",
    "model1 = try_create_model(Model1, patch_size, imgs_shape)  # Attempt to create Model1\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\", min_lr=2e-5, max_lr=1e-2)  # Train and evaluate Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model2, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model2\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model2\n",
    "model2 = try_create_model(Model2, patch_size, imgs_shape)  # Attempt to create Model2\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\", min_lr=1e-5, max_lr=7e-4)  # Train and evaluate Model2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)  # Print metrics for Model1 if available\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)  # Print metrics for Model2 if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)  # Print metrics for Model1 if available\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)  # Print metrics for Model2 if available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### EXPERIMENT 5: DISTANCE MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = DistanceMatrix(problem= problem_type)\n",
    "name = f\"DistanceMatrix\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"./Synthetic_images/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape  = load_and_preprocess_data(images_folder, image_model, problem_type, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "find_divisors(imgs_shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model1, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model1\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model1\n",
    "model1 = try_create_model(Model1, patch_size, imgs_shape)  # Attempt to create Model1\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\", min_lr=1e-7, max_lr=1e-1)  # Train and evaluate Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model2, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model2\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model2\n",
    "model2 = try_create_model(Model2, patch_size, imgs_shape)  # Attempt to create Model2\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\", min_lr=1e-5, max_lr=3e-3)  # Train and evaluate Model2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)  # Print metrics for Model1 if available\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)  # Print metrics for Model2 if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = DistanceMatrix(problem= problem_type, zoom=2)\n",
    "name = f\"DistanceMatrix_zoom2\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"./Synthetic_images/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape  = load_and_preprocess_data(images_folder, image_model, problem_type, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "find_divisors(imgs_shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model1, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model1\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model1\n",
    "model1 = try_create_model(Model1, patch_size, imgs_shape)  # Attempt to create Model1\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\", min_lr=1e-5, max_lr=5e-4)  # Train and evaluate Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model2, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model2\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model2\n",
    "model2 = try_create_model(Model2, patch_size, imgs_shape)  # Attempt to create Model2\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\", min_lr=1e-5, max_lr=1e-4)  # Train and evaluate Model2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)  # Print metrics for Model1 if available\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)  # Print metrics for Model2 if available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### EXPERIMENT 6: COMBINATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = Combination(problem= problem_type)\n",
    "name = f\"Combination\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"./Synthetic_images/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape  = load_and_preprocess_data(images_folder, image_model, problem_type, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "find_divisors(imgs_shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model1, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model1\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model1\n",
    "model1 = try_create_model(Model1, patch_size, imgs_shape)  # Attempt to create Model1\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\", min_lr=1e-7, max_lr=1e-3)  # Train and evaluate Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model2, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model2\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model2\n",
    "model2 = try_create_model(Model2, patch_size, imgs_shape)  # Attempt to create Model2\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\", min_lr=1e-4, max_lr=2e-3)  # Train and evaluate Model2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)  # Print metrics for Model1 if available\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)  # Print metrics for Model2 if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = Combination(problem= problem_type, zoom=2)\n",
    "name = f\"Combination_zoom2\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"./Synthetic_images/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape  = load_and_preprocess_data(images_folder, image_model, problem_type, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "find_divisors(imgs_shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model1, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model1\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model1\n",
    "model1 = try_create_model(Model1, patch_size, imgs_shape)  # Attempt to create Model1\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\", min_lr=2e-4, max_lr=1e-2)  # Train and evaluate Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model2, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model2\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model2\n",
    "model2 = try_create_model(Model2, patch_size, imgs_shape)  # Attempt to create Model2\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\", min_lr=5e-5, max_lr=3e-3)  # Train and evaluate Model2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)  # Print metrics for Model1 if available\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)  # Print metrics for Model2 if available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### EXPERIMENT 7: SUPERTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = SuperTML(problem= problem_type, random_seed=SEED)\n",
    "name = f\"SuperTML-EF\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"./Synthetic_images/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape  = load_and_preprocess_data(images_folder, image_model, problem_type, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "find_divisors(imgs_shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model1, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model1\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model1\n",
    "model1 = try_create_model(Model1, patch_size, imgs_shape)  # Attempt to create Model1\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\", min_lr=1e-5, max_lr=2e-3)  # Train and evaluate Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model2, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model2\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model2\n",
    "model2 = try_create_model(Model2, patch_size, imgs_shape)  # Attempt to create Model2\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\", min_lr=1e-4, max_lr=3e-3)  # Train and evaluate Model2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)  # Print metrics for Model1 if available\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)  # Print metrics for Model2 if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = SuperTML(problem= problem_type, feature_importance=True, font_size=30, random_seed=SEED)\n",
    "name = f\"SuperTML-VF_FS30\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"./Synthetic_images/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape  = load_and_preprocess_data(images_folder, image_model, problem_type, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "find_divisors(imgs_shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model1, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model1\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model1\n",
    "model1 = try_create_model(Model1, patch_size, imgs_shape)  # Attempt to create Model1\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\", min_lr=2e-4, max_lr=1.5e-3)  # Train and evaluate Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model2, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model2\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model2\n",
    "model2 = try_create_model(Model2, patch_size, imgs_shape)  # Attempt to create Model2\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\", min_lr=2e-4, max_lr=1.5e-3)  # Train and evaluate Model2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)  # Print metrics for Model1 if available\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)  # Print metrics for Model2 if available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### EXPERIMENT 8: FEATURE WRAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = FeatureWrap(problem = problem_type)\n",
    "name = f\"FeatureWrap\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"./Synthetic_images/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape  = load_and_preprocess_data(images_folder, image_model, problem_type, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "find_divisors(imgs_shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model1, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model1\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model1\n",
    "model1 = try_create_model(Model1, patch_size, imgs_shape)  # Attempt to create Model1\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\", min_lr=1e-5, max_lr=3e-3)  # Train and evaluate Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model2, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model2\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model2\n",
    "model2 = try_create_model(Model2, patch_size, imgs_shape)  # Attempt to create Model2\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\", min_lr=1e-6, max_lr=3e-2)  # Train and evaluate Model2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)  # Print metrics for Model1 if available\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)  # Print metrics for Model2 if available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### EXPERIMENT 9: BINARY IMAGE ENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = BIE(problem = problem_type)\n",
    "name = f\"BIE\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"./Synthetic_images/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape  = load_and_preprocess_data(images_folder, image_model, problem_type, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "find_divisors(imgs_shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model1, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model1\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model1\n",
    "model1 = try_create_model(Model1, patch_size, imgs_shape)  # Attempt to create Model1\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\", min_lr=1e-5, max_lr=3e-3)  # Train and evaluate Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model2, patch_size, attributes, imgs_shape, dataset_name, f\"{name}_Model2\", train_loader, val_loader, num_iter=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model2\n",
    "model2 = try_create_model(Model2, patch_size, imgs_shape)  # Attempt to create Model2\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\", min_lr=1e-6, max_lr=3e-2)  # Train and evaluate Model2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)  # Print metrics for Model1 if available\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)  # Print metrics for Model2 if available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## FINAL METRICS AND BEST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_model(base_path):\n",
    "    best_rmse = float('inf')\n",
    "    best_folder = None\n",
    "\n",
    "    # Walk through all directories and files in the base path\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        for file in files:\n",
    "            if file == f'metrics.txt':\n",
    "                file_path = os.path.join(root, file)\n",
    "                \n",
    "                # Read metrics from the file\n",
    "                with open(file_path, 'r') as f:\n",
    "                    metrics = f.read()\n",
    "                \n",
    "                # Parse the metrics into a dictionary\n",
    "                metrics_dict = {}\n",
    "                for line in metrics.splitlines():\n",
    "                    key, value = line.split(': ')\n",
    "                    metrics_dict[key.strip()] = float(value.strip())\n",
    "                \n",
    "                # Check if the current folder has a better validation loss\n",
    "                if metrics_dict['test_rmse'] < best_rmse:\n",
    "                    best_rmse = metrics_dict['test_rmse']\n",
    "                    best_folder = root\n",
    "    \n",
    "    return best_folder, best_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def read_metrics(file_path):\n",
    "    metrics = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            key, value = line.split(': ')\n",
    "            metrics[key.strip()] = float(value.strip())\n",
    "    return metrics\n",
    "\n",
    "def rename_folder(old_folder_path, prefix):\n",
    "    folder_name = os.path.basename(old_folder_path)\n",
    "    new_folder_name = f\"{prefix}_{folder_name}\"\n",
    "    parent_dir = os.path.dirname(old_folder_path)\n",
    "    new_folder_path = os.path.join(parent_dir, new_folder_name)\n",
    "    os.rename(old_folder_path, new_folder_path)\n",
    "    return new_folder_path\n",
    "\n",
    "def process_folders(root_dir):\n",
    "    prefixes = [\"TINTO\", \"BarGraph\", \"Combination\", \"DistanceMatrix\", \"IGTD\", \"REFINED\", \"SuperTML\", \"FeatureWrap\", \"BIE\"]\n",
    "    best_folders = []\n",
    "\n",
    "    for prefix in prefixes:\n",
    "        matching_folders = [f for f in os.listdir(root_dir) if f.startswith(prefix) and os.path.isdir(os.path.join(root_dir, f))]\n",
    "        if matching_folders:\n",
    "            best_folder = None\n",
    "            best_test_rmse = float('inf')\n",
    "            for folder in matching_folders:\n",
    "                metrics_file = os.path.join(root_dir, folder, 'metrics.txt')\n",
    "                if os.path.exists(metrics_file):\n",
    "                    metrics = read_metrics(metrics_file)\n",
    "                    if metrics['test_rmse'] < best_test_rmse:\n",
    "                        best_test_rmse = metrics['test_rmse']\n",
    "                        best_folder = folder\n",
    "            if best_folder:\n",
    "                new_path = rename_folder(os.path.join(root_dir, best_folder), \"TOP\")\n",
    "                best_folders.append(new_path)\n",
    "    \n",
    "    if best_folders:\n",
    "        overall_best_folder = None\n",
    "        overall_best_test_rmse = float('inf')\n",
    "        for folder in best_folders:\n",
    "            metrics_file = os.path.join(folder, 'metrics.txt')\n",
    "            if os.path.exists(metrics_file):\n",
    "                metrics = read_metrics(metrics_file)\n",
    "                if metrics['test_rmse'] < overall_best_test_rmse:\n",
    "                    overall_best_test_rmse = metrics['test_rmse']\n",
    "                    overall_best_folder = folder\n",
    "        if overall_best_folder:\n",
    "            rename_folder(overall_best_folder, \"BEST\")\n",
    "        \n",
    "    return best_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "base_path = f\"logs/Regression/{dataset_name}/ViT\"\n",
    "best_folders = process_folders(base_path)\n",
    "print(f\"Best model folder: {best_folders}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Testing1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

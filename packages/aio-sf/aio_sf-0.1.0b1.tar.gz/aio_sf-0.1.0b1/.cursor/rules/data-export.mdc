---
description: "Data export patterns and best practices"
---

# Data Export Patterns

## Parquet Export Workflow
1. **Get metadata** (optional but recommended): `get_bulk_fields(sf, object_name)`
2. **Create schema**: `create_schema_from_metadata(fields_metadata)`
3. **Execute query**: `bulk_query(sf, soql_query)`
4. **Export data**: `write_query_to_parquet_async(query_result, file_path, schema=schema)`

## Export Options
- **Inferred schema**: Let pandas/pyarrow infer types from data
- **Metadata schema**: Use Salesforce field metadata for proper typing
- **Batch processing**: Configure batch sizes for memory efficiency
- **Empty value handling**: Convert empty strings to null values

## Memory Management
```python
# For large datasets, use smaller batch sizes
writer = ParquetWriter(
    file_path="large_dataset.parquet",
    schema=schema,
    batch_size=1000,  # Smaller batches for memory efficiency
    convert_empty_to_null=True
)
writer.write_query_result(query_result)
```

## Resume Capability
```python
# Resume from job locator for interrupted queries
query_result = resume_from_locator(
    sf=sf,
    job_id="job_id_from_previous_run",
    locator="locator_from_previous_run"
)
```
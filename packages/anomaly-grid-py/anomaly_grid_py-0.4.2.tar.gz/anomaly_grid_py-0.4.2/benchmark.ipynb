{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae7JXP7Tbk4_"
      },
      "source": [
        "# Challenge: Sequence Deviation Detection\n",
        "\n",
        "This notebook creates **finite alphabet sequences datasets** (‚â§20 symbols for niche focus) designed to favor sequence modeling:\n",
        "\n",
        "1. **üîê Network Protocol State Machines** - 16 states, complex transitions, subtle violations\n",
        "2. **üß¨ Protein Folding Sequences** - 20 amino acids, biological constraints, rare misfolding\n",
        "3. **üì° Communication Protocol Analysis** - 12 symbols, timing patterns, steganographic attacks\n",
        "\n",
        "**Features:**\n",
        "- **Small vocabularies**: 12-20 symbols maximum\n",
        "- **Complex sequence dependencies**: Multi-order Markov patterns\n",
        "- **Subtle deviations**: Microscopic pattern violations\n",
        "- **Sequence-favoring design**: Traditional ML struggles with temporal patterns\n",
        "- **Difficulty**: Possible Max F1 < 0.75 through carefully built complexity"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install anomaly-grid-py"
      ],
      "metadata": {
        "id": "MGkIG7rpbmk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KspkyoEbk5D"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import random\n",
        "import hashlib\n",
        "from datetime import datetime, timedelta\n",
        "from collections import defaultdict, Counter\n",
        "import itertools\n",
        "import math\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.ensemble import RandomForestClassifier, IsolationForest, ExtraTreesClassifier\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.svm import SVC, OneClassSVM\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, f1_score, accuracy_score, precision_recall_curve,\n",
        "    average_precision_score, classification_report, precision_score,\n",
        "    recall_score, matthews_corrcoef, balanced_accuracy_score,\n",
        "    cohen_kappa_score, log_loss, brier_score_loss\n",
        ")\n",
        "\n",
        "try:\n",
        "    import anomaly_grid_py\n",
        "    ANOMALY_GRID_AVAILABLE = True\n",
        "    print(\"‚úÖ anomaly-grid-py available\")\n",
        "except ImportError:\n",
        "    ANOMALY_GRID_AVAILABLE = False\n",
        "    print(\"‚ùå Install anomaly-grid-py: pip install anomaly-grid-py\")\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "print(\"Ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSAKSlVrbk5F"
      },
      "source": [
        "## üîê Dataset 1: Network Protocol State Machines (16 States)\n",
        "\n",
        "**State transition patterns with subtle protocol violations**\n",
        "\n",
        "- **Alphabet**: 16 protocol states (finite, well-defined)\n",
        "- **Challenge**: Multi-order state dependencies with rare violations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Im_Rh59bk5F"
      },
      "outputs": [],
      "source": [
        "def generate_protocol_state_machine_dataset(n_samples=15000, contamination=0.025):\n",
        "    \"\"\"\n",
        "    Generate protocol state machine dataset.\n",
        "\n",
        "    This dataset focuses on finite alphabet sequence patterns where:\n",
        "    - 16 protocol states form complex transition patterns\n",
        "    - Normal sequences follow strict state machine rules\n",
        "    - Anomalies are subtle state transition violations\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Finite alphabet: 16 protocol states\n",
        "    PROTOCOL_STATES = [\n",
        "        'INIT', 'LISTEN', 'SYN_SENT', 'SYN_RECV', 'ESTABLISHED',\n",
        "        'FIN_WAIT1', 'FIN_WAIT2', 'CLOSE_WAIT', 'CLOSING', 'LAST_ACK',\n",
        "        'TIME_WAIT', 'CLOSED', 'AUTH', 'DATA_XFER', 'ERROR', 'RESET'\n",
        "    ]\n",
        "\n",
        "    # State transition rules (multi-order dependencies)\n",
        "    VALID_TRANSITIONS = {\n",
        "        # Order-1 transitions\n",
        "        'INIT': ['LISTEN', 'SYN_SENT'],\n",
        "        'LISTEN': ['SYN_RECV', 'CLOSED'],\n",
        "        'SYN_SENT': ['SYN_RECV', 'ESTABLISHED', 'CLOSED'],\n",
        "        'SYN_RECV': ['ESTABLISHED', 'FIN_WAIT1', 'RESET'],\n",
        "        'ESTABLISHED': ['AUTH', 'DATA_XFER', 'FIN_WAIT1', 'CLOSE_WAIT'],\n",
        "        'AUTH': ['DATA_XFER', 'ERROR', 'ESTABLISHED'],\n",
        "        'DATA_XFER': ['DATA_XFER', 'FIN_WAIT1', 'CLOSE_WAIT', 'ESTABLISHED'],\n",
        "        'FIN_WAIT1': ['FIN_WAIT2', 'CLOSING', 'TIME_WAIT'],\n",
        "        'FIN_WAIT2': ['TIME_WAIT', 'CLOSED'],\n",
        "        'CLOSE_WAIT': ['LAST_ACK', 'CLOSED'],\n",
        "        'CLOSING': ['TIME_WAIT', 'CLOSED'],\n",
        "        'LAST_ACK': ['CLOSED', 'TIME_WAIT'],\n",
        "        'TIME_WAIT': ['CLOSED', 'INIT'],\n",
        "        'CLOSED': ['INIT', 'LISTEN'],\n",
        "        'ERROR': ['RESET', 'CLOSED', 'INIT'],\n",
        "        'RESET': ['INIT', 'CLOSED']\n",
        "    }\n",
        "\n",
        "    # Order-2 transition constraints (context-dependent)\n",
        "    ORDER2_CONSTRAINTS = {\n",
        "        ('INIT', 'LISTEN'): ['SYN_RECV', 'CLOSED'],\n",
        "        ('INIT', 'SYN_SENT'): ['SYN_RECV', 'ESTABLISHED'],\n",
        "        ('SYN_RECV', 'ESTABLISHED'): ['AUTH', 'DATA_XFER'],\n",
        "        ('ESTABLISHED', 'AUTH'): ['DATA_XFER', 'ESTABLISHED'],\n",
        "        ('AUTH', 'DATA_XFER'): ['DATA_XFER', 'FIN_WAIT1'],\n",
        "        ('DATA_XFER', 'DATA_XFER'): ['DATA_XFER', 'FIN_WAIT1', 'CLOSE_WAIT'],\n",
        "        ('FIN_WAIT1', 'FIN_WAIT2'): ['TIME_WAIT', 'CLOSED'],\n",
        "        ('TIME_WAIT', 'CLOSED'): ['INIT', 'LISTEN'],\n",
        "        ('CLOSED', 'INIT'): ['LISTEN', 'SYN_SENT'],\n",
        "        ('ERROR', 'RESET'): ['INIT', 'CLOSED']\n",
        "    }\n",
        "\n",
        "    # Order-3 transition patterns (complex dependencies)\n",
        "    ORDER3_PATTERNS = {\n",
        "        ('INIT', 'SYN_SENT', 'SYN_RECV'): ['ESTABLISHED'],\n",
        "        ('SYN_RECV', 'ESTABLISHED', 'AUTH'): ['DATA_XFER'],\n",
        "        ('ESTABLISHED', 'AUTH', 'DATA_XFER'): ['DATA_XFER', 'FIN_WAIT1'],\n",
        "        ('AUTH', 'DATA_XFER', 'DATA_XFER'): ['DATA_XFER', 'FIN_WAIT1'],\n",
        "        ('DATA_XFER', 'FIN_WAIT1', 'FIN_WAIT2'): ['TIME_WAIT'],\n",
        "        ('FIN_WAIT2', 'TIME_WAIT', 'CLOSED'): ['INIT'],\n",
        "        ('TIME_WAIT', 'CLOSED', 'INIT'): ['LISTEN', 'SYN_SENT'],\n",
        "        ('CLOSED', 'INIT', 'LISTEN'): ['SYN_RECV'],\n",
        "        ('ERROR', 'RESET', 'INIT'): ['LISTEN', 'SYN_SENT']\n",
        "    }\n",
        "\n",
        "    sequences = []\n",
        "    labels = []\n",
        "    features = []\n",
        "\n",
        "    n_anomalies = int(n_samples * contamination)\n",
        "    n_normal = n_samples - n_anomalies\n",
        "\n",
        "    print(f\"Generating {n_normal} normal and {n_anomalies} anomalous protocol sequences...\")\n",
        "    print(f\"üìä Finite alphabet: {len(PROTOCOL_STATES)} states\")\n",
        "\n",
        "    # Generate normal sequences (following state machine rules)\n",
        "    for i in range(n_normal):\n",
        "        if i % 2000 == 0:\n",
        "            print(f\"  Normal sequences: {i}/{n_normal}\")\n",
        "\n",
        "        sequence_length = random.randint(50, 200)  # Long enough for decent patterns\n",
        "        sequence = generate_valid_protocol_sequence(sequence_length, PROTOCOL_STATES,\n",
        "                                                   VALID_TRANSITIONS, ORDER2_CONSTRAINTS, ORDER3_PATTERNS)\n",
        "\n",
        "        sequences.append(sequence)\n",
        "        labels.append(0)\n",
        "\n",
        "        # Create sequence-focused features\n",
        "        feature_vector = create_protocol_state_features(sequence, PROTOCOL_STATES, is_anomaly=False)\n",
        "        features.append(feature_vector)\n",
        "\n",
        "    # Generate anomalous sequences (subtle state machine violations)\n",
        "    for i in range(n_anomalies):\n",
        "        if i % 200 == 0:\n",
        "            print(f\"  Anomalous sequences: {i}/{n_anomalies}\")\n",
        "\n",
        "        # Start with a valid sequence\n",
        "        normal_idx = random.randint(0, len(sequences) - 1)\n",
        "        base_sequence = sequences[normal_idx].copy()\n",
        "\n",
        "        # Apply subtle state machine violations\n",
        "        violation_type = random.choices(\n",
        "            ['invalid_transition', 'order2_violation', 'order3_violation', 'state_skip'],\n",
        "            weights=[0.4, 0.3, 0.2, 0.1]\n",
        "        )[0]\n",
        "\n",
        "        base_sequence = apply_state_violation(base_sequence, violation_type, PROTOCOL_STATES,\n",
        "                                            VALID_TRANSITIONS, ORDER2_CONSTRAINTS, ORDER3_PATTERNS)\n",
        "\n",
        "        sequences.append(base_sequence)\n",
        "        labels.append(1)\n",
        "\n",
        "        # Create sequence-focused features\n",
        "        feature_vector = create_protocol_state_features(base_sequence, PROTOCOL_STATES, is_anomaly=True)\n",
        "        features.append(feature_vector)\n",
        "\n",
        "    # Shuffle\n",
        "    combined = list(zip(sequences, labels, features))\n",
        "    random.shuffle(combined)\n",
        "    sequences, labels, features = zip(*combined)\n",
        "\n",
        "    return list(sequences), np.array(labels), np.array(features)\n",
        "\n",
        "def generate_valid_protocol_sequence(length, states, transitions, order2_constraints, order3_patterns):\n",
        "    \"\"\"Generate a valid protocol sequence following state machine rules.\"\"\"\n",
        "    sequence = ['INIT']  # Always start with INIT\n",
        "\n",
        "    while len(sequence) < length:\n",
        "        current_state = sequence[-1]\n",
        "\n",
        "        # Check for order-3 patterns first (highest priority)\n",
        "        if len(sequence) >= 3:\n",
        "            order3_key = tuple(sequence[-3:])\n",
        "            if order3_key in order3_patterns:\n",
        "                next_state = random.choice(order3_patterns[order3_key])\n",
        "                sequence.append(next_state)\n",
        "                continue\n",
        "\n",
        "        # Check for order-2 constraints\n",
        "        if len(sequence) >= 2:\n",
        "            order2_key = tuple(sequence[-2:])\n",
        "            if order2_key in order2_constraints:\n",
        "                next_state = random.choice(order2_constraints[order2_key])\n",
        "                sequence.append(next_state)\n",
        "                continue\n",
        "\n",
        "        # Use order-1 transitions\n",
        "        if current_state in transitions:\n",
        "            next_state = random.choice(transitions[current_state])\n",
        "            sequence.append(next_state)\n",
        "        else:\n",
        "            # Fallback to random valid state\n",
        "            sequence.append(random.choice(states))\n",
        "\n",
        "    return sequence[:length]\n",
        "\n",
        "def apply_state_violation(sequence, violation_type, states, transitions, order2_constraints, order3_patterns):\n",
        "    \"\"\"Apply subtle state machine violations to create anomalies.\"\"\"\n",
        "    if len(sequence) < 5:\n",
        "        return sequence\n",
        "\n",
        "    if violation_type == 'invalid_transition':\n",
        "        # Insert invalid state transition\n",
        "        pos = random.randint(1, len(sequence) - 2)\n",
        "        current_state = sequence[pos - 1]\n",
        "        valid_next = transitions.get(current_state, [])\n",
        "        invalid_states = [s for s in states if s not in valid_next and s != current_state]\n",
        "        if invalid_states:\n",
        "            sequence[pos] = random.choice(invalid_states)\n",
        "\n",
        "    elif violation_type == 'order2_violation':\n",
        "        # Violate order-2 constraint\n",
        "        for i in range(2, len(sequence)):\n",
        "            order2_key = tuple(sequence[i-2:i])\n",
        "            if order2_key in order2_constraints:\n",
        "                valid_next = order2_constraints[order2_key]\n",
        "                invalid_states = [s for s in states if s not in valid_next]\n",
        "                if invalid_states:\n",
        "                    sequence[i] = random.choice(invalid_states)\n",
        "                    break\n",
        "\n",
        "    elif violation_type == 'order3_violation':\n",
        "        # Violate order-3 pattern\n",
        "        for i in range(3, len(sequence)):\n",
        "            order3_key = tuple(sequence[i-3:i])\n",
        "            if order3_key in order3_patterns:\n",
        "                valid_next = order3_patterns[order3_key]\n",
        "                invalid_states = [s for s in states if s not in valid_next]\n",
        "                if invalid_states:\n",
        "                    sequence[i] = random.choice(invalid_states)\n",
        "                    break\n",
        "\n",
        "    else:  # state_skip\n",
        "        # Skip a required intermediate state\n",
        "        pos = random.randint(1, len(sequence) - 2)\n",
        "        # Remove one state to create a skip\n",
        "        sequence.pop(pos)\n",
        "\n",
        "    return sequence\n",
        "\n",
        "def create_protocol_state_features(sequence, states, is_anomaly=False):\n",
        "    \"\"\"Create features that lose temporal information.\"\"\"\n",
        "\n",
        "    # State frequency features (lose temporal order)\n",
        "    state_counts = {state: sequence.count(state) for state in states}\n",
        "\n",
        "    # Basic sequence statistics (minimal temporal info)\n",
        "    sequence_stats = {\n",
        "        'length': len(sequence),\n",
        "        'unique_states': len(set(sequence)),\n",
        "        'most_common_state_freq': max(state_counts.values()) if state_counts else 0,\n",
        "        'state_diversity': len([c for c in state_counts.values() if c > 0])\n",
        "    }\n",
        "\n",
        "    # Transition frequency (some temporal info but limited)\n",
        "    transitions = {}\n",
        "    for i in range(len(sequence) - 1):\n",
        "        trans = f\"{sequence[i]}->{sequence[i+1]}\"\n",
        "        transitions[trans] = transitions.get(trans, 0) + 1\n",
        "\n",
        "    transition_stats = {\n",
        "        'num_transitions': len(transitions),\n",
        "        'max_transition_freq': max(transitions.values()) if transitions else 0,\n",
        "        'avg_transition_freq': np.mean(list(transitions.values())) if transitions else 0\n",
        "    }\n",
        "\n",
        "    # Combine features\n",
        "    feature_vector = []\n",
        "    feature_vector.extend(state_counts.values())\n",
        "    feature_vector.extend(sequence_stats.values())\n",
        "    feature_vector.extend(transition_stats.values())\n",
        "\n",
        "    # Add minimal noise\n",
        "    for _ in range(10):\n",
        "        feature_vector.append(random.gauss(0, 0.5))\n",
        "\n",
        "    return feature_vector\n",
        "\n",
        "print(\"üîê Generating Protocol State Machine Dataset...\")\n",
        "protocol_sequences, protocol_labels, protocol_features = generate_protocol_state_machine_dataset(n_samples=15000, contamination=0.025)\n",
        "\n",
        "print(f\"\\nüìä Protocol State Machine Dataset:\")\n",
        "print(f\"  Alphabet size: 16 states (finite, well-defined)\")\n",
        "print(f\"  Total samples: {len(protocol_sequences)}\")\n",
        "print(f\"  Features: {protocol_features.shape[1]}\")\n",
        "print(f\"  Normal sequences: {np.sum(protocol_labels == 0)} ({np.sum(protocol_labels == 0)/len(protocol_labels)*100:.1f}%)\")\n",
        "print(f\"  Anomalous sequences: {np.sum(protocol_labels == 1)} ({np.sum(protocol_labels == 1)/len(protocol_labels)*100:.1f}%)\")\n",
        "print(f\"  Avg sequence length: {np.mean([len(seq) for seq in protocol_sequences]):.1f}\")\n",
        "print(f\"  Max sequence length: {max([len(seq) for seq in protocol_sequences])}\")\n",
        "\n",
        "# Show examples\n",
        "print(f\"\\nüîç Example sequences:\")\n",
        "normal_idx = np.where(protocol_labels == 0)[0][0]\n",
        "anomaly_idx = np.where(protocol_labels == 1)[0][0]\n",
        "print(f\"  Normal (first 15): {protocol_sequences[normal_idx][:15]}\")\n",
        "print(f\"  Anomaly (first 15): {protocol_sequences[anomaly_idx][:15]}\")\n",
        "print(\"‚úÖ Protocol state machine dataset generated\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByTYfVqmbk5H"
      },
      "source": [
        "## üß¨ Dataset 2: Protein Folding Sequences (20 Amino Acids)\n",
        "\n",
        "**Biological sequence patterns with synthetic rare misfolding events**\n",
        "\n",
        "- **Alphabet**: 20 amino acids (standard biological alphabet)\n",
        "- **Challenge**: Complex biological constraints with rare misfolding patterns\n",
        "- **Focus**: Protein folding requires understanding of sequence context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjY8hkP9bk5H"
      },
      "outputs": [],
      "source": [
        "def generate_protein_folding_dataset(n_samples=15000, contamination=0.02):\n",
        "    \"\"\"\n",
        "    Generate impossibly difficult protein folding dataset.\n",
        "\n",
        "    This dataset focuses on biological sequence patterns where:\n",
        "    - 20 amino acids form complex folding patterns\n",
        "    - Normal sequences follow biological constraints\n",
        "    - Anomalies are rare misfolding patterns\n",
        "    \"\"\"\n",
        "\n",
        "    # Finite alphabet: 20 standard amino acids\n",
        "    AMINO_ACIDS = [\n",
        "        'A', 'R', 'N', 'D', 'C', 'Q', 'E', 'G', 'H', 'I',\n",
        "        'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V'\n",
        "    ]\n",
        "\n",
        "    # Amino acid properties for biological constraints\n",
        "    HYDROPHOBIC = ['A', 'I', 'L', 'M', 'F', 'W', 'Y', 'V']\n",
        "    HYDROPHILIC = ['R', 'N', 'D', 'Q', 'E', 'H', 'K', 'S', 'T']\n",
        "    SPECIAL = ['C', 'G', 'P']  # Cysteine, Glycine, Proline\n",
        "\n",
        "    CHARGED_POSITIVE = ['R', 'H', 'K']\n",
        "    CHARGED_NEGATIVE = ['D', 'E']\n",
        "    AROMATIC = ['F', 'W', 'Y', 'H']\n",
        "\n",
        "    # Biological folding patterns\n",
        "    ALPHA_HELIX_FAVORING = ['A', 'E', 'L', 'M']\n",
        "    BETA_SHEET_FAVORING = ['I', 'Y', 'F', 'V']\n",
        "    TURN_FAVORING = ['G', 'N', 'P', 'S']\n",
        "\n",
        "    # Biological sequence motifs (order-dependent)\n",
        "    SIGNAL_PEPTIDES = [\n",
        "        ['M', 'K', 'L', 'L', 'F'],  # Start signal\n",
        "        ['L', 'L', 'A', 'A', 'A'],  # Hydrophobic signal\n",
        "        ['A', 'L', 'A', 'L', 'A']   # Alternating pattern\n",
        "    ]\n",
        "\n",
        "    BINDING_SITES = [\n",
        "        ['R', 'G', 'D'],           # RGD motif\n",
        "        ['N', 'P', 'X', 'Y'],      # NPXY motif (X = any)\n",
        "        ['D', 'X', 'D', 'X', 'D']  # Metal binding\n",
        "    ]\n",
        "\n",
        "    # Disulfide bond patterns (Cysteine constraints)\n",
        "    DISULFIDE_PATTERNS = [\n",
        "        ['C', 'X', 'X', 'C'],      # Close disulfide\n",
        "        ['C', 'X', 'X', 'X', 'X', 'C'],  # Medium disulfide\n",
        "    ]\n",
        "\n",
        "    sequences = []\n",
        "    labels = []\n",
        "    features = []\n",
        "\n",
        "    n_anomalies = int(n_samples * contamination)\n",
        "    n_normal = n_samples - n_anomalies\n",
        "\n",
        "    print(f\"Generating {n_normal} normal and {n_anomalies} misfolded protein sequences...\")\n",
        "    print(f\"üìä Finite alphabet: {len(AMINO_ACIDS)} amino acids\")\n",
        "\n",
        "    # Generate normal sequences (following biological constraints)\n",
        "    for i in range(n_normal):\n",
        "        if i % 2000 == 0:\n",
        "            print(f\"  Normal sequences: {i}/{n_normal}\")\n",
        "\n",
        "        sequence_length = random.randint(80, 300)  # Protein length\n",
        "        sequence = generate_valid_protein_sequence(sequence_length, AMINO_ACIDS,\n",
        "                                                 HYDROPHOBIC, HYDROPHILIC, SPECIAL,\n",
        "                                                 ALPHA_HELIX_FAVORING, BETA_SHEET_FAVORING, TURN_FAVORING,\n",
        "                                                 SIGNAL_PEPTIDES, BINDING_SITES, DISULFIDE_PATTERNS)\n",
        "\n",
        "        sequences.append(sequence)\n",
        "        labels.append(0)\n",
        "\n",
        "        # Create sequence-focused features\n",
        "        feature_vector = create_protein_features(sequence, AMINO_ACIDS, HYDROPHOBIC,\n",
        "                                                HYDROPHILIC, AROMATIC, is_anomaly=False)\n",
        "        features.append(feature_vector)\n",
        "\n",
        "    # Generate anomalous sequences (misfolding patterns)\n",
        "    for i in range(n_anomalies):\n",
        "        if i % 200 == 0:\n",
        "            print(f\"  Misfolded sequences: {i}/{n_anomalies}\")\n",
        "\n",
        "        # Start with a valid sequence\n",
        "        normal_idx = random.randint(0, len(sequences) - 1)\n",
        "        base_sequence = sequences[normal_idx].copy()\n",
        "\n",
        "        # Apply subtle misfolding patterns\n",
        "        misfolding_type = random.choices(\n",
        "            ['hydrophobic_exposure', 'disulfide_disruption', 'charge_clustering', 'proline_kink'],\n",
        "            weights=[0.3, 0.25, 0.25, 0.2]\n",
        "        )[0]\n",
        "\n",
        "        base_sequence = apply_misfolding_pattern(base_sequence, misfolding_type, AMINO_ACIDS,\n",
        "                                               HYDROPHOBIC, HYDROPHILIC, CHARGED_POSITIVE,\n",
        "                                               CHARGED_NEGATIVE, SPECIAL)\n",
        "\n",
        "        sequences.append(base_sequence)\n",
        "        labels.append(1)\n",
        "\n",
        "        # Create sequence-focused features\n",
        "        feature_vector = create_protein_features(base_sequence, AMINO_ACIDS, HYDROPHOBIC,\n",
        "                                                HYDROPHILIC, AROMATIC, is_anomaly=True)\n",
        "        features.append(feature_vector)\n",
        "\n",
        "    # Shuffle\n",
        "    combined = list(zip(sequences, labels, features))\n",
        "    random.shuffle(combined)\n",
        "    sequences, labels, features = zip(*combined)\n",
        "\n",
        "    return list(sequences), np.array(labels), np.array(features)\n",
        "\n",
        "def generate_valid_protein_sequence(length, amino_acids, hydrophobic, hydrophilic, special,\n",
        "                                   alpha_helix, beta_sheet, turn, signal_peptides,\n",
        "                                   binding_sites, disulfide_patterns):\n",
        "    \"\"\"Generate a biologically valid protein sequence.\"\"\"\n",
        "    sequence = ['M']  # Always start with Methionine\n",
        "\n",
        "    # Add signal peptide (20% chance)\n",
        "    if random.random() < 0.2:\n",
        "        signal = random.choice(signal_peptides)\n",
        "        sequence.extend(signal)\n",
        "\n",
        "    while len(sequence) < length:\n",
        "        remaining = length - len(sequence)\n",
        "\n",
        "        # Choose structural element\n",
        "        structure_type = random.choices(\n",
        "            ['alpha_helix', 'beta_sheet', 'turn', 'binding_site', 'disulfide', 'random'],\n",
        "            weights=[0.3, 0.25, 0.15, 0.1, 0.05, 0.15]\n",
        "        )[0]\n",
        "\n",
        "        if structure_type == 'alpha_helix' and remaining >= 8:\n",
        "            # Add alpha helix segment\n",
        "            helix_length = min(random.randint(8, 15), remaining)\n",
        "            for _ in range(helix_length):\n",
        "                aa = random.choice(alpha_helix + hydrophobic)\n",
        "                sequence.append(aa)\n",
        "\n",
        "        elif structure_type == 'beta_sheet' and remaining >= 6:\n",
        "            # Add beta sheet segment\n",
        "            sheet_length = min(random.randint(6, 12), remaining)\n",
        "            for _ in range(sheet_length):\n",
        "                aa = random.choice(beta_sheet + hydrophobic)\n",
        "                sequence.append(aa)\n",
        "\n",
        "        elif structure_type == 'turn' and remaining >= 3:\n",
        "            # Add turn segment\n",
        "            turn_length = min(random.randint(3, 6), remaining)\n",
        "            for _ in range(turn_length):\n",
        "                aa = random.choice(turn + hydrophilic)\n",
        "                sequence.append(aa)\n",
        "\n",
        "        elif structure_type == 'binding_site' and remaining >= 3:\n",
        "            # Add binding site motif\n",
        "            motif = random.choice(binding_sites)\n",
        "            for aa in motif:\n",
        "                if aa == 'X':\n",
        "                    sequence.append(random.choice(amino_acids))\n",
        "                else:\n",
        "                    sequence.append(aa)\n",
        "                if len(sequence) >= length:\n",
        "                    break\n",
        "\n",
        "        elif structure_type == 'disulfide' and remaining >= 4:\n",
        "            # Add disulfide pattern\n",
        "            pattern = random.choice(disulfide_patterns)\n",
        "            for aa in pattern:\n",
        "                if aa == 'X':\n",
        "                    sequence.append(random.choice(amino_acids))\n",
        "                else:\n",
        "                    sequence.append(aa)\n",
        "                if len(sequence) >= length:\n",
        "                    break\n",
        "\n",
        "        else:\n",
        "            # Add random amino acid with biological bias\n",
        "            aa_type = random.choices(\n",
        "                [hydrophobic, hydrophilic, special],\n",
        "                weights=[0.4, 0.5, 0.1]\n",
        "            )[0]\n",
        "            sequence.append(random.choice(aa_type))\n",
        "\n",
        "    return sequence[:length]\n",
        "\n",
        "def apply_misfolding_pattern(sequence, misfolding_type, amino_acids, hydrophobic,\n",
        "                           hydrophilic, charged_pos, charged_neg, special):\n",
        "    \"\"\"Apply subtle misfolding patterns to create biological anomalies.\"\"\"\n",
        "    if len(sequence) < 10:\n",
        "        return sequence\n",
        "\n",
        "    if misfolding_type == 'hydrophobic_exposure':\n",
        "        # Place hydrophobic residues in exposed positions (surface)\n",
        "        # This violates the hydrophobic core principle\n",
        "        for _ in range(random.randint(2, 4)):\n",
        "            pos = random.randint(1, len(sequence) - 2)\n",
        "            # Replace with highly hydrophobic residue in wrong context\n",
        "            sequence[pos] = random.choice(['F', 'W', 'I', 'L'])\n",
        "\n",
        "    elif misfolding_type == 'disulfide_disruption':\n",
        "        # Disrupt disulfide bond patterns\n",
        "        cys_positions = [i for i, aa in enumerate(sequence) if aa == 'C']\n",
        "        if len(cys_positions) >= 2:\n",
        "            # Replace one cysteine to break disulfide bond\n",
        "            pos = random.choice(cys_positions)\n",
        "            sequence[pos] = random.choice(['S', 'T', 'A'])  # Similar but no disulfide\n",
        "\n",
        "    elif misfolding_type == 'charge_clustering':\n",
        "        # Create unfavorable charge clustering\n",
        "        cluster_start = random.randint(1, len(sequence) - 5)\n",
        "        charge_type = random.choice([charged_pos, charged_neg])\n",
        "        for i in range(cluster_start, min(cluster_start + 3, len(sequence))):\n",
        "            sequence[i] = random.choice(charge_type)\n",
        "\n",
        "    else:  # proline_kink\n",
        "        # Insert proline in secondary structure (disrupts folding)\n",
        "        # Proline creates kinks and breaks alpha helices\n",
        "        for _ in range(random.randint(1, 3)):\n",
        "            pos = random.randint(5, len(sequence) - 5)\n",
        "            sequence[pos] = 'P'  # Proline kink\n",
        "\n",
        "    return sequence\n",
        "\n",
        "def create_protein_features(sequence, amino_acids, hydrophobic, hydrophilic, aromatic, is_anomaly=False):\n",
        "    \"\"\"Create features that lose biological sequence context.\"\"\"\n",
        "\n",
        "    # Amino acid composition (loses sequence order)\n",
        "    aa_counts = {aa: sequence.count(aa) for aa in amino_acids}\n",
        "\n",
        "    # Biochemical properties (some biological meaning but limited)\n",
        "    property_counts = {\n",
        "        'hydrophobic': sum(1 for aa in sequence if aa in hydrophobic),\n",
        "        'hydrophilic': sum(1 for aa in sequence if aa in hydrophilic),\n",
        "        'aromatic': sum(1 for aa in sequence if aa in aromatic),\n",
        "        'charged': sum(1 for aa in sequence if aa in ['R', 'H', 'K', 'D', 'E'])\n",
        "    }\n",
        "\n",
        "    # Basic sequence statistics\n",
        "    sequence_stats = {\n",
        "        'length': len(sequence),\n",
        "        'unique_aa': len(set(sequence)),\n",
        "        'hydrophobic_ratio': property_counts['hydrophobic'] / len(sequence),\n",
        "        'charge_ratio': property_counts['charged'] / len(sequence)\n",
        "    }\n",
        "\n",
        "    # Combine features\n",
        "    feature_vector = []\n",
        "    feature_vector.extend(aa_counts.values())\n",
        "    feature_vector.extend(property_counts.values())\n",
        "    feature_vector.extend(sequence_stats.values())\n",
        "\n",
        "    # Minimal noise\n",
        "    for _ in range(8):\n",
        "        feature_vector.append(random.gauss(0, 0.3))\n",
        "\n",
        "    return feature_vector\n",
        "\n",
        "print(\"üß¨ Generating Protein Folding Dataset...\")\n",
        "protein_sequences, protein_labels, protein_features = generate_protein_folding_dataset(n_samples=15000, contamination=0.02)\n",
        "\n",
        "print(f\"\\nüìä Protein Folding Dataset:\")\n",
        "print(f\"  Alphabet size: 20 amino acids (standard biological alphabet)\")\n",
        "print(f\"  Total samples: {len(protein_sequences)}\")\n",
        "print(f\"  Features: {protein_features.shape[1]}\")\n",
        "print(f\"  Normal sequences: {np.sum(protein_labels == 0)} ({np.sum(protein_labels == 0)/len(protein_labels)*100:.1f}%)\")\n",
        "print(f\"  Misfolded sequences: {np.sum(protein_labels == 1)} ({np.sum(protein_labels == 1)/len(protein_labels)*100:.1f}%)\")\n",
        "print(f\"  Avg sequence length: {np.mean([len(seq) for seq in protein_sequences]):.1f}\")\n",
        "print(f\"  Max sequence length: {max([len(seq) for seq in protein_sequences])}\")\n",
        "\n",
        "# Show examples\n",
        "print(f\"\\nüîç Example sequences:\")\n",
        "normal_idx = np.where(protein_labels == 0)[0][0]\n",
        "anomaly_idx = np.where(protein_labels == 1)[0][0]\n",
        "print(f\"  Normal (first 20): {''.join(protein_sequences[normal_idx][:20])}\")\n",
        "print(f\"  Misfolded (first 20): {''.join(protein_sequences[anomaly_idx][:20])}\")\n",
        "print(\"‚úÖ Protein folding dataset generated\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTfSXkiJbk5I"
      },
      "source": [
        "## üì° Dataset 3: Communication Protocol Analysis (12 Symbols)\n",
        "\n",
        "**Digital communication patterns with steganographic timing attacks**\n",
        "\n",
        "- **Alphabet**: 12 communication symbols (minimal finite alphabet)\n",
        "- **Challenge**: Timing patterns with steganographic concealment\n",
        "- **Focus**: Communication protocols require temporal understanding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJul3K7Dbk5J"
      },
      "outputs": [],
      "source": [
        "def generate_communication_protocol_dataset(n_samples=15000, contamination=0.02):\n",
        "    \"\"\"\n",
        "    Generate impossibly difficult communication protocol dataset.\n",
        "\n",
        "    This dataset focuses on minimal finite alphabet patterns where:\n",
        "    - 12 communication symbols form complex timing patterns\n",
        "    - Normal sequences follow strict protocol timing\n",
        "    - Anomalies are steganographic timing attacks\n",
        "    \"\"\"\n",
        "\n",
        "    # Finite alphabet: 12 communication symbols\n",
        "    COMM_SYMBOLS = [\n",
        "        'START', 'SYNC', 'DATA', 'ACK', 'NACK', 'RETRY',\n",
        "        'PAUSE', 'RESUME', 'CHECK', 'ERROR', 'STOP', 'IDLE'\n",
        "    ]\n",
        "\n",
        "    # Protocol timing constraints (order-dependent)\n",
        "    TIMING_PATTERNS = {\n",
        "        # Normal timing sequences\n",
        "        ('START', 'SYNC'): ['DATA', 'CHECK'],\n",
        "        ('SYNC', 'DATA'): ['DATA', 'ACK', 'CHECK'],\n",
        "        ('DATA', 'DATA'): ['DATA', 'ACK', 'CHECK', 'PAUSE'],\n",
        "        ('DATA', 'ACK'): ['DATA', 'STOP', 'PAUSE'],\n",
        "        ('DATA', 'CHECK'): ['ACK', 'NACK', 'ERROR'],\n",
        "        ('ACK', 'DATA'): ['DATA', 'ACK', 'STOP'],\n",
        "        ('CHECK', 'ACK'): ['DATA', 'STOP'],\n",
        "        ('CHECK', 'NACK'): ['RETRY', 'ERROR'],\n",
        "        ('NACK', 'RETRY'): ['DATA', 'SYNC'],\n",
        "        ('RETRY', 'DATA'): ['DATA', 'ACK', 'CHECK'],\n",
        "        ('PAUSE', 'RESUME'): ['DATA', 'SYNC'],\n",
        "        ('RESUME', 'DATA'): ['DATA', 'ACK'],\n",
        "        ('ERROR', 'RETRY'): ['START', 'SYNC'],\n",
        "        ('STOP', 'IDLE'): ['START', 'IDLE'],\n",
        "        ('IDLE', 'START'): ['SYNC', 'DATA']\n",
        "    }\n",
        "\n",
        "    # Complex 3-symbol timing patterns\n",
        "    TIMING_3_PATTERNS = {\n",
        "        ('START', 'SYNC', 'DATA'): ['DATA', 'ACK'],\n",
        "        ('SYNC', 'DATA', 'DATA'): ['DATA', 'ACK', 'CHECK'],\n",
        "        ('DATA', 'DATA', 'ACK'): ['DATA', 'STOP'],\n",
        "        ('DATA', 'ACK', 'DATA'): ['DATA', 'ACK', 'STOP'],\n",
        "        ('DATA', 'CHECK', 'ACK'): ['DATA', 'STOP'],\n",
        "        ('DATA', 'CHECK', 'NACK'): ['RETRY', 'ERROR'],\n",
        "        ('CHECK', 'NACK', 'RETRY'): ['DATA', 'SYNC'],\n",
        "        ('NACK', 'RETRY', 'DATA'): ['DATA', 'ACK'],\n",
        "        ('ACK', 'DATA', 'DATA'): ['DATA', 'ACK', 'CHECK'],\n",
        "        ('PAUSE', 'RESUME', 'DATA'): ['DATA', 'ACK'],\n",
        "        ('ERROR', 'RETRY', 'START'): ['SYNC', 'DATA'],\n",
        "        ('STOP', 'IDLE', 'START'): ['SYNC'],\n",
        "        ('IDLE', 'START', 'SYNC'): ['DATA']\n",
        "    }\n",
        "\n",
        "    # Valid protocol flows\n",
        "    PROTOCOL_FLOWS = [\n",
        "        ['START', 'SYNC', 'DATA', 'DATA', 'ACK', 'STOP'],\n",
        "        ['START', 'SYNC', 'DATA', 'CHECK', 'ACK', 'DATA', 'STOP'],\n",
        "        ['START', 'SYNC', 'DATA', 'CHECK', 'NACK', 'RETRY', 'DATA', 'ACK', 'STOP'],\n",
        "        ['START', 'SYNC', 'DATA', 'PAUSE', 'RESUME', 'DATA', 'ACK', 'STOP'],\n",
        "        ['START', 'SYNC', 'DATA', 'ERROR', 'RETRY', 'START', 'SYNC', 'DATA', 'ACK', 'STOP']\n",
        "    ]\n",
        "\n",
        "    sequences = []\n",
        "    labels = []\n",
        "    features = []\n",
        "\n",
        "    n_anomalies = int(n_samples * contamination)\n",
        "    n_normal = n_samples - n_anomalies\n",
        "\n",
        "    print(f\"Generating {n_normal} normal and {n_anomalies} steganographic communication sequences...\")\n",
        "    print(f\"üìä Finite alphabet: {len(COMM_SYMBOLS)} symbols\")\n",
        "\n",
        "    # Generate normal sequences (following timing protocols)\n",
        "    for i in range(n_normal):\n",
        "        if i % 2000 == 0:\n",
        "            print(f\"  Normal sequences: {i}/{n_normal}\")\n",
        "\n",
        "        sequence_length = random.randint(30, 120)  # Communication session length\n",
        "        sequence = generate_valid_communication_sequence(sequence_length, COMM_SYMBOLS,\n",
        "                                                        TIMING_PATTERNS, TIMING_3_PATTERNS, PROTOCOL_FLOWS)\n",
        "\n",
        "        sequences.append(sequence)\n",
        "        labels.append(0)\n",
        "\n",
        "        # Create sequence-focused features\n",
        "        feature_vector = create_communication_features(sequence, COMM_SYMBOLS, is_anomaly=False)\n",
        "        features.append(feature_vector)\n",
        "\n",
        "    # Generate anomalous sequences (steganographic timing attacks)\n",
        "    for i in range(n_anomalies):\n",
        "        if i % 200 == 0:\n",
        "            print(f\"  Steganographic sequences: {i}/{n_anomalies}\")\n",
        "\n",
        "        # Start with a valid sequence\n",
        "        normal_idx = random.randint(0, len(sequences) - 1)\n",
        "        base_sequence = sequences[normal_idx].copy()\n",
        "\n",
        "        # Apply steganographic timing attacks\n",
        "        attack_type = random.choices(\n",
        "            ['timing_delay', 'symbol_substitution', 'pattern_disruption', 'flow_manipulation'],\n",
        "            weights=[0.3, 0.3, 0.25, 0.15]\n",
        "        )[0]\n",
        "\n",
        "        base_sequence = apply_steganographic_attack(base_sequence, attack_type, COMM_SYMBOLS,\n",
        "                                                  TIMING_PATTERNS, TIMING_3_PATTERNS)\n",
        "\n",
        "        sequences.append(base_sequence)\n",
        "        labels.append(1)\n",
        "\n",
        "        # Create sequence-focused features\n",
        "        feature_vector = create_communication_features(base_sequence, COMM_SYMBOLS, is_anomaly=True)\n",
        "        features.append(feature_vector)\n",
        "\n",
        "    # Shuffle\n",
        "    combined = list(zip(sequences, labels, features))\n",
        "    random.shuffle(combined)\n",
        "    sequences, labels, features = zip(*combined)\n",
        "\n",
        "    return list(sequences), np.array(labels), np.array(features)\n",
        "\n",
        "def generate_valid_communication_sequence(length, symbols, timing_patterns, timing_3_patterns, protocol_flows):\n",
        "    \"\"\"Generate a valid communication sequence following timing protocols.\"\"\"\n",
        "\n",
        "    # Start with a protocol flow template\n",
        "    if random.random() < 0.7:\n",
        "        base_flow = random.choice(protocol_flows).copy()\n",
        "        sequence = base_flow\n",
        "    else:\n",
        "        sequence = ['START']  # Always start with START\n",
        "\n",
        "    while len(sequence) < length:\n",
        "        # Check for 3-symbol patterns first\n",
        "        if len(sequence) >= 3:\n",
        "            pattern_3 = tuple(sequence[-3:])\n",
        "            if pattern_3 in timing_3_patterns:\n",
        "                next_symbol = random.choice(timing_3_patterns[pattern_3])\n",
        "                sequence.append(next_symbol)\n",
        "                continue\n",
        "\n",
        "        # Check for 2-symbol patterns\n",
        "        if len(sequence) >= 2:\n",
        "            pattern_2 = tuple(sequence[-2:])\n",
        "            if pattern_2 in timing_patterns:\n",
        "                next_symbol = random.choice(timing_patterns[pattern_2])\n",
        "                sequence.append(next_symbol)\n",
        "                continue\n",
        "\n",
        "        # Fallback to protocol-aware random selection\n",
        "        current_symbol = sequence[-1]\n",
        "        if current_symbol == 'START':\n",
        "            next_symbol = random.choice(['SYNC', 'DATA'])\n",
        "        elif current_symbol == 'STOP':\n",
        "            next_symbol = random.choice(['IDLE', 'START'])\n",
        "        elif current_symbol == 'IDLE':\n",
        "            next_symbol = random.choice(['START', 'IDLE'])\n",
        "        else:\n",
        "            next_symbol = random.choice(symbols)\n",
        "\n",
        "        sequence.append(next_symbol)\n",
        "\n",
        "    return sequence[:length]\n",
        "\n",
        "def apply_steganographic_attack(sequence, attack_type, symbols, timing_patterns, timing_3_patterns):\n",
        "    \"\"\"Apply steganographic timing attacks to communication sequences.\"\"\"\n",
        "    if len(sequence) < 8:\n",
        "        return sequence\n",
        "\n",
        "    if attack_type == 'timing_delay':\n",
        "        # Insert subtle timing delays (extra PAUSE/IDLE symbols)\n",
        "        for _ in range(random.randint(1, 3)):\n",
        "            pos = random.randint(2, len(sequence) - 2)\n",
        "            delay_symbol = random.choice(['PAUSE', 'IDLE'])\n",
        "            sequence.insert(pos, delay_symbol)\n",
        "\n",
        "    elif attack_type == 'symbol_substitution':\n",
        "        # Substitute symbols while maintaining protocol appearance\n",
        "        substitutions = {\n",
        "            'ACK': 'NACK',    # Flip acknowledgment\n",
        "            'DATA': 'CHECK',  # Change data to check\n",
        "            'SYNC': 'START',  # Timing manipulation\n",
        "            'PAUSE': 'IDLE'   # Subtle timing change\n",
        "        }\n",
        "\n",
        "        for _ in range(random.randint(1, 2)):\n",
        "            pos = random.randint(1, len(sequence) - 2)\n",
        "            original = sequence[pos]\n",
        "            if original in substitutions:\n",
        "                sequence[pos] = substitutions[original]\n",
        "\n",
        "    elif attack_type == 'pattern_disruption':\n",
        "        # Disrupt timing patterns\n",
        "        for i in range(2, len(sequence) - 1):\n",
        "            pattern_2 = tuple(sequence[i-2:i])\n",
        "            if pattern_2 in timing_patterns:\n",
        "                valid_next = timing_patterns[pattern_2]\n",
        "                invalid_symbols = [s for s in symbols if s not in valid_next]\n",
        "                if invalid_symbols:\n",
        "                    sequence[i] = random.choice(invalid_symbols)\n",
        "                    break\n",
        "\n",
        "    else:  # flow_manipulation\n",
        "        # Manipulate protocol flow\n",
        "        # Insert unexpected protocol transitions\n",
        "        unexpected_transitions = [\n",
        "            ('DATA', 'START'),   # Unexpected restart\n",
        "            ('ACK', 'ERROR'),    # Unexpected error\n",
        "            ('SYNC', 'STOP'),    # Premature stop\n",
        "            ('CHECK', 'IDLE')    # Unexpected idle\n",
        "        ]\n",
        "\n",
        "        transition = random.choice(unexpected_transitions)\n",
        "        # Find position to insert this transition\n",
        "        for i in range(len(sequence) - 1):\n",
        "            if sequence[i] == transition[0]:\n",
        "                sequence[i + 1] = transition[1]\n",
        "                break\n",
        "\n",
        "    return sequence\n",
        "\n",
        "def create_communication_features(sequence, symbols, is_anomaly=False):\n",
        "    \"\"\"Create features that lose temporal communication patterns.\"\"\"\n",
        "\n",
        "    # Symbol frequency (loses temporal order)\n",
        "    symbol_counts = {symbol: sequence.count(symbol) for symbol in symbols}\n",
        "\n",
        "    # Communication statistics (limited temporal info)\n",
        "    comm_stats = {\n",
        "        'length': len(sequence),\n",
        "        'unique_symbols': len(set(sequence)),\n",
        "        'data_ratio': sequence.count('DATA') / len(sequence),\n",
        "        'control_ratio': (sequence.count('START') + sequence.count('STOP')) / len(sequence),\n",
        "        'error_ratio': (sequence.count('ERROR') + sequence.count('NACK')) / len(sequence)\n",
        "    }\n",
        "\n",
        "    # Protocol flow indicators (some temporal info but limited)\n",
        "    flow_stats = {\n",
        "        'has_start': 1 if 'START' in sequence else 0,\n",
        "        'has_stop': 1 if 'STOP' in sequence else 0,\n",
        "        'has_error': 1 if 'ERROR' in sequence else 0,\n",
        "        'ack_nack_ratio': sequence.count('ACK') / max(1, sequence.count('NACK') + sequence.count('ACK'))\n",
        "    }\n",
        "\n",
        "    # Combine features\n",
        "    feature_vector = []\n",
        "    feature_vector.extend(symbol_counts.values())\n",
        "    feature_vector.extend(comm_stats.values())\n",
        "    feature_vector.extend(flow_stats.values())\n",
        "\n",
        "    # Minimal noise\n",
        "    for _ in range(6):\n",
        "        feature_vector.append(random.gauss(0, 0.2))\n",
        "\n",
        "    return feature_vector\n",
        "\n",
        "print(\"üì° Generating Communication Protocol Dataset...\")\n",
        "comm_sequences, comm_labels, comm_features = generate_communication_protocol_dataset(n_samples=15000, contamination=0.02)\n",
        "\n",
        "print(f\"\\nüìä Communication Protocol Dataset:\")\n",
        "print(f\"  Alphabet size: 12 symbols (minimal finite alphabet)\")\n",
        "print(f\"  Total samples: {len(comm_sequences)}\")\n",
        "print(f\"  Features: {comm_features.shape[1]}\")\n",
        "print(f\"  Normal sequences: {np.sum(comm_labels == 0)} ({np.sum(comm_labels == 0)/len(comm_labels)*100:.1f}%)\")\n",
        "print(f\"  Steganographic sequences: {np.sum(comm_labels == 1)} ({np.sum(comm_labels == 1)/len(comm_labels)*100:.1f}%)\")\n",
        "print(f\"  Avg sequence length: {np.mean([len(seq) for seq in comm_sequences]):.1f}\")\n",
        "print(f\"  Max sequence length: {max([len(seq) for seq in comm_sequences])}\")\n",
        "\n",
        "# Show examples\n",
        "print(f\"\\nüîç Example sequences:\")\n",
        "normal_idx = np.where(comm_labels == 0)[0][0]\n",
        "anomaly_idx = np.where(comm_labels == 1)[0][0]\n",
        "print(f\"  Normal (first 12): {comm_sequences[normal_idx][:12]}\")\n",
        "print(f\"  Steganographic (first 12): {comm_sequences[anomaly_idx][:12]}\")\n",
        "print(\"‚úÖ Communication protocol dataset generated\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilgQ4fkhbk5K"
      },
      "source": [
        "## Challenge Framework\n",
        "\n",
        "**Same comprehensive evaluation framework**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MgniBfHbk5L"
      },
      "outputs": [],
      "source": [
        "def calculate_comprehensive_metrics(y_true, y_pred, y_scores):\n",
        "    \"\"\"\n",
        "    Calculate comprehensive evaluation metrics.\n",
        "    \"\"\"\n",
        "    metrics = {}\n",
        "\n",
        "    try:\n",
        "        # Basic classification metrics\n",
        "        metrics['accuracy'] = accuracy_score(y_true, y_pred)\n",
        "        metrics['balanced_accuracy'] = balanced_accuracy_score(y_true, y_pred)\n",
        "        metrics['precision'] = precision_score(y_true, y_pred, average='binary', zero_division=0)\n",
        "        metrics['recall'] = recall_score(y_true, y_pred, average='binary', zero_division=0)\n",
        "        metrics['f1_binary'] = f1_score(y_true, y_pred, average='binary', zero_division=0)\n",
        "        metrics['f1_macro'] = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "        metrics['f1_weighted'] = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "        # Advanced metrics\n",
        "        metrics['matthews_corrcoef'] = matthews_corrcoef(y_true, y_pred)\n",
        "        metrics['cohen_kappa'] = cohen_kappa_score(y_true, y_pred)\n",
        "\n",
        "        # Probability-based metrics\n",
        "        if len(np.unique(y_true)) > 1:\n",
        "            metrics['roc_auc'] = roc_auc_score(y_true, y_scores)\n",
        "            metrics['average_precision'] = average_precision_score(y_true, y_scores)\n",
        "\n",
        "            # Precision-Recall curve analysis\n",
        "            precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
        "            f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
        "            metrics['max_f1'] = np.max(f1_scores)\n",
        "            metrics['optimal_threshold'] = thresholds[np.argmax(f1_scores)] if len(thresholds) > 0 else 0.5\n",
        "\n",
        "            # Calibration metrics\n",
        "            try:\n",
        "                metrics['brier_score'] = brier_score_loss(y_true, y_scores)\n",
        "            except:\n",
        "                metrics['brier_score'] = np.nan\n",
        "        else:\n",
        "            metrics['roc_auc'] = 0.5\n",
        "            metrics['average_precision'] = np.mean(y_true)\n",
        "            metrics['max_f1'] = 0.0\n",
        "            metrics['optimal_threshold'] = 0.5\n",
        "            metrics['brier_score'] = np.nan\n",
        "\n",
        "        # Class-specific metrics\n",
        "        tn = np.sum((y_true == 0) & (y_pred == 0))\n",
        "        fp = np.sum((y_true == 0) & (y_pred == 1))\n",
        "        fn = np.sum((y_true == 1) & (y_pred == 0))\n",
        "        tp = np.sum((y_true == 1) & (y_pred == 1))\n",
        "\n",
        "        metrics['true_negative_rate'] = tn / max(1, tn + fp)  # Specificity\n",
        "        metrics['false_positive_rate'] = fp / max(1, tn + fp)\n",
        "        metrics['false_negative_rate'] = fn / max(1, tp + fn)\n",
        "        metrics['positive_predictive_value'] = tp / max(1, tp + fp)  # Precision\n",
        "        metrics['negative_predictive_value'] = tn / max(1, tn + fn)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error calculating metrics: {e}\")\n",
        "        # Return default metrics\n",
        "        for key in ['accuracy', 'balanced_accuracy', 'precision', 'recall', 'f1_binary',\n",
        "                   'f1_macro', 'f1_weighted', 'matthews_corrcoef', 'cohen_kappa', 'roc_auc',\n",
        "                   'average_precision', 'max_f1', 'optimal_threshold', 'brier_score',\n",
        "                   'true_negative_rate', 'false_positive_rate', 'false_negative_rate',\n",
        "                   'positive_predictive_value', 'negative_predictive_value']:\n",
        "            metrics[key] = 0.0\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def run_finite_alphabet_challenge_sklearn(X, y, dataset_name, cv_folds=3):\n",
        "    \"\"\"\n",
        "    Run finite alphabet challenge for scikit-learn with comprehensive metrics.\n",
        "    \"\"\"\n",
        "    print(f\"\\n Scikit-Learn: {dataset_name}\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Cross-validation\n",
        "    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
        "\n",
        "    # Model suite\n",
        "    models = {\n",
        "        'Logistic Regression': LogisticRegression(class_weight='balanced', max_iter=5000, random_state=42),\n",
        "        'Random Forest': RandomForestClassifier(n_estimators=500, class_weight='balanced',\n",
        "                                               max_depth=20, min_samples_split=3, random_state=42),\n",
        "        'Extra Trees': ExtraTreesClassifier(n_estimators=500, class_weight='balanced',\n",
        "                                           max_depth=20, random_state=42),\n",
        "        'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss',\n",
        "                                n_estimators=500, max_depth=10, learning_rate=0.05, random_state=42),\n",
        "        'Neural Network': MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42),\n",
        "        'Naive Bayes': GaussianNB(),\n",
        "        'SGD Classifier': SGDClassifier(class_weight='balanced', random_state=42),\n",
        "        'Isolation Forest': IsolationForest(contamination='auto', random_state=42, n_estimators=300),\n",
        "        'One-Class SVM': OneClassSVM(nu=0.03, gamma='scale'),\n",
        "        'Local Outlier Factor': LocalOutlierFactor(contamination='auto', novelty=True)\n",
        "    }\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for name, model in models.items():\n",
        "        try:\n",
        "            cv_metrics = defaultdict(list)\n",
        "\n",
        "            for fold, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n",
        "                X_train, X_test = X[train_idx], X[test_idx]\n",
        "                y_train, y_test = y[train_idx], y[test_idx]\n",
        "\n",
        "                # Scaling\n",
        "                scaler = RobustScaler()\n",
        "                X_train_scaled = scaler.fit_transform(X_train)\n",
        "                X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "                # Handle unsupervised models\n",
        "                if name in ['Isolation Forest', 'One-Class SVM', 'Local Outlier Factor']:\n",
        "                    # Train on normal data only\n",
        "                    normal_data = X_train_scaled[y_train == 0]\n",
        "                    if len(normal_data) < 10:\n",
        "                        continue\n",
        "\n",
        "                    model.fit(normal_data)\n",
        "\n",
        "                    # Predict\n",
        "                    predictions = model.predict(X_test_scaled)\n",
        "                    y_pred = (predictions == -1).astype(int)\n",
        "\n",
        "                    # Get scores\n",
        "                    if hasattr(model, 'decision_function'):\n",
        "                        y_scores = -model.decision_function(X_test_scaled)\n",
        "                    else:\n",
        "                        y_scores = -model.score_samples(X_test_scaled)\n",
        "                else:\n",
        "                    # Supervised models\n",
        "                    pos_weight = len(y_train[y_train == 0]) / max(1, len(y_train[y_train == 1]))\n",
        "\n",
        "                    if hasattr(model, 'scale_pos_weight'):\n",
        "                        model.set_params(scale_pos_weight=pos_weight)\n",
        "\n",
        "                    model.fit(X_train_scaled, y_train)\n",
        "\n",
        "                    if hasattr(model, 'predict_proba'):\n",
        "                        y_scores = model.predict_proba(X_test_scaled)[:, 1]\n",
        "                    elif hasattr(model, 'decision_function'):\n",
        "                        y_scores = model.decision_function(X_test_scaled)\n",
        "                    else:\n",
        "                        y_scores = model.predict(X_test_scaled).astype(float)\n",
        "\n",
        "                    # Optimize threshold\n",
        "                    if len(np.unique(y_test)) > 1:\n",
        "                        precision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n",
        "                        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
        "                        best_threshold = thresholds[np.argmax(f1_scores)] if len(thresholds) > 0 else 0.5\n",
        "                        y_pred = (y_scores >= best_threshold).astype(int)\n",
        "                    else:\n",
        "                        y_pred = (y_scores >= 0.5).astype(int)\n",
        "\n",
        "                # Calculate metrics\n",
        "                fold_metrics = calculate_comprehensive_metrics(y_test, y_pred, y_scores)\n",
        "\n",
        "                for metric_name, metric_value in fold_metrics.items():\n",
        "                    if not np.isnan(metric_value):\n",
        "                        cv_metrics[metric_name].append(metric_value)\n",
        "\n",
        "            if len(cv_metrics['f1_macro']) > 0:\n",
        "                # Calculate statistics across folds\n",
        "                result = {\n",
        "                    'Dataset': dataset_name,\n",
        "                    'Model': name,\n",
        "                    'Type': 'Traditional ML',\n",
        "                    'CV Folds': len(cv_metrics['f1_macro'])\n",
        "                }\n",
        "\n",
        "                # Add all metrics with mean and std\n",
        "                for metric_name, values in cv_metrics.items():\n",
        "                    if len(values) > 0:\n",
        "                        result[f'{metric_name}_mean'] = np.mean(values)\n",
        "                        result[f'{metric_name}_std'] = np.std(values)\n",
        "                    else:\n",
        "                        result[f'{metric_name}_mean'] = 0.0\n",
        "                        result[f'{metric_name}_std'] = 0.0\n",
        "\n",
        "                results.append(result)\n",
        "\n",
        "                # Display key metrics\n",
        "                f1_mean = result['f1_macro_mean']\n",
        "                f1_std = result['f1_macro_std']\n",
        "                auc_mean = result['roc_auc_mean']\n",
        "                auc_std = result['roc_auc_std']\n",
        "\n",
        "                print(f\"{name:<20} | F1: {f1_mean:.4f}¬±{f1_std:.3f} | AUC: {auc_mean:.4f}¬±{auc_std:.3f}\")\n",
        "            else:\n",
        "                print(f\"{name:<20} | ‚ùå All folds failed\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"{name:<20} | ‚ùå Model error: {str(e)[:40]}...\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def run_finite_alphabet_challenge_sequence(sequences, labels, dataset_name, cv_folds=3):\n",
        "    \"\"\"\n",
        "    Run finite alphabet challenge for sequence models with comprehensive metrics.\n",
        "    \"\"\"\n",
        "    if not ANOMALY_GRID_AVAILABLE:\n",
        "        print(f\"\\n‚ö†Ô∏è Skipping sequence challenge for {dataset_name} (library not available)\")\n",
        "        return []\n",
        "\n",
        "    print(f\"\\n Challenge Sequence Models: {dataset_name}\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Cross-validation\n",
        "    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
        "\n",
        "    results = []\n",
        "    orders_to_test = [1, 2, 3, 4]  # Test higher orders for finite alphabets\n",
        "\n",
        "    for order in orders_to_test:\n",
        "        try:\n",
        "            cv_metrics = defaultdict(list)\n",
        "\n",
        "            for fold, (train_idx, test_idx) in enumerate(cv.split(sequences, labels)):\n",
        "                train_sequences = [sequences[i] for i in train_idx]\n",
        "                test_sequences = [sequences[i] for i in test_idx]\n",
        "                train_labels = labels[train_idx]\n",
        "                test_labels = labels[test_idx]\n",
        "\n",
        "                # Filter normal sequences for training\n",
        "                normal_train_sequences = [seq for seq, label in zip(train_sequences, train_labels) if label == 0]\n",
        "\n",
        "                if len(normal_train_sequences) < 30:  # Need sufficient training data\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    # Create and train detector\n",
        "                    detector = anomaly_grid_py.AnomalyDetector(max_order=order)\n",
        "                    detector.fit(normal_train_sequences)\n",
        "\n",
        "                    # Get anomaly scores\n",
        "                    anomaly_scores = detector.predict_proba(test_sequences)\n",
        "\n",
        "                    # Optimize threshold\n",
        "                    if len(np.unique(test_labels)) > 1:\n",
        "                        precision, recall, thresholds = precision_recall_curve(test_labels, anomaly_scores)\n",
        "                        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
        "                        best_threshold = thresholds[np.argmax(f1_scores)] if len(thresholds) > 0 else 0.5\n",
        "                        y_pred = (anomaly_scores >= best_threshold).astype(int)\n",
        "                    else:\n",
        "                        y_pred = (anomaly_scores >= 0.5).astype(int)\n",
        "\n",
        "                    # Calculate metrics\n",
        "                    fold_metrics = calculate_comprehensive_metrics(test_labels, y_pred, anomaly_scores)\n",
        "\n",
        "                    for metric_name, metric_value in fold_metrics.items():\n",
        "                        if not np.isnan(metric_value):\n",
        "                            cv_metrics[metric_name].append(metric_value)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"    Order {order} Fold {fold} error: {str(e)[:30]}...\")\n",
        "                    continue\n",
        "\n",
        "            if len(cv_metrics['f1_macro']) > 0:\n",
        "                # Calculate statistics across folds\n",
        "                result = {\n",
        "                    'Dataset': dataset_name,\n",
        "                    'Model': f'Anomaly-Grid-Py (order={order})',\n",
        "                    'Type': 'Sequence-Based',\n",
        "                    'CV Folds': len(cv_metrics['f1_macro'])\n",
        "                }\n",
        "\n",
        "                # Add all metrics with mean and std\n",
        "                for metric_name, values in cv_metrics.items():\n",
        "                    if len(values) > 0:\n",
        "                        result[f'{metric_name}_mean'] = np.mean(values)\n",
        "                        result[f'{metric_name}_std'] = np.std(values)\n",
        "                    else:\n",
        "                        result[f'{metric_name}_mean'] = 0.0\n",
        "                        result[f'{metric_name}_std'] = 0.0\n",
        "\n",
        "                results.append(result)\n",
        "\n",
        "                # Display key metrics\n",
        "                f1_mean = result['f1_macro_mean']\n",
        "                f1_std = result['f1_macro_std']\n",
        "                auc_mean = result['roc_auc_mean']\n",
        "                auc_std = result['roc_auc_std']\n",
        "\n",
        "                print(f\"Order {order:<15} | F1: {f1_mean:.4f}¬±{f1_std:.3f} | AUC: {auc_mean:.4f}¬±{auc_std:.3f}\")\n",
        "            else:\n",
        "                print(f\"Order {order:<15} | ‚ùå All folds failed\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Order {order:<15} | ‚ùå Order error: {str(e)[:30]}...\")\n",
        "\n",
        "    return results\n",
        "\n",
        "print(\"Framework ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzzJtPVLbk5N"
      },
      "source": [
        "## Execute Benchmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBMwbPNYbk5N"
      },
      "outputs": [],
      "source": [
        "print(\"EXECUTING\")\n",
        "print(\"=\" * 80)\n",
        "print(\"Testing on datasets with finite alphabets (‚â§20 symbols)\")\n",
        "print(\"Complex dependencies ‚Ä¢ Temporal patterns\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "all_finite_results = []\n",
        "\n",
        "# Challenge datasets\n",
        "finite_datasets = [\n",
        "    {\n",
        "        'name': 'Protocol State Machine',\n",
        "        'sequences': protocol_sequences,\n",
        "        'labels': protocol_labels,\n",
        "        'features': protocol_features,\n",
        "        'description': 'Complex state transition patterns with subtle violations',\n",
        "        'alphabet_size': 16,\n",
        "        'expected_max_f1': 0.65\n",
        "    },\n",
        "    #{\n",
        "    #    'name': 'Protein Folding',\n",
        "    #    'sequences': protein_sequences,\n",
        "    #    'labels': protein_labels,\n",
        "    #    'features': protein_features,\n",
        "    #    'description': 'Biological sequence patterns with rare misfolding',\n",
        "    #    'alphabet_size': 20,\n",
        "    #    'expected_max_f1': 0.60\n",
        "    #},\n",
        "    {\n",
        "        'name': 'Communication Protocol',\n",
        "        'sequences': comm_sequences,\n",
        "        'labels': comm_labels,\n",
        "        'features': comm_features,\n",
        "        'description': 'Steganographic timing attacks in communication',\n",
        "        'alphabet_size': 12,\n",
        "        'expected_max_f1': 0.55\n",
        "    }\n",
        "]\n",
        "\n",
        "# Execute finite alphabet challenges\n",
        "for dataset in finite_datasets:\n",
        "    print(f\"\\n CHALLENGE: {dataset['name'].upper()}\")\n",
        "    print(f\"Alphabet size: {dataset['alphabet_size']} symbols (finite, well-defined)\")\n",
        "    print(f\"Description: {dataset['description']}\")\n",
        "    print(f\"Expected max F1: < {dataset['expected_max_f1']}\")\n",
        "    print(f\"Samples: {len(dataset['sequences'])}, Features: {dataset['features'].shape[1]}\")\n",
        "    print(f\"Contamination: {np.mean(dataset['labels']):.1%}\")\n",
        "    print(f\"Avg sequence length: {np.mean([len(seq) for seq in dataset['sequences']]):.1f}\")\n",
        "    print(f\"Max sequence length: {max([len(seq) for seq in dataset['sequences']])}\")\n",
        "\n",
        "    # Challenge traditional ML (should struggle with temporal patterns)\n",
        "    sklearn_results = run_finite_alphabet_challenge_sklearn(\n",
        "        dataset['features'], dataset['labels'], dataset['name'], cv_folds=3\n",
        "    )\n",
        "    all_finite_results.extend(sklearn_results)\n",
        "\n",
        "    # Challenge sequence models (should have advantage but still struggle)\n",
        "    sequence_results = run_finite_alphabet_challenge_sequence(\n",
        "        dataset['sequences'], dataset['labels'], dataset['name'], cv_folds=3\n",
        "    )\n",
        "    all_finite_results.extend(sequence_results)\n",
        "\n",
        "    print(f\"\\n‚úÖ {dataset['name']} challenge completed\")\n",
        "\n",
        "print(f\"\\n ALL CHALLENGES COMPLETED!\")\n",
        "print(f\"Total experiments: {len(all_finite_results)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ptolydrbk5O"
      },
      "source": [
        "## Challenge Results Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jc3fF_phbk5O"
      },
      "outputs": [],
      "source": [
        "if len(all_finite_results) > 0:\n",
        "    finite_df = pd.DataFrame(all_finite_results)\n",
        "\n",
        "    print(\"CHALLENGE RESULTS\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Check if challenge succeeded and sequence advantage is clear\n",
        "    max_f1 = finite_df['f1_macro_mean'].max()\n",
        "    avg_f1 = finite_df['f1_macro_mean'].mean()\n",
        "\n",
        "    traditional_scores = finite_df[finite_df['Type'] == 'Traditional ML']['f1_macro_mean']\n",
        "    sequence_scores = finite_df[finite_df['Type'] == 'Sequence-Based']['f1_macro_mean']\n",
        "\n",
        "    print(f\"üìä FINITE ALPHABET CHALLENGE ASSESSMENT:\")\n",
        "    print(f\"  Maximum F1 achieved: {max_f1:.4f}\")\n",
        "    print(f\"  Average F1 across all models: {avg_f1:.4f}\")\n",
        "\n",
        "    if max_f1 < 0.70:\n",
        "        print(f\"  ‚úÖ CHALLENGE SUCCEEDED - Max F1 < 0.80\")\n",
        "        if max_f1 < 0.60:\n",
        "            print(f\"  üíÄ IMPOSSIBLY DIFFICULT - Max F1 < 0.60\")\n",
        "        elif max_f1 < 0.65:\n",
        "            print(f\"  üî• EXTREMELY DIFFICULT - Max F1 < 0.65\")\n",
        "    else:\n",
        "        print(f\"  ‚ö†Ô∏è  CHALLENGE FAILED - Max F1 >= 0.70, need more difficulty\")\n",
        "\n",
        "\n",
        "    print(\"\\n CHALLENGE WINNERS:\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    # Show top performers with alphabet size context\n",
        "    alphabet_sizes = {'Protocol State Machine': 16, 'Protein Folding': 20, 'Communication Protocol': 12}\n",
        "\n",
        "    for dataset_name in finite_df['Dataset'].unique():\n",
        "        dataset_results = finite_df[finite_df['Dataset'] == dataset_name]\n",
        "        best_result = dataset_results.loc[dataset_results['f1_macro_mean'].idxmax()]\n",
        "\n",
        "        emoji = \"üöÄ\" if best_result['Type'] == 'Sequence-Based' else \"ü§ñ\"\n",
        "        f1_str = f\"{best_result['f1_macro_mean']:.4f}¬±{best_result['f1_macro_std']:.3f}\"\n",
        "        alphabet_size = alphabet_sizes.get(dataset_name, '?')\n",
        "\n",
        "        print(f\"{dataset_name:<25} | Alphabet: {alphabet_size:2d} | {emoji} {best_result['Model']:<30} | F1: {f1_str}\")\n",
        "\n",
        "    print(\"\\nüíÄ anomaly-grid-py VS scikit-learn BY DATASET:\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    sequence_wins = 0\n",
        "    traditional_wins = 0\n",
        "\n",
        "    for dataset_name in finite_df['Dataset'].unique():\n",
        "        dataset_results = finite_df[finite_df['Dataset'] == dataset_name]\n",
        "\n",
        "        traditional_best = dataset_results[dataset_results['Type'] == 'Traditional ML']['f1_macro_mean'].max()\n",
        "        sequence_best = dataset_results[dataset_results['Type'] == 'Sequence-Based']['f1_macro_mean'].max()\n",
        "\n",
        "        if not pd.isna(traditional_best) and not pd.isna(sequence_best):\n",
        "            improvement = ((sequence_best - traditional_best) / traditional_best) * 100\n",
        "            alphabet_size = alphabet_sizes.get(dataset_name, '?')\n",
        "\n",
        "            if sequence_best > traditional_best:\n",
        "                winner = \"üöÄ Sequence\"\n",
        "                sequence_wins += 1\n",
        "            else:\n",
        "                winner = \"ü§ñ Traditional\"\n",
        "                traditional_wins += 1\n",
        "\n",
        "            print(f\"{dataset_name:<25} | Alphabet: {alphabet_size:2d} | {winner:<15} | Improvement: {improvement:+6.2f}%\")\n",
        "            print(f\"{'':25} | Traditional: {traditional_best:.4f} | Sequence: {sequence_best:.4f}\")\n",
        "        else:\n",
        "            print(f\"{dataset_name:<25} | Incomplete comparison\")\n",
        "\n",
        "    total_battles = sequence_wins + traditional_wins\n",
        "    if total_battles > 0:\n",
        "        print(f\"\\nüèÜ RESULTS:\")\n",
        "        print(f\"  üöÄ Sequence-Based wins: {sequence_wins}/{total_battles} ({sequence_wins/total_battles:.1%})\")\n",
        "        print(f\"  ü§ñ Traditional ML wins: {traditional_wins}/{total_battles} ({traditional_wins/total_battles:.1%})\")\n",
        "\n",
        "        if sequence_wins > traditional_wins:\n",
        "            print(f\"  ‚úÖ SEQUENCE MODELING DOMINATES on finite alphabet datasets\")\n",
        "        elif sequence_wins == traditional_wins:\n",
        "            print(f\"  ‚öñÔ∏è BALANCED PERFORMANCE between approaches\")\n",
        "        else:\n",
        "            print(f\"  ‚ö†Ô∏è TRADITIONAL ML UNEXPECTEDLY STRONG - May need more sequence-favoring design\")\n",
        "\n",
        "    print(\"\\n ANALYSIS:\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    for dataset_name in finite_df['Dataset'].unique():\n",
        "        print(f\"\\nüíÄ {dataset_name.upper()} (Alphabet: {alphabet_sizes.get(dataset_name, '?')} symbols):\")\n",
        "        dataset_results = finite_df[finite_df['Dataset'] == dataset_name].sort_values('f1_macro_mean', ascending=False)\n",
        "\n",
        "        for i, (_, row) in enumerate(dataset_results.head(6).iterrows()):\n",
        "            emoji = \"üöÄ\" if row['Type'] == 'Sequence-Based' else \"ü§ñ\"\n",
        "            f1_str = f\"{row['f1_macro_mean']:.4f}¬±{row['f1_macro_std']:.3f}\"\n",
        "            auc_str = f\"{row['roc_auc_mean']:.4f}¬±{row['roc_auc_std']:.3f}\"\n",
        "\n",
        "            print(f\"  {i+1}. {emoji} {row['Model']:<30} | F1: {f1_str} | AUC: {auc_str}\")\n",
        "\n",
        "    print(\"\\nüìä ALPHABET SIZE IMPACT ANALYSIS:\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Analyze performance by alphabet size\n",
        "    for dataset_name in finite_df['Dataset'].unique():\n",
        "        dataset_results = finite_df[finite_df['Dataset'] == dataset_name]\n",
        "        alphabet_size = alphabet_sizes.get(dataset_name, 0)\n",
        "\n",
        "        traditional_avg = dataset_results[dataset_results['Type'] == 'Traditional ML']['f1_macro_mean'].mean()\n",
        "        sequence_avg = dataset_results[dataset_results['Type'] == 'Sequence-Based']['f1_macro_mean'].mean()\n",
        "\n",
        "        if not pd.isna(traditional_avg) and not pd.isna(sequence_avg):\n",
        "            advantage = sequence_avg - traditional_avg\n",
        "            print(f\"Alphabet {alphabet_size:2d}: Traditional={traditional_avg:.3f}, Sequence={sequence_avg:.3f}, Advantage={advantage:+.3f}\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå No finite alphabet challenge results to analyze\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6iDDCbXbk5O"
      },
      "source": [
        "## üíæ Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOD7RQlWbk5O"
      },
      "outputs": [],
      "source": [
        "if len(all_finite_results) > 0:\n",
        "    # Save results\n",
        "    finite_df.to_csv('challenge_results.csv', index=False)\n",
        "    print(\"üíæ Challenge results saved to 'challenge_results.csv'\")\n",
        "\n",
        "    # Create challenge report\n",
        "    max_f1 = finite_df['f1_macro_mean'].max()\n",
        "    avg_f1 = finite_df['f1_macro_mean'].mean()\n",
        "\n",
        "    traditional_scores = finite_df[finite_df['Type'] == 'Traditional ML']['f1_macro_mean']\n",
        "    sequence_scores = finite_df[finite_df['Type'] == 'Sequence-Based']['f1_macro_mean']\n",
        "    sequence_advantage = sequence_scores.mean() - traditional_scores.mean() if len(traditional_scores) > 0 and len(sequence_scores) > 0 else 0\n",
        "\n",
        "    finite_report = f\"\"\"\n",
        "# Challenge Report\n",
        "\n",
        "## Challenge Success Assessment\n",
        "\n",
        "- **Maximum F1 achieved**: {max_f1:.4f}\n",
        "- **Average F1 across all models**: {avg_f1:.4f}\n",
        "- **Challenge Status**: {'‚úÖ SUCCEEDED' if max_f1 < 0.70 else '‚ùå FAILED'} (Target: max F1 < 0.70)\n",
        "- **Difficulty Level**: {'üíÄ IMPOSSIBLE' if max_f1 < 0.60 else 'üî• EXTREME' if max_f1 < 0.65 else '‚öîÔ∏è VERY HARD'}\n",
        "\n",
        "## Challenge Datasets\n",
        "\n",
        "### üîê Protocol State Machine (16 States)\n",
        "- **Samples**: {len(protocol_sequences)} protocol sequences\n",
        "- **Contamination**: {np.mean(protocol_labels):.1%}\n",
        "- **Challenge**: Complex multi-order state transition patterns with subtle violations\n",
        "- **Avg Length**: {np.mean([len(seq) for seq in protocol_sequences]):.1f} states\n",
        "- **Max Length**: {max([len(seq) for seq in protocol_sequences])} states\n",
        "\n",
        "### üß¨ Protein Folding (20 Amino Acids)\n",
        "- **Samples**: {len(protein_sequences)} protein sequences\n",
        "- **Contamination**: {np.mean(protein_labels):.1%}\n",
        "- **Challenge**: Biological sequence patterns with rare misfolding events\n",
        "- **Avg Length**: {np.mean([len(seq) for seq in protein_sequences]):.1f} amino acids\n",
        "- **Max Length**: {max([len(seq) for seq in protein_sequences])} amino acids\n",
        "\n",
        "### üì° Communication Protocol (12 Symbols)\n",
        "- **Samples**: {len(comm_sequences)} communication sequences\n",
        "- **Contamination**: {np.mean(comm_labels):.1%}\n",
        "- **Challenge**: Steganographic timing attacks in digital communication\n",
        "- **Avg Length**: {np.mean([len(seq) for seq in comm_sequences]):.1f} symbols\n",
        "- **Max Length**: {max([len(seq) for seq in comm_sequences])} symbols\n",
        "\n",
        "## Design Principles\n",
        "\n",
        "1. **üî§ SMALL VOCABULARIES**: 12-20 symbols maximum (finite, well-defined)\n",
        "2. **üîó COMPLEX DEPENDENCIES**: Multi-order sequence patterns (orders 1-4)\n",
        "3. **‚è∞ TEMPORAL PATTERNS**: State transitions, biological constraints, timing protocols\n",
        "\n",
        "## Challenge Results by Dataset\n",
        "\n",
        "### Winners by Alphabet Size\n",
        "\"\"\"\n",
        "\n",
        "    # Add winners by dataset\n",
        "    alphabet_sizes = {'Protocol State Machine': 16, 'Protein Folding': 20, 'Communication Protocol': 12}\n",
        "\n",
        "    for dataset_name in finite_df['Dataset'].unique():\n",
        "        dataset_results = finite_df[finite_df['Dataset'] == dataset_name]\n",
        "        best_result = dataset_results.loc[dataset_results['f1_macro_mean'].idxmax()]\n",
        "        alphabet_size = alphabet_sizes.get(dataset_name, '?')\n",
        "\n",
        "        finite_report += f\"\"\"\n",
        "**{dataset_name}** (Alphabet: {alphabet_size} symbols)\n",
        "- Winner: {best_result['Model']} ({best_result['Type']})\n",
        "- Macro F1: {best_result['f1_macro_mean']:.4f} ¬± {best_result['f1_macro_std']:.3f}\n",
        "- ROC AUC: {best_result['roc_auc_mean']:.4f} ¬± {best_result['roc_auc_std']:.3f}\n",
        "\"\"\"\n",
        "\n",
        "    # Add battle results\n",
        "    sequence_wins = 0\n",
        "    traditional_wins = 0\n",
        "\n",
        "    finite_report += f\"\"\"\n",
        "\n",
        "### Sequence vs Traditional ML Battle Results\n",
        "\n",
        "| Dataset | Alphabet Size | Winner | Traditional F1 | Sequence F1 | Improvement |\n",
        "|---------|---------------|--------|----------------|-------------|-------------|\n",
        "\"\"\"\n",
        "\n",
        "    for dataset_name in finite_df['Dataset'].unique():\n",
        "        dataset_results = finite_df[finite_df['Dataset'] == dataset_name]\n",
        "\n",
        "        traditional_best = dataset_results[dataset_results['Type'] == 'Traditional ML']['f1_macro_mean'].max()\n",
        "        sequence_best = dataset_results[dataset_results['Type'] == 'Sequence-Based']['f1_macro_mean'].max()\n",
        "\n",
        "        if not pd.isna(traditional_best) and not pd.isna(sequence_best):\n",
        "            improvement = ((sequence_best - traditional_best) / traditional_best) * 100\n",
        "            alphabet_size = alphabet_sizes.get(dataset_name, '?')\n",
        "\n",
        "            if sequence_best > traditional_best:\n",
        "                winner = \"üöÄ Sequence\"\n",
        "                sequence_wins += 1\n",
        "            else:\n",
        "                winner = \"ü§ñ Traditional\"\n",
        "                traditional_wins += 1\n",
        "\n",
        "            finite_report += f\"| {dataset_name} | {alphabet_size} | {winner} | {traditional_best:.4f} | {sequence_best:.4f} | {improvement:+.2f}% |\\n\"\n",
        "\n",
        "    # Add comprehensive analysis\n",
        "    finite_report += f\"\"\"\n",
        "\n",
        "### Overall Battle Results\n",
        "\n",
        "- **üöÄ Sequence-Based wins**: {sequence_wins}/{sequence_wins + traditional_wins} ({sequence_wins/(sequence_wins + traditional_wins):.1%})\n",
        "- **ü§ñ Traditional ML wins**: {traditional_wins}/{sequence_wins + traditional_wins} ({traditional_wins/(sequence_wins + traditional_wins):.1%})\n",
        "- **üìä Sequence dominance**: {'‚úÖ CLEAR' if sequence_wins > traditional_wins else '‚öñÔ∏è BALANCED' if sequence_wins == traditional_wins else '‚ö†Ô∏è UNEXPECTED'}\n",
        "\n",
        "### Statistical Analysis\n",
        "\n",
        "- **Total experiments**: {len(all_finite_results)} (with 3-fold CV)\n",
        "- **Performance ranges**:\n",
        "  - Traditional ML: {traditional_scores.min():.4f} - {traditional_scores.max():.4f}\n",
        "  - Sequence-based: {sequence_scores.min():.4f} - {sequence_scores.max():.4f}\n",
        "\n",
        "### Alphabet Size Impact\n",
        "\"\"\"\n",
        "\n",
        "    # Add alphabet size analysis\n",
        "    for dataset_name in finite_df['Dataset'].unique():\n",
        "        dataset_results = finite_df[finite_df['Dataset'] == dataset_name]\n",
        "        alphabet_size = alphabet_sizes.get(dataset_name, 0)\n",
        "\n",
        "        traditional_avg = dataset_results[dataset_results['Type'] == 'Traditional ML']['f1_macro_mean'].mean()\n",
        "        sequence_avg = dataset_results[dataset_results['Type'] == 'Sequence-Based']['f1_macro_mean'].mean()\n",
        "\n",
        "        if not pd.isna(traditional_avg) and not pd.isna(sequence_avg):\n",
        "            advantage = sequence_avg - traditional_avg\n",
        "            finite_report += f\"\\n- **Alphabet {alphabet_size}**: Traditional={traditional_avg:.3f}, Sequence={sequence_avg:.3f}, Advantage={advantage:+.3f}\"\n",
        "\n",
        "    finite_report += f\"\"\"\n",
        "\n",
        "## Key Findings\n",
        "\n",
        "1. **‚è∞ TEMPORAL PATTERNS MATTER**: State transitions and timing require sequence understanding\n",
        "\n",
        "---\n",
        "*Challenge completed on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n",
        "\"\"\"\n",
        "\n",
        "    # Save finite alphabet report\n",
        "    with open('challenge_report.md', 'w') as f:\n",
        "        f.write(finite_report)\n",
        "\n",
        "    print(\"üìÑ Challenge report saved to 'challenge_report.md'\")\n",
        "\n",
        "    # Display final summary\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"CHALLENGE COMPLETED!\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"üìä Total experiments: {len(all_finite_results)}\")\n",
        "    print(f\"üíæ Results: Saved to CSV and markdown files\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå No finite alphabet challenge results to save\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
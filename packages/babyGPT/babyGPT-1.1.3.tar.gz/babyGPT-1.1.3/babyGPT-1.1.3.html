 <!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd"> 
<html lang="en">
<head>
<title>
babyGPT-1.1.3.html
</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<STYLE type="text/css">
    /* For animating the color for the twitter link */
    .pcolor {
        animation-name: color-change;
        animation-duration:  5s;  
        animation-iteration-count: infinite;
    }
    @keyframes color-change {
        0% { color: red; }                     /* Start color */
        50% { color: blue; }                   /* Midpoint color */
        100% { color: green; }                 /* End color */
    }
</STYLE>

</head>
<body bgcolor="#f0f0f8">
<table width="100%" cellspacing="0" cellpadding="2" border="0" summary="heading">
<tr bgcolor="#7799ee">
<td valign=bottom>&nbsp;<br>
<font color="#ffffff" face="helvetica, arial">&nbsp;<br><big><big><strong>babyGPT</strong></big></big> (version 1.1.3, 2025-September-16)</font></td
><td align=right valign=bottom
><font color="#ffffff" face="helvetica, arial">
</font></td></tr></table>
<p><a href="#babyGPT"><tt>babyGPT</tt></a>.py<br>
<tt>
&nbsp;<br>
Version:&nbsp;1.1.3<br>
&nbsp;&nbsp;&nbsp;<br>
Author:&nbsp;Avinash&nbsp;Kak&nbsp;(kak@purdue.edu)<br>
&nbsp;<br>
Date:&nbsp;2025-September-16<br>
&nbsp;<br>
&nbsp;<br>
</tt>
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="2"> 
<TR>
<TH ALIGN=left>
<tt>
<b>Download Version 1.1.3:</b>&nbsp;  
<a HREF="https://engineering.purdue.edu/kak/distBabyGPT/babyGPT-1.1.3.tar.gz?download">gztar</a> 
&nbsp;             
<br>
<br>
&nbsp;
</tt>
</TH>
<TD>
<tt>
&nbsp;&nbsp;&nbsp;&nbsp;
Total number of downloads (all versions): 
<?php   
    $file = fopen("HowManyCounts.txt", "r") or exit("Unable to open file!");
    echo fgets($file);
    fclose($file);
?>
</tt>
<br>
<center>
<tt>
<font color="red" size="-2">
&nbsp;&nbsp;&nbsp;&nbsp;
This count is automatically updated at every rotation of
<br> 
&nbsp;&nbsp;&nbsp;&nbsp;
the weblogs (normally once every two to four days)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;
Last updated:
<?php   
    $file = fopen("LastUpdated.txt", "r") or exit("Unable to open file!");
    echo fgets($file);
    fclose($file);
?>
</font>
</tt>
</center>
</TD>
</TR>
</TABLE>
<br>
<tt>
<a HREF="babyGPT-1.1.3_CodeOnly.html">View the main module code file in your browser</a> 
&nbsp;<br>
&nbsp;<br>
<a HREF="datasets_for_babyGPT.tar.gz">Download the text datasets for babyGPT</a>
&nbsp;<br>
&nbsp;<br>
&nbsp;<br>   
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://X.com/AvinashKak1"><strong><span class="pcolor">[See my Twitter posts for examples of the latest results with babyGPT]</span></strong></a>
&nbsp;<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;<br>

<font size="+2" color="red">THE QUICKEST WAY TO START USING babyGPT:<br>
</font>
<br>


&nbsp;&nbsp;&nbsp;&nbsp;1.&nbsp;&nbsp;Download&nbsp;the&nbsp;module&nbsp;code&nbsp;archive&nbsp;by&nbsp;clicking&nbsp;on&nbsp;the&nbsp;"gztar"&nbsp;link&nbsp;shown&nbsp;above.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Unpack&nbsp;and&nbsp;install&nbsp;the&nbsp;archive&nbsp;by&nbsp;following&nbsp;the&nbsp;instruction&nbsp;in&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;INSTALLATION&nbsp;section&nbsp;of&nbsp;this&nbsp;documentation&nbsp;page.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;2.&nbsp;&nbsp;Next,&nbsp;download&nbsp;the&nbsp;training&nbsp;dataset&nbsp;by&nbsp;clicking&nbsp;on&nbsp;the&nbsp;link&nbsp;"Download&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;text&nbsp;datasets&nbsp;for&nbsp;babyGPT"&nbsp;shown&nbsp;above.&nbsp;&nbsp;See&nbsp;the&nbsp;Section&nbsp;"Training&nbsp;Datasets<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Provided"&nbsp;for&nbsp;further&nbsp;details.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;3.&nbsp;&nbsp;Now&nbsp;enter&nbsp;the&nbsp;Example&nbsp;subdirectory&nbsp;of&nbsp;the&nbsp;distribution&nbsp;and&nbsp;enter&nbsp;the&nbsp;pathname<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;the&nbsp;training&nbsp;data&nbsp;in&nbsp;the&nbsp;script<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;create_base_model_with_buffered_context.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;pathname&nbsp;is&nbsp;the&nbsp;value&nbsp;of&nbsp;the&nbsp;variable&nbsp;'articles_dir'&nbsp;near&nbsp;the&nbsp;beginning&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;script.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;4.&nbsp;&nbsp;Finally,&nbsp;execute&nbsp;the&nbsp;script&nbsp;named&nbsp;above.&nbsp;&nbsp;That's&nbsp;all!<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;<br>
<font size="+2" color="red">CHANGE&nbsp;LOG:<br>
</font>&nbsp;<br>
Version&nbsp;1.1.3:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;version&nbsp;allows&nbsp;you&nbsp;to&nbsp;use&nbsp;gradient&nbsp;accumulation&nbsp;to&nbsp;experiment&nbsp;with&nbsp;longer<br>
&nbsp;&nbsp;&nbsp;&nbsp;sequence&nbsp;lengths&nbsp;for&nbsp;transformer-based&nbsp;learning.&nbsp;&nbsp;If&nbsp;you&nbsp;don't&nbsp;wish&nbsp;to&nbsp;use&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;gradient&nbsp;accumulation&nbsp;feature,&nbsp;just&nbsp;set&nbsp;gradient_accumulation_steps&nbsp;to&nbsp;zero&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;call&nbsp;to&nbsp;the&nbsp;constructor&nbsp;for&nbsp;the&nbsp;MasterDecoderWithMasking&nbsp;class.&nbsp;&nbsp;Another<br>
&nbsp;&nbsp;&nbsp;&nbsp;improvement&nbsp;in&nbsp;this&nbsp;version&nbsp;is&nbsp;better&nbsp;documentation&nbsp;regarding&nbsp;why&nbsp;I&nbsp;designed<br>
&nbsp;&nbsp;&nbsp;&nbsp;babyGPT&nbsp;to&nbsp;be&nbsp;trained&nbsp;with&nbsp;streaming&nbsp;datasets.&nbsp;&nbsp;You&nbsp;will&nbsp;find&nbsp;this&nbsp;explanation&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;doc-string&nbsp;associated&nbsp;with&nbsp;the&nbsp;class&nbsp;TokenStreamDataset&nbsp;that&nbsp;is&nbsp;derived&nbsp;from<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;class&nbsp;torch.utils.data.IterableDataset.<br>
&nbsp;<br>
Version&nbsp;1.1.2:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;There&nbsp;was&nbsp;an&nbsp;error&nbsp;in&nbsp;the&nbsp;code&nbsp;that&nbsp;creates&nbsp;the&nbsp;vocab&nbsp;dictionary&nbsp;for&nbsp;the&nbsp;cleaned<br>
&nbsp;&nbsp;&nbsp;&nbsp;up&nbsp;tokens.&nbsp;I&nbsp;have&nbsp;fixed&nbsp;the&nbsp;error&nbsp;in&nbsp;this&nbsp;version&nbsp;and&nbsp;also&nbsp;provided&nbsp;new<br>
&nbsp;&nbsp;&nbsp;&nbsp;tokenizers&nbsp;trained&nbsp;on&nbsp;the&nbsp;athlete-news&nbsp;dataset.&nbsp;These&nbsp;are&nbsp;the&nbsp;base&nbsp;tokenizer&nbsp;with<br>
&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;vocab&nbsp;of&nbsp;size&nbsp;50002&nbsp;tokens&nbsp;and&nbsp;its&nbsp;cleaned-up&nbsp;version&nbsp;whose&nbsp;vocab&nbsp;size&nbsp;is<br>
&nbsp;&nbsp;&nbsp;&nbsp;35035.&nbsp;The&nbsp;base&nbsp;tokenizer&nbsp;was&nbsp;trained&nbsp;with&nbsp;the&nbsp;target&nbsp;vocab&nbsp;size&nbsp;set&nbsp;to&nbsp;50000.<br>
&nbsp;&nbsp;&nbsp;&nbsp;Both&nbsp;tokenizers&nbsp;are&nbsp;in&nbsp;the&nbsp;Examples&nbsp;directory&nbsp;of&nbsp;the&nbsp;distribution.<br>
&nbsp;<br>
Version&nbsp;1.1.1:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;I&nbsp;have&nbsp;made&nbsp;further&nbsp;enhancements&nbsp;to&nbsp;the&nbsp;tokenizer&nbsp;training&nbsp;code&nbsp;in&nbsp;this&nbsp;version<br>
&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;order&nbsp;to&nbsp;discard&nbsp;superfluous&nbsp;tokens,&nbsp;these&nbsp;being&nbsp;tokens&nbsp;that&nbsp;contribute&nbsp;almost<br>
&nbsp;&nbsp;&nbsp;&nbsp;nothing&nbsp;to&nbsp;the&nbsp;downstream&nbsp;learning&nbsp;tasks.&nbsp;&nbsp;For&nbsp;example,&nbsp;if&nbsp;the&nbsp;tokenizer&nbsp;training<br>
&nbsp;&nbsp;&nbsp;&nbsp;has&nbsp;learned&nbsp;two&nbsp;tokens&nbsp;'abc'&nbsp;and&nbsp;'abcd'&nbsp;and&nbsp;both&nbsp;predict&nbsp;exactly&nbsp;the&nbsp;same&nbsp;set&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;corpus&nbsp;words,&nbsp;you&nbsp;can&nbsp;discard&nbsp;one&nbsp;of&nbsp;the&nbsp;two&nbsp;without&nbsp;affecting&nbsp;the&nbsp;overall<br>
&nbsp;&nbsp;&nbsp;&nbsp;expressive&nbsp;power&nbsp;of&nbsp;all&nbsp;the&nbsp;learned&nbsp;tokens.&nbsp;Additionally,&nbsp;since&nbsp;unsupervised<br>
&nbsp;&nbsp;&nbsp;&nbsp;learning&nbsp;requires&nbsp;estimating&nbsp;the&nbsp;maximum-likelihood&nbsp;probabilities&nbsp;for&nbsp;the&nbsp;next<br>
&nbsp;&nbsp;&nbsp;&nbsp;token&nbsp;over&nbsp;all&nbsp;the&nbsp;possibilities&nbsp;at&nbsp;that&nbsp;position,&nbsp;I&nbsp;believe&nbsp;that&nbsp;getting&nbsp;rid&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;superfluous&nbsp;tokens&nbsp;can&nbsp;only&nbsp;reduce&nbsp;the&nbsp;noise&nbsp;in&nbsp;the&nbsp;estimation&nbsp;process.&nbsp;I&nbsp;refer<br>
&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;getting&nbsp;rid&nbsp;of&nbsp;such&nbsp;tokens&nbsp;as&nbsp;"cleaning&nbsp;up&nbsp;of&nbsp;the&nbsp;tokenizer&nbsp;vocabulary".&nbsp;&nbsp;In<br>
&nbsp;&nbsp;&nbsp;&nbsp;this&nbsp;module,&nbsp;the&nbsp;token&nbsp;clean-up&nbsp;logic&nbsp;is&nbsp;implemented&nbsp;in&nbsp;a&nbsp;new&nbsp;function&nbsp;called<br>
&nbsp;&nbsp;&nbsp;&nbsp;"post_training_cleanup()"&nbsp;that&nbsp;is&nbsp;defined&nbsp;for&nbsp;the&nbsp;inner&nbsp;class&nbsp;TrainTokenizer&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;babyGPT.&nbsp;&nbsp;As&nbsp;shown&nbsp;by&nbsp;the&nbsp;example&nbsp;script<br>
&nbsp;&nbsp;&nbsp;&nbsp;create_base_model_with_buffered_context.py&nbsp;in&nbsp;the&nbsp;Examples&nbsp;directory,&nbsp;I&nbsp;call&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;"post_training_cleanup()"&nbsp;function&nbsp;after&nbsp;I&nbsp;have&nbsp;trained&nbsp;a&nbsp;tokenizer&nbsp;with&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;"train_tokenizer()"&nbsp;function.<br>
&nbsp;<br>
Version&nbsp;1.1.0:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;There&nbsp;was&nbsp;a&nbsp;module&nbsp;packaging&nbsp;error&nbsp;with&nbsp;1.0.9.&nbsp;&nbsp;I&nbsp;have&nbsp;fixed&nbsp;the&nbsp;problem&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;1.1.0.<br>
&nbsp;<br>
Version&nbsp;1.0.9:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;version&nbsp;applies&nbsp;filtering&nbsp;to&nbsp;the&nbsp;training&nbsp;corpus&nbsp;for&nbsp;improving&nbsp;both&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;tokenizer&nbsp;and&nbsp;the&nbsp;working&nbsp;of&nbsp;the&nbsp;transformer&nbsp;network&nbsp;for&nbsp;next&nbsp;token&nbsp;prediction.<br>
&nbsp;&nbsp;&nbsp;&nbsp;Text&nbsp;files&nbsp;downloaded&nbsp;from&nbsp;the&nbsp;internet&nbsp;---&nbsp;and&nbsp;especially&nbsp;the&nbsp;news&nbsp;media<br>
&nbsp;&nbsp;&nbsp;&nbsp;articles&nbsp;---&nbsp;include&nbsp;long&nbsp;URL&nbsp;strings&nbsp;that&nbsp;should&nbsp;play&nbsp;no&nbsp;role&nbsp;in&nbsp;the&nbsp;training&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;tokenizer&nbsp;or&nbsp;the&nbsp;learning&nbsp;required&nbsp;by&nbsp;the&nbsp;transformer&nbsp;network.&nbsp;(It&nbsp;is&nbsp;not<br>
&nbsp;&nbsp;&nbsp;&nbsp;uncommon&nbsp;for&nbsp;the&nbsp;URL&nbsp;strings&nbsp;to&nbsp;consist&nbsp;of&nbsp;hundreds&nbsp;of&nbsp;characters.)&nbsp;&nbsp;Version<br>
&nbsp;&nbsp;&nbsp;&nbsp;1.0.9&nbsp;accepts&nbsp;a&nbsp;string&nbsp;for&nbsp;downstream&nbsp;processing&nbsp;only&nbsp;if&nbsp;it&nbsp;is&nbsp;shorter&nbsp;than&nbsp;20<br>
&nbsp;&nbsp;&nbsp;&nbsp;characters.&nbsp;I&nbsp;chose&nbsp;20&nbsp;because&nbsp;(according&nbsp;to&nbsp;Google)&nbsp;the&nbsp;average&nbsp;word&nbsp;length&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;English&nbsp;is&nbsp;only&nbsp;5&nbsp;and&nbsp;"virtually&nbsp;all,&nbsp;very&nbsp;close&nbsp;to&nbsp;100%,&nbsp;of&nbsp;English&nbsp;words&nbsp;have<br>
&nbsp;&nbsp;&nbsp;&nbsp;fewer&nbsp;than&nbsp;20&nbsp;letters."&nbsp;Words&nbsp;that&nbsp;are&nbsp;longer&nbsp;than&nbsp;20&nbsp;characters&nbsp;tend&nbsp;to&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;mostly&nbsp;scientific&nbsp;or&nbsp;technical&nbsp;jargon.<br>
&nbsp;<br>
Version&nbsp;1.0.8:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;is&nbsp;a&nbsp;PyTorch-Lightning&nbsp;compatible&nbsp;version&nbsp;of&nbsp;babyGPT.&nbsp;It&nbsp;is&nbsp;not&nbsp;so&nbsp;uncommon<br>
&nbsp;&nbsp;&nbsp;&nbsp;today&nbsp;for&nbsp;a&nbsp;university&nbsp;lab&nbsp;to&nbsp;deploy&nbsp;multiple&nbsp;low-cost&nbsp;GPUs&nbsp;for&nbsp;training&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;network.&nbsp;Using&nbsp;more&nbsp;than&nbsp;one&nbsp;GPU&nbsp;requires&nbsp;refactoring&nbsp;your&nbsp;code&nbsp;so&nbsp;that&nbsp;it<br>
&nbsp;&nbsp;&nbsp;&nbsp;conforms&nbsp;to&nbsp;the&nbsp;Lightning&nbsp;API.&nbsp;&nbsp;Version&nbsp;1.0.8&nbsp;is&nbsp;an&nbsp;attempt&nbsp;in&nbsp;that&nbsp;direction.&nbsp;In<br>
&nbsp;&nbsp;&nbsp;&nbsp;addition&nbsp;to&nbsp;code&nbsp;reorganization,&nbsp;I&nbsp;have&nbsp;also&nbsp;made&nbsp;other&nbsp;minor&nbsp;changes&nbsp;to&nbsp;make&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;code&nbsp;more&nbsp;efficient.&nbsp;&nbsp;For&nbsp;example,&nbsp;I&nbsp;eliminated&nbsp;a&nbsp;not-really-needed&nbsp;inner-loop&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;overall&nbsp;training&nbsp;loop&nbsp;for&nbsp;the&nbsp;transformer&nbsp;network.&nbsp;&nbsp;[IMPORTANT:&nbsp;You&nbsp;can&nbsp;still<br>
&nbsp;&nbsp;&nbsp;&nbsp;use&nbsp;this&nbsp;version&nbsp;for&nbsp;single-GPU&nbsp;based&nbsp;training.&nbsp;&nbsp;The&nbsp;code&nbsp;will&nbsp;automatically<br>
&nbsp;&nbsp;&nbsp;&nbsp;detect&nbsp;the&nbsp;number&nbsp;of&nbsp;GPUs&nbsp;available&nbsp;and&nbsp;proceed&nbsp;accordingly.]&nbsp;&nbsp;Finally,&nbsp;note&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;I&nbsp;have&nbsp;tested&nbsp;Lightning&nbsp;based&nbsp;execution&nbsp;of&nbsp;the&nbsp;code&nbsp;with&nbsp;only&nbsp;the&nbsp;DDP<br>
&nbsp;&nbsp;&nbsp;&nbsp;(Distributed&nbsp;Data&nbsp;Parallel)&nbsp;strategy&nbsp;for&nbsp;multi-GPU&nbsp;processing.&nbsp;&nbsp;With&nbsp;this<br>
&nbsp;&nbsp;&nbsp;&nbsp;strategy,&nbsp;the&nbsp;computational&nbsp;graph&nbsp;created&nbsp;for&nbsp;the&nbsp;model&nbsp;has&nbsp;to&nbsp;fit&nbsp;in&nbsp;each&nbsp;GPU.<br>
&nbsp;&nbsp;&nbsp;&nbsp;So&nbsp;you&nbsp;cannot&nbsp;construct&nbsp;a&nbsp;larger&nbsp;model&nbsp;just&nbsp;because&nbsp;you&nbsp;are&nbsp;using&nbsp;Lightning&nbsp;for<br>
&nbsp;&nbsp;&nbsp;&nbsp;multi-GPU&nbsp;support.&nbsp;&nbsp;All&nbsp;that&nbsp;you&nbsp;get&nbsp;(with&nbsp;the&nbsp;DDP&nbsp;strategy)&nbsp;is&nbsp;that&nbsp;the&nbsp;learning<br>
&nbsp;&nbsp;&nbsp;&nbsp;process&nbsp;will&nbsp;digest&nbsp;more&nbsp;data&nbsp;faster.&nbsp;&nbsp;For&nbsp;example,&nbsp;if&nbsp;you&nbsp;are&nbsp;using&nbsp;a&nbsp;2-GPU&nbsp;VM,<br>
&nbsp;&nbsp;&nbsp;&nbsp;your&nbsp;effective&nbsp;batch&nbsp;size&nbsp;will&nbsp;double&nbsp;because&nbsp;the&nbsp;two&nbsp;GPUs&nbsp;will&nbsp;be&nbsp;consuming&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;batches&nbsp;in&nbsp;parallel.<br>
&nbsp;<br>
Version&nbsp;1.0.7:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;There&nbsp;was&nbsp;an&nbsp;error&nbsp;in&nbsp;the&nbsp;definition&nbsp;of&nbsp;BasicDecoderWithMasking&nbsp;that&nbsp;I&nbsp;have&nbsp;fixed<br>
&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;this&nbsp;version.&nbsp;&nbsp;Despite&nbsp;the&nbsp;error,&nbsp;the&nbsp;module&nbsp;worked&nbsp;as&nbsp;intended&nbsp;but&nbsp;not&nbsp;as<br>
&nbsp;&nbsp;&nbsp;&nbsp;efficiently&nbsp;as&nbsp;one&nbsp;would&nbsp;have&nbsp;expected.<br>
&nbsp;<br>
Version&nbsp;1.0.6:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;I&nbsp;have&nbsp;fixed&nbsp;the&nbsp;error&nbsp;that&nbsp;caused&nbsp;the&nbsp;predicted&nbsp;tokens&nbsp;to&nbsp;be&nbsp;shifted&nbsp;by&nbsp;one<br>
&nbsp;&nbsp;&nbsp;&nbsp;position&nbsp;vis-a-vis&nbsp;the&nbsp;ground-truth&nbsp;tokens.<br>
&nbsp;<br>
Version&nbsp;1.0.5:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Had&nbsp;a&nbsp;URL&nbsp;error&nbsp;in&nbsp;the&nbsp;setup.py&nbsp;of&nbsp;the&nbsp;previous&nbsp;version.&nbsp;The&nbsp;rest&nbsp;of&nbsp;the&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;remains&nbsp;unchanged.<br>
&nbsp;<br>
Version&nbsp;1.0.4:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;is&nbsp;the&nbsp;first&nbsp;public&nbsp;release&nbsp;version&nbsp;of&nbsp;the&nbsp;module.&nbsp;This&nbsp;module&nbsp;was&nbsp;created<br>
&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;the&nbsp;Deep&nbsp;Learning&nbsp;class&nbsp;at&nbsp;Purdue&nbsp;University.<br>
&nbsp;<br>
&nbsp;<br>
<font size="+2" color="red">INTRODUCTION:<br>
</font>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;SPECIFIC&nbsp;GOALS&nbsp;FOR&nbsp;THIS&nbsp;MODULE:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;1)&nbsp;To&nbsp;introduce&nbsp;the&nbsp;students&nbsp;in&nbsp;Purdue's&nbsp;Deep&nbsp;Learning&nbsp;class&nbsp;to&nbsp;the&nbsp;foundational<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;concepts&nbsp;in&nbsp;how&nbsp;to&nbsp;create&nbsp;a&nbsp;Base&nbsp;Language&nbsp;Model&nbsp;through&nbsp;self-supervised<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;learning.&nbsp;&nbsp;Large&nbsp;Language&nbsp;Models&nbsp;start&nbsp;out&nbsp;as&nbsp;Base&nbsp;Models&nbsp;that&nbsp;are<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;subsequently&nbsp;fine-tuned&nbsp;with&nbsp;reinforcement&nbsp;learning.&nbsp;&nbsp;The&nbsp;focus&nbsp;of&nbsp;this&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;is&nbsp;solely&nbsp;on&nbsp;Base&nbsp;Modeling.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;2)&nbsp;To&nbsp;demonstrate&nbsp;small-scale&nbsp;large-language&nbsp;modeling&nbsp;(SS-LLM)&nbsp;that,&nbsp;for<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;educational&nbsp;purposes,&nbsp;can&nbsp;be&nbsp;run&nbsp;in&nbsp;the&nbsp;hardware&nbsp;available&nbsp;in&nbsp;a&nbsp;typical<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;university&nbsp;lab.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;3)&nbsp;To&nbsp;create&nbsp;a&nbsp;self-contained&nbsp;module&nbsp;that,&nbsp;given&nbsp;a&nbsp;set&nbsp;of&nbsp;media&nbsp;URLs,&nbsp;will<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;download&nbsp;the&nbsp;articles&nbsp;from&nbsp;those&nbsp;websites&nbsp;(assuming&nbsp;they&nbsp;are&nbsp;not&nbsp;behind<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;paywalls),&nbsp;train&nbsp;a&nbsp;BPE&nbsp;tokenizer&nbsp;for&nbsp;the&nbsp;corpus&nbsp;of&nbsp;the&nbsp;articles&nbsp;collected,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;create&nbsp;a&nbsp;Base&nbsp;Model&nbsp;from&nbsp;the&nbsp;corpus,&nbsp;and,&nbsp;subsequently,&nbsp;let&nbsp;you&nbsp;play&nbsp;with&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;model&nbsp;using&nbsp;the&nbsp;prompting&nbsp;script&nbsp;in&nbsp;the&nbsp;module.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;My&nbsp;main&nbsp;goal&nbsp;in&nbsp;babyGPT&nbsp;is&nbsp;to&nbsp;demonstrate&nbsp;that,&nbsp;for&nbsp;the&nbsp;purpose&nbsp;of&nbsp;teaching&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;learning,&nbsp;it&nbsp;is&nbsp;possible&nbsp;to&nbsp;create&nbsp;a&nbsp;small-scale&nbsp;end-to-end&nbsp;implementation&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;downloads&nbsp;a&nbsp;corpus&nbsp;of&nbsp;news&nbsp;articles,&nbsp;trains&nbsp;a&nbsp;BPE&nbsp;tokenizer&nbsp;if&nbsp;you&nbsp;need&nbsp;a&nbsp;new&nbsp;one<br>
&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;the&nbsp;domain&nbsp;of&nbsp;the&nbsp;corpus&nbsp;you&nbsp;have&nbsp;collected,&nbsp;and,&nbsp;finally,&nbsp;uses&nbsp;the&nbsp;corpus<br>
&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;training&nbsp;an&nbsp;autoregressive&nbsp;model&nbsp;for&nbsp;next-token-prediction&nbsp;based&nbsp;on<br>
&nbsp;&nbsp;&nbsp;&nbsp;unsupervised&nbsp;learning.&nbsp;After&nbsp;you&nbsp;have&nbsp;trained&nbsp;the&nbsp;model,&nbsp;you&nbsp;can&nbsp;test&nbsp;it&nbsp;with&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;prompting&nbsp;script&nbsp;that&nbsp;is&nbsp;included&nbsp;in&nbsp;the&nbsp;Examples&nbsp;directory.<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;LANGUAGE&nbsp;MODELING&nbsp;AND&nbsp;UNSUPERVISED&nbsp;LEARNING:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;There&nbsp;is&nbsp;no&nbsp;denying&nbsp;the&nbsp;fact&nbsp;that&nbsp;the&nbsp;recent&nbsp;advances&nbsp;in&nbsp;chatbots&nbsp;have&nbsp;set&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;world&nbsp;on&nbsp;fire.&nbsp;It's&nbsp;truly&nbsp;amazing&nbsp;to&nbsp;see&nbsp;a&nbsp;chatbot&nbsp;returning&nbsp;(most&nbsp;of&nbsp;the&nbsp;time)&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;smooth-reading&nbsp;well-structured&nbsp;narrative&nbsp;in&nbsp;response&nbsp;to&nbsp;a&nbsp;prompt.&nbsp;As&nbsp;if&nbsp;that&nbsp;were<br>
&nbsp;&nbsp;&nbsp;&nbsp;not&nbsp;enough,&nbsp;it&nbsp;can&nbsp;also&nbsp;supply&nbsp;you&nbsp;with&nbsp;variants&nbsp;of&nbsp;the&nbsp;same&nbsp;narrative&nbsp;depending<br>
&nbsp;&nbsp;&nbsp;&nbsp;on&nbsp;how&nbsp;you&nbsp;prompt&nbsp;it&nbsp;and&nbsp;your&nbsp;randomization&nbsp;settings&nbsp;for&nbsp;the&nbsp;bot.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;One&nbsp;would&nbsp;think&nbsp;that&nbsp;this&nbsp;degree&nbsp;of&nbsp;competency&nbsp;shown&nbsp;by&nbsp;a&nbsp;chatbot&nbsp;would&nbsp;require&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;vast&nbsp;amount&nbsp;of&nbsp;human&nbsp;annotated&nbsp;data&nbsp;for&nbsp;training&nbsp;the&nbsp;neural&nbsp;networks&nbsp;used&nbsp;for&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;bot.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;truth&nbsp;is&nbsp;exactly&nbsp;the&nbsp;opposite.&nbsp;&nbsp;Most&nbsp;of&nbsp;the&nbsp;learning&nbsp;that&nbsp;takes&nbsp;place&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;order&nbsp;to&nbsp;train&nbsp;a&nbsp;chatbot&nbsp;is&nbsp;unsupervised&nbsp;---&nbsp;that&nbsp;is,&nbsp;without&nbsp;any&nbsp;human<br>
&nbsp;&nbsp;&nbsp;&nbsp;supervision.&nbsp;The&nbsp;bot&nbsp;is&nbsp;given&nbsp;the&nbsp;simplest&nbsp;of&nbsp;the&nbsp;goals:&nbsp;To&nbsp;predict&nbsp;the&nbsp;next<br>
&nbsp;&nbsp;&nbsp;&nbsp;token&nbsp;given&nbsp;the&nbsp;tokens&nbsp;that&nbsp;have&nbsp;been&nbsp;seen&nbsp;so&nbsp;far.&nbsp;&nbsp;To&nbsp;master&nbsp;this&nbsp;goal,&nbsp;the&nbsp;bot<br>
&nbsp;&nbsp;&nbsp;&nbsp;needs&nbsp;zero&nbsp;supervision.&nbsp;&nbsp;All&nbsp;it&nbsp;needs&nbsp;to&nbsp;do&nbsp;is&nbsp;to&nbsp;use&nbsp;its&nbsp;neural&nbsp;network&nbsp;to&nbsp;make<br>
&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;prediction&nbsp;for&nbsp;the&nbsp;next&nbsp;token.&nbsp;&nbsp;And,&nbsp;at&nbsp;training&nbsp;time,&nbsp;should&nbsp;this&nbsp;prediction<br>
&nbsp;&nbsp;&nbsp;&nbsp;be&nbsp;wrong,&nbsp;to&nbsp;estimate&nbsp;the&nbsp;error&nbsp;made,&nbsp;and&nbsp;then&nbsp;to&nbsp;backpropagate&nbsp;that&nbsp;error&nbsp;while<br>
&nbsp;&nbsp;&nbsp;&nbsp;adjusting&nbsp;the&nbsp;learnable&nbsp;weights&nbsp;in&nbsp;the&nbsp;network.&nbsp;&nbsp;Until&nbsp;not&nbsp;too&nbsp;long&nbsp;ago&nbsp;most<br>
&nbsp;&nbsp;&nbsp;&nbsp;people&nbsp;would&nbsp;have&nbsp;thought&nbsp;that&nbsp;this&nbsp;type&nbsp;of&nbsp;learning&nbsp;would&nbsp;be&nbsp;much&nbsp;too&nbsp;weak&nbsp;to&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;of&nbsp;any&nbsp;practical&nbsp;use.&nbsp;But,&nbsp;as&nbsp;in&nbsp;all&nbsp;engineering,&nbsp;you&nbsp;cannot&nbsp;argue&nbsp;with&nbsp;something<br>
&nbsp;&nbsp;&nbsp;&nbsp;that&nbsp;actually&nbsp;works.&nbsp;&nbsp;One&nbsp;great&nbsp;thing&nbsp;that&nbsp;has&nbsp;come&nbsp;out&nbsp;of&nbsp;AI&nbsp;research&nbsp;of&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;last&nbsp;two&nbsp;decades&nbsp;is&nbsp;that&nbsp;unsupervised&nbsp;learning&nbsp;not&nbsp;only&nbsp;works,&nbsp;it&nbsp;actually&nbsp;lends<br>
&nbsp;&nbsp;&nbsp;&nbsp;itself&nbsp;to&nbsp;designing&nbsp;powerful&nbsp;data&nbsp;driven&nbsp;frameworks&nbsp;without&nbsp;too&nbsp;much&nbsp;human<br>
&nbsp;&nbsp;&nbsp;&nbsp;intervention.<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;TRANSFORMERS:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;unsupervised&nbsp;learning&nbsp;of&nbsp;the&nbsp;sort&nbsp;described&nbsp;above&nbsp;is&nbsp;best&nbsp;implemented&nbsp;with<br>
&nbsp;&nbsp;&nbsp;&nbsp;Transformers.&nbsp;(See&nbsp;my&nbsp;material&nbsp;for&nbsp;the&nbsp;Week&nbsp;13&nbsp;lecture&nbsp;at&nbsp;Purdue's&nbsp;Deep&nbsp;Learning<br>
&nbsp;&nbsp;&nbsp;&nbsp;class&nbsp;for&nbsp;a&nbsp;detailed&nbsp;presentation&nbsp;on&nbsp;how&nbsp;one&nbsp;can&nbsp;implement&nbsp;an&nbsp;English-to-Spanish<br>
&nbsp;&nbsp;&nbsp;&nbsp;translation&nbsp;framework&nbsp;using&nbsp;Transformers.)&nbsp;&nbsp;And&nbsp;central&nbsp;to&nbsp;a&nbsp;Transformer-based<br>
&nbsp;&nbsp;&nbsp;&nbsp;architecture&nbsp;is&nbsp;the&nbsp;notion&nbsp;of&nbsp;Attention.&nbsp;&nbsp;Attention&nbsp;means&nbsp;the&nbsp;extent&nbsp;to&nbsp;which<br>
&nbsp;&nbsp;&nbsp;&nbsp;each&nbsp;element&nbsp;at&nbsp;the&nbsp;input&nbsp;to&nbsp;a&nbsp;neural&nbsp;network&nbsp;attends&nbsp;to&nbsp;every&nbsp;other&nbsp;element&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;same&nbsp;input.&nbsp;&nbsp;For&nbsp;example,&nbsp;in&nbsp;a&nbsp;network&nbsp;for&nbsp;language&nbsp;translation,&nbsp;the&nbsp;network<br>
&nbsp;&nbsp;&nbsp;&nbsp;would&nbsp;use&nbsp;Attention&nbsp;to&nbsp;figure&nbsp;out&nbsp;the&nbsp;significance&nbsp;of&nbsp;each&nbsp;token&nbsp;in&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;source-language&nbsp;sentence&nbsp;to&nbsp;every&nbsp;other&nbsp;token&nbsp;in&nbsp;the&nbsp;same&nbsp;sentence.&nbsp;&nbsp;If&nbsp;"car"&nbsp;was<br>
&nbsp;&nbsp;&nbsp;&nbsp;one&nbsp;of&nbsp;the&nbsp;tokens&nbsp;in&nbsp;a&nbsp;sentence&nbsp;at&nbsp;the&nbsp;input&nbsp;and&nbsp;a&nbsp;subsequent&nbsp;clause&nbsp;in&nbsp;the&nbsp;same<br>
&nbsp;&nbsp;&nbsp;&nbsp;sentence&nbsp;used&nbsp;the&nbsp;pronoun&nbsp;"it"&nbsp;that&nbsp;pointed&nbsp;to&nbsp;that&nbsp;car,&nbsp;the&nbsp;network&nbsp;would&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;able&nbsp;to&nbsp;figure&nbsp;out&nbsp;the&nbsp;connection&nbsp;between&nbsp;the&nbsp;"it"&nbsp;and&nbsp;the&nbsp;"car"&nbsp;tokens&nbsp;through<br>
&nbsp;&nbsp;&nbsp;&nbsp;Attention.&nbsp;&nbsp;Along&nbsp;the&nbsp;same&nbsp;lines,&nbsp;the&nbsp;network&nbsp;would&nbsp;use&nbsp;Cross&nbsp;Attention&nbsp;to&nbsp;figure<br>
&nbsp;&nbsp;&nbsp;&nbsp;out&nbsp;the&nbsp;importance&nbsp;of&nbsp;each&nbsp;token&nbsp;in&nbsp;the&nbsp;source&nbsp;language&nbsp;to&nbsp;the&nbsp;different&nbsp;tokens<br>
&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;the&nbsp;target&nbsp;language.&nbsp;&nbsp;As&nbsp;you&nbsp;can&nbsp;imagine,&nbsp;understanding&nbsp;such&nbsp;connections<br>
&nbsp;&nbsp;&nbsp;&nbsp;between&nbsp;the&nbsp;tokens&nbsp;would&nbsp;be&nbsp;critical&nbsp;to&nbsp;any&nbsp;network&nbsp;that&nbsp;is&nbsp;learning&nbsp;how&nbsp;to<br>
&nbsp;&nbsp;&nbsp;&nbsp;translate&nbsp;a&nbsp;source&nbsp;language&nbsp;sentence&nbsp;into&nbsp;a&nbsp;target&nbsp;language&nbsp;sentence.<br>
&nbsp;<br>
&nbsp;<br>
<font size="+2" color="red">THE&nbsp;MAJOR&nbsp;COMPONENTS&nbsp;of&nbsp;babyGPT:<br>
</font>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;babyGPT&nbsp;module&nbsp;contains&nbsp;the&nbsp;following&nbsp;Python&nbsp;classes:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1)&nbsp;ArticleGatherer&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(2)&nbsp;ArticleDataset&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[supplies&nbsp;the&nbsp;data&nbsp;downloader&nbsp;for&nbsp;training]<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(3)&nbsp;TrainTokenizer&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(4)&nbsp;TransformerFG&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[borrowed&nbsp;from&nbsp;Transformers&nbsp;in&nbsp;DLStudio]<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(5)&nbsp;MasterDecoderWithMasking;&nbsp;&nbsp;&nbsp;[borrowed&nbsp;from&nbsp;Transformers&nbsp;in&nbsp;DLStudio]<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(6)&nbsp;PromptResponder<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;In&nbsp;what&nbsp;follows,&nbsp;I'll&nbsp;introduce&nbsp;each&nbsp;of&nbsp;these&nbsp;components&nbsp;one&nbsp;by&nbsp;one.&nbsp;&nbsp;Each<br>
&nbsp;&nbsp;&nbsp;&nbsp;component&nbsp;is&nbsp;a&nbsp;separate&nbsp;inner&nbsp;class&nbsp;of&nbsp;the&nbsp;main&nbsp;module&nbsp;class&nbsp;babyGPT.<br>
&nbsp;<br>
&nbsp;<br>
<font size="+1" color="blue">&nbsp;&nbsp;&nbsp;&nbsp;ArticleGatherer:<br>
</font>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;About&nbsp;the&nbsp;ArticleGatherer,&nbsp;you&nbsp;supply&nbsp;it&nbsp;with&nbsp;a&nbsp;list&nbsp;of&nbsp;URLs&nbsp;to&nbsp;media&nbsp;news&nbsp;sites.<br>
&nbsp;&nbsp;&nbsp;&nbsp;It&nbsp;then&nbsp;uses&nbsp;the&nbsp;Newspaper&nbsp;module&nbsp;(which&nbsp;understands&nbsp;the&nbsp;structure&nbsp;of&nbsp;a&nbsp;typical<br>
&nbsp;&nbsp;&nbsp;&nbsp;news&nbsp;HTML&nbsp;file)&nbsp;to&nbsp;download&nbsp;the&nbsp;articles&nbsp;from&nbsp;each&nbsp;of&nbsp;those&nbsp;URLs.&nbsp;&nbsp;It&nbsp;is<br>
&nbsp;&nbsp;&nbsp;&nbsp;important&nbsp;to&nbsp;keep&nbsp;in&nbsp;mind&nbsp;that&nbsp;ArticleGatherer&nbsp;skips&nbsp;over&nbsp;non-HTML&nbsp;article&nbsp;files<br>
&nbsp;&nbsp;&nbsp;&nbsp;at&nbsp;the&nbsp;media&nbsp;websites.&nbsp;Unfortunately,&nbsp;many&nbsp;popular&nbsp;news&nbsp;websites&nbsp;now&nbsp;hide&nbsp;their<br>
&nbsp;&nbsp;&nbsp;&nbsp;content&nbsp;behind&nbsp;paywalls&nbsp;implemented&nbsp;with&nbsp;JavaScript.&nbsp;&nbsp;[Examples&nbsp;of&nbsp;such&nbsp;websites<br>
&nbsp;&nbsp;&nbsp;&nbsp;include&nbsp;www.nyt.com,&nbsp;www.wsj.com,&nbsp;www.bbc.com,&nbsp;etc.]&nbsp;For&nbsp;obvious&nbsp;reasons,&nbsp;if&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;list&nbsp;of&nbsp;the&nbsp;URLs&nbsp;you&nbsp;provide&nbsp;ArticleGatherer&nbsp;consists&nbsp;of&nbsp;mostly&nbsp;such&nbsp;websites,<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;size&nbsp;of&nbsp;the&nbsp;corpus&nbsp;you&nbsp;create&nbsp;for&nbsp;experimenting&nbsp;with&nbsp;babyGPT&nbsp;could&nbsp;be&nbsp;much<br>
&nbsp;&nbsp;&nbsp;&nbsp;too&nbsp;small&nbsp;to&nbsp;be&nbsp;of&nbsp;any&nbsp;use.<br>
&nbsp;<br>
&nbsp;<br>
<font size="+1" color="blue">&nbsp;&nbsp;&nbsp;&nbsp;ArticleDataset:<br>
</font>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;After&nbsp;you&nbsp;have&nbsp;used&nbsp;ArticleGatherer&nbsp;to&nbsp;download&nbsp;the&nbsp;news&nbsp;articles&nbsp;for&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;training&nbsp;corpus,&nbsp;the&nbsp;next&nbsp;thing&nbsp;you&nbsp;are&nbsp;going&nbsp;to&nbsp;need&nbsp;is&nbsp;a&nbsp;dataloader.&nbsp;That's<br>
&nbsp;&nbsp;&nbsp;&nbsp;exactly&nbsp;what's&nbsp;provided&nbsp;by&nbsp;the&nbsp;ArticleDataset&nbsp;class.&nbsp;&nbsp;It&nbsp;randomly&nbsp;shuffles&nbsp;all<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;articles&nbsp;gathered&nbsp;and&nbsp;creates&nbsp;a&nbsp;number&nbsp;of&nbsp;dataloading&nbsp;streams&nbsp;equal&nbsp;to&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;batch-size&nbsp;that&nbsp;you&nbsp;are&nbsp;using&nbsp;for&nbsp;training&nbsp;babyGPT.&nbsp;The&nbsp;data&nbsp;input&nbsp;for&nbsp;the&nbsp;i^th<br>
&nbsp;&nbsp;&nbsp;&nbsp;batch&nbsp;instance&nbsp;is&nbsp;provided&nbsp;by&nbsp;the&nbsp;i^th&nbsp;stream.&nbsp;Logically&nbsp;speaking,&nbsp;you&nbsp;can&nbsp;think<br>
&nbsp;&nbsp;&nbsp;&nbsp;of&nbsp;each&nbsp;stream&nbsp;as&nbsp;a&nbsp;concatenation&nbsp;of&nbsp;the&nbsp;news&nbsp;articles&nbsp;that&nbsp;were&nbsp;randomly&nbsp;chosen<br>
&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;that&nbsp;batch&nbsp;instance.<br>
&nbsp;<br>
&nbsp;<br>
<font size="+1" color="blue">&nbsp;&nbsp;&nbsp;&nbsp;TrainTokenizer:<br>
</font>&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Tokenizers&nbsp;play&nbsp;a&nbsp;critical&nbsp;role&nbsp;in&nbsp;language&nbsp;modeling&nbsp;because&nbsp;they&nbsp;create&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;bounded&nbsp;vocabulary&nbsp;of&nbsp;the&nbsp;tokens&nbsp;that&nbsp;is&nbsp;needed&nbsp;for&nbsp;maximum-likelihood&nbsp;prediction<br>
&nbsp;&nbsp;&nbsp;&nbsp;of&nbsp;the&nbsp;next&nbsp;token&nbsp;in&nbsp;a&nbsp;narrative.&nbsp;&nbsp;The&nbsp;token&nbsp;vocabulary&nbsp;is&nbsp;generally&nbsp;constructed<br>
&nbsp;&nbsp;&nbsp;&nbsp;by&nbsp;using&nbsp;a&nbsp;split-and-merge&nbsp;approach&nbsp;in&nbsp;which&nbsp;you&nbsp;start&nbsp;by&nbsp;considering&nbsp;each<br>
&nbsp;&nbsp;&nbsp;&nbsp;different&nbsp;word&nbsp;in&nbsp;your&nbsp;corpus&nbsp;as&nbsp;a&nbsp;sequence&nbsp;of&nbsp;the&nbsp;most&nbsp;basic&nbsp;symbols,&nbsp;which&nbsp;can<br>
&nbsp;&nbsp;&nbsp;&nbsp;be&nbsp;ASCII&nbsp;characters&nbsp;if&nbsp;you&nbsp;are&nbsp;considering&nbsp;purely&nbsp;English&nbsp;text;&nbsp;or&nbsp;the&nbsp;individual<br>
&nbsp;&nbsp;&nbsp;&nbsp;bytes,&nbsp;as&nbsp;in&nbsp;the&nbsp;BPE&nbsp;(Byte&nbsp;Pair&nbsp;Encoding)&nbsp;tokenizer&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;for&nbsp;most<br>
&nbsp;&nbsp;&nbsp;&nbsp;Western&nbsp;languages;&nbsp;or&nbsp;the&nbsp;even&nbsp;more&nbsp;general&nbsp;individual&nbsp;utf-8&nbsp;encoded&nbsp;multi-byte<br>
&nbsp;&nbsp;&nbsp;&nbsp;characters&nbsp;if&nbsp;your&nbsp;goal&nbsp;is&nbsp;to&nbsp;create&nbsp;a&nbsp;language&nbsp;agnostic&nbsp;tokenizer.<br>
&nbsp;&nbsp;&nbsp;&nbsp;Subsequently,&nbsp;you&nbsp;form&nbsp;subwords&nbsp;by,&nbsp;first,&nbsp;merging&nbsp;the&nbsp;most&nbsp;basic&nbsp;constituents<br>
&nbsp;&nbsp;&nbsp;&nbsp;and,&nbsp;then,&nbsp;merging&nbsp;together&nbsp;smaller&nbsp;subwords&nbsp;into&nbsp;longer&nbsp;subwords&nbsp;by&nbsp;choosing&nbsp;at<br>
&nbsp;&nbsp;&nbsp;&nbsp;each&nbsp;iteration&nbsp;the&nbsp;most&nbsp;frequently&nbsp;occurring&nbsp;contiguously&nbsp;occurring&nbsp;pair&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;subwords.&nbsp;&nbsp;The&nbsp;merging&nbsp;process&nbsp;continues&nbsp;until&nbsp;you&nbsp;have&nbsp;reached&nbsp;the&nbsp;specified<br>
&nbsp;&nbsp;&nbsp;&nbsp;vocabulary&nbsp;size.&nbsp;&nbsp;What&nbsp;this&nbsp;logic&nbsp;implies&nbsp;is&nbsp;that&nbsp;if&nbsp;a&nbsp;long&nbsp;word&nbsp;in&nbsp;the&nbsp;corpus<br>
&nbsp;&nbsp;&nbsp;&nbsp;occurs&nbsp;sufficiently&nbsp;frequently,&nbsp;it&nbsp;will&nbsp;be&nbsp;represented&nbsp;by&nbsp;a&nbsp;single&nbsp;token.&nbsp;&nbsp;On&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;other&nbsp;hand,&nbsp;a&nbsp;relatively&nbsp;short&nbsp;word&nbsp;that&nbsp;occurs&nbsp;rarely&nbsp;in&nbsp;the&nbsp;original&nbsp;corpus<br>
&nbsp;&nbsp;&nbsp;&nbsp;could&nbsp;be&nbsp;decomposed&nbsp;into&nbsp;shorter&nbsp;tokens.&nbsp;&nbsp;It&nbsp;is&nbsp;in&nbsp;this&nbsp;manner&nbsp;that,&nbsp;with&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;WordPiece&nbsp;tokenizer,&nbsp;the&nbsp;BERT&nbsp;LLM&nbsp;has&nbsp;a&nbsp;vocabulary&nbsp;of&nbsp;around&nbsp;30,000&nbsp;tokens&nbsp;and,<br>
&nbsp;&nbsp;&nbsp;&nbsp;with&nbsp;the&nbsp;BPE&nbsp;tokenizer,&nbsp;the&nbsp;GPT-3&nbsp;has&nbsp;a&nbsp;vocabulary&nbsp;of&nbsp;50,000&nbsp;tokens.&nbsp;Without&nbsp;such<br>
&nbsp;&nbsp;&nbsp;&nbsp;tokenization,&nbsp;the&nbsp;size&nbsp;of&nbsp;the&nbsp;vocabulary&nbsp;could&nbsp;grow&nbsp;continuously&nbsp;with&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;size&nbsp;of&nbsp;the&nbsp;corpus.&nbsp;&nbsp;As&nbsp;you&nbsp;can&nbsp;imagine,&nbsp;if&nbsp;a&nbsp;language&nbsp;modeler&nbsp;is&nbsp;ingesting<br>
&nbsp;&nbsp;&nbsp;&nbsp;terabytes&nbsp;of&nbsp;text,&nbsp;the&nbsp;vocabulary&nbsp;of&nbsp;the&nbsp;words&nbsp;it&nbsp;sees&nbsp;could&nbsp;run&nbsp;into&nbsp;millions.<br>
&nbsp;&nbsp;&nbsp;&nbsp;It&nbsp;is&nbsp;not&nbsp;possible&nbsp;to&nbsp;devise&nbsp;the&nbsp;probability-based&nbsp;logic&nbsp;for&nbsp;next-word&nbsp;prediction<br>
&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;your&nbsp;underlying&nbsp;vocabulary&nbsp;is&nbsp;unbounded.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;module&nbsp;comes&nbsp;with&nbsp;a&nbsp;pretrained&nbsp;tokenizer&nbsp;with&nbsp;a&nbsp;vocab&nbsp;size&nbsp;of&nbsp;around&nbsp;50,000<br>
&nbsp;&nbsp;&nbsp;&nbsp;tokens.&nbsp;&nbsp;I&nbsp;trained&nbsp;this&nbsp;tokenizer&nbsp;using&nbsp;the&nbsp;babyGPT&nbsp;module&nbsp;on&nbsp;the&nbsp;"Athlete&nbsp;News"<br>
&nbsp;&nbsp;&nbsp;&nbsp;dataset&nbsp;created&nbsp;by&nbsp;Adrien&nbsp;Dubois.&nbsp;The&nbsp;name&nbsp;of&nbsp;the&nbsp;tokenizer&nbsp;JSON&nbsp;in&nbsp;the&nbsp;Examples<br>
&nbsp;&nbsp;&nbsp;&nbsp;directory&nbsp;is&nbsp;like:&nbsp;"XYZ_babygpt_tokenizer_PQRST.json".&nbsp;&nbsp;The&nbsp;prefix&nbsp;"XYZ"&nbsp;says<br>
&nbsp;&nbsp;&nbsp;&nbsp;that&nbsp;JSON&nbsp;was&nbsp;created&nbsp;with&nbsp;the&nbsp;tokenization&nbsp;code&nbsp;in&nbsp;version&nbsp;X.Y.Z&nbsp;of&nbsp;babyGPT.<br>
&nbsp;&nbsp;&nbsp;&nbsp;And&nbsp;"PQRST"&nbsp;is&nbsp;an&nbsp;integer&nbsp;that&nbsp;is&nbsp;the&nbsp;actual&nbsp;size&nbsp;of&nbsp;the&nbsp;token&nbsp;vocab.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Starting&nbsp;with&nbsp;Version&nbsp;1.1.1,&nbsp;you&nbsp;will&nbsp;find&nbsp;the&nbsp;tokenizer&nbsp;named&nbsp;above&nbsp;in&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;subdirectory&nbsp;named&nbsp;"tokenizer_outputs"&nbsp;in&nbsp;the&nbsp;Examples&nbsp;directory&nbsp;of&nbsp;the&nbsp;distro.<br>
&nbsp;&nbsp;&nbsp;&nbsp;You&nbsp;will&nbsp;also&nbsp;find&nbsp;a&nbsp;cleaned&nbsp;version&nbsp;of&nbsp;the&nbsp;tokenizer&nbsp;in&nbsp;a&nbsp;subdirectory&nbsp;named<br>
&nbsp;&nbsp;&nbsp;&nbsp;"cleaned_tokenizer_outputs".<br>
&nbsp;<br>
&nbsp;<br>
<font size="+1" color="blue">&nbsp;&nbsp;&nbsp;&nbsp;TransformerFG:<br>
</font>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;About&nbsp;the&nbsp;TransformerFG&nbsp;component&nbsp;of&nbsp;babyGPT,&nbsp;as&nbsp;mentioned&nbsp;already,&nbsp;language<br>
&nbsp;&nbsp;&nbsp;&nbsp;modeling&nbsp;is&nbsp;best&nbsp;carried&nbsp;out&nbsp;with&nbsp;Transformer&nbsp;based&nbsp;implementations.&nbsp;To&nbsp;that&nbsp;end,<br>
&nbsp;&nbsp;&nbsp;&nbsp;I&nbsp;borrowed&nbsp;TransformerFG&nbsp;from&nbsp;DLStudio's&nbsp;Transformers&nbsp;module.&nbsp;&nbsp;TransformerFG&nbsp;is<br>
&nbsp;&nbsp;&nbsp;&nbsp;my&nbsp;implementation&nbsp;of&nbsp;the&nbsp;concept&nbsp;of&nbsp;the&nbsp;Transformer&nbsp;as&nbsp;proposed&nbsp;by&nbsp;Vaswani&nbsp;et<br>
&nbsp;&nbsp;&nbsp;&nbsp;al.&nbsp;in&nbsp;their&nbsp;seminal&nbsp;paper&nbsp;"Attention&nbsp;is&nbsp;All&nbsp;You&nbsp;Need."&nbsp;&nbsp;The&nbsp;suffix&nbsp;"FG"&nbsp;stands<br>
&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;"First&nbsp;Generation."<br>
&nbsp;<br>
&nbsp;<br>
<font size="+1" color="blue">&nbsp;&nbsp;&nbsp;&nbsp;MasterDecoderWithMasking:<br>
</font>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;MasterDecoderWithMasking&nbsp;part&nbsp;of&nbsp;babyGPT&nbsp;has&nbsp;also&nbsp;been&nbsp;borrowed&nbsp;from<br>
&nbsp;&nbsp;&nbsp;&nbsp;DLStudio's&nbsp;Transformers&nbsp;module.&nbsp;&nbsp;To&nbsp;see&nbsp;the&nbsp;need&nbsp;for&nbsp;this&nbsp;component,&nbsp;note&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;unsupervised&nbsp;learning&nbsp;that&nbsp;is&nbsp;needed&nbsp;for&nbsp;autoregressive&nbsp;language&nbsp;modeling&nbsp;only<br>
&nbsp;&nbsp;&nbsp;&nbsp;uses&nbsp;the&nbsp;Decoder&nbsp;side&nbsp;of&nbsp;the&nbsp;Encode-Decoder&nbsp;architecture&nbsp;that&nbsp;would&nbsp;otherwise&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;needed&nbsp;for&nbsp;a&nbsp;Transformer-based&nbsp;framework&nbsp;for&nbsp;translating&nbsp;one&nbsp;language&nbsp;into<br>
&nbsp;&nbsp;&nbsp;&nbsp;another.&nbsp;An&nbsp;example&nbsp;of&nbsp;such&nbsp;a&nbsp;framework&nbsp;is&nbsp;presented&nbsp;in&nbsp;the&nbsp;notes&nbsp;for&nbsp;my&nbsp;Week&nbsp;14<br>
&nbsp;&nbsp;&nbsp;&nbsp;lecture&nbsp;at&nbsp;Purdue's&nbsp;Deep&nbsp;Learning&nbsp;class.&nbsp;That&nbsp;framework&nbsp;has&nbsp;two&nbsp;decoder<br>
&nbsp;&nbsp;&nbsp;&nbsp;implementations:&nbsp;MasterDecoder&nbsp;and&nbsp;MasterDecoderWithMasking.&nbsp;&nbsp;If&nbsp;you&nbsp;are&nbsp;engaged<br>
&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;autoregressive&nbsp;modeling,&nbsp;you&nbsp;have&nbsp;no&nbsp;choice&nbsp;but&nbsp;to&nbsp;use&nbsp;the&nbsp;"WithMasking"<br>
&nbsp;&nbsp;&nbsp;&nbsp;version&nbsp;of&nbsp;the&nbsp;decoder.&nbsp;&nbsp;As&nbsp;to&nbsp;the&nbsp;reason&nbsp;for&nbsp;the&nbsp;"Master"&nbsp;prefix&nbsp;in&nbsp;the&nbsp;name&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;decoder,&nbsp;language&nbsp;modeling&nbsp;typically&nbsp;requires&nbsp;a&nbsp;number&nbsp;of&nbsp;Transformer&nbsp;layers,<br>
&nbsp;&nbsp;&nbsp;&nbsp;with&nbsp;each&nbsp;layer&nbsp;using&nbsp;multiple&nbsp;Attention&nbsp;Heads&nbsp;to&nbsp;calculate&nbsp;what's&nbsp;known&nbsp;as&nbsp;Self<br>
&nbsp;&nbsp;&nbsp;&nbsp;Attention.&nbsp;In&nbsp;my&nbsp;DLStudio&nbsp;code,&nbsp;I&nbsp;refer&nbsp;to&nbsp;this&nbsp;layered&nbsp;organization&nbsp;of&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;Transformers&nbsp;as&nbsp;MasterEncoder&nbsp;and&nbsp;MasterDecoder,&nbsp;and&nbsp;to&nbsp;each&nbsp;Transformer&nbsp;layer&nbsp;as<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;BasicEncoder&nbsp;and&nbsp;the&nbsp;BasicDecoder.&nbsp;&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Note&nbsp;that&nbsp;there's&nbsp;an&nbsp;interesting&nbsp;difference&nbsp;between&nbsp;the&nbsp;decoder&nbsp;logic&nbsp;as&nbsp;used&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;language&nbsp;translation&nbsp;and&nbsp;what&nbsp;you&nbsp;need&nbsp;for&nbsp;unsupervised&nbsp;learning&nbsp;in&nbsp;a&nbsp;GPT:&nbsp;When<br>
&nbsp;&nbsp;&nbsp;&nbsp;used&nbsp;for&nbsp;language&nbsp;translation,&nbsp;the&nbsp;decoder&nbsp;would&nbsp;also&nbsp;calculate&nbsp;Cross&nbsp;Attention,<br>
&nbsp;&nbsp;&nbsp;&nbsp;which&nbsp;is&nbsp;the&nbsp;attention&nbsp;between&nbsp;each&nbsp;element&nbsp;of&nbsp;the&nbsp;data&nbsp;coursing&nbsp;through&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;decoder&nbsp;and&nbsp;all&nbsp;the&nbsp;elements&nbsp;at&nbsp;the&nbsp;final&nbsp;output&nbsp;of&nbsp;the&nbsp;encoder.&nbsp;&nbsp;The&nbsp;decoder&nbsp;as<br>
&nbsp;&nbsp;&nbsp;&nbsp;used&nbsp;for&nbsp;unsupervised&nbsp;learning&nbsp;in&nbsp;a&nbsp;GPT&nbsp;only&nbsp;needs&nbsp;to&nbsp;calculate&nbsp;Self&nbsp;Attention.<br>
&nbsp;<br>
&nbsp;<br>
<font size="+1" color="blue">&nbsp;&nbsp;&nbsp;&nbsp;PromptResponder:<br>
</font>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;About&nbsp;the&nbsp;final&nbsp;component&nbsp;of&nbsp;babyGPT,&nbsp;PromptResponder,&nbsp;its&nbsp;purpose&nbsp;is&nbsp;to&nbsp;put&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;trained&nbsp;babyGPT&nbsp;model&nbsp;to&nbsp;use&nbsp;by&nbsp;having&nbsp;it&nbsp;respond&nbsp;appropriately&nbsp;to&nbsp;the&nbsp;prompts<br>
&nbsp;&nbsp;&nbsp;&nbsp;supplied&nbsp;by&nbsp;a&nbsp;user.&nbsp;&nbsp;Given&nbsp;a&nbsp;prompt&nbsp;in&nbsp;the&nbsp;form&nbsp;of&nbsp;a&nbsp;sentence&nbsp;fragment,&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;PromptResponder&nbsp;uses&nbsp;its&nbsp;next-token&nbsp;prediction&nbsp;ability&nbsp;to&nbsp;keep&nbsp;on&nbsp;generating&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;tokens&nbsp;until&nbsp;it&nbsp;reaches&nbsp;the&nbsp;end-of-sentence&nbsp;token&nbsp;or&nbsp;until&nbsp;it&nbsp;has&nbsp;generated&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;specified&nbsp;number&nbsp;of&nbsp;sentences&nbsp;through&nbsp;this&nbsp;process.<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;<br>
<font size="+2" color="red">DEALING&nbsp;WITH&nbsp;THE&nbsp;PROBLEM&nbsp;OF&nbsp;CONTEXT&nbsp;DISRUPTION&nbsp;CAUSED&nbsp;BY&nbsp;THE&nbsp;"&lt;SOS&gt;"&nbsp;TOKEN:<br>
</font>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;What&nbsp;comes&nbsp;in&nbsp;the&nbsp;way&nbsp;of&nbsp;training&nbsp;babyGPT&nbsp;well&nbsp;are&nbsp;the&nbsp;textual&nbsp;discontinuities<br>
&nbsp;&nbsp;&nbsp;&nbsp;created&nbsp;by&nbsp;how&nbsp;a&nbsp;batch&nbsp;is&nbsp;constructed&nbsp;for&nbsp;each&nbsp;new&nbsp;iteration&nbsp;of&nbsp;training.&nbsp;&nbsp;As<br>
&nbsp;&nbsp;&nbsp;&nbsp;explained&nbsp;elsewhere&nbsp;in&nbsp;this&nbsp;doc&nbsp;page,&nbsp;the&nbsp;list&nbsp;of&nbsp;all&nbsp;the&nbsp;documents&nbsp;in&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;training&nbsp;corpus&nbsp;is&nbsp;first&nbsp;randomized&nbsp;and&nbsp;then&nbsp;divided&nbsp;into&nbsp;a&nbsp;number&nbsp;of&nbsp;token<br>
&nbsp;&nbsp;&nbsp;&nbsp;streams,&nbsp;with&nbsp;one&nbsp;stream&nbsp;for&nbsp;each&nbsp;batch&nbsp;instance.&nbsp;(This&nbsp;randomization&nbsp;of&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;files&nbsp;and&nbsp;the&nbsp;division&nbsp;into&nbsp;token&nbsp;streams&nbsp;is&nbsp;carried&nbsp;out&nbsp;afresh&nbsp;at&nbsp;the&nbsp;beginning<br>
&nbsp;&nbsp;&nbsp;&nbsp;of&nbsp;each&nbsp;epoch.)&nbsp;&nbsp;Subsequently,&nbsp;when&nbsp;a&nbsp;fresh&nbsp;batch&nbsp;is&nbsp;needed,&nbsp;for&nbsp;each&nbsp;batch<br>
&nbsp;&nbsp;&nbsp;&nbsp;instance&nbsp;you&nbsp;"draw"&nbsp;from&nbsp;its&nbsp;corresponding&nbsp;stream&nbsp;a&nbsp;"Context&nbsp;Window"&nbsp;number&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;tokens.&nbsp;The&nbsp;special&nbsp;&lt;SOS&gt;&nbsp;token&nbsp;is&nbsp;placed&nbsp;at&nbsp;the&nbsp;beginning&nbsp;of&nbsp;each&nbsp;such&nbsp;token<br>
&nbsp;&nbsp;&nbsp;&nbsp;sequence.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;insertion&nbsp;of&nbsp;the&nbsp;&lt;SOS&gt;&nbsp;token&nbsp;disrupts&nbsp;the&nbsp;continuity&nbsp;of&nbsp;the&nbsp;token&nbsp;streams,<br>
&nbsp;&nbsp;&nbsp;&nbsp;as&nbsp;you&nbsp;can&nbsp;well&nbsp;imagine,&nbsp;and&nbsp;it&nbsp;violates&nbsp;the&nbsp;main&nbsp;point&nbsp;of&nbsp;the&nbsp;learning&nbsp;involved<br>
&nbsp;&nbsp;&nbsp;&nbsp;which&nbsp;is&nbsp;to&nbsp;learn&nbsp;the&nbsp;continuity&nbsp;properties&nbsp;of&nbsp;the&nbsp;text.&nbsp;Since&nbsp;these&nbsp;continuities<br>
&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;context&nbsp;dependent,&nbsp;it&nbsp;would&nbsp;be&nbsp;fair&nbsp;to&nbsp;say&nbsp;that&nbsp;the&nbsp;&lt;SOS&gt;&nbsp;token&nbsp;causes&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;context&nbsp;disruption&nbsp;for&nbsp;the&nbsp;token&nbsp;that&nbsp;comes&nbsp;after&nbsp;&lt;SOS&gt;&nbsp;at&nbsp;the&nbsp;beginning&nbsp;of&nbsp;each<br>
&nbsp;&nbsp;&nbsp;&nbsp;batch&nbsp;instance.&nbsp;&nbsp;Over&nbsp;the&nbsp;years,&nbsp;various&nbsp;strategies&nbsp;have&nbsp;been&nbsp;proposed&nbsp;to<br>
&nbsp;&nbsp;&nbsp;&nbsp;circumvent&nbsp;this&nbsp;problem,&nbsp;one&nbsp;of&nbsp;the&nbsp;most&nbsp;recent&nbsp;being&nbsp;the&nbsp;"sliding-window&nbsp;based<br>
&nbsp;&nbsp;&nbsp;&nbsp;Attention"&nbsp;as&nbsp;presented&nbsp;by&nbsp;Beltagy,&nbsp;Peters,&nbsp;and&nbsp;Cohan&nbsp;in&nbsp;their&nbsp;2023&nbsp;paper<br>
&nbsp;&nbsp;&nbsp;&nbsp;"Longformer:&nbsp;The&nbsp;Long-Document&nbsp;Transformer".&nbsp;&nbsp;In&nbsp;this&nbsp;approach,&nbsp;a&nbsp;fixed-sized<br>
&nbsp;&nbsp;&nbsp;&nbsp;window&nbsp;is&nbsp;used&nbsp;to&nbsp;calculate&nbsp;the&nbsp;attention&nbsp;at&nbsp;the&nbsp;token&nbsp;that&nbsp;is&nbsp;at&nbsp;the&nbsp;center&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;window.&nbsp;&nbsp;In&nbsp;this&nbsp;manner,&nbsp;what&nbsp;is&nbsp;calculated&nbsp;for&nbsp;Self&nbsp;Attention&nbsp;is&nbsp;the&nbsp;extent<br>
&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;which&nbsp;each&nbsp;token&nbsp;attends&nbsp;to&nbsp;the&nbsp;W/2&nbsp;tokens&nbsp;on&nbsp;each&nbsp;side&nbsp;of&nbsp;the&nbsp;token&nbsp;at&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;center.&nbsp;&nbsp;As&nbsp;the&nbsp;authors&nbsp;say:&nbsp;"Using&nbsp;multiple&nbsp;stacked&nbsp;layers&nbsp;of&nbsp;such&nbsp;windowed<br>
&nbsp;&nbsp;&nbsp;&nbsp;attention&nbsp;results&nbsp;in&nbsp;a&nbsp;large&nbsp;receptive&nbsp;field,&nbsp;where&nbsp;top&nbsp;layers&nbsp;have&nbsp;access&nbsp;to&nbsp;all<br>
&nbsp;&nbsp;&nbsp;&nbsp;input&nbsp;locations&nbsp;and&nbsp;have&nbsp;the&nbsp;capacity&nbsp;to&nbsp;build&nbsp;representations&nbsp;that&nbsp;incorporate<br>
&nbsp;&nbsp;&nbsp;&nbsp;information&nbsp;across&nbsp;the&nbsp;entire&nbsp;input."<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;In&nbsp;keeping&nbsp;with&nbsp;the&nbsp;spirit&nbsp;of&nbsp;babyGPT,&nbsp;I&nbsp;have&nbsp;used&nbsp;a&nbsp;much&nbsp;simpler&nbsp;approach&nbsp;to<br>
&nbsp;&nbsp;&nbsp;&nbsp;deal&nbsp;with&nbsp;the&nbsp;context&nbsp;disruption&nbsp;problem&nbsp;caused&nbsp;by&nbsp;the&nbsp;&lt;SOS&gt;&nbsp;token.&nbsp;&nbsp;My&nbsp;solution<br>
&nbsp;&nbsp;&nbsp;&nbsp;is&nbsp;based&nbsp;on&nbsp;an&nbsp;idea&nbsp;I&nbsp;call&nbsp;"Context&nbsp;Buffer".&nbsp;&nbsp;The&nbsp;Context&nbsp;Buffer&nbsp;for&nbsp;each&nbsp;token<br>
&nbsp;&nbsp;&nbsp;&nbsp;sequence&nbsp;in&nbsp;CURRENT&nbsp;batch&nbsp;consists&nbsp;of&nbsp;the&nbsp;last&nbsp;n&nbsp;tokens&nbsp;in&nbsp;the&nbsp;corresponding<br>
&nbsp;&nbsp;&nbsp;&nbsp;token&nbsp;sequence&nbsp;in&nbsp;the&nbsp;PREVIOUS&nbsp;batch.&nbsp;&nbsp;These&nbsp;last&nbsp;n&nbsp;tokens,&nbsp;inserted&nbsp;after&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&lt;SOS&gt;&nbsp;token&nbsp;in&nbsp;the&nbsp;current&nbsp;batch,&nbsp;provided&nbsp;the&nbsp;context&nbsp;for&nbsp;the&nbsp;prediction&nbsp;at&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;first&nbsp;token&nbsp;positions&nbsp;in&nbsp;the&nbsp;current&nbsp;batch.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;To&nbsp;elaborate,&nbsp;let's&nbsp;assume&nbsp;that&nbsp;N&nbsp;is&nbsp;the&nbsp;size&nbsp;of&nbsp;the&nbsp;Context&nbsp;Window&nbsp;for&nbsp;your<br>
&nbsp;&nbsp;&nbsp;&nbsp;Transformer&nbsp;based&nbsp;processing&nbsp;of&nbsp;text&nbsp;and&nbsp;n&nbsp;is&nbsp;the&nbsp;size&nbsp;of&nbsp;the&nbsp;Context&nbsp;Buffer.&nbsp;&nbsp;At<br>
&nbsp;&nbsp;&nbsp;&nbsp;every&nbsp;training&nbsp;iteration,&nbsp;for&nbsp;each&nbsp;batch&nbsp;instance&nbsp;you&nbsp;will&nbsp;pull&nbsp;N&nbsp;fresh&nbsp;tokens<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;the&nbsp;dataloader.&nbsp;&nbsp;You&nbsp;will&nbsp;prepend&nbsp;the&nbsp;n&nbsp;last&nbsp;tokens&nbsp;for&nbsp;the&nbsp;same&nbsp;instance&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;previous&nbsp;batch&nbsp;to&nbsp;the&nbsp;sequence&nbsp;of&nbsp;N&nbsp;tokens&nbsp;supplied&nbsp;by&nbsp;the&nbsp;dataloader&nbsp;for&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;current&nbsp;batch.&nbsp;&nbsp;This&nbsp;will&nbsp;result&nbsp;in&nbsp;n+N&nbsp;tokens&nbsp;for&nbsp;transformer-based<br>
&nbsp;&nbsp;&nbsp;&nbsp;processing.&nbsp;I&nbsp;refer&nbsp;to&nbsp;n+N&nbsp;as&nbsp;the&nbsp;max_seq_length&nbsp;for&nbsp;which&nbsp;you&nbsp;have&nbsp;designed&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;transformer.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;It&nbsp;is&nbsp;interesting&nbsp;to&nbsp;note&nbsp;that&nbsp;the&nbsp;above&nbsp;mentioned&nbsp;problem&nbsp;with&nbsp;context<br>
&nbsp;&nbsp;&nbsp;&nbsp;disruption&nbsp;does&nbsp;NOT&nbsp;arise&nbsp;with&nbsp;sentence-based&nbsp;language&nbsp;modeling&nbsp;(as&nbsp;in&nbsp;BERT)<br>
&nbsp;&nbsp;&nbsp;&nbsp;since&nbsp;&lt;SOS&gt;&nbsp;is&nbsp;what&nbsp;you&nbsp;would&nbsp;want&nbsp;to&nbsp;use&nbsp;for&nbsp;designating&nbsp;the&nbsp;start&nbsp;of&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;sentence.&nbsp;&nbsp;(For&nbsp;such&nbsp;learning,&nbsp;you&nbsp;would&nbsp;also&nbsp;use&nbsp;another&nbsp;token,&nbsp;denoted&nbsp;&lt;EOS&gt;<br>
&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;the&nbsp;"End&nbsp;of&nbsp;Sequence"&nbsp;indication.)<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;<br>
<font size="+2" color="red">CONFORMING&nbsp;TO&nbsp;THE&nbsp;LIGHTNING&nbsp;API:<br>
</font>&nbsp;<br>
Right&nbsp;off&nbsp;the&nbsp;bat,&nbsp;your&nbsp;code&nbsp;must&nbsp;create&nbsp;an&nbsp;instance&nbsp;of&nbsp;the&nbsp;Lightning's&nbsp;Trainer<br>
class.&nbsp;&nbsp;In&nbsp;my&nbsp;code,&nbsp;this&nbsp;call&nbsp;looks&nbsp;like:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;trainer&nbsp;=&nbsp;&nbsp;Lightning.Trainer(devices=-1,&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;accelerator="gpu",&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;strategy='ddp',&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;enable_progress_bar=False,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;logger=logger,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;max_epochs=-1,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;log_every_n_steps=100<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;<br>
About&nbsp;the&nbsp;constructor&nbsp;parameters&nbsp;used&nbsp;above,&nbsp;I&nbsp;have&nbsp;used&nbsp;"devices=-1"&nbsp;because&nbsp;I&nbsp;want<br>
babyGPT&nbsp;to&nbsp;run&nbsp;without&nbsp;any&nbsp;changes&nbsp;in&nbsp;both&nbsp;my&nbsp;laptop&nbsp;for&nbsp;debugging&nbsp;and&nbsp;code<br>
development&nbsp;purposes&nbsp;and&nbsp;my&nbsp;2-GPU&nbsp;VM&nbsp;in&nbsp;our&nbsp;lab&nbsp;cloud.&nbsp;&nbsp;With&nbsp;the&nbsp;option&nbsp;"devices=-1",<br>
Lightning&nbsp;will&nbsp;discover&nbsp;the&nbsp;number&nbsp;of&nbsp;GPUs&nbsp;available&nbsp;and&nbsp;automatically&nbsp;set&nbsp;"devices"<br>
to&nbsp;that&nbsp;number.&nbsp;&nbsp;Understanding&nbsp;the&nbsp;option&nbsp;"strategy='ddp'is&nbsp;important&nbsp;if&nbsp;you&nbsp;want&nbsp;to<br>
have&nbsp;realistic&nbsp;expectations&nbsp;of&nbsp;what&nbsp;Lightning&nbsp;can&nbsp;do&nbsp;for&nbsp;you.&nbsp;&nbsp;The&nbsp;"ddp"&nbsp;strategy<br>
stands&nbsp;for&nbsp;"Distributed&nbsp;Data&nbsp;Parallel".&nbsp;&nbsp;This&nbsp;strategy&nbsp;launches&nbsp;a&nbsp;number&nbsp;of&nbsp;processes<br>
that&nbsp;are&nbsp;executed&nbsp;in&nbsp;parallel,&nbsp;with&nbsp;one&nbsp;process&nbsp;for&nbsp;each&nbsp;GPU.&nbsp;&nbsp;While&nbsp;each&nbsp;process<br>
creates&nbsp;its&nbsp;own&nbsp;instance&nbsp;of&nbsp;the&nbsp;dataloader&nbsp;and&nbsp;has&nbsp;its&nbsp;own&nbsp;training&nbsp;loop&nbsp;for&nbsp;forward<br>
propagation&nbsp;of&nbsp;the&nbsp;data&nbsp;and&nbsp;backpropagation&nbsp;of&nbsp;the&nbsp;loss&nbsp;gradients,&nbsp;the&nbsp;processes&nbsp;are<br>
synchronized&nbsp;for&nbsp;updating&nbsp;the&nbsp;model&nbsp;parameters.&nbsp;&nbsp;The&nbsp;updating&nbsp;of&nbsp;the&nbsp;learnable<br>
parameters&nbsp;is&nbsp;synchronized&nbsp;in&nbsp;the&nbsp;sense&nbsp;that&nbsp;it&nbsp;is&nbsp;based&nbsp;on&nbsp;the&nbsp;backpropagated&nbsp;loss<br>
gradients&nbsp;in&nbsp;all&nbsp;of&nbsp;the&nbsp;processes.<br>
&nbsp;<br>
Subsequently,&nbsp;you&nbsp;must&nbsp;call&nbsp;"fit()"&nbsp;on&nbsp;the&nbsp;Trainer&nbsp;instance&nbsp;with&nbsp;at&nbsp;least&nbsp;the&nbsp;two<br>
required&nbsp;arguments:&nbsp;the&nbsp;model&nbsp;you&nbsp;want&nbsp;Lightning&nbsp;to&nbsp;train&nbsp;and&nbsp;the&nbsp;dataloader&nbsp;to&nbsp;be<br>
used&nbsp;for&nbsp;training.&nbsp;&nbsp;Here's&nbsp;an&nbsp;example&nbsp;of&nbsp;this&nbsp;call&nbsp;in&nbsp;babyGPT:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;trainer.fit(&nbsp;model=master_decoder,&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;train_dataloaders=&nbsp;StreamingDataModule(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;data_source=dataloader,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;context_window_size=dataloader.context_window_size,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;batchsize=dataloader.batch_size,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;context_buffer_size=dataloader.context_buffer_size,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;inv_lookup_fn=dataloader.inverse_lookup)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;<br>
The&nbsp;"model"&nbsp;that&nbsp;is&nbsp;in&nbsp;the&nbsp;first&nbsp;argument&nbsp;to&nbsp;"trainer.fit()"&nbsp;is&nbsp;a&nbsp;network&nbsp;that&nbsp;you<br>
want&nbsp;Lightning&nbsp;to&nbsp;train;&nbsp;this&nbsp;model&nbsp;must&nbsp;be&nbsp;subclassed&nbsp;from&nbsp;the&nbsp;class<br>
'Lightning.LightningModule'.&nbsp;Starting&nbsp;with&nbsp;version&nbsp;1.0.8,&nbsp;in&nbsp;babyGPT,&nbsp;it&nbsp;is&nbsp;the&nbsp;class<br>
MasterDecoderWithMasking&nbsp;that&nbsp;is&nbsp;derived&nbsp;from&nbsp;'Lightning.LightningModule'.<br>
&nbsp;<br>
If&nbsp;you&nbsp;have&nbsp;been&nbsp;following&nbsp;the&nbsp;evolution&nbsp;of&nbsp;babyGPT,&nbsp;you&nbsp;will&nbsp;see&nbsp;a&nbsp;sea&nbsp;change<br>
between&nbsp;the&nbsp;versions&nbsp;1.0.7&nbsp;and&nbsp;1.0.8&nbsp;for&nbsp;how&nbsp;the&nbsp;class&nbsp;MasterDecoderWithMasking&nbsp;is<br>
implemented.&nbsp;You&nbsp;see,&nbsp;a&nbsp;network&nbsp;that&nbsp;is&nbsp;meant&nbsp;to&nbsp;be&nbsp;learned&nbsp;with&nbsp;Lightning&nbsp;must&nbsp;have<br>
following&nbsp;methods&nbsp;defined&nbsp;for&nbsp;it:&nbsp;training_step(),&nbsp;train_dataloader(),&nbsp;and<br>
configure_optimizers().&nbsp;&nbsp;The&nbsp;first,&nbsp;training_step(),&nbsp;has&nbsp;the&nbsp;code&nbsp;that&nbsp;is&nbsp;meant&nbsp;to&nbsp;be<br>
executed&nbsp;at&nbsp;every&nbsp;iteration&nbsp;of&nbsp;training.&nbsp;&nbsp;The&nbsp;second,&nbsp;train_dataloader(),&nbsp;must&nbsp;return<br>
a&nbsp;dataloader&nbsp;that&nbsp;is&nbsp;implemented&nbsp;as&nbsp;a&nbsp;Python&nbsp;generator.&nbsp;&nbsp;That&nbsp;is,&nbsp;the&nbsp;dataloader&nbsp;must<br>
return&nbsp;a&nbsp;new&nbsp;batch&nbsp;through&nbsp;a&nbsp;'yield'&nbsp;statement&nbsp;in&nbsp;a&nbsp;'while'&nbsp;loop.&nbsp;&nbsp;Lightning&nbsp;can<br>
invoke&nbsp;the&nbsp;train_dataloader()&nbsp;automatically&nbsp;to&nbsp;fetch&nbsp;the&nbsp;next&nbsp;batch&nbsp;for&nbsp;a&nbsp;new<br>
training&nbsp;cycle.&nbsp;&nbsp;Finally,&nbsp;about&nbsp;the&nbsp;function&nbsp;configure_optimizers(),&nbsp;note&nbsp;that<br>
Lightning&nbsp;allows&nbsp;for&nbsp;only&nbsp;specific&nbsp;types&nbsp;of&nbsp;optimizers&nbsp;and&nbsp;learning-rate&nbsp;schedulers<br>
for&nbsp;the&nbsp;optimizers.<br>
&nbsp;<br>
In&nbsp;the&nbsp;code&nbsp;that&nbsp;follows,&nbsp;the&nbsp;required&nbsp;three&nbsp;functions&nbsp;named&nbsp;above&nbsp;are&nbsp;in&nbsp;the<br>
definition&nbsp;of&nbsp;the&nbsp;class&nbsp;MasterDecoderWithMasking.&nbsp;Note&nbsp;that&nbsp;it&nbsp;is&nbsp;the&nbsp;network&nbsp;created<br>
for&nbsp;this&nbsp;class&nbsp;that&nbsp;carries&nbsp;out&nbsp;autoregressive&nbsp;modeling&nbsp;of&nbsp;a&nbsp;text&nbsp;corpus.&nbsp;&nbsp;[In&nbsp;the<br>
previous&nbsp;version&nbsp;of&nbsp;the&nbsp;module,&nbsp;v.1.0.7,&nbsp;all&nbsp;of&nbsp;this&nbsp;code&nbsp;was&nbsp;in&nbsp;the&nbsp;method<br>
"run_code_with_buffered_context_for_training_TransformerFG"&nbsp;of&nbsp;the&nbsp;top-level&nbsp;babyGPT<br>
class.]&nbsp;<br>
&nbsp;<br>
In&nbsp;addition&nbsp;to&nbsp;the&nbsp;above,&nbsp;here&nbsp;are&nbsp;some&nbsp;pointers&nbsp;relevant&nbsp;to&nbsp;making&nbsp;a&nbsp;software&nbsp;module<br>
ready&nbsp;for&nbsp;Lightning:&nbsp;(1)&nbsp;For&nbsp;single&nbsp;GPU&nbsp;based&nbsp;processing,&nbsp;when&nbsp;you&nbsp;cast&nbsp;a&nbsp;tensor&nbsp;to<br>
type&nbsp;CUDA,&nbsp;it&nbsp;is&nbsp;obvious&nbsp;to&nbsp;the&nbsp;system&nbsp;that&nbsp;you&nbsp;want&nbsp;to&nbsp;prepare&nbsp;that&nbsp;tensor&nbsp;for&nbsp;its<br>
storage&nbsp;in&nbsp;the&nbsp;memory&nbsp;of&nbsp;the&nbsp;GPU&nbsp;that&nbsp;is&nbsp;available.&nbsp;&nbsp;However,&nbsp;when&nbsp;you&nbsp;have&nbsp;multiple<br>
GPUs,&nbsp;how&nbsp;do&nbsp;you&nbsp;cast&nbsp;a&nbsp;tensor&nbsp;to&nbsp;be&nbsp;of&nbsp;type&nbsp;CUDA&nbsp;for&nbsp;a&nbsp;specific&nbsp;GPU?&nbsp;&nbsp;Here&nbsp;is&nbsp;an<br>
example&nbsp;of&nbsp;how&nbsp;to&nbsp;best&nbsp;address&nbsp;this&nbsp;problem:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mask&nbsp;=&nbsp;torch.ones(1,&nbsp;device=input_sequence.device,&nbsp;dtype=torch.long)&nbsp;&nbsp;&nbsp;<br>
&nbsp;<br>
The&nbsp;goal&nbsp;here&nbsp;is&nbsp;to&nbsp;create&nbsp;a&nbsp;mask&nbsp;that&nbsp;is&nbsp;needed&nbsp;for&nbsp;autoregressive&nbsp;learning.&nbsp;Recall<br>
from&nbsp;my&nbsp;Week&nbsp;14&nbsp;lecture,&nbsp;autoregressive&nbsp;modeling&nbsp;is&nbsp;all&nbsp;about&nbsp;predicting&nbsp;the&nbsp;next<br>
token&nbsp;based&nbsp;on&nbsp;the&nbsp;tokens&nbsp;seen&nbsp;so&nbsp;far&nbsp;in&nbsp;the&nbsp;input.&nbsp;This&nbsp;requires&nbsp;progressively<br>
masking&nbsp;the&nbsp;input&nbsp;sequence&nbsp;to&nbsp;indicate&nbsp;to&nbsp;the&nbsp;model&nbsp;how&nbsp;much&nbsp;of&nbsp;the&nbsp;input&nbsp;sequence&nbsp;to<br>
use&nbsp;for&nbsp;the&nbsp;next&nbsp;token&nbsp;prediction.&nbsp;That&nbsp;is,&nbsp;for&nbsp;a&nbsp;fresh&nbsp;iteration&nbsp;of&nbsp;the&nbsp;training<br>
cycle,&nbsp;you&nbsp;will&nbsp;start&nbsp;with&nbsp;the&nbsp;mask&nbsp;consisting&nbsp;of&nbsp;a&nbsp;single&nbsp;1,&nbsp;which&nbsp;tells&nbsp;the&nbsp;model<br>
to&nbsp;make&nbsp;a&nbsp;prediction&nbsp;for&nbsp;the&nbsp;token&nbsp;at&nbsp;second&nbsp;position&nbsp;in&nbsp;the&nbsp;input&nbsp;based&nbsp;on&nbsp;its<br>
knowing&nbsp;just&nbsp;the&nbsp;first&nbsp;token.&nbsp;Subsequently,&nbsp;you&nbsp;will&nbsp;concatenate&nbsp;another&nbsp;1&nbsp;to&nbsp;the<br>
mask&nbsp;and&nbsp;now&nbsp;the&nbsp;model&nbsp;will&nbsp;try&nbsp;to&nbsp;make&nbsp;a&nbsp;prediction&nbsp;for&nbsp;the&nbsp;third&nbsp;token&nbsp;in&nbsp;the&nbsp;input<br>
based&nbsp;on&nbsp;the&nbsp;its&nbsp;knowledge&nbsp;of&nbsp;the&nbsp;first&nbsp;two&nbsp;tokens.&nbsp;&nbsp;In&nbsp;the&nbsp;code&nbsp;line&nbsp;shown&nbsp;above,<br>
what&nbsp;you&nbsp;see&nbsp;the&nbsp;mask&nbsp;being&nbsp;initialized&nbsp;with&nbsp;a&nbsp;single&nbsp;'1'&nbsp;for&nbsp;a&nbsp;new&nbsp;input&nbsp;sequence.<br>
&nbsp;<br>
The&nbsp;important&nbsp;point&nbsp;is&nbsp;that&nbsp;the&nbsp;initialization&nbsp;of&nbsp;the&nbsp;mask&nbsp;tensor&nbsp;shown&nbsp;above&nbsp;must<br>
take&nbsp;place&nbsp;separately&nbsp;for&nbsp;each&nbsp;of&nbsp;the&nbsp;GPUs&nbsp;available&nbsp;to&nbsp;Lightning.&nbsp;As&nbsp;it&nbsp;turns&nbsp;out,<br>
Lightning&nbsp;creates&nbsp;a&nbsp;separate&nbsp;process&nbsp;for&nbsp;each&nbsp;of&nbsp;the&nbsp;GPUs.&nbsp;The&nbsp;question&nbsp;then&nbsp;becomes:<br>
How&nbsp;to&nbsp;inform&nbsp;each&nbsp;process&nbsp;as&nbsp;to&nbsp;the&nbsp;type&nbsp;of&nbsp;the&nbsp;CUDA&nbsp;tensor&nbsp;being&nbsp;created&nbsp;as&nbsp;shown<br>
above.&nbsp;As&nbsp;you&nbsp;can&nbsp;see,&nbsp;it&nbsp;is&nbsp;done&nbsp;with&nbsp;the&nbsp;simple&nbsp;expedient&nbsp;of&nbsp;declaring&nbsp;the&nbsp;"device"<br>
to&nbsp;be&nbsp;the&nbsp;same&nbsp;as&nbsp;for&nbsp;the&nbsp;"input_sequence".&nbsp;&nbsp;That&nbsp;makes&nbsp;completely&nbsp;unambiguous&nbsp;the<br>
destination&nbsp;of&nbsp;the&nbsp;new&nbsp;mask&nbsp;tensor.&nbsp;Regarding&nbsp;the&nbsp;"input_sequence"&nbsp;tensor,&nbsp;for&nbsp;the<br>
DDP&nbsp;(Distributed&nbsp;Data&nbsp;Parallel)&nbsp;strategy,&nbsp;each&nbsp;GPU&nbsp;process&nbsp;runs&nbsp;the&nbsp;dataloader<br>
separately.&nbsp;&nbsp;Therefore,&nbsp;each&nbsp;process&nbsp;will&nbsp;create&nbsp;its&nbsp;own&nbsp;version&nbsp;of&nbsp;the<br>
"input_sequence"&nbsp;tensor.<br>
&nbsp;<br>
Another&nbsp;thing&nbsp;to&nbsp;bear&nbsp;in&nbsp;mind&nbsp;about&nbsp;Lightning&nbsp;is&nbsp;that&nbsp;it&nbsp;assumes&nbsp;that&nbsp;all&nbsp;of&nbsp;the<br>
tensors&nbsp;created&nbsp;in&nbsp;the&nbsp;implementation&nbsp;for&nbsp;"training_step()"&nbsp;are&nbsp;meant&nbsp;to&nbsp;be&nbsp;of&nbsp;type<br>
CUDA.&nbsp;So&nbsp;you&nbsp;are&nbsp;spared&nbsp;the&nbsp;need&nbsp;to&nbsp;append&nbsp;".cuda()"&nbsp;or&nbsp;".to(device)"&nbsp;to&nbsp;the&nbsp;tensor<br>
initializations&nbsp;as&nbsp;is&nbsp;common&nbsp;for&nbsp;the&nbsp;case&nbsp;of&nbsp;single-GPU&nbsp;based&nbsp;code.<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;<br>
<font size="+2" color="red">GETTING&nbsp;RID&nbsp;OF&nbsp;SUPERFLUOUS&nbsp;TOKENS<br>
</font>&nbsp;<br>
Version&nbsp;1.1.1&nbsp;includes&nbsp;a&nbsp;new&nbsp;function&nbsp;named&nbsp;"post_training_cleanup()"&nbsp;defined&nbsp;for&nbsp;the<br>
TrainTokenizer&nbsp;class&nbsp;that&nbsp;you&nbsp;can&nbsp;invoke&nbsp;after&nbsp;you&nbsp;have&nbsp;trained&nbsp;a&nbsp;tokenizer&nbsp;in&nbsp;order<br>
to&nbsp;get&nbsp;rid&nbsp;of&nbsp;superfluous&nbsp;tokens.&nbsp;&nbsp;A&nbsp;token&nbsp;A&nbsp;is&nbsp;superfluous&nbsp;vis-a-vis&nbsp;another&nbsp;token&nbsp;B<br>
if&nbsp;A&nbsp;is&nbsp;a&nbsp;substring&nbsp;of&nbsp;B&nbsp;and&nbsp;if&nbsp;the&nbsp;number&nbsp;of&nbsp;the&nbsp;corpus&nbsp;words&nbsp;that&nbsp;contain&nbsp;the<br>
tokens&nbsp;A&nbsp;and&nbsp;B&nbsp;are&nbsp;exactly&nbsp;the&nbsp;same.&nbsp;&nbsp;<br>
&nbsp;<br>
When&nbsp;you&nbsp;invoke&nbsp;the&nbsp;function&nbsp;"post_training_cleanup()",&nbsp;the&nbsp;cleaned-up&nbsp;tokenizer&nbsp;JSON<br>
is&nbsp;deposited&nbsp;in&nbsp;the&nbsp;subdirectory:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;cleaned_tokenizer_outputs<br>
&nbsp;<br>
Ordinarily,&nbsp;the&nbsp;tokenizer&nbsp;JSON&nbsp;that&nbsp;is&nbsp;produced&nbsp;by&nbsp;the&nbsp;function&nbsp;"train_tokenizer()"&nbsp;<br>
is&nbsp;deposited&nbsp;in&nbsp;the&nbsp;subdirectory:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;tokenizer_outputs<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;<br>
<font size="+2" color="red">EXTENDING&nbsp;A&nbsp;PREVIOUSLY&nbsp;LEARNED&nbsp;TOKENIZER&nbsp;JSON:<br>
</font>&nbsp;<br>
Let's&nbsp;say&nbsp;you&nbsp;have&nbsp;trained&nbsp;a&nbsp;tokenizer&nbsp;with&nbsp;a&nbsp;target&nbsp;vocabulary&nbsp;of&nbsp;40,000&nbsp;and&nbsp;you<br>
want&nbsp;to&nbsp;extend&nbsp;the&nbsp;target&nbsp;vocabulary&nbsp;to,&nbsp;say,&nbsp;50,000&nbsp;without&nbsp;having&nbsp;to&nbsp;retrain&nbsp;the<br>
whole&nbsp;thing&nbsp;from&nbsp;scratch.&nbsp;&nbsp;How&nbsp;does&nbsp;one&nbsp;do&nbsp;that?&nbsp;&nbsp;It&nbsp;is&nbsp;an&nbsp;important&nbsp;question&nbsp;in&nbsp;a<br>
university&nbsp;lab&nbsp;because&nbsp;tokenizer&nbsp;training&nbsp;is&nbsp;CPU&nbsp;intensive&nbsp;and&nbsp;can&nbsp;take&nbsp;days&nbsp;depending<br>
on&nbsp;your&nbsp;hardware&nbsp;and&nbsp;the&nbsp;size&nbsp;of&nbsp;the&nbsp;target&nbsp;vocabulary.<br>
&nbsp;<br>
Version&nbsp;1.1.1&nbsp;allows&nbsp;you&nbsp;to&nbsp;extend&nbsp;the&nbsp;target&nbsp;vocabulary&nbsp;size&nbsp;for&nbsp;a&nbsp;previously<br>
trained&nbsp;tokenizer.&nbsp;&nbsp;It&nbsp;is&nbsp;accomplished&nbsp;with&nbsp;the&nbsp;function&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;extend_tokenizer_with_additional_training()<br>
&nbsp;<br>
defined&nbsp;for&nbsp;the&nbsp;inner&nbsp;class&nbsp;TrainTokenizer.&nbsp;The&nbsp;starting&nbsp;point&nbsp;for&nbsp;using&nbsp;this&nbsp;function<br>
should&nbsp;be&nbsp;the&nbsp;following&nbsp;script&nbsp;in&nbsp;the&nbsp;Examples&nbsp;directory:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;extend_previously_trained_tokenizer.py<br>
&nbsp;<br>
Note&nbsp;that&nbsp;this&nbsp;script&nbsp;expects&nbsp;to&nbsp;command-line&nbsp;arguments,&nbsp;with&nbsp;the&nbsp;first&nbsp;being&nbsp;the&nbsp;<br>
pathname&nbsp;to&nbsp;the&nbsp;previously&nbsp;trained&nbsp;JSON&nbsp;and&nbsp;the&nbsp;second&nbsp;the&nbsp;new&nbsp;target&nbsp;vocab&nbsp;size.<br>
Here's&nbsp;an&nbsp;example:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;python3&nbsp;&nbsp;&nbsp;extend_previously_trained_tokenizer.py&nbsp;&nbsp;&nbsp;tokenizer_outputs/112_babygpt_tokenizer_50002.json&nbsp;&nbsp;&nbsp;&nbsp;60000<br>
&nbsp;<br>
You'll&nbsp;obviously&nbsp;need&nbsp;to&nbsp;make&nbsp;sure&nbsp;that&nbsp;the&nbsp;numbers&nbsp;"112",&nbsp;"20025",&nbsp;and&nbsp;"60000"&nbsp;&nbsp;<br>
are&nbsp;specific&nbsp;to&nbsp;your&nbsp;request.<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;<br>
<font size="+2" color="red">TRAINING&nbsp;WITH&nbsp;GRADIENT&nbsp;ACCUMULATION:<br>
</font>&nbsp;<br>
With&nbsp;the&nbsp;hardware&nbsp;typically&nbsp;available&nbsp;in&nbsp;a&nbsp;university&nbsp;lab&nbsp;(say,&nbsp;you&nbsp;are&nbsp;using&nbsp;GPUs<br>
like&nbsp;NVIDIA&nbsp;A5000&nbsp;with&nbsp;24&nbsp;GB&nbsp;memory),&nbsp;you&nbsp;will&nbsp;find&nbsp;yourself&nbsp;trading&nbsp;off&nbsp;batch-size<br>
for&nbsp;max-sequence-length&nbsp;for&nbsp;the&nbsp;tokens&nbsp;you&nbsp;feed&nbsp;into&nbsp;the&nbsp;transformer&nbsp;(while&nbsp;also<br>
taking&nbsp;into&nbsp;account&nbsp;the&nbsp;embedding&nbsp;size).&nbsp;&nbsp;Batch&nbsp;size&nbsp;plays&nbsp;a&nbsp;critical&nbsp;role&nbsp;in<br>
learning&nbsp;and&nbsp;you&nbsp;want&nbsp;it&nbsp;to&nbsp;be&nbsp;as&nbsp;large&nbsp;as&nbsp;possible,&nbsp;but&nbsp;not&nbsp;at&nbsp;the&nbsp;cost&nbsp;of&nbsp;making<br>
max-sequence-length&nbsp;too&nbsp;small&nbsp;to&nbsp;be&nbsp;useful.&nbsp;&nbsp;In&nbsp;Version&nbsp;1.1.2,&nbsp;I&nbsp;was&nbsp;able&nbsp;to&nbsp;use&nbsp;a<br>
batch-size&nbsp;of&nbsp;50&nbsp;for&nbsp;a&nbsp;max-sequence-length&nbsp;of&nbsp;50&nbsp;tokens&nbsp;(and&nbsp;with&nbsp;the&nbsp;embedding&nbsp;size<br>
set&nbsp;to&nbsp;384).<br>
&nbsp;<br>
How&nbsp;does&nbsp;one&nbsp;get&nbsp;past&nbsp;the&nbsp;constraints&nbsp;described&nbsp;above&nbsp;and&nbsp;give&nbsp;demonstrations&nbsp;with<br>
longer&nbsp;token&nbsp;sequence?&nbsp;&nbsp;Gradient&nbsp;accumulation&nbsp;is&nbsp;one&nbsp;answer.&nbsp;&nbsp;Gradient&nbsp;accumulation<br>
allows&nbsp;you&nbsp;to&nbsp;reduce&nbsp;the&nbsp;batch-size&nbsp;and&nbsp;increase&nbsp;the&nbsp;maximum&nbsp;sequence&nbsp;length&nbsp;without<br>
losing&nbsp;learning&nbsp;effectiveness.&nbsp;That&nbsp;is&nbsp;because&nbsp;you&nbsp;accumulate&nbsp;the&nbsp;backpropagated<br>
gradients&nbsp;over&nbsp;multiple&nbsp;steps&nbsp;of&nbsp;training&nbsp;before&nbsp;actually&nbsp;updating&nbsp;the&nbsp;learnable<br>
parameters.&nbsp;So&nbsp;your&nbsp;effective&nbsp;batch-size&nbsp;becomes&nbsp;the&nbsp;number&nbsp;of&nbsp;accumulation&nbsp;steps<br>
times&nbsp;the&nbsp;actual&nbsp;batch&nbsp;size.<br>
&nbsp;<br>
When&nbsp;using&nbsp;Lightning,&nbsp;it&nbsp;takes&nbsp;only&nbsp;one&nbsp;extra&nbsp;statement&nbsp;in&nbsp;your&nbsp;training&nbsp;code&nbsp;if&nbsp;you<br>
want&nbsp;to&nbsp;take&nbsp;advantage&nbsp;of&nbsp;gradient&nbsp;accumulation.&nbsp;&nbsp;That&nbsp;is&nbsp;because&nbsp;Lightning&nbsp;is&nbsp;happy<br>
to&nbsp;take&nbsp;care&nbsp;of&nbsp;the&nbsp;rest&nbsp;of&nbsp;the&nbsp;details&nbsp;under&nbsp;the&nbsp;hood.&nbsp;&nbsp;However,&nbsp;as&nbsp;I&nbsp;have&nbsp;explained<br>
in&nbsp;the&nbsp;doc-string&nbsp;associated&nbsp;with&nbsp;the&nbsp;TokenStreamDataset&nbsp;class,&nbsp;babyGPT&nbsp;was&nbsp;designed<br>
specifically&nbsp;to&nbsp;work&nbsp;with&nbsp;streaming&nbsp;training&nbsp;datasets.&nbsp;&nbsp;True&nbsp;streaming&nbsp;datasets&nbsp;are<br>
of&nbsp;type&nbsp;IterableDataset&nbsp;and&nbsp;they&nbsp;do&nbsp;NOT&nbsp;lend&nbsp;themselves&nbsp;to&nbsp;distributed&nbsp;sampling&nbsp;that<br>
is&nbsp;the&nbsp;forte&nbsp;of&nbsp;PyTorch&nbsp;Lightning.&nbsp;In&nbsp;such&nbsp;cases,&nbsp;it&nbsp;takes&nbsp;a&nbsp;little&nbsp;bit&nbsp;more&nbsp;work&nbsp;to<br>
take&nbsp;advantage&nbsp;of&nbsp;gradient&nbsp;accumulation&nbsp;during&nbsp;training.&nbsp;&nbsp;In&nbsp;babyGPT,&nbsp;you&nbsp;will&nbsp;see<br>
the&nbsp;extra&nbsp;statements&nbsp;for&nbsp;gradient&nbsp;accumulation&nbsp;if&nbsp;you&nbsp;search&nbsp;for&nbsp;the&nbsp;string<br>
"gradient_accumulation_steps".&nbsp;If,&nbsp;say,&nbsp;you&nbsp;have&nbsp;set&nbsp;gradient_accumulation_steps&nbsp;to<br>
2,&nbsp;the&nbsp;backproped&nbsp;gradients&nbsp;would&nbsp;be&nbsp;accumulated&nbsp;over&nbsp;2&nbsp;steps&nbsp;before&nbsp;the&nbsp;accumulated<br>
values&nbsp;are&nbsp;used&nbsp;to&nbsp;update&nbsp;the&nbsp;model&nbsp;parameters.<br>
&nbsp;<br>
Beware&nbsp;that&nbsp;there&nbsp;is&nbsp;a&nbsp;price&nbsp;to&nbsp;pay&nbsp;for&nbsp;using&nbsp;gradient&nbsp;accumulation&nbsp;---&nbsp;your&nbsp;training<br>
time&nbsp;will&nbsp;go&nbsp;up&nbsp;by&nbsp;approximately&nbsp;the&nbsp;same&nbsp;factor&nbsp;as&nbsp;the&nbsp;number&nbsp;of&nbsp;step&nbsp;you&nbsp;are&nbsp;using<br>
for&nbsp;accumulation.&nbsp;&nbsp;Let's&nbsp;say&nbsp;that&nbsp;without&nbsp;gradient&nbsp;accumulation&nbsp;it&nbsp;takes&nbsp;you&nbsp;two&nbsp;or<br>
three&nbsp;days&nbsp;of&nbsp;training&nbsp;before&nbsp;you&nbsp;start&nbsp;seeing&nbsp;some&nbsp;evidence&nbsp;of&nbsp;learning.&nbsp;&nbsp;If&nbsp;you<br>
decide&nbsp;to&nbsp;train&nbsp;babyGPT&nbsp;with&nbsp;"gradient_accumulation_steps"&nbsp;set&nbsp;to&nbsp;2,&nbsp;now&nbsp;it&nbsp;could<br>
take&nbsp;you&nbsp;the&nbsp;better&nbsp;part&nbsp;of&nbsp;a&nbsp;week&nbsp;of&nbsp;training&nbsp;before&nbsp;you&nbsp;start&nbsp;seeing&nbsp;the&nbsp;same&nbsp;sort<br>
of&nbsp;evidence.<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;<br>
<font size="+2" color="red">EXAMPLES&nbsp;OF&nbsp;THE&nbsp;OUTPUT&nbsp;PRODUCED&nbsp;DURING&nbsp;TRAINING:<br>
</font>&nbsp;<br>
The&nbsp;examples&nbsp;in&nbsp;this&nbsp;section&nbsp;are&nbsp;based&nbsp;on&nbsp;the&nbsp;assumptions&nbsp;listed&nbsp;below.&nbsp;These<br>
examples&nbsp;were&nbsp;generated&nbsp;by&nbsp;Version&nbsp;1.1.2&nbsp;of&nbsp;babyGPT.&nbsp;&nbsp;As&nbsp;stated&nbsp;elsewhere,&nbsp;the&nbsp;use&nbsp;of<br>
gradient&nbsp;accumulation&nbsp;in&nbsp;Version&nbsp;1.1.3&nbsp;allows&nbsp;babyGPT&nbsp;to&nbsp;handle&nbsp;longer&nbsp;sequences&nbsp;than<br>
in&nbsp;the&nbsp;examples&nbsp;here.<br>
&nbsp;<br>
--&nbsp;&nbsp;You&nbsp;are&nbsp;using&nbsp;the&nbsp;"Athlete&nbsp;News"&nbsp;dataset&nbsp;that&nbsp;you&nbsp;can&nbsp;download&nbsp;from&nbsp;the&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;website&nbsp;at&nbsp;Purdue.&nbsp;&nbsp;As&nbsp;mentioned,&nbsp;this&nbsp;dataset&nbsp;was&nbsp;derived&nbsp;by&nbsp;Adrien&nbsp;Dubois&nbsp;from<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;2017-2018&nbsp;news&nbsp;reports.<br>
&nbsp;<br>
--&nbsp;&nbsp;You&nbsp;are&nbsp;training&nbsp;the&nbsp;model&nbsp;with&nbsp;the&nbsp;following&nbsp;config&nbsp;parameters:<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;GPUs&nbsp;used&nbsp;&nbsp;&nbsp;:&nbsp;&nbsp;&nbsp;TWO&nbsp;NVIDIA&nbsp;A5000&nbsp;(each&nbsp;with&nbsp;24&nbsp;GB)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Max&nbsp;Sequence&nbsp;Length&nbsp;&nbsp;&nbsp;:&nbsp;&nbsp;&nbsp;55<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Batch&nbsp;Size&nbsp;&nbsp;&nbsp;:&nbsp;&nbsp;&nbsp;50<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Embedding&nbsp;Size&nbsp;&nbsp;&nbsp;:&nbsp;&nbsp;&nbsp;384&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Transformer&nbsp;Stages&nbsp;&nbsp;&nbsp;:&nbsp;&nbsp;&nbsp;6&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Number&nbsp;of&nbsp;Attention&nbsp;Heads&nbsp;&nbsp;&nbsp;:&nbsp;&nbsp;&nbsp;8&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Learning&nbsp;Rate&nbsp;&nbsp;&nbsp;:&nbsp;&nbsp;&nbsp;10^-4&nbsp;&nbsp;(with&nbsp;Cosine&nbsp;scheduler)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Warmup&nbsp;Steps&nbsp;&nbsp;&nbsp;:&nbsp;&nbsp;&nbsp;4000<br>
&nbsp;<br>
&nbsp;<br>
--&nbsp;&nbsp;You&nbsp;are&nbsp;running&nbsp;the&nbsp;following&nbsp;script&nbsp;in&nbsp;the&nbsp;Examples&nbsp;directory&nbsp;of&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;distribution:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;create_base_model_with_buffered_context.py&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
The&nbsp;script&nbsp;named&nbsp;above&nbsp;will&nbsp;show&nbsp;the&nbsp;following&nbsp;sort&nbsp;of&nbsp;output&nbsp;every&nbsp;100&nbsp;iterations.<br>
For&nbsp;each&nbsp;output,&nbsp;it&nbsp;will&nbsp;randomly&nbsp;select&nbsp;four&nbsp;out&nbsp;of&nbsp;for&nbsp;50&nbsp;batch&nbsp;instances&nbsp;and&nbsp;present<br>
the&nbsp;information&nbsp;arranged&nbsp;as&nbsp;follows:<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Ground-truth<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;GT&nbsp;Token&nbsp;Seq<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Predicted<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Detokenized<br>
&nbsp;<br>
&nbsp;<br>
where&nbsp;"Ground-truth"&nbsp;is&nbsp;a&nbsp;segment&nbsp;of&nbsp;the&nbsp;text&nbsp;extracted&nbsp;from&nbsp;the&nbsp;downloader&nbsp;for&nbsp;each<br>
batch&nbsp;instance;&nbsp;"GT&nbsp;Token&nbsp;Seq"&nbsp;is&nbsp;tokenized&nbsp;version&nbsp;of&nbsp;the&nbsp;ground-truth;&nbsp;"Predicted"<br>
is&nbsp;the&nbsp;sequence&nbsp;of&nbsp;predictions&nbsp;at&nbsp;each&nbsp;token&nbsp;position&nbsp;by&nbsp;the&nbsp;transformer;&nbsp;and<br>
"Detokenized"&nbsp;is&nbsp;the&nbsp;output&nbsp;of&nbsp;the&nbsp;decoder&nbsp;that&nbsp;joins&nbsp;the&nbsp;tokens&nbsp;back&nbsp;into&nbsp;words.&nbsp;&nbsp;To<br>
help&nbsp;out&nbsp;with&nbsp;detokenization,&nbsp;babyGPT&nbsp;inserts&nbsp;underscores&nbsp;between&nbsp;the&nbsp;whole&nbsp;words<br>
that&nbsp;are&nbsp;dropped&nbsp;in&nbsp;the&nbsp;detokenization&nbsp;step.&nbsp;&nbsp;You&nbsp;can&nbsp;see&nbsp;these&nbsp;underscores&nbsp;in&nbsp;the<br>
outputs&nbsp;shown&nbsp;for&nbsp;"GT&nbsp;Token&nbsp;Seq".<br>
&nbsp;<br>
In&nbsp;the&nbsp;rest&nbsp;of&nbsp;this&nbsp;section,&nbsp;I&nbsp;have&nbsp;displayed&nbsp;an&nbsp;example&nbsp;of&nbsp;the&nbsp;output&nbsp;produced&nbsp;for<br>
each&nbsp;epoch:<br>
&nbsp;<br>
&nbsp;<br>
<font size="+1" color="blue">Epoch&nbsp;0&nbsp;example:<br>
</font>&nbsp;<br>
Ground-truth:&nbsp;&nbsp;&nbsp;It&nbsp;was&nbsp;just&nbsp;the&nbsp;Lakers&nbsp;&nbsp;third&nbsp;loss&nbsp;in&nbsp;11&nbsp;games&nbsp;since&nbsp;the&nbsp;All&nbsp;-&nbsp;Star&nbsp;break&nbsp;,&nbsp;&nbsp;but&nbsp;their&nbsp;fourth&nbsp;against&nbsp;the&nbsp;Warriors_<br>
GT&nbsp;Token&nbsp;Seq:&nbsp;.&nbsp;&nbsp;It&nbsp;_&nbsp;was&nbsp;_&nbsp;just&nbsp;_&nbsp;the&nbsp;_&nbsp;Lakers&nbsp;&nbsp;third&nbsp;_&nbsp;loss&nbsp;_&nbsp;in&nbsp;_&nbsp;11&nbsp;_&nbsp;games&nbsp;_&nbsp;since&nbsp;_&nbsp;the&nbsp;_&nbsp;All&nbsp;-&nbsp;Star&nbsp;_&nbsp;break&nbsp;,&nbsp;but&nbsp;_&nbsp;the&nbsp;ir&nbsp;_&nbsp;fourth&nbsp;_&nbsp;again&nbsp;st&nbsp;_&nbsp;the&nbsp;_&nbsp;War&nbsp;ri&nbsp;or&nbsp;s&nbsp;_<br>
&nbsp;&nbsp;&nbsp;Predicted:&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_<br>
&nbsp;Detokenized:&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_&nbsp;_<br>
&nbsp;<br>
&nbsp;<br>
<font size="+1" color="blue">Epoch&nbsp;1&nbsp;example:<br>
</font>&nbsp;<br>
Ground-truth:&nbsp;_with&nbsp;a&nbsp;seven&nbsp;-&nbsp;point&nbsp;lead&nbsp;.&nbsp;&nbsp;However&nbsp;,&nbsp;&nbsp;the&nbsp;Warriors&nbsp;caught&nbsp;up&nbsp;soon&nbsp;enough&nbsp;and&nbsp;then&nbsp;left&nbsp;them&nbsp;in&nbsp;a&nbsp;trail&nbsp;of&nbsp;smoke<br>
GT&nbsp;Token&nbsp;Seq:&nbsp;_&nbsp;with&nbsp;_&nbsp;a&nbsp;_&nbsp;seven&nbsp;-&nbsp;point&nbsp;_&nbsp;lead&nbsp;.&nbsp;How&nbsp;ever&nbsp;,&nbsp;the&nbsp;_&nbsp;War&nbsp;ri&nbsp;or&nbsp;s&nbsp;_&nbsp;caught&nbsp;_&nbsp;up&nbsp;_&nbsp;soon&nbsp;_&nbsp;enough&nbsp;_&nbsp;an&nbsp;d&nbsp;_&nbsp;then&nbsp;_&nbsp;left&nbsp;_&nbsp;them&nbsp;_&nbsp;in&nbsp;_&nbsp;a&nbsp;_&nbsp;tra&nbsp;il&nbsp;_&nbsp;of&nbsp;_&nbsp;smo&nbsp;ke<br>
&nbsp;&nbsp;&nbsp;Predicted:&nbsp;_&nbsp;of&nbsp;_&nbsp;a&nbsp;_&nbsp;two&nbsp;-&nbsp;time&nbsp;_&nbsp;lead&nbsp;_&nbsp;The&nbsp;ever,&nbsp;the&nbsp;_&nbsp;War&nbsp;ri&nbsp;or&nbsp;s&nbsp;_&nbsp;are&nbsp;_&nbsp;up&nbsp;_&nbsp;a&nbsp;_&nbsp;to&nbsp;_&nbsp;to&nbsp;d&nbsp;_&nbsp;the&nbsp;_&nbsp;the&nbsp;_&nbsp;the&nbsp;_&nbsp;to&nbsp;_&nbsp;the&nbsp;_&nbsp;row&nbsp;_&nbsp;_&nbsp;of&nbsp;_&nbsp;the&nbsp;y<br>
&nbsp;Detokenized:&nbsp;_of&nbsp;a&nbsp;two&nbsp;-&nbsp;time&nbsp;lead&nbsp;Theever,&nbsp;&nbsp;the&nbsp;Warriors&nbsp;are&nbsp;up&nbsp;a&nbsp;to&nbsp;tod&nbsp;the&nbsp;the&nbsp;the&nbsp;to&nbsp;the&nbsp;row&nbsp;_of&nbsp;they<br>
&nbsp;<br>
&nbsp;<br>
<font size="+1" color="blue">Epoch&nbsp;2&nbsp;example:<br>
</font>&nbsp;<br>
Ground-truth:&nbsp;Feb&nbsp;.&nbsp;&nbsp;2&nbsp;,&nbsp;&nbsp;2018&nbsp;,&nbsp;&nbsp;in&nbsp;Sacramento&nbsp;,&nbsp;&nbsp;Calif&nbsp;.&nbsp;&nbsp;(&nbsp;AP&nbsp;Photo&nbsp;/&nbsp;Rich&nbsp;Pedroncelli&nbsp;)&nbsp;Golden&nbsp;State&nbsp;Warriors&nbsp;guard&nbsp;Stephen&nbsp;Curry&nbsp;goes&nbsp;to&nbsp;the<br>
GT&nbsp;Token&nbsp;Seq:&nbsp;Fe&nbsp;b&nbsp;.&nbsp;2&nbsp;,&nbsp;2018&nbsp;,&nbsp;in&nbsp;_&nbsp;Sacramento&nbsp;,&nbsp;Cali&nbsp;f&nbsp;.&nbsp;(&nbsp;AP&nbsp;_&nbsp;Photo&nbsp;/&nbsp;Rich&nbsp;_&nbsp;Pe&nbsp;d&nbsp;ron&nbsp;cel&nbsp;li&nbsp;)&nbsp;Golden&nbsp;_&nbsp;State&nbsp;_&nbsp;War&nbsp;ri&nbsp;or&nbsp;s&nbsp;_&nbsp;guar&nbsp;d&nbsp;_&nbsp;Stephen&nbsp;_&nbsp;Curry&nbsp;_&nbsp;go&nbsp;es&nbsp;_&nbsp;to&nbsp;_&nbsp;the<br>
&nbsp;&nbsp;&nbsp;Predicted:&nbsp;Fe&nbsp;b.&nbsp;22,&nbsp;2018.&nbsp;in&nbsp;_&nbsp;the,&nbsp;Cali&nbsp;f.&nbsp;(&nbsp;AP&nbsp;_&nbsp;Photo&nbsp;/&nbsp;Mark&nbsp;_&nbsp;J&nbsp;der&nbsp;le&nbsp;_&nbsp;_&nbsp;).&nbsp;_&nbsp;State&nbsp;_&nbsp;War&nbsp;ri&nbsp;or&nbsp;s&nbsp;_&nbsp;forward&nbsp;d&nbsp;_&nbsp;Stephen&nbsp;_&nbsp;Curry&nbsp;_&nbsp;(&nbsp;_&nbsp;_&nbsp;to&nbsp;_&nbsp;the<br>
&nbsp;Detokenized:&nbsp;Feb.&nbsp;&nbsp;22,&nbsp;&nbsp;2018.&nbsp;&nbsp;in&nbsp;the,&nbsp;&nbsp;Calif.&nbsp;&nbsp;(&nbsp;AP&nbsp;Photo&nbsp;/&nbsp;Mark&nbsp;Jderle&nbsp;_&nbsp;).&nbsp;State&nbsp;Warriors&nbsp;forwardd&nbsp;Stephen&nbsp;Curry&nbsp;(&nbsp;_to&nbsp;the<br>
&nbsp;<br>
&nbsp;<br>
<font size="+1" color="blue">Epoch&nbsp;3&nbsp;example:<br>
</font>&nbsp;<br>
Ground-truth:&nbsp;got&nbsp;a&nbsp;Finals&nbsp;MVP&nbsp;[&nbsp;Andre&nbsp;Iguodala&nbsp;]&nbsp;that&nbsp;comes&nbsp;off&nbsp;their&nbsp;bench&nbsp;.&nbsp;&nbsp;&nbsp;James&nbsp;pointed&nbsp;out&nbsp;that&nbsp;the&nbsp;Warriors&nbsp;are<br>
GT&nbsp;Token&nbsp;Seq:&nbsp;got&nbsp;_&nbsp;a&nbsp;_&nbsp;Finals&nbsp;_&nbsp;MVP&nbsp;_&nbsp;[&nbsp;An&nbsp;d&nbsp;re&nbsp;_&nbsp;Igu&nbsp;od&nbsp;al&nbsp;a&nbsp;]&nbsp;that&nbsp;_&nbsp;co&nbsp;mes&nbsp;_&nbsp;off&nbsp;_&nbsp;the&nbsp;ir&nbsp;_&nbsp;bench&nbsp;.&nbsp;&nbsp;James&nbsp;_&nbsp;poin&nbsp;te&nbsp;d&nbsp;_&nbsp;out&nbsp;_&nbsp;that&nbsp;_&nbsp;the&nbsp;_&nbsp;War&nbsp;ri&nbsp;or&nbsp;s&nbsp;_&nbsp;are<br>
&nbsp;&nbsp;&nbsp;Predicted:&nbsp;got&nbsp;_&nbsp;to&nbsp;_&nbsp;lot&nbsp;_&nbsp;MVP&nbsp;_&nbsp;an&nbsp;Stephen&nbsp;d&nbsp;re&nbsp;_&nbsp;Igu&nbsp;od&nbsp;al&nbsp;a&nbsp;_,&nbsp;_&nbsp;is&nbsp;mes&nbsp;_&nbsp;to&nbsp;_&nbsp;the&nbsp;_&nbsp;_&nbsp;season&nbsp;_&nbsp;The&nbsp;The&nbsp;_&nbsp;is&nbsp;te&nbsp;d&nbsp;_&nbsp;to&nbsp;_&nbsp;the&nbsp;_&nbsp;the&nbsp;_&nbsp;War&nbsp;ri&nbsp;or&nbsp;s&nbsp;_&nbsp;have<br>
&nbsp;Detokenized:&nbsp;got&nbsp;to&nbsp;lot&nbsp;MVP&nbsp;anStephendre&nbsp;Iguodala_,&nbsp;ismes&nbsp;to&nbsp;the&nbsp;_season&nbsp;TheThe&nbsp;isted&nbsp;to&nbsp;the&nbsp;the&nbsp;Warriors&nbsp;have<br>
&nbsp;<br>
&nbsp;<br>
<font size="+1" color="blue">Epoch&nbsp;4&nbsp;example:<br>
</font>&nbsp;<br>
Ground-truth:&nbsp;_Warriors&nbsp;(&nbsp;58&nbsp;-&nbsp;24&nbsp;)&nbsp;vs&nbsp;.&nbsp;&nbsp;No&nbsp;.&nbsp;&nbsp;7&nbsp;San&nbsp;Antonio&nbsp;Spurs&nbsp;(&nbsp;47&nbsp;-&nbsp;35&nbsp;)&nbsp;How&nbsp;to&nbsp;watch&nbsp;Game&nbsp;5&nbsp;Date&nbsp;:&nbsp;Tuesday&nbsp;,&nbsp;&nbsp;April&nbsp;24_<br>
GT&nbsp;Token&nbsp;Seq:&nbsp;_&nbsp;War&nbsp;ri&nbsp;or&nbsp;s&nbsp;_&nbsp;(&nbsp;58&nbsp;-&nbsp;24&nbsp;)&nbsp;vs&nbsp;.&nbsp;No&nbsp;.&nbsp;7&nbsp;_&nbsp;San&nbsp;_&nbsp;Antonio&nbsp;_&nbsp;Spurs&nbsp;_&nbsp;(&nbsp;4&nbsp;7&nbsp;-&nbsp;35&nbsp;)&nbsp;How&nbsp;_&nbsp;to&nbsp;_&nbsp;watch&nbsp;_&nbsp;Ga&nbsp;me&nbsp;_&nbsp;5&nbsp;_&nbsp;Da&nbsp;te&nbsp;:&nbsp;Tuesday&nbsp;,&nbsp;April&nbsp;_&nbsp;24&nbsp;_<br>
&nbsp;&nbsp;&nbsp;Predicted:&nbsp;_&nbsp;War&nbsp;ri&nbsp;or&nbsp;s&nbsp;_&nbsp;in&nbsp;1&nbsp;-&nbsp;0&nbsp;)..&nbsp;Cleveland&nbsp;_&nbsp;1&nbsp;_&nbsp;seed&nbsp;_&nbsp;Antonio&nbsp;_&nbsp;Spurs&nbsp;:&nbsp;(&nbsp;10&nbsp;)&nbsp;)&nbsp;3&nbsp;)&nbsp;an&nbsp;_&nbsp;did&nbsp;_&nbsp;watch&nbsp;_&nbsp;the&nbsp;me&nbsp;_&nbsp;1&nbsp;_&nbsp;of&nbsp;vi&nbsp;_&nbsp;"&nbsp;_&nbsp;June&nbsp;_&nbsp;22,<br>
&nbsp;Detokenized:&nbsp;_Warriors&nbsp;in1&nbsp;-&nbsp;0&nbsp;).&nbsp;&nbsp;Cleveland&nbsp;1&nbsp;seed&nbsp;Antonio&nbsp;Spurs&nbsp;:&nbsp;(&nbsp;10&nbsp;)&nbsp;)&nbsp;3&nbsp;)&nbsp;an&nbsp;did&nbsp;watch&nbsp;theme&nbsp;1&nbsp;ofvi&nbsp;"&nbsp;June&nbsp;22,<br>
&nbsp;<br>
&nbsp;<br>
<font size="+1" color="blue">Epoch&nbsp;5&nbsp;example:<br>
</font>&nbsp;<br>
Ground-truth:&nbsp;Kevin&nbsp;Durant&nbsp;and&nbsp;Klay&nbsp;Thompson&nbsp;,&nbsp;&nbsp;the&nbsp;Warriors&nbsp;lost&nbsp;Draymond&nbsp;Green&nbsp;in&nbsp;the&nbsp;second&nbsp;quarter&nbsp;to&nbsp;a&nbsp;bruise&nbsp;in_<br>
GT&nbsp;Token&nbsp;Seq:&nbsp;K&nbsp;ev&nbsp;in&nbsp;_&nbsp;Durant&nbsp;_&nbsp;an&nbsp;d&nbsp;_&nbsp;K&nbsp;lay&nbsp;_&nbsp;Thom&nbsp;p&nbsp;son&nbsp;,&nbsp;the&nbsp;_&nbsp;War&nbsp;ri&nbsp;or&nbsp;s&nbsp;_&nbsp;lo&nbsp;st&nbsp;_&nbsp;Dra&nbsp;y&nbsp;mon&nbsp;d&nbsp;_&nbsp;Green&nbsp;_&nbsp;in&nbsp;_&nbsp;the&nbsp;_&nbsp;second&nbsp;_&nbsp;quarter&nbsp;_&nbsp;to&nbsp;_&nbsp;a&nbsp;_&nbsp;bruise&nbsp;_&nbsp;in&nbsp;_<br>
&nbsp;&nbsp;&nbsp;Predicted:&nbsp;K&nbsp;lay&nbsp;in&nbsp;_&nbsp;Durant&nbsp;_&nbsp;an&nbsp;d&nbsp;_&nbsp;Dra&nbsp;lay&nbsp;_&nbsp;Thom&nbsp;p&nbsp;son&nbsp;_&nbsp;Dra&nbsp;_&nbsp;War&nbsp;ri&nbsp;or&nbsp;s&nbsp;_&nbsp;an&nbsp;st&nbsp;_&nbsp;to&nbsp;y&nbsp;mon&nbsp;d&nbsp;_&nbsp;Green&nbsp;_&nbsp;an&nbsp;_&nbsp;the&nbsp;_&nbsp;second&nbsp;_&nbsp;half&nbsp;_&nbsp;to&nbsp;_&nbsp;a&nbsp;_&nbsp;four&nbsp;_&nbsp;in&nbsp;_<br>
&nbsp;Detokenized:&nbsp;Klayin&nbsp;Durant&nbsp;and&nbsp;Dralay&nbsp;Thompson&nbsp;Dra&nbsp;Warriors&nbsp;anst&nbsp;toymond&nbsp;Green&nbsp;an&nbsp;the&nbsp;second&nbsp;half&nbsp;to&nbsp;a&nbsp;four&nbsp;in_<br>
&nbsp;<br>
&nbsp;<br>
<font size="+1" color="blue">...and&nbsp;so&nbsp;on<br>
</font>&nbsp;<br>
&nbsp;<br>
&nbsp;<br>
For&nbsp;truth&nbsp;in&nbsp;advertising,&nbsp;I&nbsp;must&nbsp;hasten&nbsp;to&nbsp;add&nbsp;that,&nbsp;in&nbsp;what&nbsp;you&nbsp;see&nbsp;above,&nbsp;I&nbsp;have<br>
chosen&nbsp;some&nbsp;of&nbsp;the&nbsp;better&nbsp;looking&nbsp;examples&nbsp;for&nbsp;each&nbsp;epoch.&nbsp;&nbsp;For&nbsp;every&nbsp;good&nbsp;output<br>
example&nbsp;like&nbsp;those&nbsp;shown&nbsp;above,&nbsp;you&nbsp;will&nbsp;see&nbsp;a&nbsp;large&nbsp;number&nbsp;meaningless&nbsp;gibberish<br>
examples.&nbsp;&nbsp;As&nbsp;you&nbsp;would&nbsp;expect,&nbsp;at&nbsp;the&nbsp;beginning&nbsp;of&nbsp;training,&nbsp;all&nbsp;output&nbsp;is&nbsp;mostly<br>
gibberish.&nbsp;&nbsp;However,&nbsp;as&nbsp;the&nbsp;training&nbsp;continues,&nbsp;you&nbsp;begin&nbsp;to&nbsp;see&nbsp;more&nbsp;and&nbsp;more<br>
examples&nbsp;of&nbsp;the&nbsp;output&nbsp;that&nbsp;makes&nbsp;sense.<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;<br>
<font size="+2" color="red">INSTALLATION:<br>
</font>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;babyGPT&nbsp;class&nbsp;was&nbsp;packaged&nbsp;using&nbsp;setuptools.&nbsp;&nbsp;For&nbsp;installation,&nbsp;execute<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;following&nbsp;command&nbsp;in&nbsp;the&nbsp;source&nbsp;directory&nbsp;(this&nbsp;is&nbsp;the&nbsp;directory&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;contains&nbsp;the&nbsp;setup.py&nbsp;file&nbsp;after&nbsp;you&nbsp;have&nbsp;downloaded&nbsp;and&nbsp;uncompressed&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;gzipped&nbsp;tar&nbsp;archive&nbsp;for&nbsp;the&nbsp;module):<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sudo&nbsp;python3&nbsp;setup.py&nbsp;install<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;On&nbsp;Linux&nbsp;distributions,&nbsp;this&nbsp;will&nbsp;install&nbsp;the&nbsp;module&nbsp;file&nbsp;at&nbsp;a&nbsp;location&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;looks&nbsp;like<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;/usr/local/lib/python3.10/dist-packages/<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;you&nbsp;do&nbsp;not&nbsp;have&nbsp;root&nbsp;access,&nbsp;you&nbsp;have&nbsp;the&nbsp;option&nbsp;of&nbsp;working&nbsp;directly&nbsp;off<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;directory&nbsp;in&nbsp;which&nbsp;you&nbsp;downloaded&nbsp;the&nbsp;software&nbsp;by&nbsp;simply&nbsp;placing&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;following&nbsp;statements&nbsp;at&nbsp;the&nbsp;top&nbsp;of&nbsp;your&nbsp;scripts&nbsp;that&nbsp;use&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;babyGPT&nbsp;class:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;import&nbsp;sys<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sys.path.append(&nbsp;"pathname_to_babyGPT_directory"&nbsp;)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;To&nbsp;uninstall&nbsp;the&nbsp;module,&nbsp;simply&nbsp;delete&nbsp;the&nbsp;source&nbsp;directory,&nbsp;locate&nbsp;where&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;babyGPT&nbsp;module&nbsp;was&nbsp;installed&nbsp;with&nbsp;"locate<br>
&nbsp;&nbsp;&nbsp;&nbsp;babyGPT"&nbsp;and&nbsp;delete&nbsp;those&nbsp;files.&nbsp;&nbsp;As&nbsp;mentioned&nbsp;above,&nbsp;the&nbsp;full<br>
&nbsp;&nbsp;&nbsp;&nbsp;pathname&nbsp;to&nbsp;the&nbsp;installed&nbsp;version&nbsp;is&nbsp;likely&nbsp;to&nbsp;look&nbsp;like<br>
&nbsp;&nbsp;&nbsp;&nbsp;/usr/local/lib/python2.7/dist-packages/babyGPT*<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;you&nbsp;want&nbsp;to&nbsp;carry&nbsp;out&nbsp;a&nbsp;non-standard&nbsp;install&nbsp;of&nbsp;the&nbsp;babyGPT<br>
&nbsp;&nbsp;&nbsp;&nbsp;module,&nbsp;look&nbsp;up&nbsp;the&nbsp;on-line&nbsp;information&nbsp;on&nbsp;Disutils&nbsp;by&nbsp;pointing&nbsp;your&nbsp;browser<br>
&nbsp;&nbsp;&nbsp;&nbsp;to<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="http://docs.python.org/dist/dist.html">http://docs.python.org/dist/dist.html</a><br>
&nbsp;<br>
<font size="+2" color="red">USAGE:<br>
</font>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;you&nbsp;want&nbsp;to&nbsp;use&nbsp;babyGPT&nbsp;for&nbsp;unsupervised&nbsp;learning&nbsp;of&nbsp;a&nbsp;base&nbsp;model&nbsp;for&nbsp;a&nbsp;text<br>
&nbsp;&nbsp;&nbsp;&nbsp;corpus,&nbsp;you&nbsp;would&nbsp;need&nbsp;to&nbsp;construct&nbsp;an&nbsp;instance&nbsp;of&nbsp;the&nbsp;main&nbsp;babyGPT&nbsp;class&nbsp;and&nbsp;its<br>
&nbsp;&nbsp;&nbsp;&nbsp;supporting&nbsp;classes&nbsp;as&nbsp;follows:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;baby_gpt&nbsp;=&nbsp;babyGPT(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;max_seq_length&nbsp;=&nbsp;max_seq_length,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;batch_size&nbsp;=&nbsp;batch_size,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;embedding_size&nbsp;=&nbsp;embedding_size,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;num_basic_decoders&nbsp;=&nbsp;num_basic_decoders,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;num_atten_heads&nbsp;=&nbsp;num_atten_heads,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;optimizer_params&nbsp;=&nbsp;optimizer_params,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;num_warmup_steps&nbsp;=&nbsp;num_warmup_steps,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;masking&nbsp;=&nbsp;masking,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;verify_text_corpus&nbsp;=&nbsp;False,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;path_saved_model&nbsp;=&nbsp;{"decoder"&nbsp;:&nbsp;"./saved_decoder",&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"embedding_generator"&nbsp;:&nbsp;"./saved_embedding_generator",&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;xformer&nbsp;=&nbsp;baby_gpt.TransformerFG(&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;max_seq_length&nbsp;=&nbsp;max_seq_length,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;embedding_size&nbsp;=&nbsp;embedding_size,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;tokenizer_json&nbsp;=&nbsp;tokenizer_json,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;num_warmup_steps&nbsp;=&nbsp;num_warmup_steps,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;optimizer_params&nbsp;=&nbsp;optimizer_params,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;master_decoder&nbsp;=&nbsp;baby_gpt.MasterDecoderWithMasking(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;xformer,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;num_basic_decoders&nbsp;=&nbsp;num_basic_decoders,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;num_atten_heads&nbsp;=&nbsp;num_atten_heads,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;context_window_size&nbsp;=&nbsp;context_window_size,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;context_buffer_size&nbsp;=&nbsp;context_buffer_size,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;batch_size&nbsp;=&nbsp;batch_size,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;gradient_accumulation_steps&nbsp;=&nbsp;gradient_accumulation_steps,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;masking&nbsp;=&nbsp;masking<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;dataloader&nbsp;=&nbsp;baby_gpt.ArticleDatasetWithBufferedContext(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;gpt&nbsp;=&nbsp;baby_gpt,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;tokenizer_json&nbsp;=&nbsp;tokenizer_json,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;context_window_size&nbsp;=&nbsp;context_window_size,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;context_buffer_size&nbsp;=&nbsp;context_buffer_size,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;articles_dir&nbsp;=&nbsp;articles_dir,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;<br>
<font size="+2" color="red">THE&nbsp;Examples&nbsp;DIRECTORY:<br>
</font>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;directory&nbsp;contains&nbsp;the&nbsp;following&nbsp;six&nbsp;scripts&nbsp;for&nbsp;working&nbsp;with&nbsp;babyGPT:<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.&nbsp;&nbsp;run_gatherer.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;script&nbsp;is&nbsp;for&nbsp;collecting&nbsp;a&nbsp;corpus&nbsp;for&nbsp;experimenting&nbsp;with&nbsp;babyGPT.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;script&nbsp;requires&nbsp;a&nbsp;list&nbsp;of&nbsp;URLs&nbsp;as&nbsp;article&nbsp;sources&nbsp;as&nbsp;illustrated<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;by&nbsp;the&nbsp;following&nbsp;example:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;urls&nbsp;=&nbsp;['<a href="https://finance.yahoo.com','http://cnn.com">https://finance.yahoo.com','http://cnn.com</a>',<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'<a href="https://sports.yahoo.com">https://sports.yahoo.com</a>',<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'<a href="https://purdueexponent.org','https://slate.com">https://purdueexponent.org','https://slate.com</a>',<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'<a href="https://timesofindia.indiatimes.com">https://timesofindia.indiatimes.com</a>',<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'<a href="http://cnn.com">http://cnn.com</a>',<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'<a href="https://slate.com">https://slate.com</a>'<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;]<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.&nbsp;&nbsp;train_tokenizer.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;the&nbsp;text&nbsp;corpus&nbsp;you&nbsp;have&nbsp;collected&nbsp;is&nbsp;for&nbsp;a&nbsp;specialized&nbsp;domain&nbsp;(such<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;as&nbsp;movies,&nbsp;sports,&nbsp;healthcare,&nbsp;etc.),&nbsp;you&nbsp;are&nbsp;likely&nbsp;to&nbsp;get&nbsp;better<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;results&nbsp;from&nbsp;babyGPT&nbsp;if&nbsp;you&nbsp;first&nbsp;train&nbsp;a&nbsp;new&nbsp;tokenizer&nbsp;for&nbsp;that&nbsp;domain.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;You&nbsp;train&nbsp;a&nbsp;new&nbsp;tokenizer&nbsp;merely&nbsp;by&nbsp;invoking&nbsp;this&nbsp;script&nbsp;after&nbsp;you&nbsp;have<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;set&nbsp;its&nbsp;variable&nbsp;"articles_dir"&nbsp;so&nbsp;that&nbsp;it&nbsp;points&nbsp;to&nbsp;the&nbsp;corpus&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;directory.<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.&nbsp;&nbsp;apply_tokenizer.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;you&nbsp;have&nbsp;created&nbsp;a&nbsp;new&nbsp;JSON&nbsp;file&nbsp;for&nbsp;the&nbsp;tokenizer,&nbsp;this&nbsp;script&nbsp;is<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;just&nbsp;to&nbsp;test&nbsp;the&nbsp;tokenizer&nbsp;on&nbsp;a&nbsp;small&nbsp;txt&nbsp;file.&nbsp;&nbsp;To&nbsp;get&nbsp;started&nbsp;with&nbsp;using<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;this&nbsp;script,&nbsp;try&nbsp;it&nbsp;out&nbsp;with&nbsp;the&nbsp;following&nbsp;command&nbsp;line:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;python3&nbsp;&nbsp;apply_tokenizer.py&nbsp;&nbsp;&nbsp;text_sample_for_testing.txt&nbsp;&nbsp;&nbsp;112_babygpt_tokenizer_50002.json<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;where&nbsp;the&nbsp;sample&nbsp;file&nbsp;"text_sample_for_testing.txt"&nbsp;should&nbsp;already&nbsp;be&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;Examples&nbsp;directory&nbsp;of&nbsp;the&nbsp;distro&nbsp;and&nbsp;where&nbsp;the&nbsp;last&nbsp;arg&nbsp;is&nbsp;the&nbsp;JSON<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;you&nbsp;are&nbsp;testing.&nbsp;&nbsp;Make&nbsp;sure&nbsp;the&nbsp;name&nbsp;of&nbsp;tokenizer&nbsp;JSON&nbsp;is&nbsp;what&nbsp;you&nbsp;are<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;testing.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.&nbsp;&nbsp;extend_previously_trained_tokenizer.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;You&nbsp;need&nbsp;to&nbsp;run&nbsp;this&nbsp;script&nbsp;only&nbsp;if&nbsp;you&nbsp;wish&nbsp;to&nbsp;extend&nbsp;a&nbsp;previously<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;trained&nbsp;tokenizer&nbsp;with&nbsp;a&nbsp;larger&nbsp;target&nbsp;vocabulary.&nbsp;&nbsp;Pay&nbsp;attention&nbsp;to&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;call&nbsp;syntax&nbsp;for&nbsp;this&nbsp;script&nbsp;since&nbsp;it&nbsp;expects&nbsp;command-line&nbsp;arguments.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Here&nbsp;is&nbsp;an&nbsp;example:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;python3&nbsp;&nbsp;&nbsp;extend_previously_trained_tokenizer.py&nbsp;&nbsp;&nbsp;tokenizer_outputs/112_babygpt_tokenizer_50002.json&nbsp;&nbsp;&nbsp;&nbsp;60000<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;which&nbsp;says&nbsp;you&nbsp;want&nbsp;to&nbsp;extend&nbsp;the&nbsp;JSON&nbsp;in&nbsp;the&nbsp;penultimate&nbsp;arg&nbsp;with&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;new&nbsp;target&nbsp;vocab&nbsp;size&nbsp;of&nbsp;60000.<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.&nbsp;&nbsp;create_base_model_with_buffered_context.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;is&nbsp;the&nbsp;script&nbsp;to&nbsp;run&nbsp;if&nbsp;you&nbsp;want&nbsp;to&nbsp;create&nbsp;a&nbsp;Base&nbsp;Model&nbsp;for&nbsp;your<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;corpus.&nbsp;&nbsp;By&nbsp;Base&nbsp;Model&nbsp;I&nbsp;mean&nbsp;a&nbsp;language&nbsp;model&nbsp;acquired&nbsp;through<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;unsupervised&nbsp;learning&nbsp;from&nbsp;a&nbsp;training&nbsp;corpus.&nbsp;&nbsp;Since&nbsp;this&nbsp;script&nbsp;calls&nbsp;on<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;core&nbsp;language&nbsp;modeling&nbsp;functionality&nbsp;of&nbsp;babyGPT,&nbsp;you&nbsp;have&nbsp;to&nbsp;set&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;relatively&nbsp;large&nbsp;number&nbsp;of&nbsp;parameters&nbsp;in&nbsp;the&nbsp;script.&nbsp;&nbsp;These&nbsp;parameters<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;shown&nbsp;below:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;articles_dir<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;tokenizer_json&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;max_seq_length&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;context_window_size<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;context_buffer_size<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;batch_size&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;embedding_size<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;num_atten_heads&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;num_basic_decoders&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;optimizer_params<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;num_warmup_steps<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6.&nbsp;&nbsp;interact_with_prompts.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;is&nbsp;the&nbsp;script&nbsp;for&nbsp;interacting&nbsp;with&nbsp;a&nbsp;trained&nbsp;babyGPT&nbsp;model&nbsp;through<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;prompts.&nbsp;&nbsp;The&nbsp;idea&nbsp;is&nbsp;that&nbsp;you&nbsp;supply&nbsp;a&nbsp;small&nbsp;number&nbsp;of&nbsp;words&nbsp;(as,&nbsp;say,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;beginning&nbsp;of&nbsp;a&nbsp;new&nbsp;thought)&nbsp;as&nbsp;a&nbsp;prompt&nbsp;and&nbsp;the&nbsp;model&nbsp;supplies&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rest&nbsp;of&nbsp;the&nbsp;words&nbsp;to&nbsp;complete&nbsp;the&nbsp;thought.&nbsp;&nbsp;At&nbsp;this&nbsp;time,&nbsp;the&nbsp;model<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;extends&nbsp;your&nbsp;prompt&nbsp;until&nbsp;it&nbsp;reaches&nbsp;a&nbsp;period&nbsp;(or&nbsp;the&nbsp;end&nbsp;dictated&nbsp;by&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;size&nbsp;of&nbsp;the&nbsp;"max_seq_length"&nbsp;parameter.<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;<br>
<font size="+2" color="red">THE&nbsp;TRAINING&nbsp;DATASETS&nbsp;PROVIDED:<br>
</font>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Click&nbsp;on&nbsp;the&nbsp;following&nbsp;link&nbsp;near&nbsp;the&nbsp;beginning&nbsp;of&nbsp;this&nbsp;documentation&nbsp;page:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"Download&nbsp;the&nbsp;text&nbsp;datasets&nbsp;for&nbsp;babyGPT"&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;order&nbsp;to&nbsp;download&nbsp;the&nbsp;following&nbsp;training&nbsp;data&nbsp;archive<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;datasets_for_babyGPT.tar.gz<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Save&nbsp;the&nbsp;archive&nbsp;in&nbsp;the&nbsp;Examples&nbsp;directory&nbsp;of&nbsp;the&nbsp;distribution.&nbsp;&nbsp;Now&nbsp;execute&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;following&nbsp;command:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;tar&nbsp;zxvf&nbsp;datasets_for_babyGPT.tar.gz<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;command&nbsp;will&nbsp;create&nbsp;a&nbsp;'data'&nbsp;subdirectory&nbsp;in&nbsp;the&nbsp;'Examples'&nbsp;directory&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;deposit&nbsp;the&nbsp;datasets&nbsp;mentioned&nbsp;below&nbsp;in&nbsp;that&nbsp;subdirectory:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;saved_Adrien_News_Articles_56M<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;saved_articles_dir_12M<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;first&nbsp;is&nbsp;the&nbsp;"Athlete&nbsp;News"&nbsp;corpus&nbsp;created&nbsp;by&nbsp;Adrien&nbsp;Dubois.&nbsp;The&nbsp;suffix&nbsp;"56M"<br>
&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;the&nbsp;name&nbsp;of&nbsp;the&nbsp;corpus&nbsp;refers&nbsp;to&nbsp;the&nbsp;fact&nbsp;that&nbsp;the&nbsp;corpus&nbsp;consists&nbsp;of&nbsp;roughly<br>
&nbsp;&nbsp;&nbsp;&nbsp;56&nbsp;Million&nbsp;multi-byte&nbsp;Unicode&nbsp;characters&nbsp;with&nbsp;utf-8&nbsp;encoding.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;second&nbsp;is&nbsp;a&nbsp;much&nbsp;smaller&nbsp;corpus&nbsp;for&nbsp;debugging&nbsp;purposes.&nbsp;&nbsp;It&nbsp;is&nbsp;based&nbsp;on&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;news&nbsp;articles&nbsp;I&nbsp;downloaded&nbsp;with&nbsp;the&nbsp;"run_gatherer.py"&nbsp;script&nbsp;in&nbsp;the&nbsp;Examples&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;directory.<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;<br>
<font size="+2" color="red">BUGS:<br>
</font>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Please&nbsp;notify&nbsp;the&nbsp;author&nbsp;if&nbsp;you&nbsp;encounter&nbsp;any&nbsp;bugs.&nbsp;&nbsp;When&nbsp;sending&nbsp;email,<br>
&nbsp;&nbsp;&nbsp;&nbsp;please&nbsp;place&nbsp;the&nbsp;string&nbsp;'babyGPT'&nbsp;in&nbsp;the&nbsp;subject&nbsp;line&nbsp;to&nbsp;get&nbsp;past&nbsp;his<br>
&nbsp;&nbsp;&nbsp;&nbsp;spam&nbsp;filter.<br>
&nbsp;<br>
&nbsp;<br>
<font size="+2" color="red">ACKNOWLEDGMENTS:<br>
</font>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;I&nbsp;must&nbsp;thank&nbsp;Aditya&nbsp;Chauhan&nbsp;for&nbsp;pulling&nbsp;me&nbsp;into&nbsp;the&nbsp;world&nbsp;of&nbsp;multi-GPU&nbsp;training<br>
&nbsp;&nbsp;&nbsp;&nbsp;with&nbsp;PyTorch&nbsp;Lightning.&nbsp;&nbsp;If&nbsp;you&nbsp;find&nbsp;useful&nbsp;any&nbsp;of&nbsp;the&nbsp;pointers&nbsp;I&nbsp;have&nbsp;provided<br>
&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;making&nbsp;your&nbsp;code&nbsp;compatible&nbsp;with&nbsp;the&nbsp;Lightning&nbsp;API,&nbsp;the&nbsp;primary&nbsp;credit&nbsp;for<br>
&nbsp;&nbsp;&nbsp;&nbsp;that&nbsp;should&nbsp;go&nbsp;to&nbsp;Aditya.&nbsp;&nbsp;Aditya,&nbsp;currently&nbsp;pursuing&nbsp;a&nbsp;PhD&nbsp;in&nbsp;Purdue&nbsp;RVL,&nbsp;is<br>
&nbsp;&nbsp;&nbsp;&nbsp;also&nbsp;our&nbsp;foremost&nbsp;expert&nbsp;in&nbsp;OpenStack&nbsp;based&nbsp;cloud&nbsp;computing.&nbsp;&nbsp;I'd&nbsp;also&nbsp;like&nbsp;to<br>
&nbsp;&nbsp;&nbsp;&nbsp;thank&nbsp;Amith&nbsp;Kashyap&nbsp;and&nbsp;Adrien&nbsp;Dubois,&nbsp;both&nbsp;also&nbsp;PhD&nbsp;candidates&nbsp;in&nbsp;RVL,&nbsp;for&nbsp;many<br>
&nbsp;&nbsp;&nbsp;&nbsp;insightful&nbsp;conversations&nbsp;about&nbsp;deep&nbsp;learning,&nbsp;in&nbsp;general,&nbsp;and&nbsp;about&nbsp;mult-GPU&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;computing&nbsp;in&nbsp;particular.&nbsp;&nbsp;As&nbsp;previously&nbsp;mentioned&nbsp;in&nbsp;this&nbsp;page,&nbsp;Adrien&nbsp;is&nbsp;also&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;creator&nbsp;of&nbsp;the&nbsp;"Athlete&nbsp;News"&nbsp;dataset&nbsp;that&nbsp;I&nbsp;provide&nbsp;through&nbsp;this&nbsp;module&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;webpage&nbsp;at&nbsp;Purdue.<br>
&nbsp;<br>
&nbsp;<br>
<font size="+2" color="red">ABOUT&nbsp;THE&nbsp;AUTHOR:<br>
</font>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;author,&nbsp;Avinash&nbsp;Kak,&nbsp;is&nbsp;a&nbsp;professor&nbsp;of&nbsp;Electrical&nbsp;and&nbsp;Computer&nbsp;Engineering&nbsp;at<br>
&nbsp;&nbsp;&nbsp;&nbsp;Purdue&nbsp;University.&nbsp;&nbsp;For&nbsp;all&nbsp;issues&nbsp;related&nbsp;to&nbsp;this&nbsp;module,&nbsp;contact&nbsp;the&nbsp;author&nbsp;at<br>
&nbsp;&nbsp;&nbsp;&nbsp;"kak@purdue.edu".&nbsp;If&nbsp;you&nbsp;send&nbsp;email,&nbsp;please&nbsp;place&nbsp;the&nbsp;string&nbsp;"babyGPT"&nbsp;in&nbsp;your<br>
&nbsp;&nbsp;&nbsp;&nbsp;subject&nbsp;line&nbsp;to&nbsp;get&nbsp;past&nbsp;his&nbsp;spam&nbsp;filter.<br>
&nbsp;<br>
&nbsp;<br>
<font size="+2" color="red">COPYRIGHT:<br>
</font>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Python&nbsp;Software&nbsp;Foundation&nbsp;License<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Copyright&nbsp;2025&nbsp;Avinash&nbsp;Kak<br>
&nbsp;<br>
@endofdocs</tt></p>
<p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#aa55cc">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#ffffff" face="helvetica, arial"><big><strong>Imported Modules</strong></big></font></td></tr>
    
<tr><td bgcolor="#aa55cc"><tt>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</tt></td><td>&nbsp;</td>
<td width="100%"><table width="100%" summary="list"><tr><td width="25%" valign=top><a href="lightning.html">lightning</a><br>
<a href="matplotlib.animation.html">matplotlib.animation</a><br>
<a href="blingfire.html">blingfire</a><br>
<a href="copy.html">copy</a><br>
<a href="glob.html">glob</a><br>
<a href="itertools.html">itertools</a><br>
</td><td width="25%" valign=top><a href="json.html">json</a><br>
<a href="logging.html">logging</a><br>
<a href="math.html">math</a><br>
<a href="newspaper.html">newspaper</a><br>
<a href="torch.nn.html">torch.nn</a><br>
<a href="numpy.html">numpy</a><br>
</td><td width="25%" valign=top><a href="torch.optim.html">torch.optim</a><br>
<a href="os.html">os</a><br>
<a href="matplotlib.pyplot.html">matplotlib.pyplot</a><br>
<a href="random.html">random</a><br>
<a href="re.html">re</a><br>
<a href="signal.html">signal</a><br>
</td><td width="25%" valign=top><a href="string.html">string</a><br>
<a href="sys.html">sys</a><br>
<a href="time.html">time</a><br>
<a href="torch.html">torch</a><br>
<a href="torchvision.html">torchvision</a><br>
<a href="torchvision.transforms.html">torchvision.transforms</a><br>
</td></tr></table></td></tr></table><p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#ee77aa">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#ffffff" face="helvetica, arial"><big><strong>Classes</strong></big></font></td></tr>
    
<tr><td bgcolor="#ee77aa"><tt>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</tt></td><td>&nbsp;</td>
<td width="100%"><dl>
<dt><font face="helvetica, arial"><a href="builtins.html#object">builtins.object</a>
</font></dt><dd>
<dl>
<dt><font face="helvetica, arial"><a href="babyGPT.html#babyGPT">babyGPT</a>
</font></dt></dl>
</dd>
</dl>
 <p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#ffc8d8">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#000000" face="helvetica, arial"><a name="babyGPT">class <strong>babyGPT</strong></a>(<a href="builtins.html#object">builtins.object</a>)</font></td></tr>
    
<tr bgcolor="#ffc8d8"><td rowspan=2><tt>&nbsp;&nbsp;&nbsp;</tt></td>
<td colspan=2><tt>babyGPT(*args,&nbsp;**kwargs)<br>
&nbsp;<br>
<br>&nbsp;</tt></td></tr>
<tr><td>&nbsp;</td>
<td width="100%">Methods defined here:<br>
<dl><dt><a name="babyGPT-__init__"><strong>__init__</strong></a>(self, *args, **kwargs)</dt><dd><tt>Initialize&nbsp;self.&nbsp;&nbsp;See&nbsp;help(type(self))&nbsp;for&nbsp;accurate&nbsp;signature.</tt></dd></dl>

<dl><dt><a name="babyGPT-run_code_with_buffered_context_for_training_TransformerFG"><strong>run_code_with_buffered_context_for_training_TransformerFG</strong></a>(self, xformer, master_decoder, dataloader, checkpoint_frequency=4000, display_train_loss=False)</dt><dd><tt>Drawn&nbsp;from&nbsp;the&nbsp;training&nbsp;routines&nbsp;in&nbsp;the&nbsp;Transformer&nbsp;module&nbsp;of&nbsp;DLStudio</tt></dd></dl>

<dl><dt><a name="babyGPT-save_checkpoint_decoder"><strong>save_checkpoint_decoder</strong></a>(self, decoder, dir_name, iter_index)</dt><dd><tt>Save&nbsp;the&nbsp;decoder&nbsp;checkpoint</tt></dd></dl>

<dl><dt><a name="babyGPT-save_checkpoint_embedding_generator"><strong>save_checkpoint_embedding_generator</strong></a>(self, embedding_generator, dir_name, iter_index)</dt><dd><tt>save&nbsp;checkpoint&nbsp;for&nbsp;the&nbsp;embedding_generator</tt></dd></dl>

<dl><dt><a name="babyGPT-save_decoder"><strong>save_decoder</strong></a>(self, decoder)</dt><dd><tt>Save&nbsp;the&nbsp;trained&nbsp;decoder&nbsp;to&nbsp;a&nbsp;disk&nbsp;file</tt></dd></dl>

<dl><dt><a name="babyGPT-save_embedding_generator"><strong>save_embedding_generator</strong></a>(self, embedding_generator)</dt></dl>

<hr>
Data descriptors defined here:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><tt>dictionary&nbsp;for&nbsp;instance&nbsp;variables&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><tt>list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
<hr>
Data and other attributes defined here:<br>
<dl><dt><strong>ArticleDatasetWithBufferedContext</strong> = &lt;class 'babyGPT.babyGPT.ArticleDatasetWithBufferedContext'&gt;<dd><tt>This&nbsp;class&nbsp;supplies&nbsp;the&nbsp;'foundational'&nbsp;dataloader&nbsp;for&nbsp;training.&nbsp;When&nbsp;using&nbsp;the&nbsp;PyTorch&nbsp;Lightning&nbsp;<br>
module&nbsp;for&nbsp;for&nbsp;multi-GPU&nbsp;training,&nbsp;this&nbsp;dataloader&nbsp;is&nbsp;routed&nbsp;through&nbsp;Lightning's&nbsp;LightningDataModule&nbsp;<br>
class&nbsp;as&nbsp;you&nbsp;will&nbsp;see&nbsp;later&nbsp;in&nbsp;this&nbsp;code&nbsp;file.&nbsp;&nbsp;Lightning&nbsp;requires&nbsp;its&nbsp;dataloaders&nbsp;to&nbsp;be&nbsp;Python&nbsp;<br>
generators.<br>
&nbsp;<br>
The&nbsp;parameter&nbsp;'context_window_size'&nbsp;is&nbsp;the&nbsp;number&nbsp;of&nbsp;fresh&nbsp;tokens&nbsp;that&nbsp;the&nbsp;dataloader&nbsp;must&nbsp;supply&nbsp;in<br>
each&nbsp;training&nbsp;iteration.&nbsp;&nbsp;And&nbsp;the&nbsp;parameter&nbsp;'context_buffer_size'&nbsp;is&nbsp;the&nbsp;number&nbsp;of&nbsp;trailing&nbsp;tokens<br>
in&nbsp;the&nbsp;previous&nbsp;batch&nbsp;that&nbsp;are&nbsp;prepended&nbsp;to&nbsp;the&nbsp;fresh&nbsp;tokens&nbsp;in&nbsp;the&nbsp;current&nbsp;batch.&nbsp;The&nbsp;number&nbsp;of&nbsp;<br>
tokens&nbsp;that&nbsp;the&nbsp;transformer&nbsp;network&nbsp;sees&nbsp;is&nbsp;the&nbsp;sum&nbsp;of&nbsp;these&nbsp;two&nbsp;sizes.&nbsp;&nbsp;<br>
&nbsp;<br>
The&nbsp;sum&nbsp;of&nbsp;context_window_size&nbsp;and&nbsp;context_buffer_size&nbsp;is&nbsp;referred&nbsp;to&nbsp;as&nbsp;'max_seq_length'&nbsp;in&nbsp;the&nbsp;<br>
code.</tt></dl>

<dl><dt><strong>ArticleGatherer</strong> = &lt;class 'babyGPT.babyGPT.ArticleGatherer'&gt;<dd><tt>This&nbsp;script&nbsp;is&nbsp;for&nbsp;collecting&nbsp;data&nbsp;for&nbsp;experimenting&nbsp;with&nbsp;the&nbsp;Transformer&nbsp;based&nbsp;unsupervised&nbsp;learning&nbsp;<br>
code&nbsp;in&nbsp;baby_gpt.py.&nbsp;&nbsp;<br>
&nbsp;<br>
The&nbsp;articles&nbsp;are&nbsp;downloaded&nbsp;from&nbsp;the&nbsp;URLs&nbsp;that&nbsp;are&nbsp;specified&nbsp;by&nbsp;the&nbsp;argument&nbsp;'urls'&nbsp;in&nbsp;the&nbsp;constructor&nbsp;<br>
shown&nbsp;below.&nbsp;&nbsp;See&nbsp;the&nbsp;script&nbsp;"create_base_model_with_buffered_context.py"&nbsp;in&nbsp;the&nbsp;Examples&nbsp;directory<br>
for&nbsp;how&nbsp;to&nbsp;set&nbsp;the&nbsp;URL&nbsp;strings&nbsp;for&nbsp;this&nbsp;argument.&nbsp;&nbsp;Here&nbsp;are&nbsp;some&nbsp;examples:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;urls&nbsp;=&nbsp;['<a href="https://finance.yahoo.com','http://cnn.com">https://finance.yahoo.com','http://cnn.com</a>',<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'<a href="https://timesofindia.indiatimes.com">https://timesofindia.indiatimes.com</a>',<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'<a href="https://purdueexponent.org','https://slate.com">https://purdueexponent.org','https://slate.com</a>',&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'<a href="https://sports.yahoo.com">https://sports.yahoo.com</a>']<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;urls&nbsp;=&nbsp;['<a href="http://cnn.com">http://cnn.com</a>']<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;urls&nbsp;=&nbsp;['<a href="https://slate.com">https://slate.com</a>']<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;urls&nbsp;=&nbsp;['<a href="https://timesofindia.indiatimes.com">https://timesofindia.indiatimes.com</a>']</tt></dl>

<dl><dt><strong>AttentionHead</strong> = &lt;class 'babyGPT.babyGPT.AttentionHead'&gt;<dd><tt>Borrowed&nbsp;from&nbsp;the&nbsp;Transformers&nbsp;module&nbsp;of&nbsp;DLStudio</tt></dl>

<dl><dt><strong>BasicDecoderWithMasking</strong> = &lt;class 'babyGPT.babyGPT.BasicDecoderWithMasking'&gt;<dd><tt>Borrowed&nbsp;from&nbsp;the&nbsp;Transformers&nbsp;module&nbsp;of&nbsp;DLStudio</tt></dl>

<dl><dt><strong>EmbeddingGenerator</strong> = &lt;class 'babyGPT.babyGPT.EmbeddingGenerator'&gt;</dl>

<dl><dt><strong>MasterDecoderWithMasking</strong> = &lt;class 'babyGPT.babyGPT.MasterDecoderWithMasking'&gt;<dd><tt>This&nbsp;class&nbsp;was&nbsp;borrowed&nbsp;initially&nbsp;from&nbsp;the&nbsp;Transformers&nbsp;module&nbsp;of&nbsp;the&nbsp;DLStudio&nbsp;platform.&nbsp;&nbsp;Subsequently,&nbsp;its&nbsp;<br>
definition&nbsp;was&nbsp;significantly&nbsp;expanded&nbsp;to&nbsp;fulfill&nbsp;the&nbsp;constraints&nbsp;imposed&nbsp;by&nbsp;the&nbsp;PyTorch&nbsp;Lightning&nbsp;API.<br>
For&nbsp;information&nbsp;regarding&nbsp;the&nbsp;operation&nbsp;of&nbsp;this&nbsp;class,&nbsp;please&nbsp;visit&nbsp;the&nbsp;website&nbsp;for&nbsp;DLStudio&nbsp;at&nbsp;Purdue.</tt></dl>

<dl><dt><strong>PromptResponder</strong> = &lt;class 'babyGPT.babyGPT.PromptResponder'&gt;<dd><tt>Prompting&nbsp;a&nbsp;trained&nbsp;babyGPT&nbsp;models&nbsp;means&nbsp;that&nbsp;you&nbsp;supply&nbsp;a&nbsp;small&nbsp;number&nbsp;of&nbsp;words&nbsp;(as,&nbsp;say,&nbsp;the&nbsp;<br>
beginning&nbsp;of&nbsp;a&nbsp;new&nbsp;thought)&nbsp;as&nbsp;a&nbsp;prompt&nbsp;and&nbsp;the&nbsp;model&nbsp;supplies&nbsp;the&nbsp;rest&nbsp;of&nbsp;the&nbsp;words&nbsp;to&nbsp;complete&nbsp;<br>
the&nbsp;thought.&nbsp;&nbsp;The&nbsp;class&nbsp;comes&nbsp;with&nbsp;two&nbsp;methods,&nbsp;the&nbsp;first&nbsp;for&nbsp;extending&nbsp;your&nbsp;prompt&nbsp;until&nbsp;it&nbsp;<br>
reaches&nbsp;a&nbsp;period,&nbsp;and&nbsp;the&nbsp;second&nbsp;for&nbsp;going&nbsp;beyond&nbsp;the&nbsp;first&nbsp;period&nbsp;encountered.<br>
&nbsp;<br>
Any&nbsp;interaction&nbsp;with&nbsp;a&nbsp;trained&nbsp;GPT&nbsp;model&nbsp;has&nbsp;to&nbsp;deal&nbsp;with&nbsp;the&nbsp;following&nbsp;issue:&nbsp;&nbsp;What&nbsp;to&nbsp;do&nbsp;with<br>
the&nbsp;context&nbsp;buffer&nbsp;that&nbsp;is&nbsp;meant&nbsp;to&nbsp;be&nbsp;a&nbsp;continuation&nbsp;of&nbsp;the&nbsp;last&nbsp;part&nbsp;of&nbsp;the&nbsp;previous&nbsp;"sentence"<br>
fed&nbsp;into&nbsp;the&nbsp;transformer.&nbsp;<br>
&nbsp;<br>
Ideally,&nbsp;we&nbsp;should&nbsp;be&nbsp;placing&nbsp;in&nbsp;the&nbsp;context&nbsp;buffer&nbsp;words&nbsp;that&nbsp;create&nbsp;a&nbsp;context&nbsp;for&nbsp;the&nbsp;prompt.<br>
But&nbsp;there&nbsp;is&nbsp;no&nbsp;easy&nbsp;way&nbsp;to&nbsp;that&nbsp;without&nbsp;a&nbsp;more&nbsp;elaborate&nbsp;model.&nbsp;An&nbsp;example&nbsp;of&nbsp;more&nbsp;elaborate<br>
modeling&nbsp;would&nbsp;be&nbsp;to&nbsp;have&nbsp;the&nbsp;input&nbsp;to&nbsp;the&nbsp;transformer&nbsp;consist&nbsp;of,&nbsp;say,&nbsp;an&nbsp;SOS&nbsp;token,&nbsp;a&nbsp;special<br>
context&nbsp;token&nbsp;consisting&nbsp;possibly&nbsp;of&nbsp;integer&nbsp;index&nbsp;values&nbsp;beyond&nbsp;the&nbsp;tokenizer&nbsp;vocab,&nbsp;followed<br>
by&nbsp;a&nbsp;context&nbsp;buffer&nbsp;that&nbsp;would&nbsp;be&nbsp;the&nbsp;last&nbsp;part&nbsp;of&nbsp;the&nbsp;previous&nbsp;sentence,&nbsp;followed,&nbsp;finally,&nbsp;<br>
by&nbsp;the&nbsp;new&nbsp;input&nbsp;tokens.<br>
&nbsp;<br>
babyGPT&nbsp;gives&nbsp;you&nbsp;two&nbsp;options&nbsp;regarding&nbsp;what&nbsp;to&nbsp;do&nbsp;with&nbsp;the&nbsp;context&nbsp;buffer&nbsp;for&nbsp;your&nbsp;prompt:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;--&nbsp;&nbsp;all_zeros<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;--&nbsp;&nbsp;get_from_prompt<br>
&nbsp;<br>
With&nbsp;the&nbsp;first&nbsp;option,&nbsp;all&nbsp;of&nbsp;the&nbsp;integer&nbsp;encoding&nbsp;values&nbsp;in&nbsp;the&nbsp;context&nbsp;buffer&nbsp;are&nbsp;set&nbsp;to<br>
the&nbsp;integer&nbsp;zero.&nbsp;&nbsp;And,&nbsp;with&nbsp;the&nbsp;second&nbsp;option,&nbsp;at&nbsp;this&nbsp;time,&nbsp;the&nbsp;context&nbsp;buffer&nbsp;contains<br>
a&nbsp;portion&nbsp;or&nbsp;all&nbsp;of&nbsp;the&nbsp;prompt&nbsp;itself.&nbsp;&nbsp;If&nbsp;the&nbsp;tokenized&nbsp;version&nbsp;of&nbsp;the&nbsp;prompt&nbsp;is&nbsp;shorter<br>
than&nbsp;the&nbsp;size&nbsp;of&nbsp;the&nbsp;context&nbsp;buffer,&nbsp;only&nbsp;the&nbsp;context_buffer_size&nbsp;number&nbsp;of&nbsp;elements&nbsp;of&nbsp;the<br>
prompt&nbsp;are&nbsp;retained&nbsp;for&nbsp;the&nbsp;context&nbsp;buffer.&nbsp;&nbsp;In&nbsp;the&nbsp;opposite&nbsp;case,&nbsp;just&nbsp;the&nbsp;initial&nbsp;<br>
context_buffer_size&nbsp;number&nbsp;of&nbsp;elements&nbsp;of&nbsp;the&nbsp;prompt&nbsp;are&nbsp;retained.</tt></dl>

<dl><dt><strong>SelfAttention</strong> = &lt;class 'babyGPT.babyGPT.SelfAttention'&gt;<dd><tt>Borrowed&nbsp;from&nbsp;the&nbsp;Transformers&nbsp;module&nbsp;of&nbsp;DLStudio</tt></dl>

<dl><dt><strong>TrainTokenizer</strong> = &lt;class 'babyGPT.babyGPT.TrainTokenizer'&gt;<dd><tt>Tokenizers&nbsp;play&nbsp;a&nbsp;critical&nbsp;role&nbsp;in&nbsp;language&nbsp;modeling&nbsp;because&nbsp;they&nbsp;create&nbsp;a&nbsp;fixed-sized&nbsp;vocabulary&nbsp;<br>
for&nbsp;the&nbsp;corpus&nbsp;you&nbsp;are&nbsp;working&nbsp;with&nbsp;---&nbsp;regardless&nbsp;of&nbsp;the&nbsp;size&nbsp;of&nbsp;the&nbsp;corpus&nbsp;itself.&nbsp;&nbsp;Unless&nbsp;your&nbsp;<br>
text&nbsp;corpus&nbsp;is&nbsp;based&nbsp;on&nbsp;a&nbsp;set&nbsp;of&nbsp;documents&nbsp;frozen&nbsp;in&nbsp;time,&nbsp;ordinarily,&nbsp;as&nbsp;the&nbsp;size&nbsp;of&nbsp;a&nbsp;text&nbsp;corpus&nbsp;<br>
goes&nbsp;up,&nbsp;so&nbsp;does&nbsp;the&nbsp;size&nbsp;of&nbsp;the&nbsp;vocabulary&nbsp;---&nbsp;despite&nbsp;the&nbsp;illusion&nbsp;to&nbsp;the&nbsp;contrary&nbsp;created&nbsp;by&nbsp;<br>
the&nbsp;fixed&nbsp;sizes&nbsp;of&nbsp;the&nbsp;language&nbsp;dictionaries&nbsp;you&nbsp;have&nbsp;seen&nbsp;all&nbsp;your&nbsp;life.&nbsp;&nbsp;How&nbsp;we&nbsp;express&nbsp;ourselves&nbsp;<br>
is&nbsp;a&nbsp;living&nbsp;thing.&nbsp;&nbsp;We&nbsp;are&nbsp;constantly&nbsp;inventing&nbsp;new&nbsp;words&nbsp;and&nbsp;new&nbsp;expressions;&nbsp;these&nbsp;form&nbsp;important&nbsp;<br>
components&nbsp;of&nbsp;&nbsp;what's&nbsp;referred&nbsp;to&nbsp;as&nbsp;the&nbsp;zeitgeist.<br>
&nbsp;<br>
Having&nbsp;a&nbsp;fixed-sized&nbsp;vocab&nbsp;is&nbsp;important&nbsp;because&nbsp;the&nbsp;loss&nbsp;functions&nbsp;used&nbsp;in&nbsp;deep-learning&nbsp;network&nbsp;<br>
used&nbsp;for&nbsp;language&nbsp;processing&nbsp;are&nbsp;based&nbsp;on&nbsp;maximum-likelihood&nbsp;prediction&nbsp;of&nbsp;the&nbsp;next&nbsp;token&nbsp;given&nbsp;<br>
the&nbsp;tokens&nbsp;seen&nbsp;previously.&nbsp;&nbsp;That&nbsp;requires&nbsp;estimating&nbsp;the&nbsp;probabilities&nbsp;associated&nbsp;with&nbsp;all<br>
possible&nbsp;tokens&nbsp;at&nbsp;the&nbsp;next&nbsp;position.&nbsp;&nbsp;As&nbsp;you&nbsp;can&nbsp;imagine,&nbsp;it&nbsp;would&nbsp;be&nbsp;impossible&nbsp;to&nbsp;engage&nbsp;in&nbsp;<br>
such&nbsp;probabilistic&nbsp;reasoning&nbsp;if&nbsp;you&nbsp;did&nbsp;not&nbsp;know&nbsp;in&nbsp;advance&nbsp;the&nbsp;size&nbsp;of&nbsp;the&nbsp;vocabulary.<br>
&nbsp;<br>
Added&nbsp;in&nbsp;version&nbsp;1.0.9:&nbsp;Here's&nbsp;an&nbsp;important&nbsp;point&nbsp;to&nbsp;remember&nbsp;when&nbsp;you&nbsp;are&nbsp;training&nbsp;a&nbsp;tokenizer&nbsp;<br>
on&nbsp;media&nbsp;articles&nbsp;collected&nbsp;from&nbsp;the&nbsp;internet&nbsp;at&nbsp;large:&nbsp;The&nbsp;articles&nbsp;frequently&nbsp;contain&nbsp;long&nbsp;<br>
URL&nbsp;strings&nbsp;that&nbsp;should&nbsp;play&nbsp;no&nbsp;role&nbsp;in&nbsp;either&nbsp;training&nbsp;the&nbsp;tokenizer&nbsp;for&nbsp;a&nbsp;new&nbsp;corpus&nbsp;or&nbsp;in&nbsp;<br>
training&nbsp;a&nbsp;transformer&nbsp;network&nbsp;for&nbsp;next-token&nbsp;prediction.&nbsp;What&nbsp;makes&nbsp;this&nbsp;problem&nbsp;worse&nbsp;is&nbsp;that&nbsp;<br>
such&nbsp;strings&nbsp;may&nbsp;consist&nbsp;of&nbsp;hundreds&nbsp;of&nbsp;characters&nbsp;---&nbsp;because&nbsp;some&nbsp;media&nbsp;URLs&nbsp;these&nbsp;days&nbsp;<br>
contain&nbsp;the&nbsp;full&nbsp;title&nbsp;of&nbsp;the&nbsp;articles&nbsp;they&nbsp;point&nbsp;to.&nbsp;In&nbsp;addition,&nbsp;parts&nbsp;of&nbsp;a&nbsp;URL&nbsp;(such&nbsp;as&nbsp;the&nbsp;<br>
Query&nbsp;part)&nbsp;may&nbsp;also&nbsp;be&nbsp;encoded&nbsp;---&nbsp;that&nbsp;is,&nbsp;consist&nbsp;of&nbsp;a&nbsp;seemingly&nbsp;gibberish&nbsp;sequence&nbsp;of<br>
characters.&nbsp;&nbsp;To&nbsp;get&nbsp;rid&nbsp;of&nbsp;such&nbsp;strings,&nbsp;starting&nbsp;with&nbsp;Version&nbsp;1.0.9,&nbsp;I&nbsp;filter&nbsp;out&nbsp;all&nbsp;strings&nbsp;<br>
that&nbsp;are&nbsp;longer&nbsp;than&nbsp;20&nbsp;characters.&nbsp;&nbsp;This&nbsp;I&nbsp;do&nbsp;both&nbsp;for&nbsp;Tokenizer&nbsp;training&nbsp;and&nbsp;when&nbsp;reading&nbsp;in&nbsp;<br>
the&nbsp;text&nbsp;data&nbsp;for&nbsp;training&nbsp;the&nbsp;transformer&nbsp;network.</tt></dl>

<dl><dt><strong>TransformerFG</strong> = &lt;class 'babyGPT.babyGPT.TransformerFG'&gt;<dd><tt>This&nbsp;I&nbsp;have&nbsp;borrowed&nbsp;from&nbsp;the&nbsp;DLStudio's&nbsp;Transformers&nbsp;module.&nbsp;&nbsp;"FG"&nbsp;stands&nbsp;for&nbsp;"First&nbsp;Generation"&nbsp;---&nbsp;which&nbsp;is&nbsp;<br>
the&nbsp;Transformer&nbsp;as&nbsp;originally&nbsp;proposed&nbsp;by&nbsp;Vaswani&nbsp;et&nbsp;al.</tt></dl>

</td></tr></table></td></tr></table><p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#eeaa77">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#ffffff" face="helvetica, arial"><big><strong>Functions</strong></big></font></td></tr>
    
<tr><td bgcolor="#eeaa77"><tt>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</tt></td><td>&nbsp;</td>
<td width="100%"><dl><dt><a name="-ctrl_c_handler"><strong>ctrl_c_handler</strong></a>(signum, frame)</dt></dl>
 <dl><dt><a name="-gen"><strong>gen</strong></a>(container)</dt></dl>
</td></tr></table><p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#55aa55">
<td colspan=3 valign=bottom>&nbsp;<br>

<tr><td bgcolor="#55aa55"><tt>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</tt></td><td>&nbsp;</td>
<td width="100%"><strong>__author__</strong> = 'Avinash Kak (kak@purdue.edu)'<br>
<strong>__copyright__</strong> = '(C) 2025 Avinash Kak. Python Software Foundation.'<br>
<strong>__date__</strong> = '2025-September-16<br>
<strong>__url__</strong> = 'https://engineering.purdue.edu/kak/distBabyGPT/babyGPT-1.1.3.html'<br>
<strong>__version__</strong> = '1.1.3'</td></tr></table>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#7799ee">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#ffffff" face="helvetica, arial"><big><strong>Author</strong></big></font></td></tr>
<tr><td bgcolor="#7799ee"><tt>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</tt></td><td>&nbsp;</td>
<td width="100%">Avinash&nbsp;Kak&nbsp;(kak@purdue.edu)</td></tr></table>
</body></html>

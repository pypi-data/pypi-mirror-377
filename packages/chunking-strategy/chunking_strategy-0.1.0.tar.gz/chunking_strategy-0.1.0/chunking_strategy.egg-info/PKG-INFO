Metadata-Version: 2.4
Name: chunking-strategy
Version: 0.1.0
Summary: A comprehensive chunking library for text, documents, audio, video, and data streams
Author-email: Chunking Strategy Team <sharanharsoor@gmail.com>
Maintainer-email: Chunking Strategy Team <sharanharsoor@gmail.com>
License: MIT
Project-URL: Homepage, https://github.com/sharanharsoor/chunking
Project-URL: Repository, https://github.com/sharanharsoor/chunking
Project-URL: Issues, https://github.com/sharanharsoor/chunking/issues
Project-URL: Changelog, https://github.com/sharanharsoor/chunking/blob/main/CHANGELOG.md
Keywords: chunking,text-processing,document-processing,audio-processing,video-processing,data-streams,content-defined-chunking,semantic-chunking,RAG
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Classifier: Topic :: Text Processing :: General
Classifier: Topic :: Multimedia :: Sound/Audio
Classifier: Topic :: Multimedia :: Video
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: pydantic>=2.0.0
Requires-Dist: pyyaml>=6.0
Requires-Dist: click>=8.0.0
Requires-Dist: rich>=13.0.0
Requires-Dist: typing-extensions>=4.0.0
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Requires-Dist: pytest-cov>=4.0.0; extra == "dev"
Requires-Dist: pytest-asyncio>=0.21.0; extra == "dev"
Requires-Dist: black>=23.0.0; extra == "dev"
Requires-Dist: ruff>=0.1.0; extra == "dev"
Requires-Dist: isort>=5.12.0; extra == "dev"
Requires-Dist: mypy>=1.5.0; extra == "dev"
Requires-Dist: pre-commit>=3.0.0; extra == "dev"
Provides-Extra: text
Requires-Dist: spacy>=3.6.0; extra == "text"
Requires-Dist: nltk>=3.8; extra == "text"
Requires-Dist: sentence-transformers>=5.1.0; extra == "text"
Requires-Dist: huggingface-hub>=0.20.0; extra == "text"
Requires-Dist: tiktoken>=0.5.0; extra == "text"
Provides-Extra: document
Requires-Dist: PyPDF2>=3.0.0; extra == "document"
Requires-Dist: pdfminer.six>=20220524; extra == "document"
Requires-Dist: PyMuPDF>=1.23.0; extra == "document"
Requires-Dist: python-docx>=0.8.11; extra == "document"
Requires-Dist: beautifulsoup4>=4.12.0; extra == "document"
Requires-Dist: lxml>=4.9.0; extra == "document"
Requires-Dist: markdown>=3.5.0; extra == "document"
Provides-Extra: multimedia
Requires-Dist: opencv-python>=4.8.0; extra == "multimedia"
Requires-Dist: Pillow>=10.0.0; extra == "multimedia"
Requires-Dist: librosa>=0.10.0; extra == "multimedia"
Requires-Dist: pydub>=0.25.0; extra == "multimedia"
Requires-Dist: imageio>=2.31.0; extra == "multimedia"
Requires-Dist: moviepy>=1.0.3; extra == "multimedia"
Provides-Extra: ml
Requires-Dist: scikit-learn>=1.3.0; extra == "ml"
Requires-Dist: numpy>=1.24.0; extra == "ml"
Requires-Dist: torch>=2.0.0; extra == "ml"
Requires-Dist: transformers>=4.30.0; extra == "ml"
Requires-Dist: bitsandbytes>=0.47.0; extra == "ml"
Requires-Dist: fsspec<2025.0.0,>=2023.1.0; extra == "ml"
Provides-Extra: detection
Requires-Dist: python-magic>=0.4.27; extra == "detection"
Requires-Dist: chardet>=5.2.0; extra == "detection"
Requires-Dist: langdetect>=1.0.9; extra == "detection"
Provides-Extra: hardware
Requires-Dist: psutil>=5.9.0; extra == "hardware"
Requires-Dist: GPUtil>=1.4.0; extra == "hardware"
Requires-Dist: nvidia-ml-py3>=7.352.0; extra == "hardware"
Provides-Extra: tika
Requires-Dist: tika>=2.6.0; extra == "tika"
Requires-Dist: python-magic>=0.4.27; extra == "tika"
Provides-Extra: vectordb
Requires-Dist: qdrant-client>=1.6.0; extra == "vectordb"
Requires-Dist: weaviate-client>=3.24.0; extra == "vectordb"
Requires-Dist: pinecone-client>=2.2.0; extra == "vectordb"
Provides-Extra: all
Requires-Dist: chunking-strategy[detection,dev,document,hardware,ml,multimedia,text,tika,vectordb]; extra == "all"
Dynamic: license-file

# Chunking Strategy Library

**A comprehensive Python library for intelligent document chunking with extensive format support and streaming capabilities.**

Transform your documents into perfectly sized chunks for RAG systems, vector databases, LLM processing, and content analysis with multi-core processing and memory-efficient streaming for large files.

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![PyPI version](https://badge.fury.io/py/chunking-strategy.svg)](https://badge.fury.io/py/chunking-strategy)

---

## ðŸš€ **Quick Start**

### Installation

```bash
# Basic installation
pip install chunking-strategy

# With all features (recommended)
pip install chunking-strategy[all]

# Specific feature sets
pip install chunking-strategy[document,hardware,tika]
```

### 30-Second Example

```python
from chunking_strategy import ChunkerOrchestrator

# Auto-select best strategy for any file
orchestrator = ChunkerOrchestrator()

# Works with ANY file type - documents, code, multimedia!
result = orchestrator.chunk_file("document.pdf")      # PDF processing
result = orchestrator.chunk_file("podcast.mp3")       # Audio with silence detection
result = orchestrator.chunk_file("video.mp4")         # Video scene analysis
result = orchestrator.chunk_file("image.jpg")         # Image tiling

# Use your perfectly chunked content
for chunk in result.chunks:
    print(f"Chunk: {chunk.content}")
    print(f"Strategy used: {result.strategy_used}")
```

### Quick CLI Examples

```bash
# Simple text file chunking
python -m chunking_strategy chunk my_document.txt --strategy sentence_based

# PDF with specific output format
python -m chunking_strategy chunk report.pdf --strategy pdf_chunker --format json

# See all available strategies
python -m chunking_strategy list-strategies

# Process multiple files at once
python -m chunking_strategy batch *.txt --strategy paragraph_based --workers 4

# Get help for any command
python -m chunking_strategy chunk --help
```

### Choose Your Approach

**ðŸ¤– Intelligent Auto Selection (Recommended)**
```python
# Let the system choose the best strategy
config = {"strategies": {"primary": "auto"}}
orchestrator = ChunkerOrchestrator(config=config)
result = orchestrator.chunk_file("any_file.ext")  # Automatic optimization!
```

**ðŸŽ¯ Universal Strategies (Any file type)**
```python
# Apply ANY strategy to ANY file type
from chunking_strategy import apply_universal_strategy

result = apply_universal_strategy("paragraph", "script.py")     # Paragraph chunking on code
result = apply_universal_strategy("sentence", "document.pdf")   # Sentence chunking on PDF
result = apply_universal_strategy("rolling_hash", "data.json") # Rolling hash on JSON
```

**ðŸ”§ Specialized Chunkers (Maximum precision)**
```python
# Deep understanding of specific formats
from chunking_strategy import create_chunker

python_chunker = create_chunker("python_code")    # AST-aware Python chunking
pdf_chunker = create_chunker("pdf_chunker")       # PDF structure + images + tables
cpp_chunker = create_chunker("c_cpp_code")        # C/C++ syntax understanding
```

---

## ðŸŽ¯ **Why Choose This Library?**

### âœ¨ **Key Features**
- **Multi-Format Support**: PDF, DOC, DOCX, code files, **audio/video/images**, and more. Universal processing via Apache Tika integration
- **Multimedia Intelligence**: Smart audio chunking with silence detection, video scene analysis, image tiling for ML workflows
- **Performance Optimization**: Multi-core batch processing and memory-efficient streaming for large files
- **Batch Processing**: Process thousands of files efficiently with multiprocessing
- **Robust Processing**: Comprehensive error handling, logging, and quality metrics

### ðŸ§  **Intelligent Chunking Strategies**
- **Text-Based**: Sentence, paragraph, semantic, and topic-based chunking
- **Document-Aware**: PDF with image/table extraction, structured document processing
- **Multimedia-Smart**: Audio silence detection, video scene analysis, image tiling and patches
- **Universal**: Apache Tika integration for any file format (1,400+ formats)
- **Custom**: Easy to create domain-specific chunking strategies

### âš¡ **Performance & Scalability**
- **True Streaming Processing**: Handle multi-gigabyte files with constant memory usage through memory-mapped streaming
- **Parallel Processing**: Multi-core batch processing for multiple files
- **40 Chunking Strategies**: Comprehensive variety of text, code, document, and multimedia chunkers
- **Quality Metrics**: Built-in evaluation and optimization

### ðŸ”¥ **Key Differentiators**
- **Memory-Mapped Streaming**: Process massive documents (1GB+) that would crash other libraries
- **Format Variety**: 40 specialized chunkers vs. 5-8 in most libraries
- **True Universal Framework**: Apply any strategy to any file type
- **Token-Precise Control**: Advanced tokenizer integration (tiktoken, transformers, etc.) for LLM applications
- **Comprehensive Testing**: Extensively tested with real-world files and edge cases

---

## ðŸŽ­ **Three Powerful Approaches - Choose What Fits Your Needs**

Our library offers three distinct approaches to handle different use cases. Understanding when to use each approach will help you get the best results.

### ðŸ¤– **Auto Selection**: Intelligent Strategy Selection

**Best for**: Quick start, prototyping, general-purpose applications

The system automatically chooses the optimal strategy based on file extension and content characteristics. Zero configuration required!

```python
from chunking_strategy import ChunkerOrchestrator

# Zero configuration - just works!
orchestrator = ChunkerOrchestrator(config={"strategies": {"primary": "auto"}})

# System intelligently selects:
result = orchestrator.chunk_file("script.py")      # â†’ paragraph (preserves code structure)
result = orchestrator.chunk_file("document.txt")   # â†’ sentence (readable chunks)
result = orchestrator.chunk_file("data.json")      # â†’ fixed_size (consistent processing)
result = orchestrator.chunk_file("large_file.pdf") # â†’ rolling_hash (efficient for large files)
```

**Auto Selection Rules:**
- **Code files** (`.py`, `.js`, `.cpp`, `.java`): `paragraph` - preserves logical structure
- **Text files** (`.txt`, `.md`, `.rst`): `sentence` - optimizes readability
- **Documents** (`.pdf`, `.doc`, `.docx`): `paragraph` - maintains document structure
- **Data files** (`.json`, `.xml`, `.csv`): `fixed_size` - consistent processing
- **Large files** (>10MB): `rolling_hash` - memory efficient
- **Small files** (<1KB): `sentence` - optimal for small content

### ðŸŒ **Universal Strategies**: Any Strategy + Any File Type

**Best for**: Consistency across formats, custom workflows, RAG applications

Apply ANY chunking strategy to ANY file type through our universal framework. Perfect when you need the same chunking approach across different file formats.

```python
from chunking_strategy import apply_universal_strategy

# Same strategy works across ALL file types!
result = apply_universal_strategy("sentence", "document.pdf")    # Sentence chunking on PDF
result = apply_universal_strategy("paragraph", "script.py")      # Paragraph chunking on Python
result = apply_universal_strategy("rolling_hash", "data.xlsx")   # Rolling hash on Excel
result = apply_universal_strategy("overlapping_window", "video.mp4")  # Overlapping windows on video

# Perfect for RAG systems requiring consistent chunk sizes
for file_path in document_collection:
    result = apply_universal_strategy("fixed_size", file_path, chunk_size=1000)
    # All files get exactly 1000-character chunks regardless of format!
```

**Universal Strategies Available:**
- `fixed_size` - Consistent chunk sizes with overlap support
- `sentence` - Sentence-boundary aware chunking
- `paragraph` - Paragraph-based logical grouping
- `overlapping_window` - Sliding window with customizable overlap
- `rolling_hash` - Content-defined boundaries using hash functions

### ðŸ”§ **Specialized Chunkers**: Maximum Precision & Rich Metadata

**Best for**: Advanced applications, code analysis, document intelligence, detailed metadata requirements

Deep understanding of specific file formats with semantic boundaries and comprehensive metadata extraction.

```python
from chunking_strategy import create_chunker

# Python AST-aware chunking
python_chunker = create_chunker("python_code")
result = python_chunker.chunk("complex_script.py")
for chunk in result.chunks:
    meta = chunk.metadata.extra
    print(f"Element: {meta['element_name']} ({meta['element_type']})")
    print(f"Has docstring: {bool(meta.get('docstring'))}")
    print(f"Arguments: {meta.get('args', [])}")

# PDF with image and table extraction
pdf_chunker = create_chunker("pdf_chunker")
result = pdf_chunker.chunk("report.pdf", extract_images=True, extract_tables=True)
for chunk in result.chunks:
    if chunk.modality == ModalityType.IMAGE:
        print(f"Found image on page {chunk.metadata.page}")
    elif "table" in chunk.metadata.extra.get("chunk_type", ""):
        print(f"Found table: {chunk.content[:100]}...")

# C/C++ syntax-aware chunking
cpp_chunker = create_chunker("c_cpp_code")
result = cpp_chunker.chunk("algorithm.cpp")
for chunk in result.chunks:
    if chunk.metadata.extra.get("element_type") == "function":
        print(f"Function: {chunk.metadata.extra['element_name']}")
```

**Specialized Chunkers Available:**
- `python_code` - AST parsing, function/class boundaries, docstring extraction
- `c_cpp_code` - C/C++ syntax understanding, preprocessor directives
- `universal_code` - Multi-language code chunking (JavaScript, Go, Rust, etc.)
- `pdf_chunker` - PDF structure, images, tables, metadata
- `universal_document` - Apache Tika integration for comprehensive format support *(coming soon)*

### ðŸ“Š **Comparison: When to Use Each Approach**

| Use Case | Auto Selection | Universal Strategies | Specialized Chunkers |
|----------|---------------|---------------------|-------------------|
| **Quick prototyping** | â­â­â­â­â­ Perfect | â­â­â­ Good | â­â­ Overkill |
| **RAG systems** | â­â­â­â­ Great | â­â­â­â­â­ Perfect | â­â­â­ Good |
| **Code analysis** | â­â­â­ Good | â­â­ Basic | â­â­â­â­â­ Perfect |
| **Document intelligence** | â­â­â­ Good | â­â­ Basic | â­â­â­â­â­ Perfect |
| **Cross-format consistency** | â­â­â­ Good | â­â­â­â­â­ Perfect | â­â­ Limited |
| **Advanced applications** | â­â­â­â­ Great | â­â­â­â­ Great | â­â­â­â­â­ Perfect |

### ðŸ”® **Future File Format Support**

We're actively expanding format support. **Coming soon**:

| Category | Formats | Strategy Recommendations |
|----------|---------|------------------------|
| **Spreadsheets** | `.xls`, `.xlsx`, `.ods`, `.csv` | `fixed_size` or specialized `excel_chunker` |
| **Presentations** | `.ppt`, `.pptx`, `.odp` | `paragraph` or specialized `presentation_chunker` |
| **Data Formats** | `.parquet`, `.avro`, `.orc` | `fixed_size` or specialized `data_chunker` |
| **Media Files** | `.mp4`, `.avi`, `.mp3`, `.wav` | `overlapping_window` or specialized `media_chunker` |
| **Archives** | `.zip`, `.tar`, `.7z` | Content-aware or specialized `archive_chunker` |
| **CAD/Design** | `.dwg`, `.dxf`, `.svg` | Specialized `design_chunker` |

**Request format support**: [Open an issue](https://github.com/sharanharsoor/chunking/issues) for priority formats!

---

## ðŸ“– **Usage Examples**

### Basic Text Chunking

```python
from chunking_strategy import create_chunker

# Sentence-based chunking (best for semantic coherence)
chunker = create_chunker("sentence_based", max_sentences=3)
result = chunker.chunk("Your text here...")

# Fixed-size chunking (consistent chunk sizes)
chunker = create_chunker("fixed_size", chunk_size=1000, overlap_size=100)
result = chunker.chunk("Your text here...")

# Paragraph-based chunking (natural boundaries)
chunker = create_chunker("paragraph_based", max_paragraphs=2)
result = chunker.chunk("Your text here...")
```

### PDF Processing with Images & Tables

```python
from chunking_strategy import create_chunker

# Advanced PDF processing
chunker = create_chunker(
    "pdf_chunker",
    pages_per_chunk=1,
    extract_images=True,
    extract_tables=True,
    backend="pymupdf"  # or "pypdf2", "pdfminer"
)

result = chunker.chunk("document.pdf")

# Access different content types
for chunk in result.chunks:
    chunk_type = chunk.metadata.extra.get('chunk_type')
    if chunk_type == 'text':
        print(f"Text: {chunk.content}")
    elif chunk_type == 'image':
        print(f"Image on page {chunk.metadata.page}")
    elif chunk_type == 'table':
        print(f"Table: {chunk.content}")
```

### Universal Document Processing

```python
from chunking_strategy import create_chunker

# Process ANY file format with Apache Tika
chunker = create_chunker(
    "universal_document",
    chunk_size=1000,
    preserve_structure=True,
    extract_metadata=True
)

# Works with PDF, DOC, DOCX, Excel, PowerPoint, code files, etc.
result = chunker.chunk("any_document.docx")

print(f"File type: {result.source_info['file_type']}")
print(f"Extracted metadata: {result.source_info['tika_metadata']}")
```

### Batch Processing with Hardware Optimization

```python
from chunking_strategy.core.batch import BatchProcessor

# Automatic hardware optimization
processor = BatchProcessor()

result = processor.process_files(
    files=["doc1.pdf", "doc2.txt", "doc3.docx"],
    default_strategy="sentence_based",
    parallel_mode="process",  # Uses multiple CPU cores
    workers=None  # Auto-detected optimal worker count
)

print(f"Processed {result.total_files} files")
print(f"Created {result.total_chunks} chunks")
print(f"Performance: {result.files_per_second:.1f} files/second")
```

---

## ðŸ–¥ï¸ **Command Line Interface**

### Quick Commands

```bash
# List available strategies
chunking-strategy list-strategies

# Check your hardware capabilities
chunking-strategy hardware --recommendations

# Chunk a single file
chunking-strategy chunk document.pdf --strategy pdf_chunker --format json

# Batch process multiple files
chunking-strategy batch *.txt --strategy sentence_based --workers 4

# Use configuration file
chunking-strategy chunk document.pdf --config my_config.yaml
```

### Advanced CLI Usage

```bash
# Hardware-optimized batch processing
chunking-strategy batch documents/*.pdf \
    --strategy universal_document \
    --workers 8 \
    --mode process \
    --output-dir results \
    --format json

# PDF processing with specific backend
chunking-strategy chunk document.pdf \
    --strategy pdf_chunker \
    --backend pymupdf \
    --extract-images \
    --extract-tables \
    --pages-per-chunk 1

# Process entire directory with custom strategies per file type
chunking-strategy batch-smart ./documents/ \
    --pdf-strategy "enhanced_pdf_chunker" \
    --text-strategy "semantic" \
    --code-strategy "python_code" \
    --output-format json \
    --generate-embeddings \
    --embedding-model all-MiniLM-L6-v2

# Real-time processing with monitoring
chunking-strategy process-watch ./incoming/ \
    --auto-strategy \
    --streaming \
    --max-memory 4GB \
    --webhook http://localhost:8080/chunked \
    --metrics-dashboard
```

---

## âš™ï¸ **Configuration-Driven Processing**

### YAML Configuration

Create a `config.yaml` file:

```yaml
profile_name: "rag_optimized"

strategies:
  primary: "sentence_based"
  fallbacks: ["paragraph_based", "fixed_size"]
  configs:
    sentence_based:
      max_sentences: 3
      overlap_sentences: 1
    paragraph_based:
      max_paragraphs: 2
    fixed_size:
      chunk_size: 1000

preprocessing:
  enabled: true
  normalize_whitespace: true

postprocessing:
  enabled: true
  merge_short_chunks: true
  min_chunk_size: 100

quality_evaluation:
  enabled: true
  threshold: 0.7
```

Use with Python:

```python
from chunking_strategy import ChunkerOrchestrator

orchestrator = ChunkerOrchestrator(config_path="config.yaml")
result = orchestrator.chunk_file("document.pdf")
```

Use with CLI:

```bash
chunking-strategy chunk document.pdf --config config.yaml
```

---

## ðŸŽ­ **Complete Chunking Algorithms Reference (40 Total)**

### ðŸ“ **Text-Based Strategies** (9 strategies)
- `sentence_based` - Semantic coherence with sentence boundaries (RAG, Q&A)
- `paragraph_based` - Natural paragraph structure (Document analysis, summarization)
- `token_based` - Precise token-level chunking with multiple tokenizer support (LLM optimization)
- `semantic` - AI-powered semantic similarity with embeddings (High-quality understanding)
- `boundary_aware` - Intelligent boundary detection (Clean, readable chunks)
- `recursive` - Hierarchical multi-level chunking (Complex document structure)
- `overlapping_window` - Sliding window with customizable overlap (Context preservation)
- `fixed_length_word` - Fixed word count per chunk (Consistent word-based processing)
- `embedding_based` - Embedding similarity for boundaries (Advanced semantic understanding)

### ðŸ’» **Code-Aware Strategies** (7 strategies)
- `python_code` - AST-aware Python parsing with function/class boundaries (Python analysis)
- `c_cpp_code` - C/C++ syntax understanding with preprocessor handling (Systems programming)
- `javascript_code` - JavaScript/TypeScript AST parsing (Web development, Node.js)
- `java_code` - Java syntax parsing with package structure (Enterprise Java codebases)
- `go_code` - Go language structure awareness (Go codebase analysis)
- `css_code` - CSS rule and selector-aware chunking (Web styling analysis)
- `universal_code` - Multi-language code chunking (Cross-language processing)

### ðŸ“„ **Document-Aware Strategies** (5 strategies)
- `pdf_chunker` - Advanced PDF processing with images, tables, layout (PDF intelligence)
- `enhanced_pdf_chunker` - Premium PDF with OCR, structure analysis (Complex PDF workflows)
- `doc_chunker` - Microsoft Word document processing (Corporate documents)
- `markdown_chunker` - Markdown structure-aware (headers, lists, code blocks)
- `xml_html_chunker` - XML/HTML tag-aware with structure preservation (Web content)

### ðŸ“Š **Data Format Strategies** (2 strategies)
- `csv_chunker` - CSV row and column-aware processing (Tabular data analysis)
- `json_chunker` - JSON structure-preserving chunking (API data, configuration files)

### ðŸŽµ **Multimedia Strategies** (6 strategies)
- `time_based_audio` - Audio chunking by time intervals (Podcast transcription)
- `silence_based_audio` - Audio chunking at silence boundaries (Speech processing)
- `time_based_video` - Video chunking by time segments (Video content analysis)
- `scene_based_video` - Scene change detection for intelligent cuts (Video processing)
- `grid_based_image` - Spatial grid-based image tiling (Computer vision)
- `patch_based_image` - Overlapping patch extraction (Machine learning, patterns)

### ðŸ”§ **Content-Defined Chunking (CDC)** (7 strategies)
- `fastcdc` - Fast content-defined chunking with rolling hash (Deduplication, backup)
- `rabin_fingerprinting` - Rabin polynomial rolling hash boundaries (Content-addressable storage)
- `rolling_hash` - Generic rolling hash with configurable parameters (Variable-size chunking)
- `buzhash` - BuzHash algorithm for content boundaries (Efficient content splitting)
- `gear_cdc` - Gear-based content-defined chunking (High-performance CDC)
- `ml_cdc` - Machine learning-enhanced boundary detection (Intelligent boundaries)
- `tttd` - Two Threshold Two Divisor algorithm (Advanced CDC with dual thresholds)

### ðŸ§  **Advanced & Adaptive Strategies** (4 strategies)
- `adaptive` - Self-learning chunker that adapts based on feedback (Dynamic optimization)
- `context_enriched` - Context-aware chunking with NLP enhancement (Advanced text understanding)
- `discourse_aware` - Discourse structure and topic transition detection (Academic papers)
- `fixed_size` - Simple fixed-size chunking with overlap support (Baseline, simple needs)

### Strategy Selection Guide

```python
# For RAG systems and LLM processing
chunker = create_chunker("sentence_based", max_sentences=3)

# For vector databases with token limits
chunker = create_chunker("fixed_size", chunk_size=512)

# For document analysis and summarization
chunker = create_chunker("paragraph_based", max_paragraphs=2)

# For complex PDFs with mixed content
chunker = create_chunker("pdf_chunker", extract_images=True)

# For any file format
chunker = create_chunker("universal_document")
```

---

## ðŸŒŠ **Streaming Support for Large Files**

### Memory-Efficient Processing
The library provides comprehensive streaming capabilities for processing massive files (1GB+) with constant memory usage.

```python
from chunking_strategy import StreamingChunker

# Process huge files with constant memory usage
streamer = StreamingChunker("sentence_based",
                           block_size=64*1024,    # 64KB blocks
                           overlap_size=1024)     # 1KB overlap

# Memory usage stays constant regardless of file size
for chunk in streamer.stream_file("huge_10gb_file.txt"):
    process_chunk(chunk)  # Memory: ~10MB constant vs 10GB regular loading
```

### Streaming Advantages
- **Constant Memory Usage**: Fixed ~10-100MB footprint regardless of file size
- **Early Chunk Availability**: Start processing chunks as they're generated
- **Fault Tolerance**: Built-in checkpointing and resume capabilities
- **Better Resource Utilization**: Smooth resource usage, system-friendly

### Resume from Interruption
```python
# Automatic resume on interruption
streamer = StreamingChunker("semantic", enable_checkpoints=True)

try:
    for chunk in streamer.stream_file("massive_dataset.txt"):
        process_chunk(chunk)
except KeyboardInterrupt:
    print("Interrupted - progress saved")

# Later - resumes from last checkpoint automatically
for chunk in streamer.stream_file("massive_dataset.txt"):
    process_chunk(chunk)  # Continues from where it left off
```

### Performance Monitoring
```python
for chunk in streamer.stream_file("large_file.txt"):
    progress = streamer.get_progress()
    print(f"ðŸ“Š Progress: {progress.progress_percentage:.1f}%")
    print(f"âš¡ Throughput: {progress.throughput_mbps:.1f} MB/s")
    print(f"â±ï¸  ETA: {progress.eta_seconds:.0f}s")
```

---

## ðŸ“Š **Performance Metrics & Benchmarking**

### Comprehensive Performance Analysis
The library provides extensive performance monitoring to help you optimize strategies and understand real-world efficiency.

#### Quality Metrics
```python
from chunking_strategy.core.metrics import ChunkingQualityEvaluator

evaluator = ChunkingQualityEvaluator()
metrics = evaluator.evaluate(result.chunks)

print(f"ðŸ“ Size Consistency: {metrics.size_consistency:.3f}")      # How uniform chunk sizes are
print(f"ðŸ§  Semantic Coherence: {metrics.coherence:.3f}")           # Internal coherence of chunks
print(f"ðŸ“‹ Content Coverage: {metrics.coverage:.3f}")              # Coverage of source content
print(f"ðŸŽ¯ Boundary Quality: {metrics.boundary_quality:.3f}")      # Quality of chunk boundaries
print(f"ðŸ’¡ Information Density: {metrics.information_density:.3f}") # Information content per chunk
print(f"ðŸ† Overall Score: {metrics.overall_score:.3f}")            # Weighted combination
```

#### Performance Metrics
```python
from chunking_strategy.benchmarking import ChunkingBenchmark

benchmark = ChunkingBenchmark(enable_memory_profiling=True)
metrics = benchmark.benchmark_strategy("semantic", "document.pdf")

print(f"â±ï¸  Processing Time: {metrics.processing_time:.3f}s")
print(f"ðŸ§  Memory Usage: {metrics.memory_usage_mb:.1f} MB")
print(f"ðŸ“Š Peak Memory: {metrics.peak_memory_mb:.1f} MB")
print(f"ðŸš€ Throughput: {metrics.throughput_mb_per_sec:.1f} MB/s")
print(f"ðŸ’» CPU Usage: {metrics.cpu_usage_percent:.1f}%")
```

### Why These Metrics Matter

#### Real-World Efficiency Interpretation
- **Size Consistency > 0.8**: Predictable for vector databases with token limits
- **Semantic Coherence > 0.8**: Better for LLM understanding and Q&A systems
- **Throughput > 10 MB/s**: Suitable for real-time applications
- **Memory usage < 100MB per GB**: Efficient for batch processing

#### Strategy Comparison
```python
# Compare multiple strategies
strategies = ["sentence_based", "semantic", "fixed_size"]
results = {}

for strategy in strategies:
    results[strategy] = benchmark.benchmark_strategy(strategy, "test_doc.pdf")

best_quality = max(results, key=lambda s: results[s].quality_score)
best_speed = max(results, key=lambda s: results[s].throughput_mb_per_sec)

print(f"ðŸ† Best Quality: {best_quality}")
print(f"âš¡ Best Speed: {best_speed}")
```

---

## ðŸŽ¬ **Multimedia Support**

### Comprehensive Format Support
The library supports extensive multimedia processing with intelligent strategies for audio, video, and images. From podcast transcription to video analysis and computer vision workflows.

**ðŸ”¥ Key Multimedia Features:**
- **Smart Audio Chunking**: Silence detection, time-based segments, speech boundaries
- **Intelligent Video Processing**: Scene change detection, frame extraction, temporal analysis
- **Advanced Image Tiling**: Grid-based, patch-based, ML-ready formats
- **Rich Metadata Extraction**: Resolution, frame rates, audio properties, timestamps
- **Universal Format Support**: 1,400+ multimedia formats via Apache Tika integration

#### Audio Processing
```python
# Time-based audio chunking
audio_chunker = create_chunker(
    "time_based_audio",
    segment_duration=30,        # 30-second segments
    overlap_duration=2,         # 2-second overlap
    format_support=['mp3', 'wav', 'flac', 'ogg']
)

# Silence-based intelligent chunking
silence_chunker = create_chunker(
    "silence_based_audio",
    silence_threshold_db=-40,   # Silence detection threshold
    min_silence_duration=0.5    # Natural speech boundaries
)
```

#### Video Processing
```python
# Scene-based video chunking with intelligent cuts
scene_chunker = create_chunker(
    "scene_based_video",
    scene_change_threshold=0.3,    # Scene change sensitivity
    extract_frames=True,           # Extract key frames
    include_audio=True             # Include audio analysis
)

# Time-based video segments
video_chunker = create_chunker(
    "time_based_video",
    segment_duration=60,           # 1-minute segments
    frame_extraction_interval=10   # Extract frame every 10s
)
```

#### Image Processing
```python
# Grid-based image tiling
image_chunker = create_chunker(
    "grid_based_image",
    grid_size=(4, 4),              # 4x4 grid (16 tiles)
    tile_overlap=0.1,              # 10% overlap between tiles
    preserve_aspect_ratio=True     # Maintain proportions
)

# Patch-based for machine learning
patch_chunker = create_chunker(
    "patch_based_image",
    patch_size=(224, 224),         # 224x224 pixel patches
    stride=(112, 112)              # 50% overlap for ML workflows
)
```

### Supported Multimedia Formats
- **Audio**: MP3, WAV, FLAC, AAC, OGG, M4A, WMA
- **Video**: MP4, AVI, MOV, MKV, WMV, WebM, FLV
- **Images**: JPEG, PNG, GIF, BMP, TIFF, WebP, SVG
- **Universal**: 1,400+ formats via Apache Tika integration

### Rich Multimedia Metadata
```python
result = chunker.chunk("video_with_audio.mp4")
for chunk in result.chunks:
    metadata = chunk.metadata.extra

    if chunk.modality == ModalityType.VIDEO:
        print(f"Resolution: {metadata['width']}x{metadata['height']}")
        print(f"Frame rate: {metadata['fps']}")
        print(f"Duration: {metadata['duration_seconds']:.2f}s")
        print(f"Codec: {metadata['video_codec']}")
    elif chunk.modality == ModalityType.AUDIO:
        print(f"Sample rate: {metadata['sample_rate']}")
        print(f"Channels: {metadata['channels']}")
        print(f"Bitrate: {metadata['bitrate']}")
        print(f"Audio codec: {metadata['audio_codec']}")
    elif chunk.modality == ModalityType.IMAGE:
        print(f"Dimensions: {metadata['width']}x{metadata['height']}")
        print(f"Color space: {metadata['color_space']}")
        print(f"File size: {metadata['file_size_bytes']} bytes")
```

### CLI Multimedia Processing
```bash
# Process audio files with silence detection
chunking-strategy chunk podcast.mp3 \
    --strategy silence_based_audio \
    --silence-threshold -35 \
    --min-silence-duration 1.0 \
    --output-format json

# Batch process video files with scene detection
chunking-strategy batch videos/*.mp4 \
    --strategy scene_based_video \
    --extract-frames \
    --scene-threshold 0.3 \
    --output-dir processed_videos

# Image tiling for computer vision datasets
chunking-strategy chunk dataset_image.jpg \
    --strategy grid_based_image \
    --grid-size 8x8 \
    --tile-overlap 0.15 \
    --preserve-aspect-ratio
```

### Real-World Multimedia Use Cases
```python
# ðŸŽ™ï¸ Podcast transcription workflow
audio_chunker = create_chunker(
    "silence_based_audio",
    silence_threshold_db=-30,      # Detect natural speech pauses
    min_silence_duration=1.0,      # 1-second minimum silence
    max_segment_duration=300       # Max 5-minute segments
)
segments = audio_chunker.chunk("interview_podcast.mp3")
# Perfect for feeding to speech-to-text APIs

# ðŸŽ¬ Video content analysis
video_chunker = create_chunker(
    "scene_based_video",
    scene_change_threshold=0.25,   # Sensitive scene detection
    extract_frames=True,           # Extract key frames
    frame_interval=5,              # Every 5 seconds
    include_audio=True             # Audio analysis too
)
scenes = video_chunker.chunk("documentary.mp4")
# Ideal for content summarization and indexing

# ðŸ–¼ï¸ Computer vision dataset preparation
image_chunker = create_chunker(
    "patch_based_image",
    patch_size=(256, 256),         # Standard ML patch size
    stride=(128, 128),             # 50% overlap
    normalize_patches=True,        # Normalize pixel values
    augment_patches=False          # Disable augmentation
)
patches = image_chunker.chunk("satellite_image.tiff")
# Ready for training ML models
```

---

## ðŸ”§ **Extending the Library**

### Creating Custom Chunking Algorithms
The library provides a powerful framework for integrating your own custom algorithms with full feature support.

#### Quick Custom Algorithm Example
```python
from chunking_strategy.core.base import BaseChunker, ChunkingResult, Chunk, ChunkMetadata
from chunking_strategy.core.registry import register_chunker, ComplexityLevel

@register_chunker(
    name="word_count_chunker",
    category="text",
    description="Chunks text based on word count",
    complexity=ComplexityLevel.LOW,
    default_parameters={"words_per_chunk": 50}
)
class WordCountChunker(BaseChunker):
    def __init__(self, words_per_chunk=50, **kwargs):
        super().__init__(name="word_count_chunker", category="text", **kwargs)
        self.words_per_chunk = words_per_chunk

    def chunk(self, content, **kwargs):
        words = content.split()
        chunks = []

        for i in range(0, len(words), self.words_per_chunk):
            chunk_words = words[i:i + self.words_per_chunk]
            chunk_content = " ".join(chunk_words)

            chunk = Chunk(
                id=f"word_chunk_{i // self.words_per_chunk}",
                content=chunk_content,
                metadata=ChunkMetadata(word_count=len(chunk_words))
            )
            chunks.append(chunk)

        return ChunkingResult(chunks=chunks, strategy_used=self.name)

# Use your custom chunker
from chunking_strategy import create_chunker
chunker = create_chunker("word_count_chunker", words_per_chunk=30)
result = chunker.chunk("Your text content here")
```

#### Advanced Custom Algorithm with Streaming
```python
from chunking_strategy.core.base import StreamableChunker, AdaptableChunker
from typing import Iterator, Union

@register_chunker(name="advanced_custom")
class AdvancedCustomChunker(StreamableChunker, AdaptableChunker):
    def chunk_stream(self, content_stream: Iterator[Union[str, bytes]], **kwargs):
        """Enable streaming support for large files"""
        buffer = ""
        chunk_id = 0

        for content_piece in content_stream:
            buffer += content_piece

            # Process when buffer reaches threshold
            if len(buffer) >= self.buffer_threshold:
                chunk = self.process_buffer(buffer, chunk_id)
                yield chunk
                chunk_id += 1
                buffer = ""

        # Process remaining buffer
        if buffer:
            chunk = self.process_buffer(buffer, chunk_id)
            yield chunk

    def adapt_parameters(self, feedback_score: float, feedback_type: str):
        """Enable adaptive learning from user feedback"""
        if feedback_score < 0.5:
            self.buffer_threshold *= 0.8  # Make chunks smaller
        elif feedback_score > 0.8:
            self.buffer_threshold *= 1.2  # Make chunks larger
```

### Integration Methods

#### File-Based Loading
```python
# Save algorithm in custom_algorithms/my_algorithm.py
from chunking_strategy import load_custom_algorithms

load_custom_algorithms("custom_algorithms/")
chunker = create_chunker("my_custom_chunker")
```

#### Configuration Integration
```yaml
# config.yaml
custom_algorithms:
  - path: "custom_algorithms/sentiment_chunker.py"
    enabled: true

strategies:
  primary: "sentiment_chunker"  # Use your custom algorithm
```

#### CLI Integration
```bash
# Load and use custom algorithms via CLI
chunking-strategy --custom-algorithms custom_algorithms/ chunk document.txt --strategy my_algorithm

# Compare custom vs built-in algorithms
chunking-strategy compare document.txt --strategies my_algorithm,sentence_based,fixed_size
```

### Validation and Testing Framework
```python
from chunking_strategy.core.custom_validation import CustomAlgorithmValidator
from chunking_strategy.benchmarking import ChunkingBenchmark

# Validate your custom algorithm
validator = CustomAlgorithmValidator()
report = validator.validate_algorithm("my_custom_chunker")

print(f"âœ… Validation passed: {report.is_valid}")
for issue in report.issues:
    print(f"âš ï¸  {issue.level}: {issue.message}")

# Performance testing
benchmark = ChunkingBenchmark()
metrics = benchmark.benchmark_strategy("my_custom_chunker", "test_document.txt")
print(f"â±ï¸  Processing time: {metrics.processing_time:.3f}s")
print(f"ðŸ† Quality score: {metrics.quality_score:.3f}")
```

**For detailed custom algorithm development, see [CUSTOM_ALGORITHMS_GUIDE.md](CUSTOM_ALGORITHMS_GUIDE.md).**

---

## âš¡ **Advanced Features & Best Practices**

### Hardware Optimization
```python
from chunking_strategy.core.hardware import get_hardware_info

# Automatic hardware detection and optimization
hardware = get_hardware_info()
print(f"ðŸ–¥ï¸  CPU cores: {hardware.cpu_count}")
print(f"ðŸ§  Memory: {hardware.memory_total_gb:.1f} GB")
print(f"ðŸ“¦ Recommended batch size: {hardware.recommended_batch_size}")

# Hardware-optimized batch processing
from chunking_strategy.core.batch import BatchProcessor

processor = BatchProcessor()
result = processor.process_files(
    files=document_list,
    default_strategy="sentence_based",
    parallel_mode="process",    # Multi-core processing
    workers=None               # Auto-detected optimal count
)
```

### Comprehensive Logging & Debugging
```python
import chunking_strategy as cs

# Configure detailed logging
cs.configure_logging(
    level=cs.LogLevel.VERBOSE,     # Show detailed operations
    file_output=True,              # Save logs to file
    collect_performance=True,      # Track performance metrics
    collect_metrics=True           # Track quality metrics
)

# Enable debug mode for troubleshooting
cs.enable_debug_mode()

# Generate debug archive for bug reports
debug_archive = cs.create_debug_archive("Description of the issue")
print(f"ðŸ› Debug archive: {debug_archive['archive_path']}")
# Share this file for support

# Quick debugging examples
cs.user_info("Processing started")              # User-friendly messages
cs.debug_operation("chunk_processing", {"chunks": 42})  # Developer details
cs.performance_log({"time": 1.23, "memory": "45MB"})    # Performance tracking
```

**CLI Debugging:**
```bash
# Enable debug mode with detailed logging
chunking-strategy --debug --log-level verbose chunk document.pdf

# Collect debug information
chunking-strategy debug collect --description "PDF processing issue"

# Test logging configuration
chunking-strategy debug test-logging
```

### Quality Assessment & Adaptive Learning
```python
# Adaptive chunker learns from feedback
adaptive_chunker = create_chunker("adaptive")
result = adaptive_chunker.chunk("document.pdf")

# Simulate user feedback (0.0 = poor, 1.0 = excellent)
user_satisfaction = 0.3  # Poor results
adaptive_chunker.adapt_parameters(user_satisfaction, "quality")

# The chunker automatically adjusts its parameters for better results
result2 = adaptive_chunker.chunk("document2.pdf")  # Should perform better
```

### Error Handling with Fallbacks
```python
def robust_chunking(file_path, strategies=None):
    """Chunk with automatic fallback strategies."""
    if strategies is None:
        strategies = ["pdf_chunker", "sentence_based", "paragraph_based", "fixed_size"]

    for strategy in strategies:
        try:
            chunker = create_chunker(strategy)
            result = chunker.chunk(file_path)
            cs.user_success(f"âœ… Successfully processed with {strategy}")
            return result
        except Exception as e:
            cs.user_warning(f"âš ï¸  Strategy {strategy} failed: {e}")
            continue

    raise Exception("âŒ All chunking strategies failed")

# Guaranteed to work with automatic fallbacks
result = robust_chunking("any_document.pdf")
```

**For comprehensive debugging instructions, see [DEBUGGING_GUIDE.md](DEBUGGING_GUIDE.md).**

---

## ðŸ—ï¸ **Integration Examples**

### With Vector Databases

```python
from chunking_strategy import create_chunker
import weaviate  # or qdrant, pinecone, etc.

# Chunk documents
chunker = create_chunker("sentence_based", max_sentences=3)
result = chunker.chunk("document.pdf")

# Store in vector database
client = weaviate.Client("http://localhost:8080")

for chunk in result.chunks:
    client.data_object.create(
        {
            "content": chunk.content,
            "source": chunk.metadata.source,
            "page": chunk.metadata.page,
            "chunk_id": chunk.id
        },
        "Document"
    )
```

### With LangChain

```python
from chunking_strategy import create_chunker
from langchain.schema import Document

# Chunk with our library
chunker = create_chunker("paragraph_based", max_paragraphs=2)
result = chunker.chunk("document.pdf")

# Convert to LangChain documents
langchain_docs = [
    Document(
        page_content=chunk.content,
        metadata={
            "source": chunk.metadata.source,
            "page": chunk.metadata.page,
            "chunk_id": chunk.id
        }
    )
    for chunk in result.chunks
]
```

### With Streamlit

```python
import streamlit as st
from chunking_strategy import create_chunker

st.title("Document Chunking App")

# File upload
uploaded_file = st.file_uploader("Choose a file")

if uploaded_file:
    # Strategy selection
    strategy = st.selectbox("Chunking Strategy",
                           ["sentence_based", "paragraph_based", "fixed_size"])

    # Process file
    if st.button("Process"):
        chunker = create_chunker(strategy)
        result = chunker.chunk(uploaded_file)

        st.success(f"Created {len(result.chunks)} chunks")

        # Display chunks
        for i, chunk in enumerate(result.chunks):
            st.text_area(f"Chunk {i+1}", chunk.content, height=100)
```

---

## ðŸš€ **Performance & Hardware Optimization**

### Automatic Hardware Detection

```python
from chunking_strategy.core.hardware import get_hardware_info

# Check your system capabilities
hardware = get_hardware_info()
print(f"CPU cores: {hardware.cpu_count}")
print(f"Memory: {hardware.memory_total_gb:.1f} GB")
print(f"GPUs: {hardware.gpu_count}")
print(f"Recommended batch size: {hardware.recommended_batch_size}")
```

### Optimized Batch Processing

```python
from chunking_strategy.core.batch import BatchProcessor

processor = BatchProcessor()

# Automatic optimization based on your hardware
result = processor.process_files(
    files=file_list,
    default_strategy="universal_document",
    parallel_mode="process",  # or "thread", "sequential"
    workers=None,  # Auto-detected
    batch_size=None  # Auto-detected
)
```

### Performance Monitoring

```python
from chunking_strategy.core.metrics import ChunkingQualityEvaluator

# Evaluate chunk quality
evaluator = ChunkingQualityEvaluator()
metrics = evaluator.evaluate(result.chunks)

print(f"Quality Score: {metrics.coherence:.3f}")
print(f"Size Consistency: {metrics.size_consistency:.3f}")
print(f"Coverage: {metrics.coverage:.3f}")
```

---

## ðŸ”§ **Installation Options**

### Feature-Specific Installation

```bash
# Basic text processing
pip install chunking-strategy

# PDF processing
pip install chunking-strategy[document]

# Hardware optimization
pip install chunking-strategy[hardware]

# Universal document support (Apache Tika)
pip install chunking-strategy[tika]

# Vector database integrations
pip install chunking-strategy[vectordb]

# Everything included
pip install chunking-strategy[all]
```

### Dependencies by Feature

| Feature | Dependencies | Description |
|---------|-------------|-------------|
| `document` | PyMuPDF, PyPDF2, pdfminer.six | PDF processing with multiple backends |
| `hardware` | psutil, GPUtil | Hardware detection and optimization |
| `tika` | tika, python-magic | Universal document processing |
| `text` | spacy, nltk, sentence-transformers>=5.1.0, huggingface-hub | Advanced text processing + embeddings |
| `vectordb` | qdrant-client, weaviate-client | Vector database integrations |

---

## ðŸŽ¯ **Use Cases**

### RAG (Retrieval-Augmented Generation)

```python
# Optimal for RAG systems
chunker = create_chunker(
    "sentence_based",
    max_sentences=3,      # Good balance of context and specificity
    overlap_sentences=1   # Overlap for better retrieval
)
```

### Vector Database Indexing

```python
# Consistent sizes for vector DBs
chunker = create_chunker(
    "fixed_size",
    chunk_size=512,      # Fits most embedding models
    overlap_size=50      # Prevents information loss at boundaries
)
```

### ðŸ”® **Embeddings & Vector Database Integration**

**Complete workflow from chunking â†’ embeddings â†’ vector database:**

```python
from chunking_strategy.orchestrator import ChunkerOrchestrator
from chunking_strategy.core.embeddings import (
    EmbeddingConfig, EmbeddingModel, OutputFormat,
    embed_chunking_result, export_for_vector_db
)

# Step 1: Chunk your documents
orchestrator = ChunkerOrchestrator()
chunks = orchestrator.chunk_file("document.pdf")

# Step 2: Generate embeddings with multiple model options
config = EmbeddingConfig(
    model=EmbeddingModel.ALL_MINILM_L6_V2,    # Fast & lightweight (384D)
    # model=EmbeddingModel.ALL_MPNET_BASE_V2,  # High quality (768D)
    # model=EmbeddingModel.CLIP_VIT_B_32,      # Multimodal (512D)
    output_format=OutputFormat.FULL_METADATA,  # Include all metadata
    batch_size=32,
    normalize_embeddings=True
)

embedding_result = embed_chunking_result(chunks, config)
print(f"âœ… Generated {embedding_result.total_chunks} embeddings ({embedding_result.embedding_dim}D)")

# Step 3: Export ready for vector databases
vector_data = export_for_vector_db(embedding_result, format="dict")
# Now ready for Qdrant, Weaviate, Pinecone, ChromaDB!
```

**ðŸ”‘ HuggingFace Authentication Setup:**

1. **Get your token**: Visit https://huggingface.co/settings/tokens
2. **Method 1 - Config file**:
   ```bash
   cp config/huggingface_token.py.template config/huggingface_token.py
   # Edit and add your token: HUGGINGFACE_TOKEN = "hf_your_token_here"
   ```

3. **Method 2 - Environment variable**:
   ```bash
   export HF_TOKEN="hf_your_token_here"
   ```

**Supported Embedding Models:**

| Model | Dimensions | Use Case | Speed |
|-------|------------|----------|-------|
| `ALL_MINILM_L6_V2` | 384 | Fast development, testing | âš¡âš¡âš¡ |
| `ALL_MPNET_BASE_V2` | 768 | High quality | âš¡âš¡ |
| `ALL_DISTILROBERTA_V1` | 768 | Code embeddings | âš¡âš¡ |
| `CLIP_VIT_B_32` | 512 | Text + images | âš¡ |

**CLI Embeddings:**

```bash
# Generate embeddings for all files in directory
chunking-strategy embed documents/ --model all-MiniLM-L6-v2 --output-format full_metadata

# Batch process with specific settings
chunking-strategy embed-batch data/ --batch-size 64 --normalize
```

### Document Analysis & Summarization

```python
# Preserve document structure
chunker = create_chunker(
    "paragraph_based",
    max_paragraphs=2,
    preserve_structure=True
)
```

### Multi-Format Document Processing

```python
# Handle any file type
chunker = create_chunker(
    "universal_document",
    chunk_size=1000,
    extract_metadata=True,
    preserve_structure=True
)
```

---

## ðŸ“Š **Quality & Metrics**

### Built-in Quality Evaluation

```python
from chunking_strategy.core.metrics import ChunkingQualityEvaluator

chunker = create_chunker("sentence_based", max_sentences=3)
result = chunker.chunk("document.pdf")

# Evaluate quality
evaluator = ChunkingQualityEvaluator()
metrics = evaluator.evaluate(result.chunks)

print(f"Size Consistency: {metrics.size_consistency:.3f}")
print(f"Semantic Coherence: {metrics.coherence:.3f}")
print(f"Content Coverage: {metrics.coverage:.3f}")
print(f"Boundary Quality: {metrics.boundary_quality:.3f}")
```

### Adaptive Optimization

```python
# Chunkers can adapt based on feedback
chunker = create_chunker("fixed_size", chunk_size=1000)

# Simulate quality feedback
chunker.adapt_parameters(0.3, "quality")  # Low quality score
# Chunker automatically adjusts parameters for better quality
```

---

## ðŸ› ï¸ **Advanced Features**

### Custom Chunking Strategy

```python
from chunking_strategy.core.base import BaseChunker
from chunking_strategy.core.registry import register_chunker

@register_chunker(name="custom_chunker", category="custom")
class CustomChunker(BaseChunker):
    def chunk(self, content, **kwargs):
        # Your custom chunking logic
        chunks = self.custom_logic(content)
        return ChunkingResult(chunks=chunks)

# Use your custom chunker
chunker = create_chunker("custom_chunker")
```

### Pipeline Processing

```python
from chunking_strategy.core.pipeline import ChunkingPipeline

pipeline = ChunkingPipeline([
    ("preprocessing", preprocessor),
    ("chunking", chunker),
    ("postprocessing", postprocessor),
    ("quality_check", quality_evaluator)
])

result = pipeline.process("document.pdf")
```

### Streaming for Large Files

```python
# Memory-efficient processing of large files
chunker = create_chunker("fixed_size", chunk_size=1000)

def file_stream():
    with open("huge_file.txt", 'r') as f:
        for line in f:
            yield line

# Process without loading entire file into memory
for chunk in chunker.chunk_stream(file_stream()):
    process_chunk(chunk)
```

---

## ðŸ” **Error Handling & Debugging**

### Robust Error Handling

```python
def safe_chunking(file_path, strategies=None):
    """Chunk with fallback strategies."""
    if strategies is None:
        strategies = ["sentence_based", "paragraph_based", "fixed_size"]

    for strategy in strategies:
        try:
            chunker = create_chunker(strategy)
            return chunker.chunk(file_path)
        except Exception as e:
            print(f"Strategy {strategy} failed: {e}")
            continue

    raise Exception("All chunking strategies failed")

result = safe_chunking("document.pdf")
```

### Logging and Monitoring

```python
import logging

# Enable detailed logging
logging.basicConfig(level=logging.INFO)

# Chunking operations will now log detailed information
chunker = create_chunker("sentence_based")
result = chunker.chunk("document.pdf")

# Monitor performance
print(f"Processing time: {result.processing_time:.3f}s")
print(f"Chunks created: {len(result.chunks)}")
print(f"Average chunk size: {sum(len(c.content) for c in result.chunks) / len(result.chunks):.1f}")
```

---

## ðŸ“š **API Reference**

### Core Functions

```python
# Create chunkers
create_chunker(strategy_name, **params) -> BaseChunker

# List available strategies
list_chunkers() -> List[str]

# Get chunker metadata
get_chunker_metadata(strategy_name) -> ChunkerMetadata

# Configuration-driven processing
ChunkerOrchestrator(config_path) -> orchestrator
```

### Chunking Results

```python
# ChunkingResult attributes
result.chunks          # List[Chunk]
result.processing_time  # float
result.strategy_used    # str
result.source_info     # Dict[str, Any]
result.total_chunks    # int

# Chunk attributes
chunk.id              # str
chunk.content         # str
chunk.modality        # ModalityType
chunk.metadata        # ChunkMetadata
```

### Hardware Optimization

```python
# Hardware detection
get_hardware_info() -> HardwareInfo

# Batch configuration
get_optimal_batch_config(total_files, avg_file_size_mb) -> Dict

# Batch processing
BatchProcessor().process_files(files, strategy, **options) -> BatchResult
```

---

## ðŸ¤ **Contributing**

We welcome contributions! Please feel free to submit a Pull Request or open an issue for:
- Bug fixes and improvements
- New chunking strategies
- Documentation improvements
- Performance optimizations

### Development Setup

```bash
git clone https://github.com/sharanharsoor/chunking.git
cd chunking
pip install -e .[dev,all]
pytest tests/
```

---

## ðŸ“„ **License**

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ðŸ”— **Links**

- **Repository**: [GitHub repository](https://github.com/sharanharsoor/chunking)
- **Examples**: [Example scripts and demos](examples/)
- **Issues**: [Bug reports and feature requests](https://github.com/sharanharsoor/chunking/issues)
- **PyPI**: Package will be available on PyPI soon

---

**Ready to transform your document processing?** Install now and start chunking smarter! ðŸš€

```bash
pip install chunking-strategy[all]
```

Metadata-Version: 2.4
Name: crawlo
Version: 1.2.3
Summary: Crawlo æ˜¯ä¸€æ¬¾åŸºäºå¼‚æ­¥IOçš„é«˜æ€§èƒ½Pythonçˆ¬è™«æ¡†æ¶ï¼Œæ”¯æŒåˆ†å¸ƒå¼æŠ“å–ã€‚
Home-page: https://github.com/crawl-coder/Crawlo.git
Author: crawl-coder
Author-email: crawlo@qq.com
License: MIT
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.6
Description-Content-Type: text/markdown
Requires-Dist: aiohttp>=3.12.14
Requires-Dist: aiomysql>=0.2.0
Requires-Dist: aioredis>=2.0.1
Requires-Dist: asyncmy>=0.2.10
Requires-Dist: cssselect>=1.2.0
Requires-Dist: dateparser>=1.2.2
Requires-Dist: httpx[http2]>=0.27.0
Requires-Dist: curl-cffi>=0.13.0
Requires-Dist: lxml>=5.2.1
Requires-Dist: motor>=3.7.0
Requires-Dist: parsel>=1.9.1
Requires-Dist: pydantic>=2.11.7
Requires-Dist: pymongo>=4.11
Requires-Dist: PyMySQL>=1.1.1
Requires-Dist: python-dateutil>=2.9.0.post0
Requires-Dist: redis>=6.2.0
Requires-Dist: requests>=2.32.4
Requires-Dist: six>=1.17.0
Requires-Dist: ujson>=5.9.0
Requires-Dist: urllib3>=2.5.0
Requires-Dist: w3lib>=2.1.2
Requires-Dist: rich>=14.1.0
Requires-Dist: astor>=0.8.1
Requires-Dist: watchdog>=6.0.0
Provides-Extra: render
Requires-Dist: webdriver-manager>=4.0.0; extra == "render"
Requires-Dist: playwright; extra == "render"
Requires-Dist: selenium>=3.141.0; extra == "render"
Provides-Extra: all
Requires-Dist: bitarray>=1.5.3; extra == "all"
Requires-Dist: PyExecJS>=1.5.1; extra == "all"
Requires-Dist: pymongo>=3.10.1; extra == "all"
Requires-Dist: redis-py-cluster>=2.1.0; extra == "all"
Requires-Dist: webdriver-manager>=4.0.0; extra == "all"
Requires-Dist: playwright; extra == "all"
Requires-Dist: selenium>=3.141.0; extra == "all"

<!-- markdownlint-disable MD033 MD041 -->
<div align="center">
  <h1 align="center">Crawlo</h1>
  <p align="center">å¼‚æ­¥åˆ†å¸ƒå¼çˆ¬è™«æ¡†æ¶</p>
  <p align="center"><strong>åŸºäº asyncio çš„é«˜æ€§èƒ½å¼‚æ­¥åˆ†å¸ƒå¼çˆ¬è™«æ¡†æ¶ï¼Œæ”¯æŒå•æœºå’Œåˆ†å¸ƒå¼éƒ¨ç½²</strong></p>
  
  <p align="center">
    <a href="https://www.python.org/downloads/">
      <img src="https://img.shields.io/badge/python-3.8%2B-blue" alt="Python Version">
    </a>
    <a href="LICENSE">
      <img src="https://img.shields.io/badge/license-MIT-green" alt="License">
    </a>
    <a href="https://crawlo.readthedocs.io/">
      <img src="https://img.shields.io/badge/docs-latest-brightgreen" alt="Documentation">
    </a>
    <a href="https://github.com/crawlo/crawlo/actions">
      <img src="https://github.com/crawlo/crawlo/workflows/CI/badge.svg" alt="CI Status">
    </a>
  </p>
  
  <p align="center">
    <a href="#-ç‰¹æ€§">ç‰¹æ€§</a> â€¢
    <a href="#-å¿«é€Ÿå¼€å§‹">å¿«é€Ÿå¼€å§‹</a> â€¢
    <a href="#-å‘½ä»¤è¡Œå·¥å…·">å‘½ä»¤è¡Œå·¥å…·</a> â€¢
    <a href="#-ç¤ºä¾‹é¡¹ç›®">ç¤ºä¾‹é¡¹ç›®</a>
  </p>
</div>

<br />

<!-- ç‰¹æ€§ section -->
<div align="center">
  <h2>ğŸŒŸ ç‰¹æ€§</h2>

  <table>
    <thead>
      <tr>
        <th>ç‰¹æ€§</th>
        <th>æè¿°</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>âš¡ <strong>å¼‚æ­¥é«˜æ€§èƒ½</strong></td>
        <td>åŸºäº asyncio å®ç°ï¼Œå……åˆ†åˆ©ç”¨ç°ä»£ CPU å¤¯æ€§èƒ½</td>
      </tr>
      <tr>
        <td>ğŸŒ <strong>åˆ†å¸ƒå¼æ”¯æŒ</strong></td>
        <td>å†…ç½® Redis é˜Ÿåˆ—ï¼Œè½»æ¾å®ç°åˆ†å¸ƒå¼éƒ¨ç½²</td>
      </tr>
      <tr>
        <td>ğŸ”§ <strong>æ¨¡å—åŒ–è®¾è®¡</strong></td>
        <td>ä¸­é—´ä»¶ã€ç®¡é“ã€æ‰©å±•ç»„ä»¶ç³»ç»Ÿï¼Œæ˜“äºå®šåˆ¶å’Œæ‰©å±•</td>
      </tr>
      <tr>
        <td>ğŸ”„ <strong>æ™ºèƒ½å»é‡</strong></td>
        <td>å¤šç§å»é‡ç­–ç•¥ï¼ˆå†…å­˜ã€Redisã€Bloom Filterï¼‰</td>
      </tr>
      <tr>
        <td>âš™ï¸ <strong>çµæ´»é…ç½®</strong></td>
        <td>æ”¯æŒå¤šç§é…ç½®æ–¹å¼ï¼Œé€‚åº”ä¸åŒåœºæ™¯éœ€æ±‚</td>
      </tr>
      <tr>
        <td>ğŸ“‹ <strong>é«˜çº§æ—¥å¿—</strong></td>
        <td>æ”¯æŒæ—¥å¿—è½®è½¬ã€ç»“æ„åŒ–æ—¥å¿—ã€JSONæ ¼å¼ç­‰é«˜çº§åŠŸèƒ½</td>
      </tr>
      <tr>
        <td>ğŸ“š <strong>ä¸°å¯Œæ–‡æ¡£</strong></td>
        <td>å®Œæ•´çš„ä¸­è‹±æ–‡åŒè¯­æ–‡æ¡£å’Œç¤ºä¾‹é¡¹ç›®</td>
      </tr>
    </tbody>
  </table>
</div>

<br />

---

<!-- å¿«é€Ÿå¼€å§‹ section -->
<h2 align="center">ğŸš€ å¿«é€Ÿå¼€å§‹</h2>

### å®‰è£…

```bash
pip install crawlo
```

### åˆ›å»ºé¡¹ç›®

```bash
# åˆ›å»ºé»˜è®¤é¡¹ç›®
crawlo startproject myproject

# åˆ›å»ºåˆ†å¸ƒå¼æ¨¡æ¿é¡¹ç›®
crawlo startproject myproject distributed

# åˆ›å»ºé¡¹ç›®å¹¶é€‰æ‹©ç‰¹å®šæ¨¡å—
crawlo startproject myproject --modules mysql,redis,proxy

cd myproject
```

### ç”Ÿæˆçˆ¬è™«

```bash
# åœ¨é¡¹ç›®ç›®å½•ä¸­ç”Ÿæˆçˆ¬è™«
crawlo genspider news_spider news.example.com
```

### ç¼–å†™çˆ¬è™«

```python
from crawlo import Spider, Request, Item

class MyItem(Item):
    title = ''
    url = ''

class MySpider(Spider):
    name = 'myspider'
    
    async def start_requests(self):
        yield Request('https://httpbin.org/get', callback=self.parse)
    
    async def parse(self, response):
        yield MyItem(
            title='Example Title',
            url=response.url
        )
```

### è¿è¡Œçˆ¬è™«

```bash
# ä½¿ç”¨å‘½ä»¤è¡Œå·¥å…·è¿è¡Œçˆ¬è™«ï¼ˆæ¨èï¼‰
crawlo run myspider

# ä½¿ç”¨é¡¹ç›®è‡ªå¸¦çš„ run.py è„šæœ¬è¿è¡Œ
python run.py

# è¿è¡Œæ‰€æœ‰çˆ¬è™«
crawlo run all

# åœ¨é¡¹ç›®å­ç›®å½•ä¸­ä¹Ÿèƒ½æ­£ç¡®è¿è¡Œ
cd subdirectory
crawlo run myspider
```

---

<!-- å‘½ä»¤è¡Œå·¥å…· section -->
<h2 align="center">ğŸ“œ å‘½ä»¤è¡Œå·¥å…·</h2>

Crawlo æä¾›äº†ä¸°å¯Œçš„å‘½ä»¤è¡Œå·¥å…·æ¥å¸®åŠ©å¼€å‘å’Œç®¡ç†çˆ¬è™«é¡¹ç›®ï¼š

### è·å–å¸®åŠ©

```bash
# æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
crawlo -h
crawlo --help
crawlo help
```

### crawlo startproject

åˆ›å»ºæ–°çš„çˆ¬è™«é¡¹ç›®ã€‚

```bash
# åŸºæœ¬ç”¨æ³•
crawlo startproject <project_name> [template_type] [--modules module1,module2]

# ç¤ºä¾‹
crawlo startproject my_spider_project
crawlo startproject news_crawler simple
crawlo startproject ecommerce_spider distributed --modules mysql,proxy
```

**å‚æ•°è¯´æ˜ï¼š**
- `project_name`: é¡¹ç›®åç§°ï¼ˆå¿…é¡»æ˜¯æœ‰æ•ˆçš„Pythonæ ‡è¯†ç¬¦ï¼‰
- `template_type`: æ¨¡æ¿ç±»å‹ï¼ˆå¯é€‰ï¼‰
  - `default`: é»˜è®¤æ¨¡æ¿ - é€šç”¨é…ç½®ï¼Œé€‚åˆå¤§å¤šæ•°é¡¹ç›®
  - `simple`: ç®€åŒ–æ¨¡æ¿ - æœ€å°é…ç½®ï¼Œé€‚åˆå¿«é€Ÿå¼€å§‹
  - `distributed`: åˆ†å¸ƒå¼æ¨¡æ¿ - é’ˆå¯¹åˆ†å¸ƒå¼çˆ¬å–ä¼˜åŒ–
  - `high-performance`: é«˜æ€§èƒ½æ¨¡æ¿ - é’ˆå¯¹å¤§è§„æ¨¡é«˜å¹¶å‘ä¼˜åŒ–
  - `gentle`: æ¸©å’Œæ¨¡æ¿ - ä½è´Ÿè½½é…ç½®ï¼Œå¯¹ç›®æ ‡ç½‘ç«™å‹å¥½
- `--modules`: é€‰æ‹©è¦åŒ…å«çš„æ¨¡å—ç»„ä»¶ï¼ˆå¯é€‰ï¼‰
  - `mysql`: MySQLæ•°æ®åº“æ”¯æŒ
  - `mongodb`: MongoDBæ•°æ®åº“æ”¯æŒ
  - `redis`: Redisæ”¯æŒï¼ˆåˆ†å¸ƒå¼é˜Ÿåˆ—å’Œå»é‡ï¼‰
  - `proxy`: ä»£ç†æ”¯æŒ
  - `monitoring`: ç›‘æ§å’Œæ€§èƒ½åˆ†æ
  - `dedup`: å»é‡åŠŸèƒ½
  - `httpx`: HttpXä¸‹è½½å™¨
  - `aiohttp`: AioHttpä¸‹è½½å™¨
  - `curl`: CurlCffiä¸‹è½½å™¨

### crawlo genspider

åœ¨ç°æœ‰é¡¹ç›®ä¸­ç”Ÿæˆæ–°çš„çˆ¬è™«ã€‚

```bash
# åŸºæœ¬ç”¨æ³•
crawlo genspider <spider_name> <domain>

# ç¤ºä¾‹
crawlo genspider news_spider news.example.com
crawlo genspider product_spider shop.example.com
```

**å‚æ•°è¯´æ˜ï¼š**
- `spider_name`: çˆ¬è™«åç§°ï¼ˆå¿…é¡»æ˜¯æœ‰æ•ˆçš„Pythonæ ‡è¯†ç¬¦ï¼‰
- `domain`: ç›®æ ‡åŸŸå

### crawlo run

è¿è¡Œçˆ¬è™«ã€‚

```bash
# åŸºæœ¬ç”¨æ³•
crawlo run <spider_name>|all [--json] [--no-stats]

# ç¤ºä¾‹
crawlo run myspider
crawlo run all
crawlo run all --json --no-stats
```

**å‚æ•°è¯´æ˜ï¼š**
- `spider_name`: è¦è¿è¡Œçš„çˆ¬è™«åç§°
- `all`: è¿è¡Œæ‰€æœ‰çˆ¬è™«
- `--json`: ä»¥JSONæ ¼å¼è¾“å‡ºç»“æœ
- `--no-stats`: ä¸è®°å½•ç»Ÿè®¡ä¿¡æ¯

### crawlo list

åˆ—å‡ºé¡¹ç›®ä¸­æ‰€æœ‰å¯ç”¨çš„çˆ¬è™«ã€‚

```bash
# åŸºæœ¬ç”¨æ³•
crawlo list [--json]

# ç¤ºä¾‹
crawlo list
crawlo list --json
```

**å‚æ•°è¯´æ˜ï¼š**
- `--json`: ä»¥JSONæ ¼å¼è¾“å‡ºç»“æœ

### crawlo check

æ£€æŸ¥çˆ¬è™«å®šä¹‰çš„åˆè§„æ€§ã€‚

```bash
# åŸºæœ¬ç”¨æ³•
crawlo check [--fix] [--ci] [--json] [--watch]

# ç¤ºä¾‹
crawlo check
crawlo check --fix
crawlo check --ci
crawlo check --watch
```

**å‚æ•°è¯´æ˜ï¼š**
- `--fix`: è‡ªåŠ¨ä¿®å¤å¸¸è§é—®é¢˜
- `--ci`: CIæ¨¡å¼è¾“å‡ºï¼ˆç®€æ´æ ¼å¼ï¼‰
- `--json`: ä»¥JSONæ ¼å¼è¾“å‡ºç»“æœ
- `--watch`: ç›‘å¬æ¨¡å¼ï¼Œæ–‡ä»¶æ›´æ”¹æ—¶è‡ªåŠ¨æ£€æŸ¥

### crawlo stats

æŸ¥çœ‹çˆ¬è™«è¿è¡Œç»Ÿè®¡ä¿¡æ¯ã€‚

```bash
# åŸºæœ¬ç”¨æ³•
crawlo stats [spider_name] [--all]

# ç¤ºä¾‹
crawlo stats
crawlo stats myspider
crawlo stats myspider --all
```

**å‚æ•°è¯´æ˜ï¼š**
- `spider_name`: æŒ‡å®šè¦æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯çš„çˆ¬è™«åç§°
- `--all`: æ˜¾ç¤ºæŒ‡å®šçˆ¬è™«çš„æ‰€æœ‰å†å²è¿è¡Œè®°å½•

---

<!-- æ¶æ„è®¾è®¡ section -->
<h2 align="center">ğŸ—ï¸ æ¶æ„è®¾è®¡</h2>

### æ ¸å¿ƒç»„ä»¶è¯´æ˜

Crawlo æ¡†æ¶ç”±ä»¥ä¸‹æ ¸å¿ƒç»„ä»¶æ„æˆï¼š

<table>
  <thead>
    <tr>
      <th>ç»„ä»¶</th>
      <th>åŠŸèƒ½æè¿°</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Crawler</strong></td>
      <td>çˆ¬è™«è¿è¡Œå®ä¾‹ï¼Œç®¡ç†Spiderä¸å¼•æ“çš„ç”Ÿå‘½å‘¨æœŸ</td>
    </tr>
    <tr>
      <td><strong>Engine</strong></td>
      <td>å¼•æ“ç»„ä»¶ï¼Œåè°ƒSchedulerã€Downloaderã€Processor</td>
    </tr>
    <tr>
      <td><strong>Scheduler</strong></td>
      <td>è°ƒåº¦å™¨ï¼Œç®¡ç†è¯·æ±‚é˜Ÿåˆ—å’Œå»é‡è¿‡æ»¤</td>
    </tr>
    <tr>
      <td><strong>Downloader</strong></td>
      <td>ä¸‹è½½å™¨ï¼Œè´Ÿè´£ç½‘ç»œè¯·æ±‚ï¼Œæ”¯æŒå¤šç§å®ç°(aiohttp, httpx, curl-cffi)</td>
    </tr>
    <tr>
      <td><strong>Processor</strong></td>
      <td>å¤„ç†å™¨ï¼Œå¤„ç†å“åº”æ•°æ®å’Œç®¡é“</td>
    </tr>
    <tr>
      <td><strong>QueueManager</strong></td>
      <td>ç»Ÿä¸€çš„é˜Ÿåˆ—ç®¡ç†å™¨ï¼Œæ”¯æŒå†…å­˜é˜Ÿåˆ—å’ŒRedisé˜Ÿåˆ—çš„è‡ªåŠ¨åˆ‡æ¢</td>
    </tr>
    <tr>
      <td><strong>Filter</strong></td>
      <td>è¯·æ±‚å»é‡è¿‡æ»¤å™¨ï¼Œæ”¯æŒå†…å­˜å’ŒRedisä¸¤ç§å®ç°</td>
    </tr>
    <tr>
      <td><strong>Middleware</strong></td>
      <td>ä¸­é—´ä»¶ç³»ç»Ÿï¼Œå¤„ç†è¯·æ±‚/å“åº”çš„é¢„å¤„ç†å’Œåå¤„ç†</td>
    </tr>
    <tr>
      <td><strong>Pipeline</strong></td>
      <td>æ•°æ®å¤„ç†ç®¡é“ï¼Œæ”¯æŒå¤šç§å­˜å‚¨æ–¹å¼(æ§åˆ¶å°ã€æ•°æ®åº“ç­‰)å’Œå»é‡åŠŸèƒ½</td>
    </tr>
    <tr>
      <td><strong>Spider</strong></td>
      <td>çˆ¬è™«åŸºç±»ï¼Œå®šä¹‰çˆ¬å–é€»è¾‘</td>
    </tr>
  </tbody>
</table>

### è¿è¡Œæ¨¡å¼

Crawloæ”¯æŒä¸‰ç§è¿è¡Œæ¨¡å¼ï¼š

<table>
  <thead>
    <tr>
      <th>æ¨¡å¼</th>
      <th>æè¿°</th>
      <th>é˜Ÿåˆ—ç±»å‹</th>
      <th>è¿‡æ»¤å™¨ç±»å‹</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>standalone</strong></td>
      <td>å•æœºæ¨¡å¼</td>
      <td>å†…å­˜é˜Ÿåˆ—</td>
      <td>å†…å­˜è¿‡æ»¤å™¨</td>
    </tr>
    <tr>
      <td><strong>distributed</strong></td>
      <td>åˆ†å¸ƒå¼æ¨¡å¼</td>
      <td>Redisé˜Ÿåˆ—</td>
      <td>Redisè¿‡æ»¤å™¨</td>
    </tr>
    <tr>
      <td><strong>auto</strong></td>
      <td>è‡ªåŠ¨æ£€æµ‹æ¨¡å¼</td>
      <td>æ ¹æ®ç¯å¢ƒè‡ªåŠ¨é€‰æ‹©æœ€ä½³è¿è¡Œæ–¹å¼</td>
      <td>æ ¹æ®ç¯å¢ƒè‡ªåŠ¨é€‰æ‹©</td>
    </tr>
  </tbody>
</table>

### æ¨¡å—å±‚æ¬¡ç»“æ„

```
crawlo/
â”œâ”€â”€ cli.py                          # å‘½ä»¤è¡Œæ¥å£
â”œâ”€â”€ crawler.py                      # çˆ¬è™«è¿è¡Œå®ä¾‹
â”œâ”€â”€ project.py                      # é¡¹ç›®ç®¡ç†
â”œâ”€â”€ config.py                       # é…ç½®ç®¡ç†
â”œâ”€â”€ mode_manager.py                 # è¿è¡Œæ¨¡å¼ç®¡ç†å™¨
â”œâ”€â”€ stats_collector.py              # ç»Ÿè®¡æ”¶é›†å™¨
â”œâ”€â”€ subscriber.py                   # äº‹ä»¶è®¢é˜…å™¨
â”œâ”€â”€ task_manager.py                 # ä»»åŠ¡ç®¡ç†å™¨
â”œâ”€â”€ event.py                        # äº‹ä»¶å®šä¹‰
â”œâ”€â”€ exceptions.py                   # å¼‚å¸¸å®šä¹‰
â”œâ”€â”€
â”œâ”€â”€ core/                           # æ ¸å¿ƒç»„ä»¶
â”‚   â”œâ”€â”€ engine.py                   # å¼•æ“
â”‚   â”œâ”€â”€ scheduler.py                # è°ƒåº¦å™¨
â”‚   â”œâ”€â”€ processor.py                # å¤„ç†å™¨
â”‚
â”œâ”€â”€ spider/                         # çˆ¬è™«åŸºç±»
â”‚   â””â”€â”€ __init__.py                 # çˆ¬è™«å…ƒç±»å’ŒåŸºç±»
â”‚
â”œâ”€â”€ network/                        # ç½‘ç»œç›¸å…³
â”‚   â”œâ”€â”€ request.py                  # è¯·æ±‚å¯¹è±¡
â”‚   â””â”€â”€ response.py                 # å“åº”å¯¹è±¡
â”‚
â”œâ”€â”€ downloader/                     # ä¸‹è½½å™¨
â”‚   â”œâ”€â”€ __init__.py                 # ä¸‹è½½å™¨åŸºç±»
â”‚   â”œâ”€â”€ aiohttp_downloader.py      # AioHttpå®ç°
â”‚   â”œâ”€â”€ httpx_downloader.py        # HttpXå®ç°
â”‚   â””â”€â”€ cffi_downloader.py         # CurlCffiå®ç°
â”‚
â”œâ”€â”€ queue/                          # é˜Ÿåˆ—ç®¡ç†
â”‚   â”œâ”€â”€ __init__.py                 
â”‚   â”œâ”€â”€ queue_manager.py           # é˜Ÿåˆ—ç®¡ç†å™¨
â”‚   â”œâ”€â”€ pqueue.py                  # å†…å­˜ä¼˜å…ˆé˜Ÿåˆ—
â”‚   â””â”€â”€ redis_priority_queue.py    # Redisä¼˜å…ˆé˜Ÿåˆ—
â”‚
â”œâ”€â”€ filters/                        # è¿‡æ»¤å™¨
â”‚   â”œâ”€â”€ __init__.py                 
â”‚   â”œâ”€â”€ base_filter.py             # è¿‡æ»¤å™¨åŸºç±»
â”‚   â”œâ”€â”€ memory_filter.py           # å†…å­˜è¿‡æ»¤å™¨
â”‚   â””â”€â”€ aioredis_filter.py         # Redisè¿‡æ»¤å™¨
â”‚
â”œâ”€â”€ middleware/                     # ä¸­é—´ä»¶
â”‚   â”œâ”€â”€ __init__.py                 
â”‚   â”œâ”€â”€ middleware_manager.py      # ä¸­é—´ä»¶ç®¡ç†å™¨
â”‚   â”œâ”€â”€ default_header.py          # é»˜è®¤è¯·æ±‚å¤´
â”‚   â”œâ”€â”€ download_delay.py          # ä¸‹è½½å»¶è¿Ÿ
â”‚   â”œâ”€â”€ proxy.py                   # ä»£ç†æ”¯æŒ
â”‚   â”œâ”€â”€ request_ignore.py          # è¯·æ±‚å¿½ç•¥
â”‚   â”œâ”€â”€ response_code.py           # å“åº”ç å¤„ç†
â”‚   â”œâ”€â”€ response_filter.py         # å“åº”è¿‡æ»¤
â”‚   â””â”€â”€ retry.py                   # é‡è¯•æœºåˆ¶
â”‚
â”œâ”€â”€ pipelines/                      # æ•°æ®ç®¡é“
â”‚   â”œâ”€â”€ __init__.py                 
â”‚   â”œâ”€â”€ pipeline_manager.py        # ç®¡é“ç®¡ç†å™¨
â”‚   â”œâ”€â”€ base_pipeline.py           # ç®¡é“åŸºç±»
â”‚   â”œâ”€â”€ console_pipeline.py        # æ§åˆ¶å°è¾“å‡ºç®¡é“
â”‚   â”œâ”€â”€ json_pipeline.py           # JSONå­˜å‚¨ç®¡é“
â”‚   â”œâ”€â”€ redis_dedup_pipeline.py    # Rediså»é‡ç®¡é“
â”‚   â””â”€â”€ mysql_pipeline.py          # MySQLå­˜å‚¨ç®¡é“
â”‚
â”œâ”€â”€ extension/                      # æ‰©å±•ç»„ä»¶
â”‚   â”œâ”€â”€ __init__.py                 
â”‚   â”œâ”€â”€ log_interval.py            # å®šæ—¶æ—¥å¿—
â”‚   â”œâ”€â”€ log_stats.py               # ç»Ÿè®¡æ—¥å¿—
â”‚   â”œâ”€â”€ logging_extension.py       # æ—¥å¿—æ‰©å±•
â”‚   â”œâ”€â”€ memory_monitor.py          # å†…å­˜ç›‘æ§
â”‚   â”œâ”€â”€ performance_profiler.py    # æ€§èƒ½åˆ†æ
â”‚   â”œâ”€â”€ health_check.py            # å¥åº·æ£€æŸ¥
â”‚   â””â”€â”€ request_recorder.py        # è¯·æ±‚è®°å½•
â”‚
â”œâ”€â”€ settings/                       # é…ç½®ç³»ç»Ÿ
â”‚   â”œâ”€â”€ __init__.py                 
â”‚   â”œâ”€â”€ default_settings.py        # é»˜è®¤é…ç½®
â”‚   â””â”€â”€ setting_manager.py         # é…ç½®ç®¡ç†å™¨
â”‚
â”œâ”€â”€ utils/                          # å·¥å…·åº“
â”‚   â”œâ”€â”€ __init__.py                 
â”‚   â”œâ”€â”€ log.py                     # åŸºç¡€æ—¥å¿—å·¥å…·
â”‚   â”œâ”€â”€ request.py                 # è¯·æ±‚å·¥å…·
â”‚   â”œâ”€â”€ request_serializer.py      # è¯·æ±‚åºåˆ—åŒ–
â”‚   â””â”€â”€ func_tools.py              # å‡½æ•°å·¥å…·
â”‚
â””â”€â”€ templates/                      # æ¨¡æ¿æ–‡ä»¶
    â”œâ”€â”€ project/                   
    â””â”€â”€ spider/
```

---

<!-- é…ç½®ç³»ç»Ÿ section -->
<h2 align="center">ğŸ›ï¸ é…ç½®ç³»ç»Ÿ</h2>

### ä¼ ç»Ÿé…ç½®æ–¹å¼

```
# settings.py
PROJECT_NAME = 'myproject'
CONCURRENCY = 16
DOWNLOAD_DELAY = 1.0
QUEUE_TYPE = 'memory'  # å•æœºæ¨¡å¼
# QUEUE_TYPE = 'redis'   # åˆ†å¸ƒå¼æ¨¡å¼

# Redis é…ç½® (åˆ†å¸ƒå¼æ¨¡å¼ä¸‹ä½¿ç”¨)
REDIS_HOST = 'localhost'
REDIS_PORT = 6379
REDIS_DB = 0
REDIS_PASSWORD = ''

# æ•°æ®ç®¡é“é…ç½®
PIPELINES = [
    'crawlo.pipelines.console_pipeline.ConsolePipeline',
    'crawlo.pipelines.json_pipeline.JsonPipeline',
    'crawlo.pipelines.redis_dedup_pipeline.RedisDedupPipeline',  # Rediså»é‡ç®¡é“
    'crawlo.pipelines.mysql_pipeline.AsyncmyMySQLPipeline',      # MySQLå­˜å‚¨ç®¡é“
]

# é«˜çº§æ—¥å¿—é…ç½®
LOG_FILE = 'logs/spider.log'
LOG_LEVEL = 'INFO'
LOG_MAX_BYTES = 10 * 1024 * 1024  # 10MB
LOG_BACKUP_COUNT = 5
LOG_JSON_FORMAT = False  # è®¾ç½®ä¸ºTrueå¯ç”¨JSONæ ¼å¼

# å¯ç”¨é«˜çº§æ—¥å¿—æ‰©å±•
ADVANCED_LOGGING_ENABLED = True

# å¯ç”¨æ—¥å¿—ç›‘æ§
LOG_MONITOR_ENABLED = True
LOG_MONITOR_INTERVAL = 30
LOG_MONITOR_DETAILED_STATS = True

# æ·»åŠ æ‰©å±•
EXTENSIONS = [
    'crawlo.extension.log_interval.LogIntervalExtension',
    'crawlo.extension.log_stats.LogStats',
    'crawlo.extension.logging_extension.CustomLoggerExtension',
    'crawlo.extension.memory_monitor.MemoryMonitorExtension',
]
```

### MySQL ç®¡é“é…ç½®

Crawlo æä¾›äº†ç°æˆçš„ MySQL ç®¡é“å®ç°ï¼Œå¯ä»¥è½»æ¾å°†çˆ¬å–çš„æ•°æ®å­˜å‚¨åˆ° MySQL æ•°æ®åº“ä¸­ï¼š

```
# åœ¨ settings.py ä¸­å¯ç”¨ MySQL ç®¡é“
PIPELINES = [
    'crawlo.pipelines.mysql_pipeline.AsyncmyMySQLPipeline',
]

# MySQL æ•°æ®åº“é…ç½®
MYSQL_HOST = 'localhost'
MYSQL_PORT = 3306
MYSQL_USER = 'your_username'
MYSQL_PASSWORD = 'your_password'
MYSQL_DB = 'your_database'
MYSQL_TABLE = 'your_table_name'

# å¯é€‰çš„æ‰¹é‡æ’å…¥é…ç½®
MYSQL_BATCH_SIZE = 100
MYSQL_USE_BATCH = True
```

MySQL ç®¡é“ç‰¹æ€§ï¼š
- **å¼‚æ­¥æ“ä½œ**ï¼šåŸºäº asyncmy é©±åŠ¨ï¼Œæä¾›é«˜æ€§èƒ½çš„å¼‚æ­¥æ•°æ®åº“æ“ä½œ
- **è¿æ¥æ± **ï¼šè‡ªåŠ¨ç®¡ç†æ•°æ®åº“è¿æ¥ï¼Œæé«˜æ•ˆç‡
- **æ‰¹é‡æ’å…¥**ï¼šæ”¯æŒæ‰¹é‡æ’å…¥ä»¥æé«˜æ€§èƒ½
- **äº‹åŠ¡æ”¯æŒ**ï¼šç¡®ä¿æ•°æ®ä¸€è‡´æ€§
- **çµæ´»é…ç½®**ï¼šæ”¯æŒè‡ªå®šä¹‰è¡¨åã€æ‰¹é‡å¤§å°ç­‰å‚æ•°

### å‘½ä»¤è¡Œé…ç½®

```
# è¿è¡Œå•ä¸ªçˆ¬è™«
crawlo run myspider

# è¿è¡Œæ‰€æœ‰çˆ¬è™«
crawlo run all

# åœ¨é¡¹ç›®å­ç›®å½•ä¸­ä¹Ÿèƒ½æ­£ç¡®è¿è¡Œ
cd subdirectory
crawlo run myspider
```

---

<!-- æ ¸å¿ƒç»„ä»¶ section -->
<h2 align="center">ğŸ§© æ ¸å¿ƒç»„ä»¶</h2>

### ä¸­é—´ä»¶ç³»ç»Ÿ
çµæ´»çš„ä¸­é—´ä»¶ç³»ç»Ÿï¼Œæ”¯æŒè¯·æ±‚é¢„å¤„ç†ã€å“åº”å¤„ç†å’Œå¼‚å¸¸å¤„ç†ã€‚

### ç®¡é“ç³»ç»Ÿ
å¯æ‰©å±•çš„æ•°æ®å¤„ç†ç®¡é“ï¼Œæ”¯æŒå¤šç§å­˜å‚¨æ–¹å¼ï¼ˆæ§åˆ¶å°ã€æ•°æ®åº“ç­‰ï¼‰å’Œå»é‡åŠŸèƒ½ï¼š
- **ConsolePipeline**: æ§åˆ¶å°è¾“å‡ºç®¡é“
- **JsonPipeline**: JSONæ–‡ä»¶å­˜å‚¨ç®¡é“
- **RedisDedupPipeline**: Rediså»é‡ç®¡é“ï¼ŒåŸºäºRedisé›†åˆå®ç°åˆ†å¸ƒå¼å»é‡
- **AsyncmyMySQLPipeline**: MySQLæ•°æ®åº“å­˜å‚¨ç®¡é“ï¼ŒåŸºäºasyncmyé©±åŠ¨

### æ‰©å±•ç»„ä»¶
åŠŸèƒ½å¢å¼ºæ‰©å±•ï¼ŒåŒ…æ‹¬æ—¥å¿—ã€ç›‘æ§ã€æ€§èƒ½åˆ†æç­‰ï¼š
- **LogIntervalExtension**: å®šæ—¶æ—¥å¿—æ‰©å±•
- **LogStats**: ç»Ÿè®¡æ—¥å¿—æ‰©å±•
- **CustomLoggerExtension**: è‡ªå®šä¹‰æ—¥å¿—æ‰©å±•
- **MemoryMonitorExtension**: å†…å­˜ç›‘æ§æ‰©å±•
- **PerformanceProfilerExtension**: æ€§èƒ½åˆ†ææ‰©å±•
- **HealthCheckExtension**: å¥åº·æ£€æŸ¥æ‰©å±•
- **RequestRecorderExtension**: è¯·æ±‚è®°å½•æ‰©å±•

### è¿‡æ»¤ç³»ç»Ÿ
æ™ºèƒ½å»é‡è¿‡æ»¤ï¼Œæ”¯æŒå¤šç§å»é‡ç­–ç•¥ï¼ˆå†…å­˜ã€Redisã€Bloom Filterï¼‰ã€‚

---

<!-- ç¤ºä¾‹é¡¹ç›® section -->
<h2 align="center">ğŸ“¦ ç¤ºä¾‹é¡¹ç›®</h2>

- [OFweekåˆ†å¸ƒå¼çˆ¬è™«](examples/ofweek_distributed/) - å¤æ‚çš„åˆ†å¸ƒå¼çˆ¬è™«ç¤ºä¾‹ï¼ŒåŒ…å«Rediså»é‡åŠŸèƒ½
- [OFweekç‹¬ç«‹çˆ¬è™«](examples/ofweek_standalone/) - ç‹¬ç«‹è¿è¡Œçš„çˆ¬è™«ç¤ºä¾‹
- [OFweekæ··åˆæ¨¡å¼çˆ¬è™«](examples/ofweek_spider/) - æ”¯æŒå•æœºå’Œåˆ†å¸ƒå¼æ¨¡å¼åˆ‡æ¢çš„çˆ¬è™«ç¤ºä¾‹

---

<!-- æ–‡æ¡£ section -->
<h2 align="center">ğŸ“š æ–‡æ¡£</h2>

å®Œæ•´çš„æ–‡æ¡£è¯·è®¿é—® [Crawlo Documentation](https://crawlo.readthedocs.io/)

- [å¿«é€Ÿå¼€å§‹æŒ‡å—](docs/modules/index.md)
- [æ¨¡å—åŒ–æ–‡æ¡£](docs/modules/index.md)
- [æ ¸å¿ƒå¼•æ“æ–‡æ¡£](docs/modules/core/engine.md)
- [è°ƒåº¦å™¨æ–‡æ¡£](docs/modules/core/scheduler.md)
- [ä¸‹è½½å™¨æ–‡æ¡£](docs/modules/downloader/index.md)
- [ä¸­é—´ä»¶æ–‡æ¡£](docs/modules/middleware/index.md)
- [ç®¡é“æ–‡æ¡£](docs/modules/pipeline/index.md)
- [é˜Ÿåˆ—æ–‡æ¡£](docs/modules/queue/index.md)
- [è¿‡æ»¤å™¨æ–‡æ¡£](docs/modules/filter/index.md)
- [æ‰©å±•ç»„ä»¶æ–‡æ¡£](docs/modules/extension/index.md)

---

<!-- è´¡çŒ® section -->
<h2 align="center">ğŸ¤ è´¡çŒ®</h2>

æ¬¢è¿æäº¤ Issue å’Œ Pull Request æ¥å¸®åŠ©æ”¹è¿› Crawloï¼

---

<!-- è®¸å¯è¯ section -->
<h2 align="center">ğŸ“„ è®¸å¯è¯</h2>

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ï¼Œè¯¦æƒ…è¯·è§ [LICENSE](LICENSE) æ–‡ä»¶ã€‚

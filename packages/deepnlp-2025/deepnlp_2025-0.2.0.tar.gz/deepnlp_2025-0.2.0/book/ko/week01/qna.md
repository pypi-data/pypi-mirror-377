# Transformer, Mamba, RWKV, Jamba 아키텍처 Q&A

## Transformer 아키텍처

**Q:** Transformer에서 Self-Attention이 가지는 주요 장점은 무엇인가? 또한 추론 시 병목 현상은 어떤 부분에서 발생하는가?

**A:** **Transformer**의 핵심인 **Self-Attention(자가 어텐션)** 메커니즘 덕분에 **입력 시퀀스를 병렬 처리**할 수 있다는 점이 가장 큰 장점이다. RNN과 달리 순환 구조가 없어 모든 토큰 간의 관계를 동시에 계산하므로 **긴 의존 관계**도 효율적으로 학습할 수 있고 학습 속도도 빠르다. 다만 **추론(inference)** 단계에서는 **병목 현상**이 발생하는데, 새로운 토큰을 생성할 때마다 **이전 모든 토큰들과의 어텐션**을 계산해야 하기 때문이다. 예를 들어 시퀀스 길이가 $L$이면, 현재 토큰을 낼 때 과거 $L$개 토큰에 대한 어텐션을 모두 구해야 하므로 토큰을 하나 생성하는 데 $O(L)$의 연산이 들고, 전체적으로 보면 생성 과정이 **길이에 따라 느려지는 병목**을 겪는다. 이로 인해 Transformer 모델은 문맥 길이가 길어질수록 **추론 속도가 저하**되고 **메모리 사용량**도 크게 늘어나는 한계가 있다.

**Q:** Transformer 인코더-디코더 구조와 GPT 같은 디코더-Only 구조의 차이를 설명하라.

**A:** **Transformer 인코더-디코더 모델**은 **인코더(encoder)**와 **디코더(decoder)** 두 부분으로 구성된다. 인코더는 입력 시퀀스를 받아 내부 **컨텍스트 표현**으로 변환하고, 디코더는 이 컨텍스트와 이전까지 생성된 토큰들을 참고하여 출력 시퀀스를 한 토큰씩 생성한다. 디코더 층에서는 자기 자신에 **마스크드(masked) Self-Attention**을 적용해 미래 토큰을 보지 못하게 하며, **인코더-디코더 Attention(교차 어텐션)**으로 인코더의 컨텍스트를 참조한다. 반면 **GPT와 같은 디코더-Only 구조**는 **인코더가 없고** 디코더 하나로만 이루어진 **단일 스트림** 구조다. 오직 이전 토큰들에 대한 **Self-Attention**만 사용하여 다음 토큰을 예측하며, 별도의 인코더 입력이나 교차 어텐션이 없다. 요약하면, 인코더-디코더 모델은 **입력 시퀀스와 출력 시퀀스가 분리**되어 상호작용(교차 어텐션)을 하는 구조이고, 디코더-Only 모델은 **하나의 시퀀스**에서 **순차 생성**만을 수행하는 구조다.

**Q:** 시퀀스 길이 $L$에 대해 Transformer의 시간 복잡도와 공간 복잡도는 각각 어떻게 스케일링되는가?

**A:** **Transformer**에서 Self-Attention 연산의 **시간 복잡도**는 시퀀스 길이에 대해 **$O(L^2)$**로 증가한다. 이는 모든 토큰 쌍마다 유사도를 계산하기 때문에 연산량이 토큰 수의 제곱에 비례하기 때문이다. 마찬가지로 **메모리(공간) 복잡도**도 어텐션 가중치 행렬 등을 저장해야 하므로 **$O(L^2)$**로 스케일된다. 예를 들어, 토큰 수 $L$이 두 배로 늘어나면 계산량과 메모리 사용량은 네 배 수준으로 늘어나므로, 매우 긴 시퀀스를 처리할 때 Transformer는 연산 비용과 메모리 측면에서 비효율적이다.

## Mamba 아키텍처

**Q:** Mamba가 Transformer 대비 갖는 가장 큰 장점은 무엇인가? Mamba는 어떻게 어텐션의 $O(n^2)$ 병목을 피할 수 있었는지 설명하라.

**A:** **Mamba**는 2024년에 제안된 **새로운 시퀀스 모델**로, **어텐션 없이도** 긴 시퀀스를 효과적으로 처리할 수 있다는 점이 가장 큰 장점이다. **선택적 상태 공간 모델(Selective SSM)** 기반의 순환 구조를 사용하여 **시퀀스 길이에 선형적으로(scale linear)** 처리 시간이 증가하도록 설계되었기에, Transformer처럼 토큰 쌍마다 계산을 하지 않고도 긴 문맥을 다룰 수 있다. Mamba는 내부적으로 **RNN처럼 한 토큰씩 은닉 상태를 갱신**하지만, **하드웨어 친화적인 병렬화 알고리즘**을 도입하여 순차 처리의 병목을 해결했다. 그 결과 **어텐션의 $O(n^2)$ 연산을 피하면서**도 토큰들 간 정보를 주고받을 수 있게 되었고, 실제로 **추론 시 처리 속도가 Transformer 대비 5배 이상** 높다는 보고가 있다. 요약하면, Mamba의 구조 덕분에 **거의 무한에 가까운 길이의 시퀀스**도 실용적으로 다룰 수 있고, 긴 문맥에서도 **계산 효율과 메모리 사용 측면에서 우수**하다.

**Q:** Mamba의 Selective SSM에서 "선택적"인 동작은 무엇을 뜻하는가? 이로 인해 언어 모델에서 어떤 효과를 얻을 수 있었는가?

**A:** **Selective SSM**에서 "**선택적**"이라는 것은 **상태 공간 모델의 계수(예: 상태 전이 행렬)가 입력 토큰의 함수로 동적으로 결정**된다는 뜻이다. 즉, 모든 시점에 동일한 방식으로 상태를 업데이트하는 것이 아니라, **현재 토큰의 내용에 따라 이전 정보를 얼마나 유지하거나 잊을지 조절**하는 것이다. 이는 마치 RNN의 **게이트(gate)**처럼 동작하여 **중요한 정보는 오래 간직하고 불필요한 정보는 빠르게 망각**하게 한다. 이러한 선택적 상태 제어 덕분에 Mamba는 **토큰 간의 내용 기반 의존성**을 효과적으로 표현할 수 있게 되었고, 고정된 SSM으로는 어려웠던 **자연어와 같은 이산 토큰 데이터**에서도 높은 성능을 발휘할 수 있었다.

**Q:** Mamba-3B 모델이 보여준 성능 관련 특징을 언급하라 (예: 동일 크기 Transformer와의 비교, 두 배 큰 Transformer와의 비교 등).

**A:** **Mamba-3B**는 파라미터 규모 3억 수준의 Mamba 모델로, **동일한 크기의 Transformer보다 더 우수한 성능**을 보였으며 **파라미터 수가 두 배인 Transformer와도 맞먹는 성능**을 달성했다고 보고되었다. 이는 Mamba 아키텍처의 효율성 덕분에 **더 작은 모델로도 Transformer의 성능을 뛰어넘거나 대등하게** 낼 수 있음을 시사한다. 다시 말해, Mamba-3B는 3B 규모의 Transformer보다 언어 모델링 능력이 좋았고, 6B 규모의 Transformer와 유사한 결과를 보임으로써 **모델 크기 대비 뛰어난 성능 효율**을 입증했다. 이러한 결과는 Mamba의 **아키텍처 혁신**이 실제 모델 성능 향상으로 이어졌음을 보여준다.

## RWKV 아키텍처

**Q:** RWKV 아키텍처가 Transformer의 어떤 단점을 해결하기 위해 나왔는지 설명하라. 또한 Transformer와 RNN의 장점을 각각 어떤 방식으로 결합했는가?

**A:** **RWKV**는 **Transformer의 한계를 극복**하기 위해 고안된 모델로, **긴 문맥 처리와 높은 자원 소모** 문제에 대한 대안으로 등장했다. Transformer는 어텐션 연산의 제약으로 **컨텍스트 길이에 한계**가 있고 대용량 GPU 자원이 필요하다는 단점이 있는데, RWKV는 **RNN 계열**의 아이디어를 도입하여 **사실상 제약 없는 문맥 길이**를 지원한다. **Transformer의 장점**인 **병렬 학습** 능력을 그대로 받아들여, 학습 시에는 전체 시퀀스를 한꺼번에 처리(특수한 형태의 어텐션 수식으로 변환)함으로써 GPU 효율을 확보했고, **RNN의 장점**인 **순차 추론 효율**을 결합하여 추론 시에는 토큰을 **한 개씩 RNN처럼 생성**하도록 만들었다. 요약하면, RWKV는 **학습 단계에서는 Transformer처럼 빠르고**, **추론 단계에서는 RNN처럼 가볍게** 동작하도록 함으로써 두 구조의 이점을 모두 취한 혼합형 아키텍처다.

**Q:** RWKV의 추론 방식은 Transformer와 어떻게 다르며, 이로 인해 얻는 이점은 무엇인가? (힌트: KV 캐시 vs 은닉 상태)

**A:** **Transformer**는 추론 시 이전 토큰들의 **KV 캐시**를 모두 저장해 두고, 매 생성 스텝마다 그 **전체와 어텐션을 계산**하는 방식을 취한다. 반면 **RWKV**에서는 각 레이어가 **자신의 은닉 상태(state)**를 가지고 있고, 새로운 토큰이 들어오면 **이전 상태를 업데이트**하는 방식으로 동작한다. 따라서 이전 모든 토큰 정보를 거대한 KV 캐시로 보관할 필요 없이 **고정 크기의 은닉 상태만 유지**하면 된다. 이 차이로 인해 얻는 가장 큰 이점은 **메모리 효율과 속도**다. RWKV는 문맥이 길어져도 메모리 사용량이 거의 증가하지 않고, **토큰당 계산량이 일정**하므로 (어텐션처럼 토큰 수에 따라 증가하지 않음) **아주 긴 입력에서도 일관된 속도**를 낸다. 즉, RWKV는 Transformer 대비 **긴 문서 처리에 유리**하며, **저사양 장치**에서도 대용량 LLM을 상대적으로 원활히 구동할 수 있게 해준다.

**Q:** RWKV의 이름이 뜻하는 바는 무엇이며, Time-mix와 Channel-mix의 역할은 무엇인지 간략히 정리하라.

**A:** **RWKV**는 **Receptance, Weight, Key, Value**의 약자로, 네트워크의 4가지 주요 파라미터 이름에서 유래했다. 여기서 **Receptance (R)**는 **과거 정보를 받아들이는 게이트** 역할을 하고, **Weight (W)**는 **과거 정보에 부여하는 지수적 시간 가중치** (시간이 지남에 따라 이전 영향력을 서서히 감소시키는 계수)이며, **Key (K)**와 **Value (V)**는 각각 키/값 벡터로 **현재 토큰이 전달하는 정보**를 나타낸다.

RWKV 아키텍처의 각 레이어는 두 단계로 나뉘는데, **Time-mix** 단계와 **Channel-mix** 단계가 그것이다. **Time-mix**는 현재 토큰의 입력을 이전 토큰들의 **누적된 Key/Value 정보와 섞어**주는 단계로, R과 W 게이트를 이용해 **이전 상태를 감쇠(decay)시키며 새로운 정보를 통합**한다. 이는 Transformer에서 **어텐션이 시간축 정보를 통합하는 역할**을 대신한다고 볼 수 있다.

다음으로 **Channel-mix**는 각 토큰의 **채널(피처) 방향의 변환**을 수행하는 단계로, 전형적인 **Feed-Forward Network(FFN)**처럼 **토큰별 비선형 변환**을 적용한다. 이 과정에서 이전 토큰의 출력 일부도 입력으로 활용하여 **게이트를 통한 조정**이 이뤄지며, Transformer의 FFN과 유사한 역할을 한다. 정리하면, RWKV의 Time-mix는 **순차적인 정보 혼합**(시간 축 처리)을, Channel-mix는 **특성 차원의 혼합**(채널 처리)을 담당하여 **어텐션 없이도 토큰 간 의존성과 토큰 내부 변환을 모두 수행**하도록 설계되어 있다.

## Jamba 아키텍처

**Q:** Jamba 아키텍처에서 Transformer 층과 Mamba 층은 어떤 비율로 배치되는가? 이러한 설계가 메모리 및 속도 측면에 어떤 이점을 주는지 설명하라.

**A:** **Jamba**는 **Transformer 층과 Mamba 층을 혼합**한 **하이브리드 아키텍처**다. 구체적으로 하나의 Transformer (Attention) 층 뒤에 **여러 개의 Mamba 층**이 따라오는 형태로 쌓이는데, **"1 : 7"의 비율**이 대표적인 구성이다. 예를 들어 32개의 레이어로 구성된 Jamba 모델이라면 그 중 **4개 층만 어텐션**을 사용하고, 나머지 **28개 층은 Mamba**로 이루어진다.

이렇게 **드문드문 어텐션을 삽입**하고 대부분을 Mamba로 채움으로써, **전역적인 패턴 처리**는 간헐적으로 등장하는 어텐션 층이 담당하고 **나머지 상호작용은 효율적인 Mamba 층**들이 처리하도록 한다. 이 설계는 **메모리 사용량과 속도를 크게 개선**하는데, 특히 어텐션 층이 적으므로 **KV 캐시를 저장해야 할 층 수가 줄어들어 전체 메모리 풋프린트가 작아지고**, **긴 문맥을 처리할 때도** 소수의 어텐션만 계산하면 되므로 **Transformer 대비 훨씬 빠른 토큰 처리 속도**를 얻을 수 있다. 실제로 보고된 바에 따르면, Jamba는 **동일 규모의 일반 Transformer보다 메모리를 1/2 수준만 사용**하면서 **128K 토큰 길이 입력에 대해 3배 이상의 속도**로 텍스트를 생성해냈다.

**Q:** Jamba가 MoE를 도입한 이유는 무엇인가? 활성 파라미터와 총 파라미터의 개념을 들어 설명하라.

**A:** **Jamba**는 모델 용량을 키우면서도 효율을 유지하기 위해 **MoE (Mixture-of-Experts, 전문가 혼합)** 기법을 도입했다. 구체적으로, 일부 Transformer의 **MLP 층을 MoE 레이어로 대체**하여 **여러 개의 Expert 네트워크**를 두고, **각 토큰마다 상위 몇 개의 Expert만 활성화**되도록 한다. 예를 들어 Jamba에서는 하나의 MoE 레이어에 16개의 Expert MLP가 있고, **토큰마다 가장 관련 높은 2개의 Expert만 가동(top-2 gating)**되도록 설계되었다.

이때 **총 파라미터**란 모든 Expert들을 포함한 전체 모델의 파라미터 수를 의미하고, **활성 파라미터**는 **실제 한 번의 추론에서 활성화되어 계산에 사용되는 파라미터 수**를 뜻한다. Jamba의 경우 MoE 도입으로 **총 파라미터 수는 매우 크게 증가(예: 52억 → 520억 등)**하지만, 매 토큰마다 **극히 일부(예: 상위 2개 Expert)**의 파라미터만 사용되므로 **실제 활성 파라미터 규모는 제한**된다. 예를 들어 Jamba 7B 모델은 MoE를 통해 **총 약 520억 개의 파라미터**를 가지지만, **실제 활성화되는 것은 약 120억 개**에 불과하다.

이렇게 함으로써 **모델의 총 용량(capacity)**은 크게 늘려 **성능 향상**을 도모하면서도, **추론 시 계산량과 메모리 사용량은 활성 파라미터 수준으로 억제**하여 효율을 유지할 수 있다. 요컨대, MoE 도입으로 Jamba는 **"큰 모델의 똑똑함"을 가지되 "작은 모델의 비용"만 치르는 효과**를 얻은 것이다.

**Q:** Jamba가 지원하는 최대 컨텍스트(문맥) 길이는 얼마나 되며, 이렇게 긴 문맥을 처리하면서도 성능을 유지하는 비결은 무엇인가?

**A:** **Jamba**는 무려 **256K(25만 6천) 토큰**에 달하는 초장문의 **컨텍스트 윈도우**를 지원한다. 이는 현재 공개된 Transformer 계열 모델 중 **가장 긴 수준의 문맥 처리 능력**이며, 이 덕분에 아주 긴 문서를 한 번에 입력하여 질의응답이나 요약을 수행하는 것이 가능하다.

이렇게 긴 문맥을 다루고도 성능을 유지할 수 있는 비결은 앞선 설계 요소들에 있다. 우선, **어텐션 층 수를 최소화**하고 대부분을 Mamba로 구성했기 때문에 **긴 입력에 대해 어텐션 연산으로 인한 부담이 매우 적다**. 또한 Mamba 층은 **선형 시간**에 동작하므로 문맥 길이가 늘어나도 계산 비용이 크게 늘지 않는다. 실제 실험에서 Jamba는 **128K 토큰 입력을 하나의 80GB GPU에서 처리**해냈으며, 동일 규모의 일반 Transformer는 메모리 한계로 이를 처리하지 못하는 반면 Jamba는 **무리 없이 동작하면서도 출력 품질을 최신 LLM 수준으로 유지**했다. 정리하면, Jamba의 아키텍처는 **긴 문맥을 효율적으로 처리하도록 특화**되어 있으며, 덕분에 **긴 입력에서도 빠른 추론과 우수한 성능을 동시에 달성**할 수 있다.

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe0e1606-699a-4f1c-8348-209cf4ca84e6",
   "metadata": {},
   "source": [
    "\n",
    "# üåç Analyzing Sea Level Rise Using Earth Data in the Cloud\n",
    "\n",
    "### This notebook is entirely based on Jinbo Wang's [tutorial](https://github.com/betolink/the-coding-club/blob/main/notebooks/Earthdata_webinar_20220727.ipynb)\n",
    "\n",
    "<img alt=\"Sea level rise infographic\" src=\"https://www.nasa.gov/wp-content/uploads/2020/08/sealevel_main-causes3-16.jpg\" width=\"800px\" />\n",
    "\n",
    "--- \n",
    "\n",
    "**We are going to reproduce the plot from this infographic** Source: [ NASA-led Study Reveals the Causes of Sea Level Rise Since 1900 ](https://grace.jpl.nasa.gov/news/113/nasa-led-study-reveals-the-causes-of-sea-level-rise-since-1900/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dad6a05-9004-4ecb-af4b-ecb132a919ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import earthaccess\n",
    "\n",
    "print(f\"using earthaccess v{earthaccess.__version__}\")\n",
    "\n",
    "auth = earthaccess.login()\n",
    "# are we authenticated?\n",
    "if not auth.authenticated:\n",
    "    # ask for credentials and persist them in a .netrc file\n",
    "    auth.login(strategy=\"interactive\", persist=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5cb775-ac1a-4bd8-9c47-0d1baaabd718",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Why earthaccess?\n",
    "\n",
    "earthaccess is a Python library that simplifies data discovery and access to NASA Earth science data by providing an abstraction layer for NASA‚Äôs Common Metadata Repository (CMR) Search API so that searching for data can be done using a simpler notation instead of low level HTTP queries. \n",
    "\n",
    "## Authentication in the cloud\n",
    "\n",
    "If the collection is a cloud-hosted collection we can print the `summary()` and get the S3 credential endpoint. These S3 credentials are temporary and we can use them with third party libraries such as s3fs, boto3 or awscli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecafd00-d5d5-4c10-99a8-5e48f2614407",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# We'll get 4 collections that match with our keywords\n",
    "collections = earthaccess.search_datasets(\n",
    "    keyword=\"SEA SURFACE HEIGHT\",\n",
    "    cloud_hosted=True,\n",
    "    count=4,\n",
    ")\n",
    "\n",
    "# Let's print 2 collections\n",
    "for collection in collections[0:2]:\n",
    "    # pprint(collection.summary())\n",
    "    print(\n",
    "        pprint(collection.summary()),\n",
    "        collection.abstract(),\n",
    "        \"\\n\",\n",
    "        collection[\"umm\"][\"DOI\"],\n",
    "        \"\\n\\n\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a3afde-18a4-4bc0-9351-5a7b623c3780",
   "metadata": {},
   "source": [
    "## A year of data \n",
    "\n",
    "Things to keep in mind:\n",
    "\n",
    "* this particular dataset has data until 2019\n",
    "* this is a global dataset, each granule represents the whole world\n",
    "* temporal coverage is 1 granule each 5 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffa7990-94cf-4efa-a788-8cdd9d0d0467",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "granules = earthaccess.search_data(\n",
    "    short_name=\"SEA_SURFACE_HEIGHT_ALT_GRIDS_L4_2SATS_5DAY_6THDEG_V_JPL2205\",\n",
    "    count=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30fed42-2890-4d50-a57b-cf497c8fc57b",
   "metadata": {},
   "source": [
    "## Working with the URLs directly\n",
    "\n",
    "If we chose, we can use `earthdata` to grab the file's URLs and then download them with another library (if we have a `.netrc` file configured with NASA's EDL credentials)\n",
    "Getting the links to our data is quiet simple with the `data_links()` method on each of the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fb1dd3-5392-4bd2-ad17-b16702604ae6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the collection is cloud hosted, but we can access it out of AWS with the regular HTTPS URL\n",
    "granules[0].data_links(access=\"out_of_region\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a217ae0f-bc15-468f-b080-00ddc12814b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "granules[0].data_links(access=\"direct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c59864-eef9-4728-9635-0175e60756a7",
   "metadata": {},
   "source": [
    "## POC: streaming into xarray\n",
    "\n",
    "We can use `earthaccess` to stream files directly into xarray, this will work well for cases where:\n",
    "\n",
    "* Data is in NetCDF/HDF5/Zaar format\n",
    "  * xarray can read bytes directly for remote datasets only with **`h5netcdf`** and **`scipy`** back-ends, if we deal with a format that won't be recognized by these 2 backends xarray will raise an exception.\n",
    "* Data is not big data (multi TB)\n",
    "  * not fully tested with Dask distributed\n",
    "* Data is gridded\n",
    "  * xarray works better with homogeneous coordinates, working with swath data will be cumbersome.\n",
    "* Data is chunked using reasonable large sizes (1MB or more)\n",
    "  * If our files are chunked in small pieces the access time will be orders of magnitude bigger than just downloading the data and accessing it locally.\n",
    "  \n",
    "Opening a year of SSH (SEA_SURFACE_HEIGHT_ALT_GRIDS_L4_2SATS_5DAY_6THDEG_V_JPL1812) data (1.1 GB approx) can take up to 5 minutes streaming the data out of region (not in AWS)\n",
    "The reason for this is not that the data transfer is order of magnitude slower but due the client libraries not fetching data concurrently and the metadata of the files in HDF is usually not consolidated like in Zaar, hence h5netcdf has to issue a lot of requests to get the info it needs.\n",
    "\n",
    "> Note: we are looping through each year and getting the metadata for the first granule in May"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a874e56b-a5c0-4562-a865-15ae73b9df4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  storing the resulting granule metadata\n",
    "granules = []\n",
    "\n",
    "# we just grab 1 granule from May for each year of the dataset\n",
    "for year in range(1999, 2019):\n",
    "    print(f\"Retrieving data for year: {year}\")\n",
    "    results = earthaccess.search_data(\n",
    "        doi=\"10.5067/SLREF-CDRV3\",\n",
    "        temporal=(f\"{year}-05\", f\"{year}-05\"),\n",
    "        count=1,\n",
    "    )\n",
    "    granules.append(results[0])\n",
    "print(f\"Total granules: {len(granules)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b63ca2f-c94c-4d4a-a620-a086ee66137f",
   "metadata": {},
   "source": [
    "### What does `earthaccess.open()` do?\n",
    "\n",
    "`earthaccess.open()` takes a list of results from `earthaccess.search_data()` or a list of URLs and creates a list of Python File-like objects that can be used in our code as if the remote files were local. When executed in AWS the file system used is [S3FS](https://github.com/fsspec/s3fs) when we open files outside of AWS we get a regular HTTPS file session. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab14a07-8783-4be7-89b5-a30c672bc117",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import xarray as xr\n",
    "\n",
    "fileset = earthaccess.open(granules)\n",
    "\n",
    "print(f\" Using {type(fileset[0])} filesystem\")\n",
    "\n",
    "ds = xr.open_mfdataset(fileset, chunks={})\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786efff1-91d7-4960-8063-f5626f96bcde",
   "metadata": {},
   "source": [
    "## Plotting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381d255f-60b3-4464-884a-436cda1f69b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(\n",
    "    ds.SLA.where((ds.SLA >= 0) & (ds.SLA < 10))\n",
    "    .std(\"Time\")\n",
    "    .plot(figsize=(14, 6), x=\"Longitude\", y=\"Latitude\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc02507-629d-4c26-abcb-5355b50c766f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyproj import Geod\n",
    "\n",
    "\n",
    "def ssl_area(lats):\n",
    "    \"\"\"Calculate the area associated with a 1/6 by 1/6 degree box at latitude specified in \"lats\".\n",
    "\n",
    "    Parameters:\n",
    "        lats: a list or numpy array of size N the latitudes of interest.\n",
    "\n",
    "    Returns:\n",
    "        Array (N) area values (unit: m^2)\n",
    "    \"\"\"\n",
    "    # Define WGS84 as CRS:\n",
    "    geod = Geod(ellps=\"WGS84\")\n",
    "    # Set offsets for a box that will be centered at 0, lat\n",
    "    dx = 1 / 12.0\n",
    "    lon_offset = np.array((-dx, dx, dx, -dx))\n",
    "    lat_offset = np.array((-dx, -dx, dx, dx))\n",
    "    out = []\n",
    "    for lat in lats:\n",
    "        # Run a geodesic area calculator for the given box\n",
    "        c_area, *_ = geod.polygon_area_perimeter(lon_offset, lat + lat_offset)\n",
    "        out.append(c_area)\n",
    "    return np.array(out)\n",
    "\n",
    "\n",
    "# note: they rotated the data in the last release, this operation used to be (1,-1)\n",
    "ssh_area = ssl_area(ds.Latitude.data).reshape(-1, 1)\n",
    "print(ssh_area.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40efdd8a-5faa-4a50-a331-7c6b9e5af83a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This dataset was moved from opendap\n",
    "granule = earthaccess.search_data(concept_id=\"C2491724765-POCLOUD\")[0].data_links()[0]\n",
    "gmsl = earthaccess.open([granule])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ef1122-8185-4d68-ab94-88e19a8173f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (16, 4)\n",
    "\n",
    "img = plt.imread(\"underwater.jpeg\")\n",
    "\n",
    "fig, axs = plt.subplots()\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "def global_mean(SLA, **kwargs):\n",
    "    dout = ((SLA * ssh_area).sum() / (SLA / SLA * ssh_area).sum()) * 1000\n",
    "    return dout\n",
    "\n",
    "\n",
    "result = ds.SLA.groupby(\"Time\").apply(global_mean)\n",
    "\n",
    "plt.xlabel(\"Time (year)\", fontsize=16)\n",
    "plt.ylabel(\"Global Mean SLA (meter)\", fontsize=12)\n",
    "# axs.imshow(img, aspect='auto')\n",
    "plt.grid(True)\n",
    "\n",
    "historic_ts = xr.open_dataset(gmsl)\n",
    "\n",
    "result.plot(ax=axs, color=\"orange\", marker=\"o\", label=\"satellite record\")\n",
    "historic_ts[\"global_average_sea_level_change\"].plot(\n",
    "    ax=axs, label=\"Historical in-situ record\", color=\"lightblue\"\n",
    ")\n",
    "\n",
    "x0, x1 = axs.get_xlim()\n",
    "y0, y1 = axs.get_ylim()\n",
    "axs.imshow(img, extent=[x0, x1, y0, y1], aspect=\"auto\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

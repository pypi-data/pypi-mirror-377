{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud optimized access to NASA data with earthaccess and virtualizarr\n",
    "This notebook will focus on the usage of `earthaccess.open_virtual_dataset` and `earthaccess.open_virtual_mfdataset` to create cloud optimized reference files for the data stored in the cloud.\n",
    "\n",
    "All of the examples in this tutorial load data over https (`access=\"indirect\"`). However, there is a **significant** speed improvement when using these functions in-cloud and enabling `access=\"direct\"`. For example, using managed cloud JupyterHubs like NASA VEDA or 2i2c Openscapes. This is because the data is streamed directly from cloud storage to cloud compute.\n",
    "\n",
    "> WARNING: This feature is current experimental and may change in the future. This feature relies on NASA DMR++ metadata files which may not always be present for your dataset and you may get a `FileNotFoundError`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import earthaccess\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NASA JPL Multiscale Ultrahigh Resolution (MUR) Sea Surface Temperature (SST) dataset - 0.01 degree resolution\n",
    "results = earthaccess.search_data(\n",
    "    temporal=(\"2010-01-01\", \"2010-01-31\"), short_name=\"MUR-JPL-L4-GLOB-v4.1\"\n",
    ")\n",
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mur = earthaccess.open_virtual_mfdataset(\n",
    "    results,\n",
    "    access=\"indirect\",\n",
    "    load=True,\n",
    "    concat_dim=\"time\",\n",
    "    coords=\"all\",\n",
    "    compat=\"override\",\n",
    "    combine_attrs=\"drop_conflicts\",\n",
    ")\n",
    "mur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{mur.nbytes / 1e9} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mur.isel(time=0).sel(\n",
    "    lat=slice(20, 45), lon=slice(-95, -50)\n",
    ").analysed_sst.plot.pcolormesh(x=\"lon\", y=\"lat\", cmap=\"plasma\", figsize=(8, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save virtual reference file and load with xarray\n",
    "If you have a dataset you frequently access or you want to share this blueprint file with others, it is recommended to create a virtual reference file that points to the data in the cloud. This allows xarray to rapidly load the dataset as if it was a [Zarr store](https://zarr.dev/).\n",
    "\n",
    "Notice below that `load=False`. This means that the output of `open_virtual_mfdataset` is a virtual xarray Dataset that contains only chunk information and metadata. You can modify this dataset, then save it to a virtual reference file (as JSON), and then simply load that file with xarray. For more information on virtual reference files, see the [virtualizarr documentation](https://virtualizarr.readthedocs.io/en/latest/).\n",
    "\n",
    "Sample workflow:\n",
    "1. Open a dataset with `open_virtual_mfdataset` with `load=False`\n",
    "2. Modify the dataset as needed\n",
    "3. Save the dataset to a virtual reference file with `vds.virtualize.to_kerchunk(...)`\n",
    "4. Load the virtual reference file with `xr.open_dataset(..., engine='kerchunk')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mur_vds = earthaccess.open_virtual_mfdataset(\n",
    "    results,\n",
    "    access=\"indirect\",\n",
    "    load=False,\n",
    "    concat_dim=\"time\",\n",
    "    coords=\"all\",\n",
    "    compat=\"override\",\n",
    "    combine_attrs=\"drop_conflicts\",\n",
    ")\n",
    "mur_vds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of what's inside this virtual dataset\n",
    "print(mur_vds.analysed_sst.data)\n",
    "print(mur_vds.analysed_sst.data.manifest.dict()[\"0.0.1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mur_vds.virtualize.to_kerchunk(filepath=\"mur_kerchunk.json\", format=\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fs = earthaccess.get_fsspec_https_session()\n",
    "\n",
    "ds = xr.open_dataset(\n",
    "    \"mur_kerchunk.json\",\n",
    "    engine=\"kerchunk\",\n",
    "    chunks={},\n",
    "    storage_options={\n",
    "        \"remote_protocol\": fs.protocol,\n",
    "        \"remote_options\": fs.storage_options,\n",
    "    },\n",
    ")\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read datasets with groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NASA TEMPO NO2 tropospheric and stratospheric columns V03\n",
    "results = earthaccess.search_data(count=2, doi=\"10.5067/IS-40e/TEMPO/NO2_L2.003\")\n",
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthaccess.open_virtual_dataset(results[0], group=\"product\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced: Preprocess the datasets\n",
    "You can also preprocess the datasets before saving the virtual reference file. This is useful if you want to apply a function to the datasets before concatentaion. For example, the `SWOT_L2_LR_SSH_Expert_2.0` dataset (from [NASA JPL SWOT satellite](https://www.jpl.nasa.gov/missions/surface-water-and-ocean-topography-swot/)) is an [L2 product](https://www.earthdata.nasa.gov/learn/earth-observation-data-basics/data-processing-levels) where each file represents a single pass of the satellite. If you want to combine all the passes into a single dataset, you can concatenate the datasets using `cycle_number` and `pass_number` which are only found in the attributes of each netcdf file.\n",
    "\n",
    "The `preprocess` function and argument allows us to turn those attributes into dimensions first, and then concatenate along this new dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = earthaccess.search_data(\n",
    "    count=10, temporal=(\"2023\"), short_name=\"SWOT_L2_LR_SSH_Expert_2.0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "def preprocess(ds: xr.Dataset) -> xr.Dataset:\n",
    "    # Add cycle number and pass_number as dimensions\n",
    "    return ds.expand_dims([\"cycle_num\", \"pass_num\"]).assign_coords(\n",
    "        cycle_num=[ds.attrs[\"cycle_number\"]], pass_num=[ds.attrs[\"pass_number\"]]\n",
    "    )\n",
    "\n",
    "\n",
    "swot = earthaccess.open_virtual_mfdataset(\n",
    "    results,\n",
    "    access=\"indirect\",\n",
    "    load=False,\n",
    "    preprocess=preprocess,\n",
    "    concat_dim=\"pass_num\",\n",
    "    coords=\"all\",\n",
    "    compat=\"override\",\n",
    "    combine_attrs=\"drop_conflicts\",\n",
    ")\n",
    "swot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

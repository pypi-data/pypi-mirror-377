#!/usr/bin/env python3
"""
Binance Public Data Collector

Ultra-fast historical data collection using Binance's official public data repository.
10-100x faster than API calls, with complete historical coverage.

Data source: https://data.binance.vision/data/spot/monthly/klines/
"""

import argparse
import csv
import hashlib
import json
import logging
import shutil
import tempfile
import urllib.parse
import urllib.request
import warnings
import zipfile
from datetime import datetime, timedelta
from pathlib import Path

import pandas as pd

from ..gap_filling.universal_gap_filler import UniversalGapFiller


class BinancePublicDataCollector:
    """Ultra-fast spot data collection using Binance's public data repository."""

    def __init__(self, symbol="SOLUSDT", start_date="2020-08-15", end_date="2025-03-20"):
        """Initialize collector with date range."""
        self.symbol = symbol
        self.start_date = datetime.strptime(start_date, "%Y-%m-%d")
        # Make end_date inclusive of the full day (23:59:59)
        self.end_date = datetime.strptime(end_date, "%Y-%m-%d").replace(
            hour=23, minute=59, second=59
        )
        self.base_url = "https://data.binance.vision/data/spot/monthly/klines"
        self.output_dir = Path(__file__).parent.parent / "sample_data"

        # Available timeframes on Binance public data
        self.available_timeframes = [
            "1s",
            "1m",
            "3m",
            "5m",
            "15m",
            "30m",
            "1h",
            "2h",
            "4h",
            "6h",
            "8h",
            "12h",
            "1d",
            "3d",
            "1w",
            "1mo",
        ]

        # Popular symbols with known availability (for validation)
        self.known_symbols = {
            "BTCUSDT": "2017-08-17",
            "ETHUSDT": "2017-08-17",
            "SOLUSDT": "2020-08-11",
            "ADAUSDT": "2018-04-17",
            "DOTUSDT": "2020-08-19",
            "LINKUSDT": "2019-01-16",
        }

        # Validate date range and symbol
        self._validate_parameters()

        print("Binance Public Data Collector")
        print(f"Symbol: {self.symbol}")
        print(
            f"Date Range: {self.start_date.strftime('%Y-%m-%d')} to {self.end_date.strftime('%Y-%m-%d')}"
        )
        print(f"Data Source: {self.base_url}")

    def _validate_parameters(self):
        """Validate date range and symbol parameters."""
        today = datetime.now().date()
        yesterday = today - timedelta(days=1)

        # Check for future dates
        if self.end_date.date() > yesterday:
            warnings.warn(
                f"‚ö†Ô∏è  Requested end date {self.end_date.strftime('%Y-%m-%d')} is in the future. "
                f"Binance public data is typically available up to {yesterday}. "
                f"Recent data may not be available and requests may fail with 404 errors.",
                UserWarning,
                stacklevel=2,
            )

        # Check symbol availability
        if self.symbol in self.known_symbols:
            symbol_start = datetime.strptime(self.known_symbols[self.symbol], "%Y-%m-%d").date()
            if self.start_date.date() < symbol_start:
                warnings.warn(
                    f"‚ö†Ô∏è  Requested start date {self.start_date.strftime('%Y-%m-%d')} is before "
                    f"{self.symbol} listing date ({symbol_start}). "
                    f"Data before {symbol_start} is not available.",
                    UserWarning,
                    stacklevel=2,
                )
        else:
            # Unknown symbol - provide general guidance
            logging.info(
                f"‚ÑπÔ∏è  Symbol {self.symbol} availability not verified. "
                f"Known symbols: {list(self.known_symbols.keys())}. "
                f"If requests fail with 404 errors, check symbol availability on Binance."
            )

    def generate_monthly_urls(self, timeframe):
        """Generate list of monthly ZIP file URLs to download."""
        urls = []
        current_date = self.start_date.replace(day=1)  # Start of month

        while current_date <= self.end_date:
            year_month = current_date.strftime("%Y-%m")
            filename = f"{self.symbol}-{timeframe}-{year_month}.zip"
            url = f"{self.base_url}/{self.symbol}/{timeframe}/{filename}"
            urls.append((url, year_month, filename))

            # Move to next month
            if current_date.month == 12:
                current_date = current_date.replace(year=current_date.year + 1, month=1)
            else:
                current_date = current_date.replace(month=current_date.month + 1)

        return urls

    def download_and_extract_month(self, url, filename):
        """Download and extract a single monthly ZIP file."""
        print(f"  Downloading {filename}...")

        try:
            with tempfile.NamedTemporaryFile() as temp_zip:
                # Download ZIP file
                with urllib.request.urlopen(url, timeout=60) as response:
                    if response.status == 200:
                        shutil.copyfileobj(response, temp_zip)
                        temp_zip.flush()
                    else:
                        print(f"    ‚ö†Ô∏è  HTTP {response.status} - {filename} not available")
                        return []

                # Extract CSV data
                with zipfile.ZipFile(temp_zip.name, "r") as zf:
                    csv_filename = filename.replace(".zip", ".csv")
                    if csv_filename in zf.namelist():
                        with zf.open(csv_filename) as csv_file:
                            csv_content = csv_file.read().decode("utf-8")
                            return list(csv.reader(csv_content.strip().split("\n")))
                    else:
                        print(f"    ‚ö†Ô∏è  CSV file not found in {filename}")
                        return []

        except Exception as e:
            print(f"    ‚ùå Error downloading {filename}: {e}")
            return []

    def _detect_header_intelligent(self, raw_data):
        """Intelligent header detection - determine if first row is data or header."""
        if not raw_data:
            return False

        first_row = raw_data[0]
        if len(first_row) < 6:
            return False

        # Header detection heuristics
        try:
            # Test if first field is numeric timestamp
            timestamp_val = int(first_row[0])

            # ‚úÖ BOUNDARY FIX: Support both milliseconds (13-digit) AND microseconds (16-digit) formats
            # Valid timestamp ranges:
            # Milliseconds: 1000000000000 (2001) to 9999999999999 (2286)
            # Microseconds: 1000000000000000 (2001) to 9999999999999999 (2286)
            is_valid_milliseconds = 1000000000000 <= timestamp_val <= 9999999999999
            is_valid_microseconds = 1000000000000000 <= timestamp_val <= 9999999999999999

            if is_valid_milliseconds or is_valid_microseconds:
                # Test if other fields are numeric (prices/volumes)
                for i in [1, 2, 3, 4, 5]:  # OHLCV fields
                    float(first_row[i])
                return False  # All numeric = data row
            else:
                return True  # Invalid timestamp = likely header

        except (ValueError, IndexError):
            # Non-numeric first field = header
            return True

    def process_raw_data(self, raw_data):
        """Convert raw Binance CSV data with comprehensive timestamp format tracking and transition detection."""
        processed_data = []
        self.corruption_log = getattr(self, "corruption_log", [])

        # Initialize comprehensive format tracking
        self.format_stats = {
            "milliseconds": {
                "count": 0,
                "first_seen": None,
                "last_seen": None,
                "sample_values": [],
            },
            "microseconds": {
                "count": 0,
                "first_seen": None,
                "last_seen": None,
                "sample_values": [],
            },
            "unknown": {"count": 0, "errors": []},
        }
        self.format_transitions = []  # Track format changes
        self.current_format = None

        # Intelligent header detection
        has_header = self._detect_header_intelligent(raw_data)
        start_row = 1 if has_header else 0

        # Store header detection results for metadata
        self._header_detected = has_header
        self._header_content = raw_data[0][:6] if has_header else None
        self._data_start_row = start_row

        if has_header:
            print(f"    üìã Header detected: {raw_data[0][:6]}")
        else:
            print("    üìä Pure data format detected (no header)")

        format_change_logged = False

        for row_idx, row in enumerate(raw_data[start_row:], start=start_row):
            if len(row) >= 6:  # Binance format has 12 columns but we need first 6
                try:
                    # Binance format: [timestamp, open, high, low, close, volume, close_time, quote_volume, count, taker_buy_volume, taker_buy_quote_volume, ignore]
                    timestamp_raw = int(row[0])

                    # Comprehensive format detection with transition tracking
                    detected_format, timestamp_seconds, validation_result = (
                        self._analyze_timestamp_format(timestamp_raw, row_idx)
                    )

                    # Track format transitions
                    if self.current_format is None:
                        self.current_format = detected_format
                        print(f"    üéØ Initial timestamp format: {detected_format}")
                    elif self.current_format != detected_format and detected_format != "unknown":
                        self.format_transitions.append(
                            {
                                "row_index": row_idx,
                                "from_format": self.current_format,
                                "to_format": detected_format,
                                "timestamp_value": timestamp_raw,
                            }
                        )
                        self.current_format = detected_format
                        if not format_change_logged:
                            print(
                                f"    üîÑ Format transition detected: {self.format_transitions[-1]['from_format']} ‚Üí {detected_format}"
                            )
                            format_change_logged = True

                    # Update format statistics
                    self.format_stats[detected_format]["count"] += 1
                    if self.format_stats[detected_format]["first_seen"] is None:
                        self.format_stats[detected_format]["first_seen"] = row_idx
                    self.format_stats[detected_format]["last_seen"] = row_idx

                    # Store sample values (first 3 per format)
                    if len(self.format_stats[detected_format]["sample_values"]) < 3:
                        self.format_stats[detected_format]["sample_values"].append(timestamp_raw)

                    # Skip if validation failed
                    if not validation_result["valid"]:
                        self.corruption_log.append(validation_result["error_details"])
                        continue

                    # ‚úÖ CRITICAL FIX: Use UTC to match Binance's native timezone
                    # Eliminates artificial DST gaps caused by local timezone conversion
                    dt = datetime.utcfromtimestamp(timestamp_seconds)

                    # ‚úÖ BOUNDARY FIX: Don't filter per-monthly-file to preserve month boundaries
                    # Enhanced processing: capture all 11 essential Binance columns for complete microstructure analysis
                    processed_row = [
                        dt.strftime("%Y-%m-%d %H:%M:%S"),  # date (from open_time)
                        float(row[1]),  # open
                        float(row[2]),  # high
                        float(row[3]),  # low
                        float(row[4]),  # close
                        float(row[5]),  # volume (base asset volume)
                        # Additional microstructure columns for professional analysis
                        datetime.utcfromtimestamp(
                            int(row[6]) / (1000000 if len(str(int(row[6]))) >= 16 else 1000)
                        ).strftime("%Y-%m-%d %H:%M:%S"),  # close_time
                        float(row[7]),  # quote_asset_volume
                        int(row[8]),  # number_of_trades
                        float(row[9]),  # taker_buy_base_asset_volume
                        float(row[10]),  # taker_buy_quote_asset_volume
                    ]
                    processed_data.append(processed_row)

                except (ValueError, OSError, OverflowError) as e:
                    self.format_stats["unknown"]["count"] += 1
                    error_details = {
                        "row_index": row_idx,
                        "error_type": "timestamp_parse_error",
                        "error_message": str(e),
                        "raw_row": row[:10] if len(row) > 10 else row,
                    }
                    self.corruption_log.append(error_details)
                    self.format_stats["unknown"]["errors"].append(error_details)
                    continue
            else:
                # Record insufficient columns
                self.corruption_log.append(
                    {
                        "row_index": row_idx,
                        "error_type": "insufficient_columns",
                        "column_count": len(row),
                        "raw_row": row,
                    }
                )

        # Report comprehensive format analysis
        self._report_format_analysis()

        return processed_data

    def _analyze_timestamp_format(self, timestamp_raw, row_idx):
        """Comprehensive timestamp format analysis with validation."""
        digit_count = len(str(timestamp_raw))

        # Enhanced format detection logic
        if digit_count >= 16:  # Microseconds (16+ digits) - 2025+ format
            format_type = "microseconds"
            timestamp_seconds = timestamp_raw / 1000000
            min_bound = 1262304000000000  # 2010-01-01 00:00:00 (microseconds)
            max_bound = 1893456000000000  # 2030-01-01 00:00:00 (microseconds)

        elif digit_count >= 10:  # Milliseconds (10-15 digits) - Legacy format
            format_type = "milliseconds"
            timestamp_seconds = timestamp_raw / 1000
            min_bound = 1262304000000  # 2010-01-01 00:00:00 (milliseconds)
            max_bound = 1893456000000  # 2030-01-01 00:00:00 (milliseconds)

        else:  # Unknown format (less than 10 digits)
            format_type = "unknown"
            timestamp_seconds = None
            min_bound = max_bound = None

        # Enhanced validation with detailed error reporting
        if format_type == "unknown":
            validation_result = {
                "valid": False,
                "error_details": {
                    "row_index": row_idx,
                    "error_type": "unknown_timestamp_format",
                    "timestamp_value": timestamp_raw,
                    "digit_count": digit_count,
                    "expected_formats": "milliseconds (10-15 digits) or microseconds (16+ digits)",
                    "raw_row": f"Timestamp too short: {digit_count} digits",
                },
            }
        elif timestamp_raw < min_bound or timestamp_raw > max_bound:
            validation_result = {
                "valid": False,
                "error_details": {
                    "row_index": row_idx,
                    "error_type": "invalid_timestamp_range",
                    "timestamp_value": timestamp_raw,
                    "timestamp_format": format_type,
                    "digit_count": digit_count,
                    "valid_range": f"{min_bound} to {max_bound}",
                    "parsed_date": "out_of_range",
                    "raw_row": f"Out of valid {format_type} range (2010-2030)",
                },
            }
        else:
            validation_result = {"valid": True}

        return format_type, timestamp_seconds, validation_result

    def _report_format_analysis(self):
        """Report comprehensive format analysis with transition detection."""
        total_rows = sum(stats["count"] for stats in self.format_stats.values())

        print("    üìà COMPREHENSIVE FORMAT ANALYSIS:")

        for format_type, stats in self.format_stats.items():
            if stats["count"] > 0:
                percentage = (stats["count"] / total_rows) * 100 if total_rows > 0 else 0
                print(f"      {format_type.upper()}: {stats['count']:,} rows ({percentage:.1f}%)")

                if format_type != "unknown" and stats["sample_values"]:
                    first_sample = stats["sample_values"][0]
                    print(
                        f"        Sample: {first_sample} (rows {stats['first_seen']}-{stats['last_seen']})"
                    )

        # Report format transitions
        if len(self.format_transitions) > 0:
            print(f"    üîÑ FORMAT TRANSITIONS DETECTED: {len(self.format_transitions)}")
            for i, transition in enumerate(self.format_transitions[:3]):  # Show first 3
                print(
                    f"      #{i + 1}: Row {transition['row_index']} - {transition['from_format']} ‚Üí {transition['to_format']}"
                )
                print(f"          Timestamp: {transition['timestamp_value']}")
            if len(self.format_transitions) > 3:
                print(f"      ... and {len(self.format_transitions) - 3} more transitions")
        else:
            print(
                f"    ‚úÖ SINGLE FORMAT: No transitions detected - consistent {self.current_format}"
            )

        # Store format analysis results for metadata
        self._format_analysis_summary = {
            "total_rows_analyzed": total_rows,
            "formats_detected": {
                fmt: stats["count"]
                for fmt, stats in self.format_stats.items()
                if stats["count"] > 0
            },
            "transitions_detected": len(self.format_transitions),
            "transition_details": self.format_transitions,
            "primary_format": self.current_format,
            "format_consistency": len(self.format_transitions) == 0,
        }

    def collect_timeframe_data(self, timeframe):
        """Collect complete historical data for a single timeframe with full 11-column microstructure format."""
        print(f"\n{'=' * 60}")
        print(f"COLLECTING {timeframe.upper()} DATA FROM BINANCE PUBLIC REPOSITORY")
        print(f"{'=' * 60}")

        if timeframe not in self.available_timeframes:
            print(f"‚ùå Timeframe {timeframe} not available")
            return None

        # Generate monthly URLs
        monthly_urls = self.generate_monthly_urls(timeframe)
        print(f"Monthly files to download: {len(monthly_urls)}")

        # Collect data from all months
        all_data = []
        successful_downloads = 0

        for url, year_month, filename in monthly_urls:
            raw_month_data = self.download_and_extract_month(url, filename)
            if raw_month_data:
                processed_data = self.process_raw_data(raw_month_data)
                all_data.extend(processed_data)
                successful_downloads += 1
                print(f"    ‚úÖ {len(processed_data):,} bars from {year_month}")
            else:
                print(f"    ‚ö†Ô∏è  No data from {year_month}")

        print("\nCollection Summary:")
        print(f"  Successful downloads: {successful_downloads}/{len(monthly_urls)}")
        print(f"  Total bars collected: {len(all_data):,}")

        if all_data:
            # Sort by timestamp to ensure chronological order
            all_data.sort(key=lambda x: x[0])
            print(f"  Pre-filtering range: {all_data[0][0]} to {all_data[-1][0]}")

            # ‚úÖ BOUNDARY FIX: Apply final date range filtering after combining all monthly data
            # This preserves month boundaries while respecting the requested date range
            filtered_data = []
            for row in all_data:
                row_dt = datetime.strptime(row[0], "%Y-%m-%d %H:%M:%S")
                if self.start_date <= row_dt <= self.end_date:
                    filtered_data.append(row)

            print(f"  Post-filtering: {len(filtered_data):,} bars in requested range")
            if filtered_data:
                print(f"  Final range: {filtered_data[0][0]} to {filtered_data[-1][0]}")

            return filtered_data

        return all_data

    def generate_metadata(self, timeframe, data, collection_stats):
        """Generate comprehensive metadata for 11-column microstructure format."""
        if not data:
            return {}

        # Calculate statistics
        prices = []
        volumes = []
        for row in data:
            prices.extend([row[2], row[3]])  # high, low
            volumes.append(row[5])

        return {
            "version": "4.0.0",
            "generator": "BinancePublicDataCollector",
            "generation_timestamp": datetime.utcnow().isoformat() + "Z",
            "data_source": "Binance Public Data Repository",
            "data_source_url": self.base_url,
            "market_type": "spot",
            "symbol": self.symbol,
            "timeframe": timeframe,
            "collection_method": "direct_download",
            "target_period": {
                "start": self.start_date.isoformat(),
                "end": self.end_date.isoformat(),
                "total_days": (self.end_date - self.start_date).days,
            },
            "actual_bars": len(data),
            "date_range": {
                "start": data[0][0] if data else None,
                "end": data[-1][0] if data else None,
            },
            "statistics": {
                "price_min": min(prices) if prices else 0,
                "price_max": max(prices) if prices else 0,
                "volume_total": sum(volumes) if volumes else 0,
                "volume_mean": sum(volumes) / len(volumes) if volumes else 0,
            },
            "collection_performance": collection_stats,
            "data_integrity": {
                "chronological_order": True,
                "data_hash": self._calculate_data_hash(data),
                "corruption_detected": len(getattr(self, "corruption_log", [])) > 0,
                "corrupted_rows_count": len(getattr(self, "corruption_log", [])),
                "corruption_details": getattr(self, "corruption_log", []),
                "header_detection": {
                    "header_found": getattr(self, "_header_detected", False),
                    "header_content": getattr(self, "_header_content", None),
                    "data_start_row": getattr(self, "_data_start_row", 0),
                },
            },
            "timestamp_format_analysis": getattr(
                self,
                "_format_analysis_summary",
                {
                    "total_rows_analyzed": 0,
                    "formats_detected": {},
                    "transitions_detected": 0,
                    "transition_details": [],
                    "primary_format": "unknown",
                    "format_consistency": True,
                    "analysis_note": "Format analysis not available - may be legacy collection",
                },
            ),
            "enhanced_microstructure_format": {
                "format_version": "4.0.0",
                "total_columns": len(data[0]) if data else 11,
                "enhanced_features": [
                    "quote_asset_volume",
                    "number_of_trades",
                    "taker_buy_base_asset_volume",
                    "taker_buy_quote_asset_volume",
                    "close_time",
                ],
                "analysis_capabilities": [
                    "order_flow_analysis",
                    "liquidity_metrics",
                    "market_microstructure",
                    "trade_weighted_prices",
                    "institutional_data_patterns",
                ],
                "professional_features": True,
                "api_format_compatibility": True,
            },
            "compliance": {
                "zero_magic_numbers": True,
                "temporal_integrity": True,
                "authentic_spot_data_only": True,
                "official_binance_source": True,
                "binance_format_transition_aware": True,
                "supports_milliseconds_microseconds": True,
                "full_binance_microstructure_format": True,
                "professional_trading_ready": True,
            },
        }

    def _calculate_data_hash(self, data):
        """Calculate hash of data for integrity verification."""
        data_string = "\n".join(",".join(map(str, row)) for row in data)
        return hashlib.sha256(data_string.encode()).hexdigest()

    def save_to_csv(self, timeframe, data, collection_stats):
        """Save data to CSV file with full 11-column microstructure format and metadata."""
        if not data:
            print(f"‚ùå No data to save for {timeframe}")
            return None

        # Generate filename
        start_date_str = self.start_date.strftime("%Y%m%d")
        end_date_str = datetime.strptime(data[-1][0], "%Y-%m-%d %H:%M:%S").strftime("%Y%m%d")
        duration = f"{(self.end_date - self.start_date).days / 365.25:.1f}y"
        filename = (
            f"binance_spot_{self.symbol}-{timeframe}_{start_date_str}-{end_date_str}_{duration}.csv"
        )
        filepath = self.output_dir / filename

        # Ensure output directory exists
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # Generate metadata
        metadata = self.generate_metadata(timeframe, data, collection_stats)

        # Write CSV with metadata headers
        with open(filepath, "w", newline="") as f:
            # Write metadata headers
            f.write(f"# Binance Spot Market Data v{metadata['version']}\n")
            f.write(f"# Generated: {metadata['generation_timestamp']}\n")
            f.write(f"# Source: {metadata['data_source']}\n")
            f.write(
                f"# Market: {metadata['market_type'].upper()} | Symbol: {metadata['symbol']} | Timeframe: {metadata['timeframe']}\n"
            )
            f.write(f"# Coverage: {metadata['actual_bars']:,} bars\n")
            f.write(
                f"# Period: {metadata['date_range']['start']} to {metadata['date_range']['end']}\n"
            )
            f.write(
                f"# Collection: {collection_stats['method']} in {collection_stats['duration']:.1f}s\n"
            )
            f.write(f"# Data Hash: {metadata['data_integrity']['data_hash'][:16]}...\n")
            f.write(
                "# Compliance: Zero-Magic-Numbers, Temporal-Integrity, Official-Binance-Source\n"
            )
            f.write("#\n")

            # Write enhanced CSV header and data with all microstructure columns
            writer = csv.writer(f)
            writer.writerow(
                [
                    "date",
                    "open",
                    "high",
                    "low",
                    "close",
                    "volume",
                    "close_time",
                    "quote_asset_volume",
                    "number_of_trades",
                    "taker_buy_base_asset_volume",
                    "taker_buy_quote_asset_volume",
                ]
            )
            writer.writerows(data)

        # Save metadata as JSON
        metadata_filepath = filepath.with_suffix(".metadata.json")
        with open(metadata_filepath, "w") as f:
            json.dump(metadata, f, indent=2)

        file_size_mb = filepath.stat().st_size / (1024 * 1024)
        print(f"\n‚úÖ Created: {filepath.name} ({file_size_mb:.1f} MB)")
        print(f"‚úÖ Metadata: {metadata_filepath.name}")

        return filepath

    def collect_multiple_timeframes(self, timeframes=None):
        """Collect data for multiple timeframes with full 11-column microstructure format."""
        if timeframes is None:
            timeframes = ["1m", "3m", "5m", "15m", "30m", "1h", "2h"]

        print("BINANCE PUBLIC DATA ULTRA-FAST COLLECTION")
        print(f"Timeframes: {timeframes}")
        print("=" * 80)

        results = {}
        overall_start = datetime.now()

        for i, timeframe in enumerate(timeframes):
            print(f"\nPROGRESS: {i + 1}/{len(timeframes)} timeframes")

            tf_start = datetime.now()
            data = self.collect_timeframe_data(timeframe)
            tf_duration = (datetime.now() - tf_start).total_seconds()

            if data:
                collection_stats = {
                    "method": "direct_download",
                    "duration": tf_duration,
                    "bars_per_second": len(data) / tf_duration if tf_duration > 0 else 0,
                }

                filepath = self.save_to_csv(timeframe, data, collection_stats)
                if filepath:
                    results[timeframe] = filepath
            else:
                print(f"‚ùå Failed to collect {timeframe} data")

        overall_duration = (datetime.now() - overall_start).total_seconds()

        print("\n" + "=" * 80)
        print("ULTRA-FAST COLLECTION COMPLETE")
        print(f"Total time: {overall_duration:.1f} seconds ({overall_duration / 60:.1f} minutes)")
        print(f"Generated {len(results)} files:")

        for timeframe, filepath in results.items():
            file_size_mb = filepath.stat().st_size / (1024 * 1024)
            print(f"  {timeframe}: {filepath.name} ({file_size_mb:.1f} MB)")

        return results

    def validate_csv_file(self, csv_filepath, expected_timeframe=None):
        """
        Comprehensive validation of CSV file data integrity, completeness, and quality.

        Args:
            csv_filepath: Path to CSV file to validate
            expected_timeframe: Expected timeframe (e.g., '30m') for interval validation

        Returns:
            dict: Validation results with detailed analysis
        """
        print(f"\n{'=' * 60}")
        print(f"VALIDATING: {csv_filepath.name}")
        print(f"{'=' * 60}")

        validation_results = {
            "validation_timestamp": datetime.utcnow().isoformat() + "Z",
            "file_path": str(csv_filepath),
            "file_exists": csv_filepath.exists(),
            "file_size_mb": 0,
            "total_errors": 0,
            "total_warnings": 0,
            "validation_summary": "UNKNOWN",
        }

        if not csv_filepath.exists():
            validation_results["validation_summary"] = "FAILED - File not found"
            validation_results["total_errors"] = 1
            return validation_results

        validation_results["file_size_mb"] = csv_filepath.stat().st_size / (1024 * 1024)

        try:
            # Load CSV data efficiently
            print("Loading and parsing CSV data...")
            df = pd.read_csv(csv_filepath, comment="#")
            validation_results["total_bars"] = len(df)
            print(f"  ‚úÖ Loaded {len(df):,} data bars")

            # 1. BASIC STRUCTURE VALIDATION
            print("\n1. BASIC STRUCTURE VALIDATION")
            structure_validation = self._validate_csv_structure(df)
            validation_results["structure_validation"] = structure_validation
            print(f"  Columns: {structure_validation['status']}")
            if structure_validation["errors"]:
                for error in structure_validation["errors"]:
                    print(f"    ‚ùå {error}")
                    validation_results["total_errors"] += 1

            # 2. DATE/TIME VALIDATION
            print("\n2. DATE/TIME VALIDATION")
            datetime_validation = self._validate_datetime_sequence(df, expected_timeframe)
            validation_results["datetime_validation"] = datetime_validation
            print(
                f"  Date Range: {datetime_validation['date_range']['start']} to {datetime_validation['date_range']['end']}"
            )
            print(f"  Duration: {datetime_validation['duration_days']:.1f} days")
            print(f"  Gaps Found: {datetime_validation['gaps_found']}")
            print(f"  Sequence: {datetime_validation['chronological_order']}")

            if datetime_validation["errors"]:
                for error in datetime_validation["errors"]:
                    print(f"    ‚ùå {error}")
                    validation_results["total_errors"] += 1
            if datetime_validation["warnings"]:
                for warning in datetime_validation["warnings"]:
                    print(f"    ‚ö†Ô∏è  {warning}")
                    validation_results["total_warnings"] += 1

            # 3. OHLCV DATA QUALITY VALIDATION
            print("\n3. OHLCV DATA QUALITY VALIDATION")
            ohlcv_validation = self._validate_ohlcv_quality(df)
            validation_results["ohlcv_validation"] = ohlcv_validation
            print(
                f"  Price Range: ${ohlcv_validation['price_range']['min']:.4f} - ${ohlcv_validation['price_range']['max']:.4f}"
            )
            print(
                f"  Volume Range: {ohlcv_validation['volume_stats']['min']:.2f} - {ohlcv_validation['volume_stats']['max']:,.0f}"
            )
            print(f"  OHLC Logic Errors: {ohlcv_validation['ohlc_errors']}")
            print(f"  Negative/Zero Values: {ohlcv_validation['negative_zero_values']}")

            if ohlcv_validation["errors"]:
                for error in ohlcv_validation["errors"]:
                    print(f"    ‚ùå {error}")
                    validation_results["total_errors"] += 1
            if ohlcv_validation["warnings"]:
                for warning in ohlcv_validation["warnings"]:
                    print(f"    ‚ö†Ô∏è  {warning}")
                    validation_results["total_warnings"] += 1

            # 4. EXPECTED COVERAGE VALIDATION
            print("\n4. EXPECTED COVERAGE VALIDATION")
            coverage_validation = self._validate_expected_coverage(df, expected_timeframe)
            validation_results["coverage_validation"] = coverage_validation
            print(f"  Expected Bars: {coverage_validation['expected_bars']:,}")
            print(f"  Actual Bars: {coverage_validation['actual_bars']:,}")
            print(f"  Coverage: {coverage_validation['coverage_percentage']:.1f}%")

            # 5. STATISTICAL ANOMALY DETECTION
            print("\n5. STATISTICAL ANOMALY DETECTION")
            anomaly_validation = self._validate_statistical_anomalies(df)
            validation_results["anomaly_validation"] = anomaly_validation
            print(f"  Price Outliers: {anomaly_validation['price_outliers']}")
            print(f"  Volume Outliers: {anomaly_validation['volume_outliers']}")
            print(f"  Suspicious Patterns: {anomaly_validation['suspicious_patterns']}")

            # FINAL VALIDATION SUMMARY
            if validation_results["total_errors"] == 0:
                if validation_results["total_warnings"] == 0:
                    validation_results["validation_summary"] = "PERFECT - No errors or warnings"
                    print("\n‚úÖ VALIDATION RESULT: PERFECT")
                    print("   No errors or warnings found. Data quality is excellent.")
                else:
                    validation_results["validation_summary"] = (
                        f"GOOD - {validation_results['total_warnings']} warnings"
                    )
                    print("\n‚úÖ VALIDATION RESULT: GOOD")
                    print(
                        f"   No errors, but {validation_results['total_warnings']} warnings found."
                    )
            else:
                validation_results["validation_summary"] = (
                    f"FAILED - {validation_results['total_errors']} errors, {validation_results['total_warnings']} warnings"
                )
                print("\n‚ùå VALIDATION RESULT: FAILED")
                print(
                    f"   {validation_results['total_errors']} errors and {validation_results['total_warnings']} warnings found."
                )

        except Exception as e:
            validation_results["validation_summary"] = f"ERROR - {str(e)}"
            validation_results["total_errors"] += 1
            print(f"‚ùå Validation failed with exception: {e}")

        return validation_results

    def _validate_csv_structure(self, df):
        """Validate CSV has correct structure and columns."""
        # Enhanced expected columns for complete microstructure data
        expected_columns = [
            "date",
            "open",
            "high",
            "low",
            "close",
            "volume",
            "close_time",
            "quote_asset_volume",
            "number_of_trades",
            "taker_buy_base_asset_volume",
            "taker_buy_quote_asset_volume",
        ]

        # Legacy format for backward compatibility
        legacy_columns = ["date", "open", "high", "low", "close", "volume"]

        errors = []
        warnings = []

        # Check if it's enhanced or legacy format
        has_enhanced_format = all(col in df.columns for col in expected_columns)
        has_legacy_format = all(col in df.columns for col in legacy_columns)

        if has_enhanced_format:
            # Validate enhanced format
            missing_columns = [col for col in expected_columns if col not in df.columns]
            if missing_columns:
                errors.append(f"Missing enhanced columns: {missing_columns}")
        elif has_legacy_format:
            # Legacy format detected
            warnings.append(
                "Legacy format detected - missing microstructure columns for advanced analysis"
            )
            missing_enhanced = [col for col in expected_columns if col not in df.columns]
            warnings.append(f"Enhanced features unavailable: {missing_enhanced}")
        else:
            # Neither format complete
            missing_basic = [col for col in legacy_columns if col not in df.columns]
            errors.append(f"Missing basic required columns: {missing_basic}")

        extra_columns = [col for col in df.columns if col not in expected_columns]
        if extra_columns:
            warnings.append(f"Unexpected extra columns: {extra_columns}")

        # Check for empty data
        if len(df) == 0:
            errors.append("CSV file is empty (no data rows)")

        return {
            "status": "VALID" if not errors else "INVALID",
            "format_type": "enhanced"
            if has_enhanced_format
            else "legacy"
            if has_legacy_format
            else "incomplete",
            "errors": errors,
            "warnings": warnings,
            "columns_found": list(df.columns),
            "expected_columns": expected_columns,
            "legacy_columns": legacy_columns,
        }

    def _validate_datetime_sequence(self, df, expected_timeframe):
        """Validate datetime sequence is complete and chronological."""
        errors = []
        warnings = []
        gaps_found = 0

        # Convert date column to datetime
        try:
            df["datetime"] = pd.to_datetime(df["date"])
        except Exception as e:
            errors.append(f"Failed to parse dates: {e}")
            return {"status": "INVALID", "errors": errors, "warnings": warnings}

        # Check chronological order
        is_sorted = df["datetime"].is_monotonic_increasing

        # Find gaps if we have expected timeframe
        gap_details = []
        if expected_timeframe and len(df) > 1:
            # Calculate expected interval in minutes
            interval_map = {"1m": 1, "3m": 3, "5m": 5, "15m": 15, "30m": 30, "1h": 60, "2h": 120}
            expected_interval = interval_map.get(expected_timeframe, 0)

            if expected_interval > 0:
                expected_delta = pd.Timedelta(minutes=expected_interval)

                # Check for gaps
                for i in range(1, len(df)):
                    actual_delta = df["datetime"].iloc[i] - df["datetime"].iloc[i - 1]
                    if actual_delta > expected_delta:
                        gaps_found += 1
                        gap_details.append(
                            {
                                "position": i,
                                "expected_time": (
                                    df["datetime"].iloc[i - 1] + expected_delta
                                ).isoformat(),
                                "actual_time": df["datetime"].iloc[i].isoformat(),
                                "gap_duration": str(actual_delta - expected_delta),
                            }
                        )

                        # Record every single gap for complete validation tracking
                        warnings.append(
                            f"Gap at position {i}: expected {expected_delta}, got {actual_delta}"
                        )

        if not is_sorted:
            errors.append("Timestamps are not in chronological order")

        if gaps_found > 10:
            errors.append(f"Too many gaps found: {gaps_found} (data may be incomplete)")
        elif gaps_found > 0:
            warnings.append(f"{gaps_found} timestamp gaps found (market closures or data issues)")

        return {
            "status": "VALID" if not errors else "INVALID",
            "errors": errors,
            "warnings": warnings,
            "date_range": {
                "start": df["datetime"].min().isoformat(),
                "end": df["datetime"].max().isoformat(),
            },
            "duration_days": (df["datetime"].max() - df["datetime"].min()).days,
            "chronological_order": is_sorted,
            "gaps_found": gaps_found,
            "gap_details": gap_details,  # Complete gap details for thorough analysis
        }

    def _validate_ohlcv_quality(self, df):
        """Validate OHLCV data quality and logical consistency."""
        errors = []
        warnings = []

        # Check for negative or zero values
        negative_zero_count = 0
        for col in ["open", "high", "low", "close"]:
            negative_zero = (df[col] <= 0).sum()
            if negative_zero > 0:
                errors.append(f"Found {negative_zero} negative/zero values in {col}")
                negative_zero_count += negative_zero

        # Check volume (can be zero but not negative)
        negative_volume = (df["volume"] < 0).sum()
        if negative_volume > 0:
            errors.append(f"Found {negative_volume} negative volume values")

        zero_volume = (df["volume"] == 0).sum()
        if zero_volume > 0:
            warnings.append(f"Found {zero_volume} zero volume bars")

        # Check OHLC logic: High >= Low, Open/Close within High/Low range
        ohlc_errors = 0

        # High should be >= Low
        high_low_errors = (df["high"] < df["low"]).sum()
        if high_low_errors > 0:
            errors.append(f"Found {high_low_errors} bars where High < Low")
            ohlc_errors += high_low_errors

        # Open should be within High/Low range
        open_range_errors = ((df["open"] > df["high"]) | (df["open"] < df["low"])).sum()
        if open_range_errors > 0:
            errors.append(f"Found {open_range_errors} bars where Open is outside High/Low range")
            ohlc_errors += open_range_errors

        # Close should be within High/Low range
        close_range_errors = ((df["close"] > df["high"]) | (df["close"] < df["low"])).sum()
        if close_range_errors > 0:
            errors.append(f"Found {close_range_errors} bars where Close is outside High/Low range")
            ohlc_errors += close_range_errors

        return {
            "status": "VALID" if not errors else "INVALID",
            "errors": errors,
            "warnings": warnings,
            "price_range": {
                "min": min(df["low"].min(), df["high"].min(), df["open"].min(), df["close"].min()),
                "max": max(df["low"].max(), df["high"].max(), df["open"].max(), df["close"].max()),
            },
            "volume_stats": {
                "min": df["volume"].min(),
                "max": df["volume"].max(),
                "mean": df["volume"].mean(),
            },
            "ohlc_errors": ohlc_errors,
            "negative_zero_values": negative_zero_count,
        }

    def _validate_expected_coverage(self, df, expected_timeframe):
        """Validate data coverage matches expected timeframe and duration."""
        warnings = []

        if not expected_timeframe or len(df) == 0:
            return {"status": "SKIPPED", "warnings": ["Cannot validate coverage without timeframe"]}

        # Calculate expected bars based on timeframe and actual date range
        df["datetime"] = pd.to_datetime(df["date"])
        start_time = df["datetime"].min()
        end_time = df["datetime"].max()
        duration = end_time - start_time

        # Calculate expected number of bars
        interval_map = {"1m": 1, "3m": 3, "5m": 5, "15m": 15, "30m": 30, "1h": 60, "2h": 120}
        interval_minutes = interval_map.get(expected_timeframe, 0)

        if interval_minutes > 0:
            expected_bars = int(duration.total_seconds() / (interval_minutes * 60)) + 1
            actual_bars = len(df)
            coverage_percentage = (actual_bars / expected_bars) * 100

            if coverage_percentage < 95:
                warnings.append(
                    f"Low coverage: {coverage_percentage:.1f}% (may indicate missing data)"
                )
            elif coverage_percentage > 105:
                warnings.append(
                    f"High coverage: {coverage_percentage:.1f}% (may indicate duplicate data)"
                )
        else:
            expected_bars = 0
            coverage_percentage = 0
            warnings.append(f"Unknown timeframe '{expected_timeframe}' for coverage calculation")

        return {
            "status": "VALID" if not warnings else "WARNING",
            "warnings": warnings,
            "expected_bars": expected_bars,
            "actual_bars": len(df),
            "coverage_percentage": coverage_percentage,
            "duration_days": duration.days,
        }

    def _validate_statistical_anomalies(self, df):
        """Detect statistical anomalies in price and volume data."""
        warnings = []

        # Calculate basic statistics
        price_cols = ["open", "high", "low", "close"]

        # Price outliers (using IQR method)
        price_outliers = 0
        for col in price_cols:
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            outliers = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()
            price_outliers += outliers

        # Volume outliers
        vol_Q1 = df["volume"].quantile(0.25)
        vol_Q3 = df["volume"].quantile(0.75)
        vol_IQR = vol_Q3 - vol_Q1
        vol_upper_bound = vol_Q3 + 1.5 * vol_IQR
        volume_outliers = (df["volume"] > vol_upper_bound).sum()

        # Suspicious patterns
        suspicious_patterns = 0

        # Check for repeated identical prices (suspicious)
        for col in price_cols:
            repeated = df[col].value_counts()
            max_repeats = repeated.max()
            if max_repeats > len(df) * 0.1:  # More than 10% identical values
                warnings.append(f"Suspicious: {col} has {max_repeats} repeated values")
                suspicious_patterns += 1

        if price_outliers > len(df) * 0.05:  # More than 5% outliers
            warnings.append(
                f"High number of price outliers: {price_outliers} ({100 * price_outliers / len(df):.1f}%)"
            )

        if volume_outliers > len(df) * 0.02:  # More than 2% volume outliers
            warnings.append(
                f"High number of volume outliers: {volume_outliers} ({100 * volume_outliers / len(df):.1f}%)"
            )

        return {
            "status": "VALID" if not warnings else "WARNING",
            "warnings": warnings,
            "price_outliers": price_outliers,
            "volume_outliers": volume_outliers,
            "suspicious_patterns": suspicious_patterns,
        }

    def update_metadata_with_validation(self, csv_filepath, validation_results):
        """Update metadata JSON file with validation results."""
        metadata_filepath = csv_filepath.with_suffix(".metadata.json")

        if metadata_filepath.exists():
            with open(metadata_filepath, "r") as f:
                metadata = json.load(f)
        else:
            metadata = {}

        # Add validation results to metadata
        metadata["validation"] = validation_results

        # Update compliance status based on validation
        compliance = metadata.get("compliance", {})
        if validation_results["total_errors"] == 0:
            compliance["data_validation_passed"] = True
            compliance["validation_summary"] = validation_results["validation_summary"]
        else:
            compliance["data_validation_passed"] = False
            compliance["validation_summary"] = validation_results["validation_summary"]
            compliance["validation_errors"] = validation_results["total_errors"]
            compliance["validation_warnings"] = validation_results["total_warnings"]

        metadata["compliance"] = compliance

        # Save updated metadata with JSON serialization fix
        def convert_numpy_types(obj):
            """Convert numpy types to Python native types for JSON serialization."""
            if hasattr(obj, "item"):
                return obj.item()
            elif isinstance(obj, dict):
                return {key: convert_numpy_types(value) for key, value in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy_types(item) for item in obj]
            else:
                return obj

        with open(metadata_filepath, "w") as f:
            json.dump(convert_numpy_types(metadata), f, indent=2)

        print(f"‚úÖ Updated metadata: {metadata_filepath.name}")

    def apply_gap_filling_to_validated_files(self):
        """Apply comprehensive gap filling to validated data files using authentic Binance API data"""

        try:
            print("\nüîß INTEGRATED GAP FILLING SYSTEM")
            print("Primary Source: Binance REST API (Authentic Data Only)")
            print("=" * 60)

            # Initialize gap filling components
            gap_filler = UniversalGapFiller()

            # Find CSV files to check for gaps
            csv_files = list(Path(self.output_dir).glob("*.csv"))

            if not csv_files:
                print("‚ùå No CSV files found for gap filling")
                return

            # Filter to only files for this symbol
            symbol_files = [f for f in csv_files if self.symbol in f.name]

            if not symbol_files:
                print(f"‚ùå No CSV files found for symbol {self.symbol}")
                return

            print(f"üîç Analyzing {len(symbol_files)} files for gaps...")

            total_gaps_detected = 0
            total_gaps_filled = 0
            total_gaps_failed = 0
            files_processed = 0
            results = []

            for csv_file in symbol_files:
                print(f"\nüìÅ Processing: {csv_file.name}")

                # Extract timeframe from filename
                file_timeframe = self._extract_timeframe_from_filename(csv_file.name)
                print(f"   üìä Detected timeframe: {file_timeframe}")

                # Use the proper UniversalGapFiller process_file method
                result = gap_filler.process_file(csv_file, file_timeframe)
                results.append(result)
                files_processed += 1

                # Update totals
                total_gaps_detected += result["gaps_detected"]
                total_gaps_filled += result["gaps_filled"]
                total_gaps_failed += result["gaps_failed"]

                # Report per-file results
                if result["gaps_detected"] == 0:
                    print(f"   ‚úÖ No gaps found in {file_timeframe}")
                else:
                    success_rate = result["success_rate"]
                    status = "‚úÖ" if success_rate == 100.0 else "‚ö†Ô∏è" if success_rate > 0 else "‚ùå"
                    print(
                        f"   {status} {result['gaps_filled']}/{result['gaps_detected']} gaps filled ({success_rate:.1f}%)"
                    )

            # Comprehensive summary
            print("\n" + "=" * 60)
            print("üìä GAP FILLING SUMMARY")
            print("=" * 60)

            for result in results:
                if result["gaps_detected"] > 0:
                    status = (
                        "‚úÖ"
                        if result["success_rate"] == 100.0
                        else "‚ö†Ô∏è"
                        if result["success_rate"] > 0
                        else "‚ùå"
                    )
                    print(
                        f"{status} {result['timeframe']:>3}: {result['gaps_filled']:>2}/{result['gaps_detected']:>2} gaps filled ({result['success_rate']:>5.1f}%)"
                    )

            print("-" * 60)
            overall_success = (
                (total_gaps_filled / total_gaps_detected * 100)
                if total_gaps_detected > 0
                else 100.0
            )
            print(
                f"üéØ OVERALL: {total_gaps_filled}/{total_gaps_detected} gaps filled ({overall_success:.1f}%)"
            )

            if overall_success == 100.0:
                print("üéâ ALL GAPS FILLED SUCCESSFULLY!")
                print("‚úÖ Datasets are now 100% gapless and ready for production use")
            else:
                print(
                    f"‚ö†Ô∏è  {total_gaps_failed} gaps failed to fill (may be legitimate exchange outages)"
                )
                print("üìã Review failed gaps to confirm they are legitimate market closures")

            print(f"\nFiles processed: {files_processed}")
            print("Data source: Authentic Binance REST API")
            print("Gap filling protocol: API-first validation (no synthetic data)")

        except Exception as e:
            print(f"‚ùå Gap filling error: {e}")
            print("‚ö†Ô∏è  Continuing without gap filling...")
            import traceback

            traceback.print_exc()

    def _extract_timeframe_from_filename(self, filename):
        """Extract timeframe from filename (e.g., 'SOLUSDT-15m-data.csv' -> '15m')"""
        for tf in [
            "1s",
            "1m",
            "3m",
            "5m",
            "15m",
            "30m",
            "1h",
            "2h",
            "4h",
            "6h",
            "8h",
            "12h",
            "1d",
            "3d",
            "1w",
            "1mo",
        ]:
            if f"-{tf}_" in filename or f"-{tf}-" in filename:
                return tf
        return "15m"  # Default


def main():
    """Main execution function with CLI argument support."""
    parser = argparse.ArgumentParser(
        description="Ultra-fast Binance spot data collector with validation"
    )
    parser.add_argument(
        "--symbol", default="SOLUSDT", help="Trading pair symbol (default: SOLUSDT)"
    )
    parser.add_argument(
        "--timeframes",
        default="1m,3m,5m,15m,30m,1h,2h",
        help="Comma-separated timeframes (default: 1m,3m,5m,15m,30m,1h,2h)",
    )
    parser.add_argument(
        "--start", default="2020-08-15", help="Start date YYYY-MM-DD (default: 2020-08-15)"
    )
    parser.add_argument(
        "--end", default="2025-03-20", help="End date YYYY-MM-DD (default: 2025-03-20)"
    )
    parser.add_argument(
        "--validate-only",
        action="store_true",
        help="Only validate existing CSV files, do not collect new data",
    )
    parser.add_argument(
        "--validate-files", nargs="+", help="Specific CSV files to validate (with --validate-only)"
    )
    parser.add_argument(
        "--no-validation",
        action="store_true",
        help="Skip validation after collection (not recommended)",
    )

    args = parser.parse_args()

    print("Binance Public Data Ultra-Fast Collector with Validation")
    print("Official Binance data repository - 10-100x faster than API")
    print("=" * 80)

    # Initialize collector
    collector = BinancePublicDataCollector(
        symbol=args.symbol, start_date=args.start, end_date=args.end
    )

    if args.validate_only:
        # VALIDATION-ONLY MODE
        print("üîç VALIDATION-ONLY MODE")

        if args.validate_files:
            # Validate specific files
            files_to_validate = [Path(f) for f in args.validate_files]
        else:
            # Auto-discover CSV files in sample_data directory
            pattern = f"*{args.symbol}*.csv"
            files_to_validate = list(collector.output_dir.glob(pattern))

        if not files_to_validate:
            print("‚ùå No CSV files found to validate")
            return 1

        print(f"Found {len(files_to_validate)} files to validate:")
        for file_path in files_to_validate:
            print(f"  üìÑ {file_path.name}")

        validation_summary = []
        for csv_file in files_to_validate:
            # Extract timeframe from filename for validation
            timeframe = None
            for tf in ["1m", "3m", "5m", "15m", "30m", "1h", "2h", "4h", "1d"]:
                if f"-{tf}_" in csv_file.name:
                    timeframe = tf
                    break

            # Validate file
            validation_result = collector.validate_csv_file(csv_file, timeframe)

            # Update metadata with validation results
            collector.update_metadata_with_validation(csv_file, validation_result)

            validation_summary.append(
                {
                    "file": csv_file.name,
                    "status": validation_result["validation_summary"],
                    "errors": validation_result["total_errors"],
                    "warnings": validation_result["total_warnings"],
                }
            )

        # Print validation summary
        print("\n" + "=" * 80)
        print("VALIDATION SUMMARY")
        print("=" * 80)

        perfect_files = 0
        good_files = 0
        failed_files = 0

        for summary in validation_summary:
            if summary["errors"] == 0:
                if summary["warnings"] == 0:
                    status_icon = "‚úÖ"
                    perfect_files += 1
                else:
                    status_icon = "‚ö†Ô∏è "
                    good_files += 1
            else:
                status_icon = "‚ùå"
                failed_files += 1

            print(f"{status_icon} {summary['file']}: {summary['status']}")
            if summary["errors"] > 0 or summary["warnings"] > 0:
                print(f"   ‚îî‚îÄ {summary['errors']} errors, {summary['warnings']} warnings")

        print("\nOVERALL RESULTS:")
        print(f"  ‚úÖ Perfect: {perfect_files} files")
        print(f"  ‚ö†Ô∏è  Good: {good_files} files")
        print(f"  ‚ùå Failed: {failed_files} files")

        if failed_files == 0:
            print("\nüéâ ALL VALIDATIONS PASSED!")
            return 0
        else:
            print(f"\n‚ö†Ô∏è  {failed_files} files failed validation")
            return 1

    else:
        # COLLECTION MODE (with optional validation)
        timeframes = [tf.strip() for tf in args.timeframes.split(",")]
        print(f"Collecting timeframes: {timeframes}")

        # Collect data
        results = collector.collect_multiple_timeframes(timeframes)

        if results:
            print(f"\nüöÄ ULTRA-FAST COLLECTION SUCCESS: Generated {len(results)} datasets")

            # Auto-validation after collection (unless disabled)
            if not args.no_validation:
                print("\nüîç AUTO-VALIDATION AFTER COLLECTION")
                validation_passed = 0
                validation_failed = 0

                for timeframe, csv_file in results.items():
                    validation_result = collector.validate_csv_file(csv_file, timeframe)
                    collector.update_metadata_with_validation(csv_file, validation_result)

                    if validation_result["total_errors"] == 0:
                        validation_passed += 1
                    else:
                        validation_failed += 1

                print(
                    f"\nVALIDATION RESULTS: {validation_passed} passed, {validation_failed} failed"
                )

                if validation_failed == 0:
                    print("üéâ ALL FILES VALIDATED SUCCESSFULLY!")
                    print("Ready for ML training, backtesting, and production use")

                    # AUTOMATIC GAP FILLING - Now using comprehensive gap detection and filling
                    collector.apply_gap_filling_to_validated_files()

                else:
                    print("‚ö†Ô∏è  Some files failed validation - check errors above")

            return 0
        else:
            print("‚ùå Collection failed")
            return 1


if __name__ == "__main__":
    exit(main())

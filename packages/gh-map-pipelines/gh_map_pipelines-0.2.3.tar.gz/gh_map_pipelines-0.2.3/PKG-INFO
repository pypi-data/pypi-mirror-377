Metadata-Version: 2.1
Name: gh-map-pipelines
Version: 0.2.3
Summary: Map pipelines Github in Parquet and SQLite with incremental processing and rate limit handling
License: MIT
Author: Cristiano Henrique dos Santos
Author-email: cristianovisk@gmail.com
Requires-Python: >=3.12,<4.0
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.12
Requires-Dist: fastapi (>=0.116.2,<0.117.0)
Requires-Dist: httpx (>=0.28.1,<0.29.0)
Requires-Dist: pandas (>=2.3.1,<3.0.0)
Requires-Dist: pyarrow (>=21.0.0,<22.0.0)
Requires-Dist: pydantic (>=2.11.9,<3.0.0)
Requires-Dist: pyyaml (>=6.0.2,<7.0.0)
Requires-Dist: rich (>=14.0.0,<15.0.0)
Requires-Dist: typer (>=0.16.0,<0.17.0)
Requires-Dist: uvicorn (>=0.35.0,<0.36.0)
Project-URL: Bug Tracker, https://github.com/cristianovisk/gh_map_pipelines/issues
Project-URL: Code, https://github.com/cristianovisk/gh_map_pipelines
Project-URL: Documentation, https://github.com/cristianovisk/gh_map_pipelines/blob/main/README.md
Description-Content-Type: text/markdown

# gh_map_pipelines v2 - Improved Version

## üìã Principais Melhorias

### 1. **Grava√ß√£o Incremental**
- Dados s√£o salvos em lotes (batch) durante o processamento
- N√£o h√° perda de dados em caso de interrup√ß√£o
- Arquivos Parquet s√£o atualizados incrementalmente
- Batch size configur√°vel (padr√£o: 10 itens)

### 2. **Tratamento Inteligente de Rate Limit**
- Detecta erro 429 automaticamente
- Aguarda tempo configur√°vel (padr√£o: 60 segundos)
- Registra ocorr√™ncias em tabela `rate_limit_log`
- Verifica hist√≥rico antes de fazer novas requisi√ß√µes
- Retry autom√°tico com backoff

### 3. **Sistema de Retomada (Resume)**
- Rastreia progresso em tabelas de controle
- Permite retomar de onde parou com `--resume`
- N√£o reprocessa workflows j√° analisados
- Mant√©m contador de tentativas para cada item

### 4. **Novos Comandos CLI**

## üöÄ Uso

### Instala√ß√£o
```bash
# Se estiver usando poetry
poetry install

# Ou instale diretamente
pip install -e .
```

### Comandos Dispon√≠veis

#### 1. Processamento Completo
```bash
# Processar organiza√ß√£o do zero
gh_map process --org <nome-da-org>

# Com configura√ß√µes customizadas
gh_map process --org <nome-da-org> --batch-size 20 --wait 120
```

#### 2. Retomar Processamento
```bash
# Retomar de onde parou (ap√≥s interrup√ß√£o ou rate limit)
gh_map process --org <nome-da-org> --mode resume

# Ou use o atalho
gh_map resume --org <nome-da-org>
```

#### 3. Processar Apenas Uses (ap√≥s coletar repos e workflows)
```bash
# √ötil quando repos e workflows j√° foram coletados
gh_map process --org <nome-da-org> --mode uses-only
```

#### 4. Verificar Status
```bash
# Ver estat√≠sticas do processamento
gh_map status --org <nome-da-org>

# Ou atrav√©s do modo
gh_map process --org <nome-da-org> --mode status
```

#### 5. Reset de Status (mant√©m dados)
```bash
# Reseta status de processamento mas mant√©m dados coletados
gh_map reset --org <nome-da-org> --confirm
```

#### 6. Limpar Dados
```bash
# Limpar todos os dados
gh_map clean --org <nome-da-org> --confirm

# Limpar mas manter reposit√≥rios
gh_map clean --org <nome-da-org> --confirm --keep-repos
```

#### 7. Exportar Dados
```bash
# Exportar para JSON
gh_map export --org <nome-da-org> --format json --output data.json

# Exportar para CSV (gera 3 arquivos)
gh_map export --org <nome-da-org> --format csv --output export
```

## üìä Estrutura de Dados

### Tabelas SQLite

#### 1. `repositorios`
- Informa√ß√µes dos reposit√≥rios da organiza√ß√£o

#### 2. `workflow_runs`
- Workflows do GitHub Actions de cada reposit√≥rio

#### 3. `uses_workflows`
- Actions utilizadas em cada workflow

#### 4. `workflow_status` (Nova)
- Controle de processamento de cada workflow
- Status: `completed`, `error`, `rate_limited`, `not_found`, `yaml_error`, `skipped`
- Contador de tentativas

#### 5. `processing_status` (Nova)
- Estado geral do processamento
- Checkpoints para retomada

#### 6. `rate_limit_log` (Nova)
- Hist√≥rico de rate limits encontrados
- Usado para evitar requisi√ß√µes durante per√≠odo de espera

### Arquivos Parquet
- `repos.parquet` - Reposit√≥rios
- `workflow_runs.parquet` - Workflows
- `uses_workflows.parquet` - Uses encontrados

## üîÑ Fluxo de Processamento

1. **Coleta de Reposit√≥rios**
   - Lista p√°ginas da API incrementalmente
   - Salva em lotes no banco e parquet
   - Checkpoint a cada p√°gina processada

2. **Coleta de Workflows**
   - Processa reposit√≥rio por reposit√≥rio
   - Pula reposit√≥rios j√° processados
   - Salva incrementalmente

3. **Extra√ß√£o de Uses**
   - Baixa YAMLs dos workflows
   - Extrai actions recursivamente
   - Marca status de cada workflow
   - Retry autom√°tico em caso de falha

## üõ°Ô∏è Resili√™ncia

### Cen√°rios Tratados:
- **Rate Limit (429)**: Aguarda e tenta novamente
- **Timeout**: Retry autom√°tico
- **Erro 404**: Marca como `not_found` e continua
- **YAML inv√°lido**: Marca como `yaml_error` e continua
- **Interrup√ß√£o**: Use `--resume` para continuar

### Limites de Retry:
- M√°ximo de 3 tentativas por workflow
- Workflows com muitos erros s√£o pulados
- Rate limit reseta contador ap√≥s sucesso

## üìà Monitoramento

### Status Report mostra:
- Total de reposit√≥rios coletados
- Total de workflows encontrados
- Total de uses extra√≠dos
- Workflows processados com sucesso
- Workflows com erro
- Workflows aguardando retry (rate limited)

## üí° Dicas de Uso

1. **Para grandes organiza√ß√µes**: Use batch size maior (ex: `--batch-size 50`)

2. **Se encontrar muitos rate limits**: Aumente tempo de espera (ex: `--wait 300`)

3. **Para debug**: Acompanhe os logs coloridos no console

4. **Consulta durante processamento**: Os dados s√£o salvos incrementalmente, permitindo consultas SQL/Parquet mesmo durante a coleta

5. **Processamento em etapas**:
   ```bash
   # Primeiro, colete apenas reposit√≥rios
   gh_map process --org myorg
   # (interrompa com Ctrl+C ap√≥s coletar repos)
   
   # Depois, continue com workflows e uses
   gh_map resume --org myorg
   ```

## üîç Consultas SQL √öteis

```sql
-- Workflows pendentes de processamento
SELECT COUNT(*) FROM workflow_runs wr
LEFT JOIN workflow_status ws ON wr.node_id = ws.node_id
WHERE ws.status IS NULL OR ws.status != 'completed';

-- Top 10 actions mais usadas
SELECT use, COUNT(*) as count 
FROM uses_workflows 
GROUP BY use 
ORDER BY count DESC 
LIMIT 10;

-- Reposit√≥rios com mais workflows
SELECT r.name, COUNT(w.id) as workflow_count
FROM repositorios r
JOIN workflow_runs w ON r.id = w.id_repo
GROUP BY r.name
ORDER BY workflow_count DESC;
```

## üêõ Troubleshooting

### Problema: Rate limit constante
**Solu√ß√£o**: Aumente o tempo de espera ou execute em hor√°rios com menos tr√°fego

### Problema: Mem√≥ria insuficiente para grandes orgs
**Solu√ß√£o**: Reduza o batch size para processar menos itens por vez

### Problema: Processo travado
**Solu√ß√£o**: Use `Ctrl+C` para interromper e depois `gh_map resume` para continuar

## üìù Notas de Implementa√ß√£o

### Diferen√ßas da v1:
1. N√£o carrega tudo em mem√≥ria antes de salvar
2. Salva incrementalmente em lotes configur√°veis
3. Rate limit com retry inteligente e backoff
4. Sistema de checkpoint para retomada
5. M√∫ltiplos modos de opera√ß√£o
6. Comandos auxiliares (status, reset, clean, export)
7. Melhor tratamento de erros e edge cases


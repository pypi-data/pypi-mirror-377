from typing import Any, Callable, Dict, List, Optional, Sequence

from hep_data_llm.cache import CacheType
from hep_data_llm.hint_files import InjectedFile
from hep_data_llm.models import ModelInfo
from hep_data_llm.plot import plot
from hep_data_llm.query_config import (
    HintCollectionConfig,
    ProfileConfig,
    PromptConfig,
)
from hep_data_llm.run_in_docker import DockerRunResult
from hep_data_llm.usage_info import UsageInfo


def _build_profile_config() -> ProfileConfig:
    prompt_cfg = PromptConfig(text="prompt", hint_collection="prompt_hints")
    modify_cfg = PromptConfig(text="modify", hint_collection="modify_hints")
    return ProfileConfig(
        hint_collections={
            "prompt_hints": HintCollectionConfig(
                hint_files=["prompt.txt"],
                python_files=["helpers/prompt_helper.py", "shared.py"],
            ),
            "modify_hints": HintCollectionConfig(
                hint_files=["modify.txt"],
                python_files=["shared.py"],
            ),
        },
        prompts={"prompt": prompt_cfg, "modify_prompt": modify_cfg},
        model_name="test-model",
        docker_image="docker-image",
    )


def test_plot_respects_ignore_cache_flags(monkeypatch, tmp_path) -> None:
    config = _build_profile_config()

    monkeypatch.setattr("hep_data_llm.plot.load_config", lambda profile: config)

    hint_calls: List[bool] = []
    python_calls: List[bool] = []
    captured_injected_files: List[List[str]] = []

    def fake_load_hint_files(
        hint_files: List[str], ignore_cache: bool = False
    ) -> List[str]:
        hint_calls.append(ignore_cache)
        return ["hint contents"]

    monkeypatch.setattr("hep_data_llm.plot.load_hint_files", fake_load_hint_files)

    def fake_load_python_files(
        python_files: Sequence[str], ignore_cache: bool = False
    ) -> List[InjectedFile]:
        python_calls.append(ignore_cache)
        return [
            InjectedFile(name=python_file.split("/")[-1], content="")
            for python_file in python_files
        ]

    monkeypatch.setattr("hep_data_llm.plot.load_python_files", fake_load_python_files)

    dummy_model = ModelInfo(
        model_name="test-model",
        input_cost_per_million=0.0,
        output_cost_per_million=0.0,
        endpoint=None,
    )

    monkeypatch.setattr(
        "hep_data_llm.plot.load_models", lambda: {"test-model": dummy_model}
    )
    monkeypatch.setattr(
        "hep_data_llm.plot.process_model_request",
        lambda models, all_models, default: ["test-model"],
    )

    ignore_flags: Dict[str, Optional[bool]] = {"code": None, "llm": None}

    def fake_code_it_up(
        fh_out: Any,
        model: ModelInfo,
        prompt_write_code: str,
        prompt_fix_code: str,
        code_policies: List[Any],
        max_iter: int,
        called_code: str,
        prompt_args: Dict[str, str],
        docker_image: str,
        ignore_code_cache: bool,
        ignore_llm_cache: bool,
        llm_usage_callback: Optional[Callable[[str, UsageInfo], None]],
        docker_usage_callback: Optional[Callable[[str, DockerRunResult], None]],
        injected_files: Sequence[InjectedFile] | None = None,
    ) -> tuple[DockerRunResult, str, bool]:
        ignore_flags["code"] = ignore_code_cache
        ignore_flags["llm"] = ignore_llm_cache
        captured_injected_files.append(
            [injected.name for injected in injected_files or []]
        )
        usage = UsageInfo(
            model=model.model_name,
            elapsed=1.0,
            prompt_tokens=10,
            completion_tokens=5,
            total_tokens=15,
            cost=0.01,
        )
        if llm_usage_callback is not None:
            llm_usage_callback("Run 1", usage)
        docker_result = DockerRunResult(
            stdout="**Success**",
            stderr="",
            elapsed=2.0,
            png_files=[],
            exit_code=0,
        )
        if docker_usage_callback is not None:
            docker_usage_callback("Run 1", docker_result)
        return docker_result, "print('hi')", True

    monkeypatch.setattr("hep_data_llm.plot.code_it_up", fake_code_it_up)

    output_path = tmp_path / "out.md"

    plot(
        "Question?",
        output_path,
        None,
        {CacheType.HINTS, CacheType.CODE},
        error_info=True,
        n_iter=1,
        docker_image=None,
        profile="test-profile",
    )

    assert hint_calls == [True, True]
    assert python_calls == [True, True]
    assert ignore_flags["code"] is True
    assert ignore_flags["llm"] is False
    assert captured_injected_files[0] == ["prompt_helper.py", "shared.py"]

    hint_calls.clear()
    python_calls.clear()
    ignore_flags["code"] = None
    ignore_flags["llm"] = None

    plot(
        "Another question?",
        output_path,
        None,
        {CacheType.LLM},
        error_info=True,
        n_iter=1,
        docker_image=None,
        profile="test-profile",
    )

    assert hint_calls == [False, False]
    assert python_calls == [False, False]
    assert ignore_flags["code"] is False
    assert ignore_flags["llm"] is True
    assert captured_injected_files[1] == ["prompt_helper.py", "shared.py"]

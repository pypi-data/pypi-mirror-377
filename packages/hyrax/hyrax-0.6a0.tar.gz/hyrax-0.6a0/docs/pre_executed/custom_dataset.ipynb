{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with Hyrax Custom Dataset Classes\n",
    "\n",
    "In this notebook we are going to build up a custom dataset class for hyrax, and show how you can use the \n",
    "`prepare` verb in hyrax to test various aspects of your new dataclass.\n",
    "\n",
    "First we will create some data in the form of 1000 random 10x10 tensors, fake filenames for these tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "num_tensors = 1000\n",
    "\n",
    "# Generate filenames\n",
    "alphabet = list(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\")\n",
    "filename_length = 15\n",
    "filenames = [\"\".join(list(rng.choice(alphabet, 15))) for _ in range(num_tensors)]\n",
    "\n",
    "# Generate tensors\n",
    "shape = (3, 10, 10)\n",
    "random_data = {file: torch.from_numpy(rng.random(size=shape, dtype=np.float32)) for file in filenames}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a custom Dataset class\n",
    "\n",
    "We will treat these tensors as if they are on the filesystem, and write a dataclass that gives hyrax access to \n",
    "these \"files\" treating `_read_tensor` as a library function which returns a torch.Tensor from our \"files\", \n",
    "and `_list_filenames` as a library function which lists the filenames in a particular path.\n",
    "\n",
    "The first thing we need to do is make a new class derived from HyraxDataset and torch.Dataset as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from hyrax.data_sets import HyraxDataset\n",
    "from pathlib import Path\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "class MyDataset(HyraxDataset, Dataset):\n",
    "    def __init__(self, config: dict, data_location: Union[Path, str] = None):\n",
    "        self.filenames = MyDataset._list_filenames(data_location)\n",
    "\n",
    "        super().__init__(config)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"data\": {\"image\": MyDataset.get_image(self.filenames[idx])},\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    @staticmethod\n",
    "    def _list_filenames(path_to_data):\n",
    "        \"\"\"This is a pretend implementation so we ignore path_to_data\"\"\"\n",
    "        global filenames\n",
    "        return filenames\n",
    "\n",
    "    def get_image(self, index):\n",
    "        \"\"\"Pretend to read specific data from the disk.\"\"\"\n",
    "        filename = self.filenames[index]\n",
    "        global random_data\n",
    "        return random_data[filename]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key aspects of this class that you will need to replicate are:\n",
    "\n",
    "* `__init__` must call `super().__init__(config)` This is important for hyrax to function appropriately, and \n",
    "gives you access to hyrax's config in other functions should you want it later. You will probably want to \n",
    "access `config[\"general\"][\"data_dir\"]` to figure out what directory to start in.\n",
    "\n",
    "* `__getitem__` You must implement this function, it takes an index and return the appropriate torch.Tensor\n",
    "for your data.\n",
    "\n",
    "* `__len__` must return the length of your tensorial data.\n",
    "\n",
    "Note that all of these are instance methods that use `self` as the first argument. This `self` is the current\n",
    "`MyDataset` object, and allows you to set and get values as is done with `self.filenames` in the code above.\n",
    "\n",
    "The functions `_list_filenames()` and `_read_tensor()` are both reading our fake data, and are there so we \n",
    "have an effective demonstration. The functional organization of your analogous file reading code is entirely \n",
    "up to you!\n",
    "\n",
    "\n",
    "We're now going to start up Hyrax and use the `prepare` verb to create an instance of this class and see\n",
    "that it works correctly. Note that we have set `config[\"general][\"data_dir\"]` to specify the location of our\n",
    "data for the `__init__`  function we wrote earlier, as well as the `config[\"data_set\"][\"name\"]` to the \n",
    "name of our class, so that Hyrax knows to use our dataset class rather than one of the built-in ones.\n",
    "\n",
    "Our `h.prepare()` line in the script will have the effect of calling our `__init__` function with the \n",
    "current hyrax config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-09-12 12:15:10,407 hyrax:INFO] Runtime Config read from: /Users/drew/code/hyrax/src/hyrax/hyrax_default_config.toml\n",
      "[2025-09-12 12:15:12,179 hyrax.data_sets.data_provider:INFO] No fields were specified for 'data'. The request will be modified to select all by default. You can specify `fields` in `model_inputs`.\n",
      "[2025-09-12 12:15:12,208 hyrax.prepare:INFO] Finished Prepare\n"
     ]
    }
   ],
   "source": [
    "import hyrax\n",
    "\n",
    "h = hyrax.Hyrax()\n",
    "h.config[\"general\"][\"data_dir\"] = \"/fake/path/to/some/data\"\n",
    "h.config[\"data_set\"][\"name\"] = \"MyDataset\"\n",
    "\n",
    "dataset = h.prepare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "The object we recieved from `h.prepare()` is an instance of our dataset, which we can test for functionality\n",
    "\n",
    "We're going to index into the dataset object with `[]` this has the effect of calling our `__getitem__` function\n",
    "and returning the result.\n",
    "\n",
    "We're also going to call `len()` on the dataset which will have the effect of calling our `__len__` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking __getitem__ ...\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'object_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mChecking __getitem__ ...\u001b[39m\u001b[33m\"\u001b[39m, end=\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m item = \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mShape of our first element, should be \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtorch.Size([3,10,10])\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m: \u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(item.shape, end=\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/hyrax/src/hyrax/data_sets/data_provider.py:134\u001b[39m, in \u001b[36mDataProvider.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx) -> \u001b[38;5;28mdict\u001b[39m:\n\u001b[32m    119\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"This method returns data for a given index.\u001b[39;00m\n\u001b[32m    120\u001b[39m \n\u001b[32m    121\u001b[39m \u001b[33;03m    It is also a wrapper that allows this class to be treated as a PyTorch\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    132\u001b[39m \u001b[33;03m        A dictionary containing the requested data from the prepared datasets.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresolve_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/hyrax/src/hyrax/data_sets/data_provider.py:440\u001b[39m, in \u001b[36mDataProvider.resolve_data\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    437\u001b[39m \u001b[38;5;66;03m# Because there is machinery in the consuming code that expects an \"object_id\"\u001b[39;00m\n\u001b[32m    438\u001b[39m \u001b[38;5;66;03m# key in the returned data, we will add that here if a primary dataset.\u001b[39;00m\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.primary_dataset:\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     returned_data[\u001b[33m\"\u001b[39m\u001b[33mobject_id\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mreturned_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprimary_dataset\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprimary_dataset_id_field_name\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m returned_data\n",
      "\u001b[31mKeyError\u001b[39m: 'object_id'"
     ]
    }
   ],
   "source": [
    "print(\"Checking __getitem__ ...\", end=\"\\n\\n\")\n",
    "item = dataset[0]\n",
    "\n",
    "print('Shape of our first element, should be \"torch.Size([3,10,10])\": ')\n",
    "print(item.shape, end=\"\\n\\n\")\n",
    "\n",
    "print(\"Type of our first element, should be \\\"<class 'torch.Tensor'>\\\": \")\n",
    "print(type(item), end=\"\\n\\n\")\n",
    "\n",
    "print(\"Checking __len__ ...\\n\\nShould print 0: \")\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset class is suitable for training or inference with Hyrax; however, you may want to read on to learn\n",
    "about more advanced features such as custom IDs for your data elements, metadata, and configuration access.\n",
    "\n",
    "Below is a short example that uses the HyraxAutoencoder built-in model, demonstrating that training is possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyrax\n",
    "\n",
    "h = hyrax.Hyrax()\n",
    "h.config[\"general\"][\"data_dir\"] = \"/fake/path/to/some/data\"\n",
    "h.config[\"data_set\"][\"name\"] = \"MyDataset\"\n",
    "h.config[\"model\"][\"name\"] = \"HyraxAutoencoder\"\n",
    "\n",
    "h.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending to support visualization\n",
    "\n",
    "This section is primarily concerned with binding different sorts of metadata to your dataset. This metadata\n",
    "is used by the Hyrax visualization components to identify the source data of your latent space representation\n",
    "and link it back to a particular object/event in your astronomical dataset. \n",
    "\n",
    "When we built `MyDataclass` above, we invisibly picked up two major aspects from `HyraxDataset`:\n",
    "\n",
    "1. Unique IDs: Every tensor in our dataset got an ID of a sequential zero-based index, which was exactly the \n",
    "argument to `__getitem__`/`[]`. This list of ids is available as an iterator by calling `ids()` on the dataset\n",
    "object. These IDs are used in inference results and visualizations of the data, but they can be overriden.\n",
    "\n",
    "2. Metadata Interface: Every `HyraxDataset` can provide an astropy `Table` of values in the same order as \n",
    "their `__getitem__`/`[]` This allows each tensor in the dataset to have associated scalar data such as ra/dec, \n",
    "ephemeris parameters, redshift, magnitude, etc. For our class there currently is no metadata.\n",
    "\n",
    "Below is how we would access the metadata and IDs demonstrating the default behavior if your custom class\n",
    "does no overrides:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyrax\n",
    "\n",
    "h = hyrax.Hyrax()\n",
    "h.config[\"general\"][\"data_dir\"] = \"/fake/path/to/some/data\"\n",
    "h.config[\"data_set\"][\"name\"] = \"MyDataset\"\n",
    "\n",
    "dataset = h.prepare()\n",
    "\n",
    "print(\"\\nIDs:\")\n",
    "print(f\"list(dataset.ids())[0:10] = {list(dataset.ids())[0:10]}\")\n",
    "\n",
    "\n",
    "print(\"\\nMetadata field list:\")\n",
    "print(f\"dataset.metadata_fields() = {dataset.metadata_fields()} (there is no metadata)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding IDs\n",
    "\n",
    "We're going to use the filename in our fake data as IDs by adding a single `ids()` method to our `MyDataset` \n",
    "object. The most expedient way to do this will be to redefine the entire class below. Note that functions \n",
    "marked with a comment are just the same as earlier.\n",
    "\n",
    "Note that the `ids()` function is required to return a generator, so we will use a `for` loop and `yield`\n",
    "each sequential value. This interface allows Hyrax to partially enumerate the IDs in a dataset when that\n",
    "is desirable. It is easy enough to get all the ids in order with `list(dataset.ids())`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from hyrax.data_sets import HyraxDataset\n",
    "\n",
    "\n",
    "class MyDataset(HyraxDataset, Dataset):\n",
    "    def ids(self):\n",
    "        for filename in self.filenames:\n",
    "            yield filename\n",
    "\n",
    "    # Unchanged from before below this comment ...\n",
    "    def __init__(self, config: dict):\n",
    "        self.filenames = MyDataset._list_filenames(config[\"general\"][\"data_dir\"])\n",
    "        super().__init__(config)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return MyDataset._read_tensor(self.filenames[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    @staticmethod\n",
    "    def _list_filenames(path_to_data):\n",
    "        \"\"\"This is a pretend implementation so we ignore path_to_data\"\"\"\n",
    "        global filenames\n",
    "        return filenames\n",
    "\n",
    "    @staticmethod\n",
    "    def _read_tensor(filename):\n",
    "        \"\"\"Pretend to read a particular tensor from the disk.\"\"\"\n",
    "        global random_data\n",
    "        return random_data[filename]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running `prepare` again on our newly defined dataset class, we can see that the ids are now the fake \n",
    "\"filenames\" we generated at the top of the notebook, rather than sequential integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyrax\n",
    "\n",
    "h = hyrax.Hyrax()\n",
    "h.config[\"general\"][\"data_dir\"] = \"/fake/path/to/some/data\"\n",
    "h.config[\"data_set\"][\"name\"] = \"MyDataset\"\n",
    "\n",
    "dataset = h.prepare()\n",
    "\n",
    "print(\"\\nIDs:\")\n",
    "print(f\"list(dataset.ids())[0:5] = {list(dataset.ids())[0:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Metadata\n",
    "\n",
    "Now we are going to generate some fake metadata for our fake data. This will take the form of \n",
    "random ra/dec pairs for each fake object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import astropy.units as u\n",
    "from astropy.coordinates import SkyCoord\n",
    "\n",
    "ras = rng.uniform(low=0.0, high=360.0, size=num_tensors) * u.deg\n",
    "decs = rng.uniform(low=-90.0, high=90.0, size=num_tensors) * u.deg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to override metadata we will provide `HyraxDataset` with an astropy table containing all of the metadata in the constructor for our class as shown below. We do this in `__init__` by passing an astropy table of our metadata to `super().__init__` as a second, optional argument.\n",
    "\n",
    "Note the new function `_read_metadata()` which constructs this table. On a real dataset this function would\n",
    "most likely call astropy's `Table.read` [high level interface](https://docs.astropy.org/en/latest/io/unified.html) to construct a table directly from your catalog.\n",
    "\n",
    "As before we re-implement the entire class below with small modifications marked with comments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from hyrax.data_sets import HyraxDataset\n",
    "\n",
    "\n",
    "class MyDataset(HyraxDataset, Dataset):\n",
    "    def __init__(self, config: dict):\n",
    "        self.filenames = MyDataset._list_filenames(config[\"general\"][\"data_dir\"])\n",
    "        metadata_table = MyDataset._read_metadata(config[\"general\"][\"data_dir\"])\n",
    "        super().__init__(config, metadata_table=metadata_table)\n",
    "\n",
    "    def _read_metadata(path_to_data):\n",
    "        \"\"\"This is a pretend implementation so we don't use the path passed, which you might use\n",
    "        to find your .csv/.fits/.tsv catalog file and call astropy's Table.read().\n",
    "\n",
    "        We simply construct a table from our mock data\"\"\"\n",
    "        from astropy.table import Table\n",
    "\n",
    "        global ras, decs, filenames\n",
    "        return Table({\"object_id\": filenames, \"ra\": ras, \"dec\": decs})\n",
    "\n",
    "    # Unchanged from before below this comment ...\n",
    "    def ids(self):\n",
    "        for filename in self.filenames:\n",
    "            yield filename\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return MyDataset._read_tensor(self.filenames[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    @staticmethod\n",
    "    def _list_filenames(path_to_data):\n",
    "        \"\"\"This is a pretend implementation so we ignore path_to_data\"\"\"\n",
    "        global filenames\n",
    "        return filenames\n",
    "\n",
    "    @staticmethod\n",
    "    def _read_tensor(filename):\n",
    "        \"\"\"Pretend to read a particular tensor from the disk.\"\"\"\n",
    "        global random_data\n",
    "        return random_data[filename]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our dataset class supports metadata, we can access the metadata interface directly on the dataset object using the `metadata_fields` and `metadata` functions on the dataset object.\n",
    "\n",
    "- `metadata_fields` lists the available fields, in our case only \"ra\" and \"dec\" are available, but \n",
    "this is only because that is what was defined in the cell above\n",
    "- `metadata` takes a list (or array) of indexes, and a list (or array) of valid fields. It returns a numpy rec-array of the selected metadata fields for the selected data indexes. It is essentially\n",
    "equivalent to `metadata_table[indexes][fields].as_array()` where `metadata_table` is the original astropy table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyrax\n",
    "from astropy.table import Table\n",
    "\n",
    "h = hyrax.Hyrax()\n",
    "h.config[\"general\"][\"data_dir\"] = \"/fake/path/to/some/data\"\n",
    "h.config[\"data_set\"][\"name\"] = \"MyDataset\"\n",
    "\n",
    "dataset = h.prepare()\n",
    "\n",
    "print(\"\\nMetadata field list:\")\n",
    "print(f\"dataset.metadata_fields() = {dataset.metadata_fields()}\")\n",
    "print(f'Table(dataset.metadata([1, 3, 4], \"ra\")) =>')\n",
    "Table(dataset.metadata([1, 3, 4], [\"ra\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a Dataset capable of 'ra' and 'dec' metadata, we can do a full analysis with hyrax, `train`ing the model, `infer`ing the latent space,`umap`ping the latent space to a 2d representation, and `visualize`-ing the result.\n",
    "\n",
    "At time of writing `visualize` requres \"object_id\", \"ra\" and \"dec\" fields to be defined in order to work at all. Note the appearance of those same fields in the visualizer table to the immediate right of the \"x\" and \"y\" values for the 2d projected latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyrax\n",
    "\n",
    "h = hyrax.Hyrax()\n",
    "h.config[\"general\"][\"data_dir\"] = \"/fake/path/to/some/data\"\n",
    "h.config[\"data_set\"][\"name\"] = \"MyDataset\"\n",
    "h.config[\"model\"][\"name\"] = \"HyraxAutoencoder\"\n",
    "\n",
    "h.train()\n",
    "h.infer()\n",
    "h.umap()\n",
    "h.visualize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyrax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

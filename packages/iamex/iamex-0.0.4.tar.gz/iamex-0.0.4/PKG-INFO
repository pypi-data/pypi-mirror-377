Metadata-Version: 2.4
Name: iamex
Version: 0.0.4
Summary: Librer√≠a Python simple y poderosa para acceder a m√∫ltiples modelos de inteligencia artificial de forma unificada
Home-page: https://github.com/IA-Mexico/iamex
Author: Inteligencia Artificial M√©xico
Author-email: Inteligencia Artificial M√©xico <hostmaster@iamex.io>
Maintainer-email: Inteligencia Artificial M√©xico <hostmaster@iamex.io>
License: MIT
Project-URL: Homepage, https://github.com/IA-Mexico/iamex
Project-URL: Documentation, https://github.com/IA-Mexico/iamex#readme
Project-URL: Repository, https://github.com/IA-Mexico/iamex.git
Project-URL: Bug Tracker, https://github.com/IA-Mexico/iamex/issues
Project-URL: Source Code, https://github.com/IA-Mexico/iamex
Keywords: ai,machine learning,inference,models,iamex,artificial intelligence,chat,conversational ai,llm,nlp,python,api client
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.7
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.7
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: requests>=2.25.0
Provides-Extra: dev
Requires-Dist: pytest>=6.0; extra == "dev"
Requires-Dist: pytest-cov>=2.0; extra == "dev"
Requires-Dist: black>=21.0; extra == "dev"
Requires-Dist: flake8>=3.8; extra == "dev"
Requires-Dist: mypy>=0.800; extra == "dev"
Dynamic: author
Dynamic: home-page
Dynamic: license-file
Dynamic: requires-python

# iamex

Librer√≠a Python simple y poderosa para acceder a m√∫ltiples modelos de inteligencia artificial de forma unificada.

[![PyPI version](https://badge.fury.io/py/iamex.svg)](https://badge.fury.io/py/iamex)
[![Python 3.7+](https://img.shields.io/badge/python-3.7+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## üöÄ Instalaci√≥n

```bash
pip install iamex
```

## ü§ñ Modelos Disponibles

- **iam-adv-mex** - Modelo avanzado en espa√±ol
- **IAM-lite** - Modelo ligero en espa√±ol
- **IAM-advanced** - Modelo avanzado en espa√±ol (recomendado)
- **iam-lite-mex** - Modelo ligero en espa√±ol

## üìù Uso R√°pido

### Env√≠o Simple de Prompts

```python
from iamex import send_prompt

# Uso b√°sico - devuelve solo el contenido
response = send_prompt(
    prompt="¬øQu√© es Python?",
    api_key="tu_api_key_aqui",
    model="IAM-advanced"
)
print(response)
# Salida: "Python es un lenguaje de programaci√≥n..."
```

### Conversaciones con Mensajes (Nuevo en v0.0.4)

```python
from iamex import send_messages

# Formato de conversaci√≥n
messages = [
    {"role": "system", "content": "Eres un asistente experto en programaci√≥n"},
    {"role": "user", "content": "Explica qu√© es una funci√≥n lambda"}
]

response = send_messages(messages, "tu_api_key_aqui", "IAM-advanced")
print(response)
# Salida: "Una funci√≥n lambda es una funci√≥n an√≥nima..."
```

## üîß Funcionalidades Principales

### 1. Solo Contenido vs Respuesta Completa (Nuevo en v0.0.4)

Por defecto, `iamex` devuelve solo el texto de la respuesta. Si necesitas metadatos adicionales, usa `full_response=True`:

```python
# Solo contenido (default)
content = send_prompt("Hola", api_key, "IAM-advanced")
print(content)  # "¬°Hola! ¬øEn qu√© puedo ayudarte?"

# Respuesta completa con metadatos
full_response = send_prompt("Hola", api_key, "IAM-advanced", full_response=True)
print(full_response)  # {"choices": [...], "usage": {"tokens": 25}, ...}
```

### 2. Par√°metros de Control

```python
# Con l√≠mite de tokens
response = send_prompt(
    prompt="Explica la IA en detalle",
    api_key="tu_api_key",
    model="IAM-advanced",
    max_tokens=100  # Limitar respuesta
)

# Con mensaje del sistema
response = send_prompt(
    prompt="¬øC√≥mo funciona un bucle for?",
    api_key="tu_api_key",
    model="IAM-advanced",
    system_prompt="Eres un tutor de programaci√≥n para principiantes"
)
```

### 3. Conversaciones Avanzadas (Nuevo en v0.0.4)

```python
# Conversaci√≥n multi-turno
messages = [
    {"role": "system", "content": "Eres un asistente de cocina"},
    {"role": "user", "content": "¬øC√≥mo hago pasta?"},
    {"role": "assistant", "content": "Para hacer pasta necesitas..."},
    {"role": "user", "content": "¬øY qu√© salsa me recomiendas?"}
]

response = send_messages(messages, api_key, "IAM-advanced")
```

## üõ†Ô∏è Uso Avanzado

### Cliente Personalizado

```python
from iamex import PromptClient

# Inicializar el cliente con tu API key
client = PromptClient(api_key="tu_api_key_aqui")

# Enviar un prompt (usa modelo por defecto 'IAM-advanced')
response = client.send_prompt(
    prompt="Explica qu√© es la inteligencia artificial"
)
print(response)

# Con respuesta completa
full_response = client.send_prompt("¬øQu√© es la IA?", full_response=True)
print(full_response['usage']['total_tokens'])  # Ver tokens usados
```

### Par√°metros Avanzados

El m√©todo `send_prompt` y `PromptClient` aceptan par√°metros adicionales para control fino:

```python
response = client.send_prompt(
    prompt="Tu prompt aqu√≠",
    model="IAM-advanced",
    temperature=0.5,        # Controla la creatividad (0.0 - 1.0)
    max_tokens=1000,        # M√°ximo n√∫mero de tokens en la respuesta
    top_p=0.9,             # Controla la diversidad de la respuesta
    top_k=50,              # Limita las opciones de tokens
    repetition_penalty=1.1, # Evita repeticiones
    presence_penalty=0.1,   # Penaliza tokens ya presentes
    frequency_penalty=0.1,  # Penaliza tokens frecuentes
    stream=False            # Respuesta en streaming
)
```

### Par√°metros por Defecto

Si no especificas par√°metros, se usan estos valores optimizados:

- `model`: `"IAM-advanced"`
- `temperature`: `0.3`
- `max_tokens`: `12000`
- `top_p`: `0.9`
- `top_k`: `50`
- `repetition_penalty`: `1.1`
- `presence_penalty`: `0.1`
- `frequency_penalty`: `0.1`
- `stream`: `False`

## üìä Estructura de Datos

### Estructura del Payload

La funci√≥n `send_prompt` env√≠a autom√°ticamente este payload a la API:

```json
{
  "apikey": "tu_api_key_aqui",
  "model": "tu_modelo",
  "prompt": "tu_prompt"
}
```

Para `send_messages`:

```json
{
  "apikey": "tu_api_key_aqui",
  "model": "tu_modelo",
  "messages": [
    {"role": "system", "content": "..."},
    {"role": "user", "content": "..."}
  ]
}
```

**Par√°metros obligatorios:**
- `apikey`: Tu clave de API para autenticaci√≥n
- `model`: El modelo de IA a usar
- `prompt` o `messages`: Tu consulta o conversaci√≥n

**Par√°metros opcionales:**
- `system_prompt`: Instrucciones del sistema (solo con `send_prompt`)
- `max_tokens`: L√≠mite de tokens en la respuesta
- `temperature`, `top_p`, `top_k`: Par√°metros de generaci√≥n
- `full_response`: Control del tipo de respuesta (v0.0.4)

## üìö Funciones Disponibles

### `send_prompt()`
```python
send_prompt(
    prompt: str,           # Tu pregunta o instrucci√≥n
    api_key: str,          # Clave de API (obt√©n en dev.iamex.io)
    model: str,            # Modelo a usar
    full_response: bool = False,  # Respuesta completa o solo contenido
    max_tokens: int = None,       # L√≠mite de tokens (opcional)
    system_prompt: str = None,    # Mensaje del sistema (opcional)
    **kwargs                      # Par√°metros adicionales
)
```

### `send_messages()` (Nuevo en v0.0.4)
```python
send_messages(
    messages: list,        # Lista de mensajes de conversaci√≥n
    api_key: str,          # Clave de API
    model: str,            # Modelo a usar
    full_response: bool = False,  # Respuesta completa o solo contenido
    max_tokens: int = None,       # L√≠mite de tokens (opcional)
    **kwargs                      # Par√°metros adicionales
)
```

### `PromptClient`
```python
client = PromptClient(api_key="tu_api_key")
client.send_prompt(prompt, model="IAM-advanced", **kwargs)
client.send_messages(messages, model="IAM-advanced", **kwargs)
client.get_models()  # Obtener modelos disponibles
```

## üîê Obtener API Key

1. Visita [dev.iamex.io](https://dev.iamex.io)
2. Reg√≠strate o inicia sesi√≥n
3. Obt√©n tu API key
4. Adquiere tokens seg√∫n tus necesidades

## ‚ö†Ô∏è Manejo de Errores

```python
try:
    response = send_prompt("Test", "api_key_invalida", "IAM-advanced")
    print(response)
except Exception as e:
    print(f"Error: {e}")
    # Salida: Error 401: API Key inv√°lida o no encontrada
```

### Errores Comunes
- **401**: API Key inv√°lida o no encontrada
- **400**: Par√°metros incorrectos
- **500**: Error interno del servidor
- **502**: Servidor temporalmente no disponible

## üìö Ejemplos Completos

### Chat B√°sico
```python
from iamex import send_messages

def chat_simple():
    api_key = "tu_api_key_aqui"
    model = "IAM-advanced"
    
    messages = [
        {"role": "system", "content": "Eres un asistente √∫til y amigable"}
    ]
    
    while True:
        user_input = input("T√∫: ")
        if user_input.lower() == 'salir':
            break
            
        messages.append({"role": "user", "content": user_input})
        
        try:
            response = send_messages(messages, api_key, model)
            print(f"IA: {response}")
            messages.append({"role": "assistant", "content": response})
        except Exception as e:
            print(f"Error: {e}")

chat_simple()
```

### An√°lisis de Texto
```python
from iamex import send_prompt

def analizar_sentimiento(texto, api_key):
    prompt = f"Analiza el sentimiento del siguiente texto: '{texto}'"
    
    response = send_prompt(
        prompt=prompt,
        api_key=api_key,
        model="IAM-advanced",
        system_prompt="Eres un experto en an√°lisis de sentimientos. Responde solo: Positivo, Negativo o Neutral."
    )
    
    return response

# Uso
sentimiento = analizar_sentimiento("¬°Me encanta este producto!", "tu_api_key")
print(sentimiento)  # "Positivo"
```

### Uso con M√©tricas (v0.0.4)
```python
from iamex import send_prompt

# Para obtener informaci√≥n de uso
response = send_prompt(
    "Explica brevemente qu√© es Python",
    api_key="tu_api_key",
    model="IAM-advanced",
    full_response=True
)

# Extraer m√©tricas
if isinstance(response, dict):
    usage = response.get('data', {}).get('response', {}).get('usage', {})
    print(f"Tokens usados: {usage.get('total_tokens', 'N/A')}")
    print(f"Costo estimado: ${usage.get('total_tokens', 0) * 0.001:.4f}")
```

## üèóÔ∏è Desarrollo

### Instalaci√≥n para Desarrollo
```bash
git clone https://github.com/IA-Mexico/iamex.git
cd iamex
pip install -e ".[dev]"
```

### Ejecutar Tests
```bash
pytest tests/
```

### Formatear C√≥digo
```bash
black src/ tests/
```

## üìÑ Licencia

Este proyecto est√° bajo la Licencia MIT. Ver [LICENSE](LICENSE) para m√°s detalles.

## üÜò Soporte

- **Documentaci√≥n**: [GitHub](https://github.com/IA-Mexico/iamex)
- **Issues**: [GitHub Issues](https://github.com/IA-Mexico/iamex/issues)
- **Email**: hostmaster@iamex.io
- **Portal de desarrolladores**: [dev.iamex.io](https://dev.iamex.io)

## üìã Changelog

### v0.0.4 (Actual)
- ‚úÖ **NUEVO**: Par√°metro `full_response` para controlar el tipo de respuesta
- ‚úÖ **NUEVO**: Funci√≥n `send_messages` para conversaciones con formato de mensajes
- ‚úÖ **NUEVO**: Soporte para conversaciones multi-turno
- ‚úÖ **MEJORADO**: Extracci√≥n inteligente de contenido de respuestas
- ‚úÖ **MEJORADO**: Compatibilidad hacia atr√°s mantenida
- ‚úÖ **DOCUMENTACI√ìN**: Gu√≠as completas y ejemplos pr√°cticos

### v0.0.3
- ‚úÖ **NUEVO**: Par√°metro opcional `max_tokens` en funci√≥n `send_prompt`
- ‚úÖ **MEJORADO**: Control de longitud de respuestas del modelo
- ‚úÖ **MEJORADO**: Endpoint real de iam-hub implementado
- ‚úÖ **OPTIMIZADO**: Estructura de payload exacta para la API
- ‚úÖ **DOCUMENTACI√ìN**: Gu√≠as de uso para max_tokens

### v0.0.2
- ‚úÖ **NUEVO**: Funci√≥n simple `send_prompt(prompt, api_key, model)`
- ‚úÖ **NUEVO**: Soporte completo para autenticaci√≥n con API key
- ‚úÖ **NUEVO**: Conexi√≥n directa al endpoint real de iam-hub
- ‚úÖ **MEJORADO**: Estructura de payload exacta que espera la API

### v0.0.1
- Versi√≥n inicial con cliente b√°sico `PromptClient`
- Soporte para m√∫ltiples modelos de inferencia
- Modelo por defecto: `IAM-advanced`
- Par√°metros optimizados seg√∫n la API

---

**¬°Desarrollado con ‚ù§Ô∏è por Inteligencia Artificial M√©xico!**

# Пример конфигурации переменных окружения для Kraken LLM Framework
# Скопируйте этот файл в .env и настройте под ваши модели

# =============================================================================
# ОСНОВНЫЕ НАСТРОЙКИ (для обратной совместимости)
# =============================================================================

# Основная модель (если используете только одну)
LLM_ENDPOINT=http://localhost:8080
LLM_TOKEN=your_api_token_here
LLM_MODEL=chat
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=2000

# =============================================================================
# СПЕЦИАЛИЗИРОВАННЫЕ МОДЕЛИ
# =============================================================================

# Chat модель для диалогов и генерации текста
CHAT_ENDPOINT=http://localhost:8080
CHAT_TOKEN=your_chat_token
CHAT_MODEL=chat
# Дополнительные параметры для chat модели
CHAT_TEMPERATURE=0.7
CHAT_MAX_TOKENS=2000

# Completion модель для автодополнения (legacy /v1/completions)
COMPLETION_ENDPOINT=http://localhost:8081
COMPLETION_TOKEN=your_completion_token
COMPLETION_MODEL=completion
COMPLETION_TEMPERATURE=0.5
COMPLETION_MAX_TOKENS=1000

# Embedding модель для векторных представлений
EMBEDDING_ENDPOINT=http://localhost:8082
EMBEDDING_TOKEN=your_embedding_token
EMBEDDING_MODEL=embedding
# Размерность эмбеддингов (если поддерживается)
EMBEDDING_DIMENSIONS=1536

# Multimodal модель для работы с изображениями и текстом
MULTIMODAL_ENDPOINT=http://localhost:8083
MULTIMODAL_TOKEN=your_multimodal_token
MULTIMODAL_MODEL=multimodal
MULTIMODAL_TEMPERATURE=0.6

# ASR модель для работы с речью
ASR_ENDPOINT=http://localhost:8084
ASR_TOKEN=your_asr_token
ASR_MODEL=asr
# Настройки ASR
ASR_LANGUAGE=ru
ASR_QUALITY=high

# Reasoning модель с поддержкой thinking токенов
REASONING_ENDPOINT=http://localhost:8085
REASONING_TOKEN=your_reasoning_token
REASONING_MODEL=thinking
REASONING_TEMPERATURE=0.1
REASONING_MAX_THINKING_TOKENS=5000

# =============================================================================
# ГЛОБАЛЬНЫЕ НАСТРОЙКИ
# =============================================================================

# Режим API (openai_compatible, direct, custom)
LLM_API_MODE=openai_compatible
LLM_API_VERSION=v1

# Таймауты (в секундах)
LLM_CONNECT_TIMEOUT=30
LLM_READ_TIMEOUT=60
LLM_WRITE_TIMEOUT=30

# Повторы при ошибках
LLM_MAX_RETRIES=3
LLM_RETRY_DELAY=1

# Логирование
LLM_LOG_LEVEL=INFO

# =============================================================================
# НАСТРОЙКИ РЕЖИМОВ РАБОТЫ
# =============================================================================

# Structured Output
LLM_OUTLINES_SO_MODE=true
LLM_STRUCTURED_MODE=auto  # auto, outlines, native, json

# Streaming
LLM_STREAM=false
LLM_STREAM_BUFFER_SIZE=1024

# Adaptive режим
LLM_ADAPTIVE_MODE=true
LLM_CAPABILITY_DETECTION_TIMEOUT=10
LLM_PERFORMANCE_TRACKING=true

# =============================================================================
# НАСТРОЙКИ СПЕЦИФИЧНЫХ КЛИЕНТОВ
# =============================================================================

# ReasoningLLMClient
LLM_REASONING_TYPE=prompt_based  # prompt_based, native_thinking
LLM_ENABLE_COT=true
LLM_MAX_REASONING_STEPS=10
LLM_EXPOSE_THINKING=true

# MultimodalLLMClient
LLM_MAX_IMAGE_SIZE=20971520  # 20MB в байтах
LLM_MAX_AUDIO_DURATION=300   # 5 минут
LLM_MAX_VIDEO_DURATION=60    # 1 минута
LLM_AUTO_RESIZE_IMAGES=true

# ASRClient
LLM_ENABLE_TIMESTAMPS=true
LLM_ENABLE_SPEAKER_DIARIZATION=false
LLM_MAX_SPEAKERS=10
LLM_VAD_THRESHOLD=0.5

# =============================================================================
# ПРИМЕРЫ КОНФИГУРАЦИЙ ДЛЯ ПОПУЛЯРНЫХ ПРОВАЙДЕРОВ
# =============================================================================

# OpenAI API
# LLM_ENDPOINT=https://api.openai.com/v1
# LLM_TOKEN=sk-your_openai_api_key
# LLM_MODEL=gpt-4o

# Anthropic Claude
# LLM_ENDPOINT=https://api.anthropic.com/v1
# LLM_TOKEN=your_anthropic_api_key
# LLM_MODEL=claude-3-sonnet-20240229

# Локальный Ollama
# LLM_ENDPOINT=http://localhost:11434/v1
# LLM_TOKEN=dummy_token
# LLM_MODEL=llama3.1

# Локальный vLLM
# LLM_ENDPOINT=http://localhost:8000/v1
# LLM_TOKEN=dummy_token
# LLM_MODEL=meta-llama/Llama-3.1-8B-Instruct

# Локальный Text Generation WebUI
# LLM_ENDPOINT=http://localhost:5000/v1
# LLM_TOKEN=dummy_token
# LLM_MODEL=your_model_name

# =============================================================================
# НАСТРОЙКИ ДЛЯ РАЗРАБОТКИ И ТЕСТИРОВАНИЯ
# =============================================================================

# Включить отладочный режим
DEBUG=false

# Сохранять логи запросов
LOG_REQUESTS=false

# Директория для временных файлов
TEMP_DIR=/tmp/kraken_llm

# Максимальный размер лог файла
MAX_LOG_SIZE=10485760  # 10MB

# =============================================================================
# НАСТРОЙКИ БЕЗОПАСНОСТИ
# =============================================================================

# Проверка SSL сертификатов
LLM_SSL_VERIFY=true

# Максимальный размер ответа (в байтах)
LLM_MAX_RESPONSE_SIZE=10485760  # 10MB

# Таймаут для определения возможностей модели
LLM_CAPABILITY_TIMEOUT=30

# =============================================================================
# ДОПОЛНИТЕЛЬНЫЕ НАСТРОЙКИ
# =============================================================================

# Кастомные пути для API (если LLM_API_MODE=custom)
LLM_CHAT_COMPLETIONS_PATH=/v1/chat/completions
LLM_COMPLETIONS_PATH=/v1/completions
LLM_EMBEDDINGS_PATH=/v1/embeddings
LLM_MODELS_PATH=/v1/models

# Настройки для работы с прокси
# HTTP_PROXY=http://proxy.example.com:8080
# HTTPS_PROXY=https://proxy.example.com:8080
# NO_PROXY=localhost,127.0.0.1

# =============================================================================
# ПРИМЕЧАНИЯ
# =============================================================================

# 1. Все переменные с префиксом LLM_ автоматически загружаются в LLMConfig
# 2. Специализированные переменные (CHAT_, EMBEDDING_, etc.) используются 
#    утилитой определения возможностей моделей
# 3. Для локальных моделей часто можно использовать dummy_token в качестве токена
# 4. Убедитесь, что endpoint'ы доступны и модели запущены
# 5. Для production использования рекомендуется настроить все таймауты и лимиты
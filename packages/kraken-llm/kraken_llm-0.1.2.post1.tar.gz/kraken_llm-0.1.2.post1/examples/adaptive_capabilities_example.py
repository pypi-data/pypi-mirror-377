#!/usr/bin/env python3
"""
–†–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏.
"""

import asyncio
import os
import time
import json
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from dotenv import load_dotenv
from pydantic import BaseModel

from kraken_llm.client.adaptive import AdaptiveLLMClient, AdaptiveConfig, ModelCapability
from kraken_llm.client.factory import ClientFactory
from kraken_llm.config.settings import LLMConfig
from kraken_llm.client.standard import StandardLLMClient
from kraken_llm.client.streaming import StreamingLLMClient
from kraken_llm.client.structured import StructuredLLMClient
from kraken_llm.client.reasoning import ReasoningLLMClient
from kraken_llm.client.multimodal import MultimodalLLMClient
from kraken_llm.client.embeddings import EmbeddingsLLMClient
from kraken_llm.client.completion import CompletionLLMClient

load_dotenv()

@dataclass
class ModelConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    name: str
    endpoint: str
    token: str
    model: str
    description: str

@dataclass
class TestResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏"""
    capability: str
    client_type: str
    model_config: str
    success: bool
    response_time: float
    error: Optional[str] = None
    response_preview: Optional[str] = None

class TestModel(BaseModel):
    """–¢–µ—Å—Ç–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è structured output"""
    name: str
    value: int
    description: str

def get_model_configs() -> List[ModelConfig]:
    """–ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π –∏–∑ .env"""
    configs = []
    
    # Chat –º–æ–¥–µ–ª—å (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç /v1/chat/completions)
    if all([os.getenv("CHAT_ENDPOINT"), os.getenv("CHAT_TOKEN"), os.getenv("CHAT_MODEL")]):
        configs.append(ModelConfig(
            name="chat_model",
            endpoint=os.getenv("CHAT_ENDPOINT"),
            token=os.getenv("CHAT_TOKEN"),
            model=os.getenv("CHAT_MODEL"),
            description="Chat –º–æ–¥–µ–ª—å (/v1/chat/completions)"
        ))
    
    # Completion –º–æ–¥–µ–ª—å (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç /v1/completions)
    if all([os.getenv("COMPLETION_ENDPOINT"), os.getenv("COMPLETION_TOKEN"), os.getenv("COMPLETION_MODEL")]):
        configs.append(ModelConfig(
            name="completion_model",
            endpoint=os.getenv("COMPLETION_ENDPOINT"),
            token=os.getenv("COMPLETION_TOKEN"),
            model=os.getenv("COMPLETION_MODEL"),
            description="Completion –º–æ–¥–µ–ª—å (/v1/completions)"
        ))
    
    # –ú–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    if all([os.getenv("EMBEDDING_ENDPOINT"), os.getenv("EMBEDDING_TOKEN"), os.getenv("EMBEDDING_MODEL")]):
        configs.append(ModelConfig(
            name="embedding_model",
            endpoint=os.getenv("EMBEDDING_ENDPOINT"),
            token=os.getenv("EMBEDDING_TOKEN"),
            model=os.getenv("EMBEDDING_MODEL"),
            description="–ú–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"
        ))
    
    # –î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ - –æ—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å
    if not configs and all([os.getenv("LLM_ENDPOINT"), os.getenv("LLM_TOKEN"), os.getenv("LLM_MODEL")]):
        configs.append(ModelConfig(
            name="default_model",
            endpoint=os.getenv("LLM_ENDPOINT"),
            token=os.getenv("LLM_TOKEN"),
            model=os.getenv("LLM_MODEL"),
            description="–û—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å (–æ–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å)"
        ))
    
    return configs

async def test_basic_chat_completion(client, model_name: str) -> TestResult:
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –±–∞–∑–æ–≤–æ–≥–æ chat completion"""
    start_time = time.time()
    
    messages = [
        {"role": "user", "content": "–ü—Ä–∏–≤–µ—Ç! –û—Ç–≤–µ—Ç—å –æ–¥–Ω–∏–º —Å–ª–æ–≤–æ–º: '–†–∞–±–æ—Ç–∞–µ—Ç'"}
    ]
    
    try:
        response = await client.chat_completion(messages, max_tokens=10)
        response_time = time.time() - start_time
        
        return TestResult(
            capability="chat_completion",
            client_type=type(client).__name__,
            model_config=model_name,
            success=True,
            response_time=response_time,
            response_preview=str(response)[:100] if response else None
        )
    except Exception as e:
        return TestResult(
            capability="chat_completion",
            client_type=type(client).__name__,
            model_config=model_name,
            success=False,
            response_time=time.time() - start_time,
            error=str(e)
        )

async def test_streaming_completion(client, model_name: str) -> TestResult:
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ streaming completion"""
    start_time = time.time()
    
    messages = [
        {"role": "user", "content": "–°—á–∏—Ç–∞–π –æ—Ç 1 –¥–æ 5"}
    ]
    
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ª–∏ –∫–ª–∏–µ–Ω—Ç streaming
        if not hasattr(client, 'chat_completion_stream'):
            return TestResult(
                capability="streaming",
                client_type=type(client).__name__,
                model_config=model_name,
                success=False,
                response_time=time.time() - start_time,
                error=f"{type(client).__name__} –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç streaming"
            )
        
        response_stream = await client.chat_completion_stream(messages, max_tokens=50)
        
        # –°–æ–±–∏—Ä–∞–µ–º streaming –æ—Ç–≤–µ—Ç
        full_response = ""
        chunk_count = 0
        
        async for chunk in response_stream:
            chunk_count += 1
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –æ—Ç–≤–µ—Ç–æ–≤
            if hasattr(chunk, 'choices') and chunk.choices and len(chunk.choices) > 0:
                choice = chunk.choices[0]
                
                # OpenAI —Ñ–æ—Ä–º–∞—Ç
                if hasattr(choice, 'delta') and hasattr(choice.delta, 'content'):
                    content = choice.delta.content
                    if content:
                        full_response += content
                        
                # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
                elif hasattr(choice, 'text'):
                    full_response += choice.text
                    
            # –ü—Ä–æ—Å—Ç–æ–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç
            elif isinstance(chunk, str):
                full_response += chunk
                
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            if chunk_count > 50:
                break
        
        response_time = time.time() - start_time
        
        return TestResult(
            capability="streaming",
            client_type=type(client).__name__,
            model_config=model_name,
            success=True,
            response_time=response_time,
            response_preview=f"Streaming: {chunk_count} —á–∞–Ω–∫–æ–≤, {len(full_response)} —Å–∏–º–≤–æ–ª–æ–≤: {full_response[:50]}..."
        )
        
    except Exception as e:
        return TestResult(
            capability="streaming",
            client_type=type(client).__name__,
            model_config=model_name,
            success=False,
            response_time=time.time() - start_time,
            error=str(e)
        )

async def test_structured_output(client, model_name: str) -> TestResult:
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ structured output"""
    start_time = time.time()
    
    messages = [
        {"role": "user", "content": "–í–µ—Ä–Ω–∏ JSON —Å name='test', value=42, description='—Ç–µ—Å—Ç–æ–≤—ã–π –æ–±—ä–µ–∫—Ç'"}
    ]
    
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ª–∏ –∫–ª–∏–µ–Ω—Ç structured output
        if hasattr(client, 'chat_completion_structured'):
            # –î–ª—è StructuredLLMClient –∏ CompletionLLMClient
            response = await client.chat_completion_structured(
                messages, 
                response_model=TestModel,
                max_tokens=100
            )
            response_time = time.time() - start_time
            
            return TestResult(
                capability="structured_output",
                client_type=type(client).__name__,
                model_config=model_name,
                success=True,
                response_time=response_time,
                response_preview=str(response)[:100] if response else None
            )
        else:
            # –î–ª—è –∫–ª–∏–µ–Ω—Ç–æ–≤ –±–µ–∑ structured support - –Ω–µ —Ç–µ—Å—Ç–∏—Ä—É–µ–º
            return TestResult(
                capability="structured_output",
                client_type=type(client).__name__,
                model_config=model_name,
                success=False,
                response_time=time.time() - start_time,
                error=f"{type(client).__name__} –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç structured output. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ StructuredLLMClient –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤."
            )
        
    except Exception as e:
        return TestResult(
            capability="structured_output",
            client_type=type(client).__name__,
            model_config=model_name,
            success=False,
            response_time=time.time() - start_time,
            error=str(e)
        )

async def test_function_calling(client, model_name: str) -> TestResult:
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ function calling"""
    start_time = time.time()
    
    # –ü—Ä–æ—Å—Ç–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    def get_weather(city: str) -> str:
        return f"–ü–æ–≥–æ–¥–∞ –≤ {city}: —Å–æ–ª–Ω–µ—á–Ω–æ, +20¬∞C"
    
    messages = [
        {"role": "user", "content": "–ö–∞–∫–∞—è –ø–æ–≥–æ–¥–∞ –≤ –ú–æ—Å–∫–≤–µ?"}
    ]
    
    functions = [{
        "name": "get_weather",
        "description": "–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ–≥–æ–¥–µ –≤ –≥–æ—Ä–æ–¥–µ",
        "parameters": {
            "type": "object",
            "properties": {
                "city": {
                    "type": "string",
                    "description": "–ù–∞–∑–≤–∞–Ω–∏–µ –≥–æ—Ä–æ–¥–∞"
                }
            },
            "required": ["city"]
        }
    }]
    
    try:
        if hasattr(client, 'register_function'):
            client.register_function("get_weather", get_weather, "–ü–æ–ª—É—á–∏—Ç—å –ø–æ–≥–æ–¥—É")
        
        response = await client.chat_completion(
            messages, 
            functions=functions,
            max_tokens=100
        )
        response_time = time.time() - start_time
        
        return TestResult(
            capability="function_calling",
            client_type=type(client).__name__,
            model_config=model_name,
            success=True,
            response_time=response_time,
            response_preview=str(response)[:100] if response else None
        )
    except Exception as e:
        return TestResult(
            capability="function_calling",
            client_type=type(client).__name__,
            model_config=model_name,
            success=False,
            response_time=time.time() - start_time,
            error=str(e)
        )

async def test_embeddings(client, model_name: str) -> TestResult:
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ embeddings"""
    start_time = time.time()
    
    texts = ["–ü—Ä–∏–≤–µ—Ç –º–∏—Ä", "Hello world", "–¢–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç"]
    
    try:
        if hasattr(client, 'get_embeddings'):
            response = await client.get_embeddings(texts)
        elif hasattr(client, 'create_embeddings'):
            response = await client.create_embeddings(texts)
        else:
            raise Exception("Embeddings –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è —ç—Ç–∏–º –∫–ª–∏–µ–Ω—Ç–æ–º")
        
        response_time = time.time() - start_time
        
        return TestResult(
            capability="embeddings",
            client_type=type(client).__name__,
            model_config=model_name,
            success=True,
            response_time=response_time,
            response_preview=f"Embeddings –¥–ª—è {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤ –ø–æ–ª—É—á–µ–Ω—ã"
        )
    except Exception as e:
        return TestResult(
            capability="embeddings",
            client_type=type(client).__name__,
            model_config=model_name,
            success=False,
            response_time=time.time() - start_time,
            error=str(e)
        )

async def test_reasoning(client, model_name: str) -> TestResult:
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ reasoning capabilities"""
    start_time = time.time()
    
    messages = [
        {"role": "user", "content": "–†–µ—à–∏ –∑–∞–¥–∞—á—É: –µ—Å–ª–∏ —É –º–µ–Ω—è –µ—Å—Ç—å 10 —è–±–ª–æ–∫ –∏ —è —Å—ä–µ–ª 3, —Å–∫–æ–ª—å–∫–æ –æ—Å—Ç–∞–ª–æ—Å—å?"}
    ]
    
    try:
        if hasattr(client, 'reasoning_completion'):
            response = await client.reasoning_completion(
                messages, 
                problem_type="math",
                max_tokens=100
            )
        else:
            response = await client.chat_completion(messages, max_tokens=100)
        
        response_time = time.time() - start_time
        
        return TestResult(
            capability="reasoning",
            client_type=type(client).__name__,
            model_config=model_name,
            success=True,
            response_time=response_time,
            response_preview=str(response)[:100] if response else None
        )
    except Exception as e:
        return TestResult(
            capability="reasoning",
            client_type=type(client).__name__,
            model_config=model_name,
            success=False,
            response_time=time.time() - start_time,
            error=str(e)
        )

async def test_adaptive_capabilities(model_config: ModelConfig) -> List[TestResult]:
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ AdaptiveLLMClient –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    print(f"\n=== –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ {model_config.name} ({model_config.description}) ===")
    
    config = LLMConfig(
        endpoint=model_config.endpoint,
        api_key=model_config.token,
        model=model_config.model,
        temperature=0.7
    )
    
    adaptive_config = AdaptiveConfig(
        capability_detection_timeout=5.0,
        enable_performance_tracking=True
    )
    
    results = []
    
    try:
        async with AdaptiveLLMClient(config, adaptive_config) as client:
            print("1. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–∏:")
            
            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –æ–±–Ω–æ–≤–ª—è–µ–º –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏
            capabilities = await client.get_model_capabilities(force_refresh=True)
            print(f"   –û–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏: {[cap.value for cap in capabilities]}")
            
            # –ü–æ–ª—É—á–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
            capabilities_dict = await client.detect_model_capabilities()
            
            print("2. –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è—Ö:")
            for capability, supported in capabilities_dict.items():
                status = "‚úì" if supported else "‚úó"
                print(f"   {status} {capability}: {supported}")
            
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º –±–∞–∑–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏
            print("3. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π:")
            
            # Chat completion
            result = await test_basic_chat_completion(client, model_config.name)
            results.append(result)
            print(f"   Chat completion: {'‚úì' if result.success else '‚úó'} ({result.response_time:.2f}s)")
            
            # Smart completion
            messages = [{"role": "user", "content": "–ü—Ä–∏–≤–µ—Ç! –†–∞—Å—Å–∫–∞–∂–∏ –∫–æ—Ä–æ—Ç–∫—É—é —à—É—Ç–∫—É."}]
            try:
                response = await client.smart_completion(messages, max_tokens=100)
                print(f"   Smart completion: ‚úì ({len(str(response))} —Å–∏–º–≤–æ–ª–æ–≤)")
            except Exception as e:
                print(f"   Smart completion: ‚úó ({str(e)[:50]}...)")
            
            # –ü–æ–ª—É—á–∞–µ–º –æ—Ç—á–µ—Ç –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            performance_report = client.get_performance_report()
            
            if performance_report["model_info"]:
                model_info = performance_report["model_info"]
                print(f"   –ú–æ–¥–µ–ª—å: {model_info['name']}")
                print(f"   –ü—Ä–æ–≤–∞–π–¥–µ—Ä: {model_info['provider']}")
                print(f"   –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏: {len(model_info['capabilities'])}")
    
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è AdaptiveLLMClient: {e}")
        results.append(TestResult(
            capability="adaptive_client",
            client_type="AdaptiveLLMClient",
            model_config=model_config.name,
            success=False,
            response_time=0.0,
            error=str(e)
        ))
    
    return results


def get_suitable_tests_for_client(client_name: str, model_name: str) -> List[tuple]:
    """–ü–æ–ª—É—á–∏—Ç—å –ø–æ–¥—Ö–æ–¥—è—â–∏–µ —Ç–µ—Å—Ç—ã –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ç–∏–ø–∞ –∫–ª–∏–µ–Ω—Ç–∞ –∏ –º–æ–¥–µ–ª–∏"""
    
    # –ë–∞–∑–æ–≤—ã–µ —Ç–µ—Å—Ç—ã –¥–ª—è chat/completion –∫–ª–∏–µ–Ω—Ç–æ–≤
    base_tests = [
        ("chat_completion", test_basic_chat_completion),
        ("streaming", test_streaming_completion),
        ("function_calling", test_function_calling),
        ("reasoning", test_reasoning),
    ]
    
    # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Ç–µ—Å—Ç—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤
    if client_name == "structured":
        # StructuredLLMClient - —Ç–æ–ª—å–∫–æ structured output
        return [("structured_output", test_structured_output)]
    
    elif client_name == "embeddings":
        # –î–ª—è embeddings –∫–ª–∏–µ–Ω—Ç–∞ —Ç–æ–ª—å–∫–æ —Ç–µ—Å—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        if "embedding" in model_name.lower():
            return [("embeddings", test_embeddings)]
        else:
            return []  # –ù–µ —Ç–µ—Å—Ç–∏—Ä—É–µ–º embeddings –∫–ª–∏–µ–Ω—Ç –Ω–∞ chat/completion –º–æ–¥–µ–ª—è—Ö
    
    elif client_name == "completion":
        # Completion –∫–ª–∏–µ–Ω—Ç –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –±–∞–∑–æ–≤—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ + structured —á–µ—Ä–µ–∑ –ø—Ä–æ–º–ø—Ç –∏–Ω–∂–∏–Ω–∏—Ä–∏–Ω–≥
        return [
            ("chat_completion", test_basic_chat_completion),
            ("streaming", test_streaming_completion),
            ("structured_output", test_structured_output),  # –ß–µ—Ä–µ–∑ –ø—Ä–æ–º–ø—Ç –∏–Ω–∂–∏–Ω–∏—Ä–∏–Ω–≥
        ]
    
    elif client_name in ["standard", "streaming", "multimodal"]:
        # –≠—Ç–∏ –∫–ª–∏–µ–Ω—Ç—ã –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç structured output –Ω–∞—Ç–∏–≤–Ω–æ
        tests = base_tests.copy()
            
        # –î–æ–±–∞–≤–ª—è–µ–º embeddings —Ç–æ–ª—å–∫–æ –¥–ª—è embedding –º–æ–¥–µ–ª–µ–π
        if "embedding" in model_name.lower():
            tests.append(("embeddings", test_embeddings))
            
        return tests
    
    elif client_name == "reasoning":
        # ReasoningLLMClient –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç structured output
        tests = base_tests.copy()
        
        # –î–æ–±–∞–≤–ª—è–µ–º embeddings —Ç–æ–ª—å–∫–æ –¥–ª—è embedding –º–æ–¥–µ–ª–µ–π
        if "embedding" in model_name.lower():
            tests.append(("embeddings", test_embeddings))
            
        return tests
    
    return base_tests

async def test_all_client_types(model_config: ModelConfig) -> List[TestResult]:
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö —Ç–∏–ø–æ–≤ –∫–ª–∏–µ–Ω—Ç–æ–≤ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    print(f"\n=== –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤ –¥–ª—è {model_config.name} ===")
    
    config = LLMConfig(
        endpoint=model_config.endpoint,
        api_key=model_config.token,
        model=model_config.model,
        temperature=0.7
    )
    
    all_results = []
    
    # –°–ø–∏—Å–æ–∫ –∫–ª–∏–µ–Ω—Ç–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    client_types = [
        ("standard", StandardLLMClient),
        ("streaming", StreamingLLMClient),
        ("structured", StructuredLLMClient),
        ("reasoning", ReasoningLLMClient),
        ("multimodal", MultimodalLLMClient),
        ("embeddings", EmbeddingsLLMClient),
        ("completion", CompletionLLMClient),
    ]
    
    for client_name, client_class in client_types:
        print(f"\n--- –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ {client_name} –∫–ª–∏–µ–Ω—Ç–∞ ---")
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–æ–¥—Ö–æ–¥—è—â–∏–µ —Ç–µ—Å—Ç—ã –¥–ª—è —ç—Ç–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞
        suitable_tests = get_suitable_tests_for_client(client_name, model_config.name)
        
        if not suitable_tests:
            print(f"   ‚ö† –ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö —Ç–µ—Å—Ç–æ–≤ –¥–ª—è {client_name} –∫–ª–∏–µ–Ω—Ç–∞ —Å –º–æ–¥–µ–ª—å—é {model_config.name}")
            continue
        
        try:
            async with client_class(config) as client:
                
                for test_name, test_func in suitable_tests:
                    try:
                        result = await test_func(client, model_config.name)
                        all_results.append(result)
                        
                        status = "‚úì" if result.success else "‚úó"
                        time_str = f"{result.response_time:.2f}s"
                        
                        if result.success:
                            preview = result.response_preview[:30] + "..." if result.response_preview and len(result.response_preview) > 30 else result.response_preview
                            print(f"   {status} {test_name}: {time_str} - {preview}")
                        else:
                            error_preview = result.error[:50] + "..." if result.error and len(result.error) > 50 else result.error
                            print(f"   {status} {test_name}: {time_str} - {error_preview}")
                            
                    except Exception as e:
                        print(f"   ‚úó {test_name}: –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∞ - {str(e)[:50]}...")
                        all_results.append(TestResult(
                            capability=test_name,
                            client_type=client_name,
                            model_config=model_config.name,
                            success=False,
                            response_time=0.0,
                            error=str(e)
                        ))
                        
        except Exception as e:
            print(f"   ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å {client_name} –∫–ª–∏–µ–Ω—Ç: {str(e)[:100]}...")
            all_results.append(TestResult(
                capability="client_creation",
                client_type=client_name,
                model_config=model_config.name,
                success=False,
                response_time=0.0,
                error=str(e)
            ))
    
    return all_results

async def test_client_factory(model_config: ModelConfig) -> List[TestResult]:
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ ClientFactory"""
    print(f"\n=== –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ ClientFactory –¥–ª—è {model_config.name} ===")
    
    config = LLMConfig(
        endpoint=model_config.endpoint,
        api_key=model_config.token,
        model=model_config.model,
        temperature=0.7
    )
    
    results = []
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å–æ–∑–¥–∞–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –∫–ª–∏–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ —Ñ–∞–±—Ä–∏–∫—É
    factory_methods = [
        ("adaptive", ClientFactory.create_adaptive_client),
        ("standard", ClientFactory.create_standard_client),
        ("streaming", ClientFactory.create_streaming_client),
        ("structured", ClientFactory.create_structured_client),
        ("reasoning", ClientFactory.create_reasoning_client),
        ("multimodal", ClientFactory.create_multimodal_client),
        ("embeddings", ClientFactory.create_embeddings_client),
    ]
    
    for method_name, factory_method in factory_methods:
        start_time = time.time()
        
        try:
            client = factory_method(config)
            
            # –ü—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç - –±–∞–∑–æ–≤—ã–π chat completion
            messages = [{"role": "user", "content": "–¢–µ—Å—Ç"}]
            
            if hasattr(client, 'chat_completion'):
                response = await client.chat_completion(messages, max_tokens=10)
                
                results.append(TestResult(
                    capability="factory_creation",
                    client_type=f"Factory_{method_name}",
                    model_config=model_config.name,
                    success=True,
                    response_time=time.time() - start_time,
                    response_preview="–ö–ª–∏–µ–Ω—Ç —Å–æ–∑–¥–∞–Ω –∏ —Ä–∞–±–æ—Ç–∞–µ—Ç"
                ))
                print(f"   ‚úì {method_name}: –°–æ–∑–¥–∞–Ω –∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω")
            else:
                results.append(TestResult(
                    capability="factory_creation",
                    client_type=f"Factory_{method_name}",
                    model_config=model_config.name,
                    success=False,
                    response_time=time.time() - start_time,
                    error="–ù–µ—Ç –º–µ—Ç–æ–¥–∞ chat_completion"
                ))
                print(f"   ‚úó {method_name}: –ù–µ—Ç –º–µ—Ç–æ–¥–∞ chat_completion")
                
            await client.close()
            
        except Exception as e:
            results.append(TestResult(
                capability="factory_creation",
                client_type=f"Factory_{method_name}",
                model_config=model_config.name,
                success=False,
                response_time=time.time() - start_time,
                error=str(e)
            ))
            print(f"   ‚úó {method_name}: {str(e)[:50]}...")
    
    return results

def print_summary_report(all_results: List[TestResult]):
    """–ü–µ—á–∞—Ç—å –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
    print("\n" + "="*80)
    print("üìä –ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø")
    print("="*80)
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –º–æ–¥–µ–ª—è–º
    by_model = {}
    for result in all_results:
        if result.model_config not in by_model:
            by_model[result.model_config] = []
        by_model[result.model_config].append(result)
    
    for model_name, model_results in by_model.items():
        print(f"\nüîß –ú–æ–¥–µ–ª—å: {model_name}")
        print("-" * 50)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º
        capabilities_stats = {}
        for result in model_results:
            if result.capability not in capabilities_stats:
                capabilities_stats[result.capability] = {"success": 0, "total": 0}
            capabilities_stats[result.capability]["total"] += 1
            if result.success:
                capabilities_stats[result.capability]["success"] += 1
        
        print("–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:")
        for capability, stats in capabilities_stats.items():
            success_rate = stats["success"] / stats["total"] * 100
            status = "‚úì" if success_rate > 50 else "‚ö†" if success_rate > 0 else "‚úó"
            print(f"  {status} {capability}: {stats['success']}/{stats['total']} ({success_rate:.0f}%)")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–ª–∏–µ–Ω—Ç–∞–º
        client_stats = {}
        for result in model_results:
            if result.client_type not in client_stats:
                client_stats[result.client_type] = {"success": 0, "total": 0}
            client_stats[result.client_type]["total"] += 1
            if result.success:
                client_stats[result.client_type]["success"] += 1
        
        print("\n–ö–ª–∏–µ–Ω—Ç—ã:")
        for client_type, stats in client_stats.items():
            success_rate = stats["success"] / stats["total"] * 100
            status = "‚úì" if success_rate > 50 else "‚ö†" if success_rate > 0 else "‚úó"
            print(f"  {status} {client_type}: {stats['success']}/{stats['total']} ({success_rate:.0f}%)")
    
    # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    total_tests = len(all_results)
    successful_tests = sum(1 for r in all_results if r.success)
    success_rate = successful_tests / total_tests * 100 if total_tests > 0 else 0
    
    print(f"\nüìà –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    print(f"   –í—Å–µ–≥–æ —Ç–µ—Å—Ç–æ–≤: {total_tests}")
    print(f"   –£—Å–ø–µ—à–Ω—ã—Ö: {successful_tests}")
    print(f"   –ü—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞: {success_rate:.1f}%")
    
    # –¢–æ–ø –æ—à–∏–±–æ–∫
    errors = [r.error for r in all_results if r.error]
    if errors:
        print(f"\n‚ùå –ß–∞—Å—Ç—ã–µ –æ—à–∏–±–∫–∏:")
        error_counts = {}
        for error in errors:
            short_error = error[:100] + "..." if len(error) > 100 else error
            error_counts[short_error] = error_counts.get(short_error, 0) + 1
        
        for error, count in sorted(error_counts.items(), key=lambda x: x[1], reverse=True)[:5]:
            print(f"   {count}x: {error}")

def generate_recommendations(all_results: List[TestResult]) -> Dict[str, List[str]]:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    recommendations = {
        "chat_model": [],
        "embedding_model": [],
        "general": []
    }
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –º–æ–¥–µ–ª—è–º
    by_model = {}
    for result in all_results:
        if result.model_config not in by_model:
            by_model[result.model_config] = []
        by_model[result.model_config].append(result)
    
    for model_name, model_results in by_model.items():
        model_recs = []
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —É—Å–ø–µ—à–Ω—ã–µ –∫–ª–∏–µ–Ω—Ç—ã
        successful_clients = set()
        failed_clients = set()
        
        for result in model_results:
            if result.success:
                successful_clients.add(result.client_type)
            else:
                failed_clients.add(result.client_type)
        
        if model_name == "chat_model":
            if "StandardLLMClient" in successful_clients:
                model_recs.append("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ StandardLLMClient –¥–ª—è –±–∞–∑–æ–≤—ã—Ö chat completion –∑–∞–¥–∞—á")
            
            if "StructuredLLMClient" in successful_clients:
                model_recs.append("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ StructuredLLMClient –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤")
            
            if "ReasoningLLMClient" in successful_clients:
                model_recs.append("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ ReasoningLLMClient –¥–ª—è –∑–∞–¥–∞—á, —Ç—Ä–µ–±—É—é—â–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π")
            
            if "streaming" in [r.capability for r in model_results if not r.success]:
                model_recs.append("‚ö†Ô∏è Streaming –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ - –ø—Ä–æ–≤–µ—Ä—å—Ç–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é")
            
            if "function_calling" in [r.capability for r in model_results if r.success]:
                model_recs.append("‚úÖ Function calling –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è - –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã")
        
        elif model_name == "embedding_model":
            if "EmbeddingsLLMClient" in successful_clients:
                model_recs.append("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ EmbeddingsLLMClient –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏")
            
            if any(r.capability == "chat_completion" and not r.success for r in model_results):
                model_recs.append("‚ÑπÔ∏è –≠—Ç–∞ –º–æ–¥–µ–ª—å –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–∞ —Ç–æ–ª—å–∫–æ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤, –Ω–µ –¥–ª—è —á–∞—Ç–∞")
        
        recommendations[model_name] = model_recs
    
    # –û–±—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    general_recs = []
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —É—Å–ø–µ—Ö–∞
    total_tests = len(all_results)
    successful_tests = sum(1 for r in all_results if r.success)
    success_rate = successful_tests / total_tests * 100 if total_tests > 0 else 0
    
    if success_rate > 70:
        general_recs.append("üéâ –û—Ç–ª–∏—á–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å! –ë–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ —Ñ—É–Ω–∫—Ü–∏–π —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
    elif success_rate > 40:
        general_recs.append("‚ö†Ô∏è –£–º–µ—Ä–µ–Ω–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ –∫–ª–∏–µ–Ω—Ç—ã")
    else:
        general_recs.append("‚ùå –ù–∏–∑–∫–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —Å–µ—Ä–≤–∏—Å–æ–≤")
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —á–∞—Å—Ç—ã–µ –æ—à–∏–±–∫–∏
    errors = [r.error for r in all_results if r.error]
    if "HTTP 404" in str(errors):
        general_recs.append("üîß –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –æ—à–∏–±–∫–∏ 404 - –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å endpoint'–æ–≤")
    
    if "response_model –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω" in str(errors):
        general_recs.append("üìù –î–ª—è StructuredLLMClient –≤—Å–µ–≥–¥–∞ —É–∫–∞–∑—ã–≤–∞–π—Ç–µ response_model")
    
    if "–Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç" in str(errors):
        general_recs.append("üéØ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–ª–∏–µ–Ω—Ç—ã –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∑–∞–¥–∞—á")
    
    recommendations["general"] = general_recs
    
    return recommendations

def generate_capabilities_summary(all_results: List[TestResult]) -> Dict[str, Dict[str, List[str]]]:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–≤–æ–¥–∫–∏ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏"""
    summary = {}
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –º–æ–¥–µ–ª—è–º
    by_model = {}
    for result in all_results:
        if result.model_config not in by_model:
            by_model[result.model_config] = []
        by_model[result.model_config].append(result)
    
    for model_name, model_results in by_model.items():
        model_summary = {
            "confirmed_capabilities": [],
            "working_clients": [],
            "failed_capabilities": [],
            "statistics": {}
        }
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏
        capabilities_status = {}
        clients_status = {}
        
        for result in model_results:
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º
            if result.capability not in capabilities_status:
                capabilities_status[result.capability] = {"success": 0, "total": 0}
            capabilities_status[result.capability]["total"] += 1
            if result.success:
                capabilities_status[result.capability]["success"] += 1
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–ª–∏–µ–Ω—Ç–∞–º
            if result.client_type not in clients_status:
                clients_status[result.client_type] = {"success": 0, "total": 0}
            clients_status[result.client_type]["total"] += 1
            if result.success:
                clients_status[result.client_type]["success"] += 1
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ (—É—Å–ø–µ—à–Ω–æ—Å—Ç—å > 50%)
        for capability, stats in capabilities_status.items():
            success_rate = stats["success"] / stats["total"] * 100
            if success_rate > 50:
                model_summary["confirmed_capabilities"].append(
                    f"{capability} ({stats['success']}/{stats['total']}, {success_rate:.0f}%)"
                )
            else:
                model_summary["failed_capabilities"].append(
                    f"{capability} ({stats['success']}/{stats['total']}, {success_rate:.0f}%)"
                )
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–±–æ—Ç–∞—é—â–∏–µ –∫–ª–∏–µ–Ω—Ç—ã
        for client_type, stats in clients_status.items():
            success_rate = stats["success"] / stats["total"] * 100
            if success_rate > 50:
                model_summary["working_clients"].append(
                    f"{client_type} ({stats['success']}/{stats['total']}, {success_rate:.0f}%)"
                )
        
        # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_tests = len(model_results)
        successful_tests = sum(1 for r in model_results if r.success)
        model_summary["statistics"] = {
            "total_tests": total_tests,
            "successful_tests": successful_tests,
            "success_rate": successful_tests / total_tests * 100 if total_tests > 0 else 0
        }
        
        summary[model_name] = model_summary
    
    return summary

def print_capabilities_summary(summary: Dict[str, Dict[str, List[str]]]):
    """–ü–µ—á–∞—Ç—å —Å–≤–æ–¥–∫–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–µ–π"""
    print("\n" + "="*80)
    print("üìã –°–í–û–î–ù–´–ô –ò–¢–û–ì: –ü–û–î–¢–í–ï–†–ñ–î–ï–ù–ù–´–ï –í–û–ó–ú–û–ñ–ù–û–°–¢–ò –ú–û–î–ï–õ–ï–ô")
    print("="*80)
    
    for model_name, model_data in summary.items():
        print(f"\nüéØ –ú–û–î–ï–õ–¨: {model_name.upper()}")
        print("-" * 60)
        
        stats = model_data["statistics"]
        print(f"üìä –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {stats['successful_tests']}/{stats['total_tests']} —Ç–µ—Å—Ç–æ–≤ —É—Å–ø–µ—à–Ω–æ ({stats['success_rate']:.1f}%)")
        
        if model_data["confirmed_capabilities"]:
            print(f"\n‚úÖ –ü–û–î–¢–í–ï–†–ñ–î–ï–ù–ù–´–ï –í–û–ó–ú–û–ñ–ù–û–°–¢–ò:")
            for capability in model_data["confirmed_capabilities"]:
                print(f"   ‚Ä¢ {capability}")
        else:
            print(f"\n‚ùå –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        
        if model_data["working_clients"]:
            print(f"\nüîß –†–ê–ë–û–ß–ò–ï –ö–õ–ò–ï–ù–¢–´:")
            for client in model_data["working_clients"]:
                print(f"   ‚Ä¢ {client}")
        
        if model_data["failed_capabilities"]:
            print(f"\n‚ö†Ô∏è –ü–†–û–ë–õ–ï–ú–ù–´–ï –í–û–ó–ú–û–ñ–ù–û–°–¢–ò:")
            for capability in model_data["failed_capabilities"]:
                print(f"   ‚Ä¢ {capability}")

def print_recommendations(recommendations: Dict[str, List[str]]):
    """–ü–µ—á–∞—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"""
    print("\n" + "="*80)
    print("üí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Æ")
    print("="*80)
    
    for model_name, recs in recommendations.items():
        if recs and model_name != "general":
            print(f"\nüîß {model_name.upper()}:")
            for rec in recs:
                print(f"   {rec}")
    
    if recommendations.get("general"):
        print(f"\nüåü –û–ë–©–ò–ï –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
        for rec in recommendations["general"]:
            print(f"   {rec}")

def save_results_to_json(all_results: List[TestResult], filename: str = "test_results.json"):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ JSON —Ñ–∞–π–ª"""
    results_data = []
    for result in all_results:
        results_data.append({
            "capability": result.capability,
            "client_type": result.client_type,
            "model_config": result.model_config,
            "success": result.success,
            "response_time": result.response_time,
            "error": result.error,
            "response_preview": result.response_preview,
            "timestamp": time.time()
        })
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∏ —Å–≤–æ–¥–∫—É –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π
    recommendations = generate_recommendations(all_results)
    capabilities_summary = generate_capabilities_summary(all_results)
    
    final_data = {
        "test_results": results_data,
        "capabilities_summary": capabilities_summary,
        "recommendations": recommendations,
        "summary": {
            "total_tests": len(all_results),
            "successful_tests": sum(1 for r in all_results if r.success),
            "success_rate": sum(1 for r in all_results if r.success) / len(all_results) * 100 if all_results else 0,
            "test_timestamp": time.time()
        }
    }
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(final_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filename}")


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è."""
    print("üöÄ –†–ê–°–®–ò–†–ï–ù–ù–û–ï –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –í–°–ï–• –í–û–ó–ú–û–ñ–ù–û–°–¢–ï–ô –ú–û–î–ï–õ–ï–ô")
    print("=" * 80)
    print("–¢–µ—Å—Ç–∏—Ä—É–µ–º –≤—Å–µ –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –∫–ª–∏–µ–Ω—Ç—ã –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏")
    print("=" * 80)
    
    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π
    model_configs = get_model_configs()
    
    if not model_configs:
        print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π –º–æ–¥–µ–ª–µ–π –≤ .env —Ñ–∞–π–ª–µ")
        return
    
    print(f"üìã –ù–∞–π–¥–µ–Ω–æ {len(model_configs)} –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π –º–æ–¥–µ–ª–µ–π:")
    for config in model_configs:
        print(f"   ‚Ä¢ {config.name}: {config.description}")
        print(f"     Endpoint: {config.endpoint}")
        print(f"     Model: {config.model}")
    
    all_results = []
    
    try:
        for model_config in model_configs:
            print(f"\nüîÑ –ù–∞—á–∏–Ω–∞–µ–º —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏: {model_config.name}")
            
            # 1. –¢–µ—Å—Ç–∏—Ä—É–µ–º AdaptiveLLMClient
            adaptive_results = await test_adaptive_capabilities(model_config)
            all_results.extend(adaptive_results)
            
            # 2. –¢–µ—Å—Ç–∏—Ä—É–µ–º –≤—Å–µ —Ç–∏–ø—ã –∫–ª–∏–µ–Ω—Ç–æ–≤
            client_results = await test_all_client_types(model_config)
            all_results.extend(client_results)
            
            # 3. –¢–µ—Å—Ç–∏—Ä—É–µ–º ClientFactory
            factory_results = await test_client_factory(model_config)
            all_results.extend(factory_results)
            
            print(f"‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏: {model_config.name}")
        
        # –ü–µ—á–∞—Ç–∞–µ–º –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
        print_summary_report(all_results)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Å–≤–æ–¥–Ω—ã–π –∏—Ç–æ–≥ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π
        capabilities_summary = generate_capabilities_summary(all_results)
        print_capabilities_summary(capabilities_summary)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        recommendations = generate_recommendations(all_results)
        print_recommendations(recommendations)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        save_results_to_json(all_results, "kraken_capabilities_test_results.json")
        
        print("\nüéâ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û –£–°–ü–ï–®–ù–û!")
        
    except Exception as e:
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
        import traceback
        traceback.print_exc()
        
        # –í—Å–µ —Ä–∞–≤–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ, —á—Ç–æ —É—Å–ø–µ–ª–∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å
        if all_results:
            save_results_to_json(all_results, "kraken_capabilities_test_results_partial.json")


if __name__ == "__main__":
    asyncio.run(main())
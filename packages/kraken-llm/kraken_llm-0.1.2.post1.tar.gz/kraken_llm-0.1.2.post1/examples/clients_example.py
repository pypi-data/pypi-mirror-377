#!/usr/bin/env python3
"""
–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ Kraken LLM.

–î–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç:
1. Reasoning –∫–ª–∏–µ–Ω—Ç –¥–ª—è –ø–æ—à–∞–≥–æ–≤–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è
2. Multimodal –∫–ª–∏–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
3. Adaptive –∫–ª–∏–µ–Ω—Ç —Å –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π
"""

import asyncio
from pathlib import Path
import os
from pydantic import BaseModel
from typing import Optional
from dotenv import load_dotenv

from kraken_llm import (
    LLMConfig,
    ReasoningLLMClient,
    ReasoningConfig,
    MultimodalLLMClient,
    MultimodalConfig,
    AdaptiveLLMClient,
    AdaptiveConfig
)

load_dotenv()

# –ú–æ–¥–µ–ª—å –¥–ª—è structured —Ä–µ–∂–∏–º–∞
class WeatherInfo(BaseModel):
    city: str
    temperature: int
    condition: str
    humidity: Optional[int] = None
    wind_speed: Optional[int] = None

async def demo_reasoning_client():
    """
    –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π.

    ReasoningLLMClient –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç:
    - Chain of Thought (CoT) —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è
    - –ü–æ—à–∞–≥–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–¥–∞—á
    - Streaming —Ä–µ–∂–∏–º –¥–ª—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
    - –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π
    """
    print("=== –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è ReasoningLLMClient ===")
    print("–ö–ª–∏–µ–Ω—Ç –¥–ª—è –ø–æ—à–∞–≥–æ–≤–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π Chain of Thought")

    config = LLMConfig(
        endpoint=os.getenv("LLM_ENDPOINT"),
        api_key=os.getenv("LLM_TOKEN"),
        model=os.getenv("LLM_MODEL")
    )

    reasoning_config = ReasoningConfig(
        enable_cot=True,
        max_reasoning_steps=5,
        reasoning_temperature=0.1,
        extract_confidence=True
    )

    client = ReasoningLLMClient(config, reasoning_config)
    task1 = "–í –º–∞–≥–∞–∑–∏–Ω–µ –±—ã–ª–æ 150 —è–±–ª–æ–∫. –ü—Ä–æ–¥–∞–ª–∏ 60% –æ—Ç –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞. –°–∫–æ–ª—å–∫–æ —è–±–ª–æ–∫ –æ—Å—Ç–∞–ª–æ—Å—å?"
    # –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∑–∞–¥–∞—á–∞
    print(f"\n1. –†–µ—à–µ–Ω–∏–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∑–∞–¥–∞—á–∏:\n{task1}\n")
    messages = [{
        "role": "user",
        "content": "–†–µ—à–∏ –∑–∞–¥–∞—á—É: " + task1
    }]

    try:
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º streaming —Ä–µ–∂–∏–º
        print("–†–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏:")
        reasoning_steps = []

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ª–∏ –º–µ—Ç–æ–¥ async generator
        result = client.reasoning_completion(
            messages=messages,
            problem_type="math",
            enable_streaming=True
        )

        # –ï—Å–ª–∏ —ç—Ç–æ async generator, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∫ –ø–æ—Ç–æ–∫
        if hasattr(result, '__aiter__'):
            async for step in result:
                reasoning_steps.append(step)
                print(f"\n–®–∞–≥ {len(reasoning_steps)}:")
                print(f"  –ú—ã—Å–ª—å: {step.thought}")
                if hasattr(step, 'action') and step.action:
                    print(f"  –î–µ–π—Å—Ç–≤–∏–µ: {step.action}")
                if hasattr(step, 'observation') and step.observation:
                    print(f"  –†–µ–∑—É–ª—å—Ç–∞—Ç: {step.observation}")
                if hasattr(step, 'confidence') and step.confidence:
                    print(f"  –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {step.confidence:.2f}")

                # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à—É—é –∑–∞–¥–µ—Ä–∂–∫—É –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ streaming —ç—Ñ—Ñ–µ–∫—Ç–∞
                await asyncio.sleep(0.1)
        else:
            # –ï—Å–ª–∏ —ç—Ç–æ –æ–±—ã—á–Ω—ã–π –æ–±—ä–µ–∫—Ç, –∂–¥–µ–º –µ–≥–æ
            reasoning_chain = await result
            print(
                f"–ü–æ–ª—É—á–µ–Ω–∞ —Ü–µ–ø–æ—á–∫–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —Å {len(reasoning_chain.steps)} —à–∞–≥–∞–º–∏")

            for i, step in enumerate(reasoning_chain.steps, 1):
                print(f"\n–®–∞–≥ {i}:")
                print(f"  –ú—ã—Å–ª—å: {step.thought}")
                if hasattr(step, 'action') and step.action:
                    print(f"  –î–µ–π—Å—Ç–≤–∏–µ: {step.action}")
                if hasattr(step, 'observation') and step.observation:
                    print(f"  –†–µ–∑—É–ª—å—Ç–∞—Ç: {step.observation}")
                if hasattr(step, 'confidence') and step.confidence:
                    print(f"  –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {step.confidence:.2f}")

            print(f"\n–§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç: {reasoning_chain.final_answer}")

    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –≤ reasoning completion: {e}")
        # Fallback –∫ –æ–±—ã—á–Ω–æ–º—É —Ä–µ–∂–∏–º—É
        try:
            response = await client.chat_completion(messages)
            print(f"–û–±—ã—á–Ω—ã–π –æ—Ç–≤–µ—Ç: {response}")
        except Exception as fallback_error:
            print(f"Fallback —Ç–∞–∫–∂–µ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {fallback_error}")

    # –õ–æ–≥–∏—á–µ—Å–∫–∞—è –∑–∞–¥–∞—á–∞
    task2 = "–í—Å–µ –∫–æ—à–∫–∏ - –º–ª–µ–∫–æ–ø–∏—Ç–∞—é—â–∏–µ. –í—Å–µ –º–ª–µ–∫–æ–ø–∏—Ç–∞—é—â–∏–µ - –∂–∏–≤–æ—Ç–Ω—ã–µ. –ú—É—Ä–∫–∞ - –∫–æ—à–∫–∞. –ß—Ç–æ –º–æ–∂–Ω–æ —Å–∫–∞–∑–∞—Ç—å –æ –ú—É—Ä–∫–µ?"
    print(f"\n2. –†–µ—à–µ–Ω–∏–µ –ª–æ–≥–∏—á–µ—Å–∫–æ–π –∑–∞–¥–∞—á–∏:\n{task2}\n")
    logic_messages = [{
        "role": "user",
        "content": "–î–∞–Ω–æ: " + task2
    }]

    try:
        print("–õ–æ–≥–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏:")
        logic_steps = []

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø –≤–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è
        result = client.reasoning_completion(
            messages=logic_messages,
            problem_type="logic",
            enable_streaming=True
        )

        if hasattr(result, '__aiter__'):
            async for step in result:
                logic_steps.append(step)
                print(f"\n–®–∞–≥ {len(logic_steps)}:")
                print(f"  –õ–æ–≥–∏—á–µ—Å–∫–∏–π –≤—ã–≤–æ–¥: {step.thought}")
                if hasattr(step, 'confidence') and step.confidence:
                    print(f"  –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {step.confidence:.2f}")

                # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à—É—é –∑–∞–¥–µ—Ä–∂–∫—É –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ streaming —ç—Ñ—Ñ–µ–∫—Ç–∞
                await asyncio.sleep(0.1)
        else:
            reasoning_chain = await result
            print(
                f"–ü–æ–ª—É—á–µ–Ω–∞ —Ü–µ–ø–æ—á–∫–∞ –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —Å {len(reasoning_chain.steps)} —à–∞–≥–∞–º–∏")

            for i, step in enumerate(reasoning_chain.steps, 1):
                print(f"\n–®–∞–≥ {i}:")
                print(f"  –õ–æ–≥–∏—á–µ—Å–∫–∏–π –≤—ã–≤–æ–¥: {step.thought}")
                if hasattr(step, 'confidence') and step.confidence:
                    print(f"  –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {step.confidence:.2f}")

            print(f"\n–§–∏–Ω–∞–ª—å–Ω—ã–π –≤—ã–≤–æ–¥: {reasoning_chain.final_answer}")

        print(
            f"\n–õ–æ–≥–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {len(logic_steps) if logic_steps else len(reasoning_chain.steps)} —à–∞–≥–æ–≤")

    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –≤ –ª–æ–≥–∏—á–µ—Å–∫–æ–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–∏: {e}")
        # Fallback –∫ –æ–±—ã—á–Ω–æ–º—É —Ä–µ–∂–∏–º—É
        try:
            response = await client.chat_completion(logic_messages)
            print(f"–û–±—ã—á–Ω—ã–π –æ—Ç–≤–µ—Ç: {response}")
        except Exception as fallback_error:
            print(f"Fallback —Ç–∞–∫–∂–µ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {fallback_error}")


async def demo_multimodal_client():
    """
    –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞.

    MultimodalLLMClient –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç:
    - –ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (vision completion)
    - –†–∞–±–æ—Ç—É —Å –∞—É–¥–∏–æ —Ñ–∞–π–ª–∞–º–∏
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    - –í–∞–ª–∏–¥–∞—Ü–∏—é –º–µ–¥–∏–∞ —Ñ–∞–π–ª–æ–≤
    """
    print("\n=== –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è MultimodalLLMClient ===")
    print("–ö–ª–∏–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏, –∞—É–¥–∏–æ –∏ –≤–∏–¥–µ–æ")

    config = LLMConfig()
    multimodal_config = MultimodalConfig(
        max_image_size=10 * 1024 * 1024,  # 10MB
        auto_resize_images=True
    )

    client = MultimodalLLMClient(config, multimodal_config)

    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (–ø—Ä–æ—Å—Ç–æ–π PNG 1x1 –ø–∏–∫—Å–µ–ª—å)
    test_image_path = Path("test_image.png")

    # –°–æ–∑–¥–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ PNG –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
    png_data = bytes.fromhex(
        "89504e470d0a1a0a0000000d49484452000000010000000108060000001f15c4"
        "890000000a4944415478da6300010000050001d72cc82f0000000049454e44ae426082"
    )

    try:
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ—Å—Ç–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        with open(test_image_path, 'wb') as f:
            f.write(png_data)

        print("1. –ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è:")

        try:
            response = await client.vision_completion(
                text_prompt="–û–ø–∏—à–∏ —á—Ç–æ —Ç—ã –≤–∏–¥–∏—à—å –Ω–∞ —ç—Ç–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏",
                images=[test_image_path],
                detail_level="high"
            )

            print(f"–û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏: {response}")

        except Exception as e:
            print(f"Vision completion –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è: {e}")
            print(
                "–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –î–ª—è —Ä–∞–±–æ—Ç—ã —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –Ω—É–∂–Ω–∞ –º–æ–¥–µ–ª—å —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π vision (–Ω–∞–ø—Ä–∏–º–µ—Ä, Qwen2.5-VL)")

        # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤
        print("\n2. –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã:")
        formats = MultimodalLLMClient.get_supported_formats()
        for media_type, format_list in formats.items():
            print(f"  {media_type}: {', '.join(format_list)}")

        # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        print("\n3. –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è:")
        image_content = MultimodalLLMClient.create_image_url_content(
            test_image_path,
            detail="low"
        )
        print(f"–¢–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {image_content['type']}")
        print(f"URL –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å: {image_content['image_url']['url'][:50]}...")

    finally:
        # –£–¥–∞–ª—è–µ–º —Ç–µ—Å—Ç–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        if test_image_path.exists():
            test_image_path.unlink()


async def demo_adaptive_client():
    """
    –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞.

    AdaptiveLLMClient –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏:
    - –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏
    - –í—ã–±–∏—Ä–∞–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã
    - –ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç fallback –º–µ—Ö–∞–Ω–∏–∑–º—ã
    - –°–æ–±–∏—Ä–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    """
    print("\n=== –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è AdaptiveLLMClient ===")
    print("–ö–ª–∏–µ–Ω—Ç —Å –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –∏ –≤—ã–±–æ—Ä–æ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞")

    config = LLMConfig()
    adaptive_config = AdaptiveConfig(
        auto_fallback=True,
        prefer_streaming=False,
        enable_performance_tracking=True
    )

    client = AdaptiveLLMClient(config, adaptive_config)

    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–∏
    print("1. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–∏:")
    try:
        capabilities = await client.get_model_capabilities()
        print(
            f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏: {[cap.value for cap in capabilities]}")

    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π: {e}")
        return

    # –£–º–Ω—ã–π completion
    print("\n2. –£–º–Ω—ã–π completion (–∞–≤—Ç–æ–≤—ã–±–æ—Ä —Ä–µ–∂–∏–º–∞):")

    test_cases = [
        {
            "name": "–û–±—ã—á–Ω—ã–π –≤–æ–ø—Ä–æ—Å",
            "messages": [{"role": "user", "content": "–ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ –¥–µ–ª–∞?"}],
            "expected_mode": "standard"
        },
        {
            "name": "–ó–∞–ø—Ä–æ—Å –Ω–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ",
            "messages": [{"role": "user", "content": "–û–±—ä—è—Å–Ω–∏ –ø–æ—à–∞–≥–æ–≤–æ, –∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç —Ñ–æ—Ç–æ—Å–∏–Ω—Ç–µ–∑"}],
            "expected_mode": "reasoning"
        },
        {
            "name": "–ó–∞–ø—Ä–æ—Å JSON",
            "messages": [{"role": "user", "content": "–í–µ—Ä–Ω–∏ JSON —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –ø–æ–≥–æ–¥–µ –≤ –ú–æ—Å–∫–≤–µ"}],
            "expected_mode": "structured",
            "response_model": WeatherInfo
        }
    ]

    for test_case in test_cases:
        print(f"\n  –¢–µ—Å—Ç: {test_case['name']}")
        try:
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            completion_params = {
                "messages": test_case["messages"],
                "max_tokens": 1000,
                "preferred_mode": test_case["expected_mode"]
            }

            # –î–ª—è structured —Ä–µ–∂–∏–º–∞ –¥–æ–±–∞–≤–ª—è–µ–º response_model
            if test_case["expected_mode"] == "structured" and "response_model" in test_case:
                completion_params["response_model"] = test_case["response_model"]

            response = await client.smart_completion(**completion_params)
            print(f"  –û—Ç–≤–µ—Ç: {response}")

        except Exception as e:
            print(f"  –û—à–∏–±–∫–∞: {e}")

    # –û—Ç—á–µ—Ç –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    print("\n3. –û—Ç—á–µ—Ç –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:")
    performance_report = client.get_performance_report()

    if performance_report["model_info"]:
        model_info = performance_report["model_info"]
        print(f"  –ú–æ–¥–µ–ª—å: {model_info['name']}")
        print(f"  –ü—Ä–æ–≤–∞–π–¥–µ—Ä: {model_info['provider']}")
        print(f"  –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏: {len(model_info['capabilities'])}")

    if performance_report["performance_metrics"]:
        print("  –ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:")
        for mode, metrics in performance_report["performance_metrics"].items():
            print(f"    {mode}:")
            print(f"      –ó–∞–ø—Ä–æ—Å–æ–≤: {metrics['total_requests']}")
            print(f"      –£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {metrics['success_rate']:.2%}")
            print(f"      –°—Ä–µ–¥–Ω—è—è –∑–∞–¥–µ—Ä–∂–∫–∞: {metrics['avg_latency']:.3f}s")

    # –¢–µ—Å—Ç fallback –º–µ—Ö–∞–Ω–∏–∑–º–∞
    print("\n4. –¢–µ—Å—Ç fallback –º–µ—Ö–∞–Ω–∏–∑–º–∞:")
    try:
        # –ü—Ä–æ–±—É–µ–º –∑–∞–ø—Ä–æ—Å —Å –Ω–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        response = await client.smart_completion(
            messages=[{"role": "user", "content": "–¢–µ—Å—Ç fallback"}],
            preferred_mode="nonexistent_mode"
        )
        print(f"  Fallback —Å—Ä–∞–±–æ—Ç–∞–ª: {response}")

    except Exception as e:
        print(f"  Fallback –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {e}")


async def demo_streaming_comparison():
    """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –æ–±—ã—á–Ω–æ–≥–æ –∏ streaming —Ä–µ–∂–∏–º–æ–≤"""
    print("\n=== –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∂–∏–º–æ–≤ ===")
    print("–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –æ–±—ã—á–Ω–æ–≥–æ –∏ streaming —Ä–µ–∂–∏–º–æ–≤")

    config = LLMConfig()
    adaptive_client = AdaptiveLLMClient(config)

    test_prompt = "–†–∞—Å—Å–∫–∞–∂–∏ –∫–æ—Ä–æ—Ç–∫—É—é –∏—Å—Ç–æ—Ä–∏—é –ø—Ä–æ –∫–æ—Ç–∞"
    messages = [{"role": "user", "content": test_prompt}]

    # –û–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º
    print("1. –û–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º:")
    import time
    start_time = time.time()

    try:
        response = await adaptive_client.smart_completion(messages, max_tokens=100)
        normal_time = time.time() - start_time
        print(f"  –í—Ä–µ–º—è: {normal_time:.2f}s")
        print(f"  –û—Ç–≤–µ—Ç: {response[:100]}...")

    except Exception as e:
        print(f"  –û—à–∏–±–∫–∞: {e}")
        normal_time = 0

    # Streaming —Ä–µ–∂–∏–º
    print("\n2. Streaming —Ä–µ–∂–∏–º:")
    print("  –û—Ç–≤–µ—Ç –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏: ", end='', flush=True)
    start_time = time.time()

    try:
        chunks = []
        full_response = ""

        async for chunk in adaptive_client.chat_completion_stream(messages, max_tokens=100):
            chunks.append(chunk)
            full_response += chunk
            print(chunk, end='', flush=True)

        streaming_time = time.time() - start_time
        print(f"\n  –í—Ä–µ–º—è: {streaming_time:.2f}s")
        print(f"  –ß–∞–Ω–∫–æ–≤: {len(chunks)}")
        print(f"  –ü–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç: {len(full_response)} —Å–∏–º–≤–æ–ª–æ–≤")

        if normal_time > 0:
            print(f"  –†–∞–∑–Ω–∏—Ü–∞: {abs(streaming_time - normal_time):.2f}s")

    except Exception as e:
        print(f"  –û—à–∏–±–∫–∞: {e}")


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏"""
    print("üöÄ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤ Kraken LLM")
    print("=" * 60)

    try:
        # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è reasoning –∫–ª–∏–µ–Ω—Ç–∞
        await demo_reasoning_client()

        # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è multimodal –∫–ª–∏–µ–Ω—Ç–∞
        await demo_multimodal_client()

        # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è adaptive –∫–ª–∏–µ–Ω—Ç–∞
        await demo_adaptive_client()

        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∂–∏–º–æ–≤
        await demo_streaming_comparison()

    except Exception as e:
        print(f"–û–±—â–∞—è –æ—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()

    print("\n‚úÖ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    print("\n–ù–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:")
    print("- üß† Reasoning –∫–ª–∏–µ–Ω—Ç –¥–ª—è –ø–æ—à–∞–≥–æ–≤–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è")
    print("- üñºÔ∏è  Multimodal –∫–ª–∏–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏")
    print("- ü§ñ Adaptive –∫–ª–∏–µ–Ω—Ç —Å –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π")
    print("- üìä –ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ fallback –º–µ—Ö–∞–Ω–∏–∑–º—ã")
    print("- üîÑ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞")


if __name__ == "__main__":
    asyncio.run(main())

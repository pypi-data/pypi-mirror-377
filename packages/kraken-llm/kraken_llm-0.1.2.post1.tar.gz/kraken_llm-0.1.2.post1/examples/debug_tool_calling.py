#!/usr/bin/env python3
"""
–û—Ç–ª–∞–¥–∫–∞ Tool Calling –≤ –ø–æ—Ç–æ–∫–æ–≤–æ–º —Ä–µ–∂–∏–º–µ.
"""

import asyncio
import json
import os
from dotenv import load_dotenv

from kraken_llm.client.streaming import StreamingLLMClient
from kraken_llm.config.settings import LLMConfig

load_dotenv()

async def debug_tool_calling():
    """–û—Ç–ª–∞–¥–∫–∞ tool calling —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º."""
    print("=== –û—Ç–ª–∞–¥–∫–∞ Tool Calling ===")
    
    config = LLMConfig(
        endpoint=os.getenv("LLM_ENDPOINT"),
        api_key=os.getenv("LLM_TOKEN"),
        model=os.getenv("LLM_MODEL"),
        temperature=0.3
    )
    
    async with StreamingLLMClient(config) as client:
        
        # –ü—Ä–æ—Å—Ç–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        def get_weather(city: str) -> str:
            """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ–≥–æ–¥–µ –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º –≥–æ—Ä–æ–¥–µ."""
            return f"–í –≥–æ—Ä–æ–¥–µ {city} —Å–µ–π—á–∞—Å —Å–æ–ª–Ω–µ—á–Ω–æ, +20¬∞C"
        
        # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç
        client.register_tool("get_weather", get_weather, "–ü–æ–ª—É—á–∏—Ç—å –ø–æ–≥–æ–¥—É –≤ –≥–æ—Ä–æ–¥–µ")
        
        print(f"–ó–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã: {client.get_registered_tools()}")
        
        # –ü—Ä–æ—Å—Ç–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞
        tools = [
            {
                "type": "function",
                "function": {
                    "name": "get_weather",
                    "description": "–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ–≥–æ–¥–µ –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º –≥–æ—Ä–æ–¥–µ",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "city": {
                                "type": "string",
                                "description": "–ù–∞–∑–≤–∞–Ω–∏–µ –≥–æ—Ä–æ–¥–∞"
                            }
                        },
                        "required": ["city"]
                    }
                }
            }
        ]
        
        # –ü—Ä–æ—Å—Ç–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –¥–æ–ª–∂–Ω–æ –≤—ã–∑–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç
        messages = [
            {"role": "user", "content": "–ö–∞–∫–∞—è –ø–æ–≥–æ–¥–∞ –≤ –ú–æ—Å–∫–≤–µ? –ò—Å–ø–æ–ª—å–∑—É–π —Ñ—É–Ω–∫—Ü–∏—é get_weather."}
        ]
        
        print("–û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ —Å tool calling...")
        print("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:")
        print(f"  - tools: {json.dumps(tools, indent=2, ensure_ascii=False)}")
        print(f"  - tool_choice: auto")
        print(f"  - messages: {messages}")
        
        print("\n–û—Ç–≤–µ—Ç:")
        print("-" * 50)
        
        chunk_count = 0
        content_chunks = []
        
        try:
            async for chunk in client.chat_completion_stream(
                messages=messages,
                tools=tools,
                tool_choice="auto"
            ):
                chunk_count += 1
                print(f"Chunk #{chunk_count}: '{chunk}'", end="", flush=True)
                content_chunks.append(chunk)
                
        except Exception as e:
            print(f"\n–û—à–∏–±–∫–∞: {e}")
            import traceback
            traceback.print_exc()
        
        print(f"\n{'-' * 50}")
        print(f"–ü–æ–ª—É—á–µ–Ω–æ {chunk_count} chunks")
        print(f"–û–±—â–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç: {''.join(content_chunks)}")


async def debug_simple_request():
    """–û—Ç–ª–∞–¥–∫–∞ –ø—Ä–æ—Å—Ç–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –±–µ–∑ tool calling."""
    print("\n=== –û—Ç–ª–∞–¥–∫–∞ –ø—Ä–æ—Å—Ç–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ ===")
    
    config = LLMConfig(
        endpoint=os.getenv("LLM_ENDPOINT"),
        api_key=os.getenv("LLM_TOKEN"),
        model=os.getenv("LLM_MODEL"),
        temperature=0.3
    )
    
    async with StreamingLLMClient(config) as client:
        messages = [
            {"role": "user", "content": "–ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ –¥–µ–ª–∞?"}
        ]
        
        print("–û—Ç–ø—Ä–∞–≤–∫–∞ –ø—Ä–æ—Å—Ç–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞...")
        print("–û—Ç–≤–µ—Ç:")
        print("-" * 30)
        
        chunk_count = 0
        async for chunk in client.chat_completion_stream(messages=messages):
            chunk_count += 1
            print(chunk, end="", flush=True)
        
        print(f"\n{'-' * 30}")
        print(f"–ü–æ–ª—É—á–µ–Ω–æ {chunk_count} chunks")


async def debug_function_calling():
    """–û—Ç–ª–∞–¥–∫–∞ function calling (—É—Å—Ç–∞—Ä–µ–≤—à–∏–π API)."""
    print("\n=== –û—Ç–ª–∞–¥–∫–∞ Function Calling ===")
    
    config = LLMConfig(
        endpoint=os.getenv("LLM_ENDPOINT"),
        api_key=os.getenv("LLM_TOKEN"),
        model=os.getenv("LLM_MODEL"),
        temperature=0.3
    )
    
    async with StreamingLLMClient(config) as client:
        
        def get_weather(city: str) -> str:
            """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ–≥–æ–¥–µ –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º –≥–æ—Ä–æ–¥–µ."""
            return f"–í –≥–æ—Ä–æ–¥–µ {city} —Å–µ–π—á–∞—Å —Å–æ–ª–Ω–µ—á–Ω–æ, +20¬∞C"
        
        client.register_function("get_weather", get_weather, "–ü–æ–ª—É—á–∏—Ç—å –ø–æ–≥–æ–¥—É –≤ –≥–æ—Ä–æ–¥–µ")
        
        functions = [
            {
                "name": "get_weather",
                "description": "–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ–≥–æ–¥–µ –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º –≥–æ—Ä–æ–¥–µ",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "city": {
                            "type": "string",
                            "description": "–ù–∞–∑–≤–∞–Ω–∏–µ –≥–æ—Ä–æ–¥–∞"
                        }
                    },
                    "required": ["city"]
                }
            }
        ]
        
        messages = [
            {"role": "user", "content": "–ö–∞–∫–∞—è –ø–æ–≥–æ–¥–∞ –≤ –ú–æ—Å–∫–≤–µ? –ò—Å–ø–æ–ª—å–∑—É–π —Ñ—É–Ω–∫—Ü–∏—é get_weather."}
        ]
        
        print("–û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ —Å function calling...")
        print("–û—Ç–≤–µ—Ç:")
        print("-" * 30)
        
        chunk_count = 0
        async for chunk in client.chat_completion_stream(
            messages=messages,
            functions=functions,
            function_call="auto"
        ):
            chunk_count += 1
            print(chunk, end="", flush=True)
        
        print(f"\n{'-' * 30}")
        print(f"–ü–æ–ª—É—á–µ–Ω–æ {chunk_count} chunks")


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ—Ç–ª–∞–¥–∫–∏."""
    print("üîç –û—Ç–ª–∞–¥–∫–∞ Tool/Function Calling")
    print("=" * 50)
    
    try:
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä–∏–º –ø—Ä–æ—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å
        await debug_simple_request()
        
        # –ó–∞—Ç–µ–º function calling
        await debug_function_calling()
        
        # –ò –Ω–∞–∫–æ–Ω–µ—Ü tool calling
        await debug_tool_calling()
        
        print("\n‚úÖ –û—Ç–ª–∞–¥–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
        
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ª–∞–¥–∫–∏: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
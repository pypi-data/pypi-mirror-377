#!/usr/bin/env python3
"""
–î–µ—Ç–∞–ª—å–Ω–∞—è –æ—Ç–ª–∞–¥–∫–∞ Tool Calling —Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º chunks.
"""

import asyncio
import json
import os
from dotenv import load_dotenv

from kraken_llm.client.streaming import StreamingLLMClient
from kraken_llm.config.settings import LLMConfig

load_dotenv()

class DebugStreamingClient(StreamingLLMClient):
    """–ö–ª–∏–µ–Ω—Ç —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏."""
    
    async def _process_stream_chunk(self, chunk, function_call_buffer, tool_calls_buffer, original_messages):
        """–ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è."""
        print(f"\nüîç DEBUG: –ü–æ–ª—É—á–µ–Ω chunk: {chunk}")
        
        if hasattr(chunk, 'choices') and chunk.choices:
            choice = chunk.choices[0]
            print(f"üîç DEBUG: Choice: {choice}")
            
            if hasattr(choice, 'delta'):
                delta = choice.delta
                print(f"üîç DEBUG: Delta: {delta}")
                
                if hasattr(delta, 'content') and delta.content:
                    print(f"üîç DEBUG: Content: '{delta.content}'")
                
                if hasattr(delta, 'function_call') and delta.function_call:
                    print(f"üîç DEBUG: Function call: {delta.function_call}")
                
                if hasattr(delta, 'tool_calls') and delta.tool_calls:
                    print(f"üîç DEBUG: Tool calls: {delta.tool_calls}")
        
        # –í—ã–∑—ã–≤–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥
        return await super()._process_stream_chunk(chunk, function_call_buffer, tool_calls_buffer, original_messages)


async def debug_with_detailed_logging():
    """–û—Ç–ª–∞–¥–∫–∞ —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º chunks."""
    print("=== –î–µ—Ç–∞–ª—å–Ω–∞—è –æ—Ç–ª–∞–¥–∫–∞ Tool Calling ===")
    
    config = LLMConfig(
        endpoint=os.getenv("LLM_ENDPOINT"),
        api_key=os.getenv("LLM_TOKEN"),
        model=os.getenv("LLM_MODEL"),
        temperature=0.1  # –ù–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è –±–æ–ª–µ–µ –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è
    )
    
    async with DebugStreamingClient(config) as client:
        
        def get_weather(city: str) -> str:
            """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ–≥–æ–¥–µ –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º –≥–æ—Ä–æ–¥–µ."""
            print(f"üéØ FUNCTION CALLED: get_weather(city='{city}')")
            return f"–í –≥–æ—Ä–æ–¥–µ {city} —Å–µ–π—á–∞—Å —Å–æ–ª–Ω–µ—á–Ω–æ, +20¬∞C"
        
        client.register_tool("get_weather", get_weather, "–ü–æ–ª—É—á–∏—Ç—å –ø–æ–≥–æ–¥—É –≤ –≥–æ—Ä–æ–¥–µ")
        
        tools = [
            {
                "type": "function",
                "function": {
                    "name": "get_weather",
                    "description": "–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ–≥–æ–¥–µ –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º –≥–æ—Ä–æ–¥–µ",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "city": {
                                "type": "string",
                                "description": "–ù–∞–∑–≤–∞–Ω–∏–µ –≥–æ—Ä–æ–¥–∞"
                            }
                        },
                        "required": ["city"]
                    }
                }
            }
        ]
        
        messages = [
            {"role": "user", "content": "–ö–∞–∫–∞—è –ø–æ–≥–æ–¥–∞ –≤ –ú–æ—Å–∫–≤–µ? –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–π —Ñ—É–Ω–∫—Ü–∏—é get_weather –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏."}
        ]
        
        print("–û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ —Å tool calling...")
        print("–û—Ç–≤–µ—Ç:")
        print("-" * 50)
        
        chunk_count = 0
        content_chunks = []
        
        try:
            async for chunk in client.chat_completion_stream(
                messages=messages,
                tools=tools,
                tool_choice="auto"
            ):
                chunk_count += 1
                print(f"üì¶ Chunk #{chunk_count}: '{chunk}'", end="", flush=True)
                content_chunks.append(chunk)
                
        except Exception as e:
            print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")
            import traceback
            traceback.print_exc()
        
        print(f"\n{'-' * 50}")
        print(f"–ü–æ–ª—É—á–µ–Ω–æ {chunk_count} chunks")
        print(f"–û–±—â–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç: {''.join(content_chunks)}")


async def debug_direct_openai_call():
    """–ü—Ä—è–º–æ–π –≤—ã–∑–æ–≤ OpenAI API –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è."""
    print("\n=== –ü—Ä—è–º–æ–π –≤—ã–∑–æ–≤ OpenAI API ===")
    
    import openai
    
    client = openai.AsyncOpenAI(
        base_url=os.getenv("LLM_ENDPOINT"),
        api_key=os.getenv("LLM_TOKEN")
    )
    
    tools = [
        {
            "type": "function",
            "function": {
                "name": "get_weather",
                "description": "–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ–≥–æ–¥–µ –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º –≥–æ—Ä–æ–¥–µ",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "city": {
                            "type": "string",
                            "description": "–ù–∞–∑–≤–∞–Ω–∏–µ –≥–æ—Ä–æ–¥–∞"
                        }
                    },
                    "required": ["city"]
                }
            }
        }
    ]
    
    messages = [
        {"role": "user", "content": "–ö–∞–∫–∞—è –ø–æ–≥–æ–¥–∞ –≤ –ú–æ—Å–∫–≤–µ? –ò—Å–ø–æ–ª—å–∑—É–π —Ñ—É–Ω–∫—Ü–∏—é get_weather."}
    ]
    
    print("–ü—Ä—è–º–æ–π –≤—ã–∑–æ–≤ OpenAI streaming API...")
    
    try:
        stream = await client.chat.completions.create(
            model=os.getenv("LLM_MODEL"),
            messages=messages,
            tools=tools,
            tool_choice="auto",
            stream=True,
            temperature=0.1
        )
        
        chunk_count = 0
        print("Raw chunks:")
        print("-" * 30)
        
        async for chunk in stream:
            chunk_count += 1
            print(f"Raw chunk #{chunk_count}: {chunk}")
            
            if chunk.choices:
                choice = chunk.choices[0]
                if choice.delta.content:
                    print(f"  Content: '{choice.delta.content}'")
                if choice.delta.tool_calls:
                    print(f"  Tool calls: {choice.delta.tool_calls}")
                if hasattr(choice.delta, 'function_call') and choice.delta.function_call:
                    print(f"  Function call: {choice.delta.function_call}")
        
        print(f"\n{'-' * 30}")
        print(f"–ü–æ–ª—É—á–µ–Ω–æ {chunk_count} raw chunks")
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä—è–º–æ–≥–æ –≤—ã–∑–æ–≤–∞: {e}")
        import traceback
        traceback.print_exc()
    
    await client.close()


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ—Ç–ª–∞–¥–∫–∏."""
    print("üîç –î–µ—Ç–∞–ª—å–Ω–∞—è –æ—Ç–ª–∞–¥–∫–∞ Tool/Function Calling")
    print("=" * 60)
    
    try:
        await debug_with_detailed_logging()
        await debug_direct_openai_call()
        
        print("\n‚úÖ –î–µ—Ç–∞–ª—å–Ω–∞—è –æ—Ç–ª–∞–¥–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
        
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ª–∞–¥–∫–∏: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
#!/usr/bin/env python3
"""
–ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –æ—Å–Ω–æ–≤–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π Kraken LLM Framework.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ–ª—å–∫–æ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ —Ä–∞–±–æ—á–∏–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –∫–ª–∏–µ–Ω—Ç-–º–æ–¥–µ–ª—å.
"""

import asyncio
import os
from dotenv import load_dotenv
from pydantic import BaseModel

from kraken_llm.config.settings import LLMConfig
from kraken_llm.client.standard import StandardLLMClient
from kraken_llm.client.streaming import StreamingLLMClient
from kraken_llm.client.structured import StructuredLLMClient
from kraken_llm.client.embeddings import EmbeddingsLLMClient

load_dotenv()

class ResponseModel(BaseModel):
    """–ú–æ–¥–µ–ª—å –æ—Ç–≤–µ—Ç–∞ –¥–ª—è structured output"""
    message: str
    status: str
    code: int

async def test_chat_model():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ chat –º–æ–¥–µ–ª–∏ —Å –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–º–∏ –∫–ª–∏–µ–Ω—Ç–∞–º–∏"""
    print("üîÑ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Chat Model...")
    
    config = LLMConfig(
        endpoint=os.getenv("CHAT_ENDPOINT"),
        api_key=os.getenv("CHAT_TOKEN"),
        model=os.getenv("CHAT_MODEL"),
        temperature=0.7
    )
    
    # 1. StandardLLMClient - –±–∞–∑–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    print("  üìù StandardLLMClient...")
    async with StandardLLMClient(config) as client:
        response = await client.chat_completion(
            [{"role": "user", "content": "–ü—Ä–∏–≤–µ—Ç! –û—Ç–≤–µ—Ç—å '–†–∞–±–æ—Ç–∞–µ—Ç'"}],
            max_tokens=10
        )
        print(f"    ‚úÖ –û—Ç–≤–µ—Ç: {response}")
    
    # 2. StreamingLLMClient - –ø–æ—Ç–æ–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    print("  üåä StreamingLLMClient...")
    async with StreamingLLMClient(config) as client:
        full_response = ""
        async for chunk in client.chat_completion_stream(
            [{"role": "user", "content": "–°—á–∏—Ç–∞–π –æ—Ç 1 –¥–æ 3"}],
            max_tokens=20
        ):
            if hasattr(chunk, 'choices') and chunk.choices:
                choice = chunk.choices[0]
                if hasattr(choice, 'delta') and hasattr(choice.delta, 'content'):
                    content = choice.delta.content
                    if content:
                        full_response += content
        print(f"    ‚úÖ Streaming –æ—Ç–≤–µ—Ç: {full_response.strip()}")
    
    # 3. StructuredLLMClient - —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã
    print("  üìä StructuredLLMClient...")
    async with StructuredLLMClient(config) as client:
        response = await client.chat_completion_structured(
            [{"role": "user", "content": "–í–µ—Ä–Ω–∏ JSON: message='–¢–µ—Å—Ç', status='ok', code=200"}],
            response_model=ResponseModel,
            max_tokens=50
        )
        print(f"    ‚úÖ Structured –æ—Ç–≤–µ—Ç: {response}")

async def test_completion_model():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ completion –º–æ–¥–µ–ª–∏ —Å –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–º–∏ –∫–ª–∏–µ–Ω—Ç–∞–º–∏"""
    print("\nüîÑ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Completion Model...")
    
    config = LLMConfig(
        endpoint=os.getenv("COMPLETION_ENDPOINT"),
        api_key=os.getenv("COMPLETION_TOKEN"),
        model=os.getenv("COMPLETION_MODEL"),
        temperature=0.7
    )
    
    # 1. StandardLLMClient - –±–∞–∑–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    print("  üìù StandardLLMClient...")
    async with StandardLLMClient(config) as client:
        response = await client.chat_completion(
            [{"role": "user", "content": "–ü—Ä–∏–≤–µ—Ç! –û—Ç–≤–µ—Ç—å '–†–∞–±–æ—Ç–∞–µ—Ç'"}],
            max_tokens=10
        )
        print(f"    ‚úÖ –û—Ç–≤–µ—Ç: {response}")
    
    # 2. StreamingLLMClient - –ø–æ—Ç–æ–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã (—Ä–∞–±–æ—Ç–∞—é—Ç –ª—É—á—à–µ)
    print("  üåä StreamingLLMClient...")
    async with StreamingLLMClient(config) as client:
        full_response = ""
        chunk_count = 0
        async for chunk in client.chat_completion_stream(
            [{"role": "user", "content": "–°—á–∏—Ç–∞–π –æ—Ç 1 –¥–æ 3"}],
            max_tokens=20
        ):
            chunk_count += 1
            if hasattr(chunk, 'choices') and chunk.choices:
                choice = chunk.choices[0]
                if hasattr(choice, 'delta') and hasattr(choice.delta, 'content'):
                    content = choice.delta.content
                    if content:
                        full_response += content
            if chunk_count > 10:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è —Ç–µ—Å—Ç–∞
                break
        print(f"    ‚úÖ Streaming –æ—Ç–≤–µ—Ç ({chunk_count} —á–∞–Ω–∫–æ–≤): {full_response.strip()}")

async def test_embedding_model():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ embedding –º–æ–¥–µ–ª–∏"""
    print("\nüîÑ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Embedding Model...")
    
    config = LLMConfig(
        endpoint=os.getenv("EMBEDDING_ENDPOINT"),
        api_key=os.getenv("EMBEDDING_TOKEN"),
        model=os.getenv("EMBEDDING_MODEL")
    )
    
    # EmbeddingsLLMClient - –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥—è—â–∏–π –∫–ª–∏–µ–Ω—Ç
    print("  üî¢ EmbeddingsLLMClient...")
    async with EmbeddingsLLMClient(config) as client:
        texts = ["–ü—Ä–∏–≤–µ—Ç –º–∏—Ä", "Hello world", "–¢–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç"]
        embeddings = await client.get_embeddings(texts)
        
        if hasattr(embeddings, 'data') and embeddings.data:
            first_embedding = embeddings.data[0].embedding
            print(f"    ‚úÖ –ü–æ–ª—É—á–µ–Ω—ã —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤")
            print(f"    üìè –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {len(first_embedding)}")
            print(f"    üî¢ –ü–µ—Ä–≤—ã–µ 5 –∑–Ω–∞—á–µ–Ω–∏–π: {first_embedding[:5]}")
        else:
            print(f"    ‚úÖ Embeddings –ø–æ–ª—É—á–µ–Ω—ã: {type(embeddings)}")

async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –±—ã—Å—Ç—Ä–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏"""
    print("üöÄ –ë–´–°–¢–†–ê–Ø –ü–†–û–í–ï–†–ö–ê –í–û–ó–ú–û–ñ–ù–û–°–¢–ï–ô KRAKEN LLM")
    print("=" * 50)
    print("–¢–µ—Å—Ç–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ —Ä–∞–±–æ—á–∏–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏")
    print("=" * 50)
    
    try:
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º chat –º–æ–¥–µ–ª—å
        if all([os.getenv("CHAT_ENDPOINT"), os.getenv("CHAT_TOKEN"), os.getenv("CHAT_MODEL")]):
            await test_chat_model()
        else:
            print("‚ö†Ô∏è Chat –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞ –≤ .env")
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º completion –º–æ–¥–µ–ª—å
        if all([os.getenv("COMPLETION_ENDPOINT"), os.getenv("COMPLETION_TOKEN"), os.getenv("COMPLETION_MODEL")]):
            await test_completion_model()
        else:
            print("‚ö†Ô∏è Completion –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞ –≤ .env")
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º embedding –º–æ–¥–µ–ª—å
        if all([os.getenv("EMBEDDING_ENDPOINT"), os.getenv("EMBEDDING_TOKEN"), os.getenv("EMBEDDING_MODEL")]):
            await test_embedding_model()
        else:
            print("‚ö†Ô∏è Embedding –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞ –≤ .env")
        
        print("\n‚úÖ –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
        print("\nüí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
        print("  ‚Ä¢ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ StreamingLLMClient –¥–ª—è –ø–æ—Ç–æ–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤")
        print("  ‚Ä¢ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ StructuredLLMClient –¥–ª—è JSON –æ—Ç–≤–µ—Ç–æ–≤")
        print("  ‚Ä¢ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ EmbeddingsLLMClient —Ç–æ–ª—å–∫–æ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
        print("  ‚Ä¢ –ò–∑–±–µ–≥–∞–π—Ç–µ CompletionLLMClient —Å chat endpoints")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏: {e}")

if __name__ == "__main__":
    asyncio.run(main())
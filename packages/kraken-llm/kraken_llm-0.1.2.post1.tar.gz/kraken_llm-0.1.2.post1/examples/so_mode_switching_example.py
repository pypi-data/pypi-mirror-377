"""
–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –≤—Å–µ—Ö —Ä–µ–∂–∏–º–æ–≤ structured output –≤ Kraken LLM.

–≠—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç:
1. OpenAI –Ω–∞—Ç–∏–≤–Ω—ã–π structured output (non-streaming)
2. OpenAI –Ω–∞—Ç–∏–≤–Ω—ã–π structured output (streaming)
3. Outlines structured output (non-streaming)
4. Outlines structured output (—Ä–µ–∞–ª—å–Ω—ã–π streaming —Å –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–º –ø–∞—Ä—Å–∏–Ω–≥–æ–º)
5. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤—Å–µ—Ö —Ä–µ–∂–∏–º–æ–≤
6. –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤
"""

import asyncio
import os
import time
from typing import List, Optional
from pydantic import BaseModel, Field
from dotenv import load_dotenv

from kraken_llm.client.structured import StructuredLLMClient
from kraken_llm.config.settings import LLMConfig

load_dotenv()


class TaskResponse(BaseModel):
    """–ú–æ–¥–µ–ª—å –æ—Ç–≤–µ—Ç–∞ –¥–ª—è –∑–∞–¥–∞—á–∏."""
    task_name: str = Field(description="–ù–∞–∑–≤–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏")
    priority: int = Field(ge=1, le=5, description="–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –æ—Ç 1 –¥–æ 5")
    estimated_hours: float = Field(
        ge=0.5, le=40.0, description="–û—Ü–µ–Ω–∫–∞ –≤—Ä–µ–º–µ–Ω–∏ –≤ —á–∞—Å–∞—Ö")
    tags: List[str] = Field(description="–¢–µ–≥–∏ –∑–∞–¥–∞—á–∏")
    is_urgent: bool = Field(description="–§–ª–∞–≥ —Å—Ä–æ—á–Ω–æ—Å—Ç–∏")
    description: Optional[str] = Field(
        default=None, description="–û–ø–∏—Å–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏")


class ProjectSummary(BaseModel):
    """–ú–æ–¥–µ–ª—å —Ä–µ–∑—é–º–µ –ø—Ä–æ–µ–∫—Ç–∞."""
    project_name: str = Field(description="–ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞")
    tasks: List[TaskResponse] = Field(description="–°–ø–∏—Å–æ–∫ –∑–∞–¥–∞—á")
    total_estimated_hours: float = Field(ge=0.0, description="–û–±—â–µ–µ –≤—Ä–µ–º—è")
    completion_percentage: float = Field(
        ge=0.0, le=100.0, description="–ü—Ä–æ—Ü–µ–Ω—Ç –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è")
    next_milestone: str = Field(description="–°–ª–µ–¥—É—é—â–∏–π —ç—Ç–∞–ø")


async def demo_simple_task(client: StructuredLLMClient, mode_name: str):
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø—Ä–æ—Å—Ç–æ–π –∑–∞–¥–∞—á–∏."""
    print(f"\n=== {mode_name} —Ä–µ–∂–∏–º - –ü—Ä–æ—Å—Ç–∞—è –∑–∞–¥–∞—á–∞ ===")

    messages = [
        {
            "role": "system",
            "content": "–¢—ã –ø–æ–º–æ—â–Ω–∏–∫ –ø–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—é –ø—Ä–æ–µ–∫—Ç–∞–º–∏. –°–æ–∑–¥–∞–≤–∞–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –æ –∑–∞–¥–∞—á–∞—Ö."
        },
        {
            "role": "user",
            "content": "–°–æ–∑–¥–∞–π –∑–∞–¥–∞—á—É '–ù–∞–ø–∏—Å–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é' —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º 3, –æ—Ü–µ–Ω–∫–æ–π 8 —á–∞—Å–æ–≤, —Ç–µ–≥–∞–º–∏ ['docs', 'writing'] –∏ –ø–æ–º–µ—Ç—å –∫–∞–∫ —Å—Ä–æ—á–Ω—É—é"
        }
    ]

    try:
        result = await client.chat_completion(
            messages=messages,
            response_model=TaskResponse,
            stream=False
        )

        print(f"–ó–∞–¥–∞—á–∞: {result.task_name}")
        print(f"–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: {result.priority}")
        print(f"–í—Ä–µ–º—è: {result.estimated_hours} —á–∞—Å–æ–≤")
        print(f"–¢–µ–≥–∏: {result.tags}")
        print(f"–°—Ä–æ—á–Ω–∞—è: {'–î–∞' if result.is_urgent else '–ù–µ—Ç'}")
        if result.description:
            print(f"–û–ø–∏—Å–∞–Ω–∏–µ: {result.description}")

    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –≤ {mode_name} —Ä–µ–∂–∏–º–µ: {e}")


async def demo_complex_project(client: StructuredLLMClient, mode_name: str):
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Å–ª–æ–∂–Ω–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞."""
    print(f"\n=== {mode_name} —Ä–µ–∂–∏–º - –°–ª–æ–∂–Ω—ã–π –ø—Ä–æ–µ–∫—Ç ===")

    messages = [
        {
            "role": "system",
            "content": "–¢—ã –ø–æ–º–æ—â–Ω–∏–∫ –ø–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—é –ø—Ä–æ–µ–∫—Ç–∞–º–∏. –°–æ–∑–¥–∞–≤–∞–π –¥–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—é–º–µ –ø—Ä–æ–µ–∫—Ç–æ–≤."
        },
        {
            "role": "user",
            "content": """–°–æ–∑–¥–∞–π —Ä–µ–∑—é–º–µ –ø—Ä–æ–µ–∫—Ç–∞ '–í–µ–±-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ' —Å —Ç—Ä–µ–º—è –∑–∞–¥–∞—á–∞–º–∏:
            1. –î–∏–∑–∞–π–Ω UI (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç 2, 12 —á–∞—Å–æ–≤, —Ç–µ–≥–∏ ['design', 'ui'])
            2. Backend API (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç 1, 20 —á–∞—Å–æ–≤, —Ç–µ–≥–∏ ['backend', 'api'], —Å—Ä–æ—á–Ω–∞—è)
            3. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç 3, 8 —á–∞—Å–æ–≤, —Ç–µ–≥–∏ ['testing', 'qa'])
            
            –û–±—â–µ–µ –≤—Ä–µ–º—è 40 —á–∞—Å–æ–≤, –∑–∞–≤–µ—Ä—à–µ–Ω–æ 25%, —Å–ª–µ–¥—É—é—â–∏–π —ç—Ç–∞–ø '–†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ MVP'"""
        }
    ]

    try:
        result = await client.chat_completion(
            messages=messages,
            response_model=ProjectSummary,
            stream=False
        )

        print(f"–ü—Ä–æ–µ–∫—Ç: {result.project_name}")
        print(f"–û–±—â–µ–µ –≤—Ä–µ–º—è: {result.total_estimated_hours} —á–∞—Å–æ–≤")
        print(f"–ó–∞–≤–µ—Ä—à–µ–Ω–æ: {result.completion_percentage}%")
        print(f"–°–ª–µ–¥—É—é—â–∏–π —ç—Ç–∞–ø: {result.next_milestone}")
        print(f"–ó–∞–¥–∞—á–∏ ({len(result.tasks)}):")

        for i, task in enumerate(result.tasks, 1):
            print(f"  {i}. {task.task_name}")
            print(
                f"     –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: {task.priority}, –í—Ä–µ–º—è: {task.estimated_hours}—á")
            print(
                f"     –¢–µ–≥–∏: {task.tags}, –°—Ä–æ—á–Ω–∞—è: {'–î–∞' if task.is_urgent else '–ù–µ—Ç'}")

    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –≤ {mode_name} —Ä–µ–∂–∏–º–µ: {e}")


async def demo_all_modes_comparison(openai_client: StructuredLLMClient, outlines_client: StructuredLLMClient):
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –≤—Å–µ—Ö —Ä–µ–∂–∏–º–æ–≤ structured output."""
    print(f"\n=== –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö —Ä–µ–∂–∏–º–æ–≤ Structured Output ===")

    messages = [
        {
            "role": "system",
            "content": "–°–æ–∑–¥–∞–π –∑–∞–¥–∞—á—É –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ —Å –ø–æ–ª–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π."
        },
        {
            "role": "user",
            "content": "–°–æ–∑–¥–∞–π –∑–∞–¥–∞—á—É '–ö–æ–¥-—Ä–µ–≤—å—é API' —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º 2, –æ—Ü–µ–Ω–∫–æ–π 3 —á–∞—Å–∞, —Ç–µ–≥–∞–º–∏ ['review', 'api'] –∏ –ø–æ–º–µ—Ç—å –∫–∞–∫ —Å—Ä–æ—á–Ω—É—é"
        }
    ]

    results = {}

    # 1. OpenAI Non-streaming
    print("\n1) OpenAI Non-streaming:")
    try:
        start_time = time.time()
        result = await openai_client.chat_completion(
            messages=messages,
            response_model=TaskResponse,
            stream=False
        )
        execution_time = time.time() - start_time
        results["openai_non_stream"] = {
            "time": execution_time, "result": result, "success": True}
        print(f"   –£—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ –∑–∞ {execution_time:.3f}s: {result.task_name}")
        print(
            f"      –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: {result.priority}, –í—Ä–µ–º—è: {result.estimated_hours}—á")
    except Exception as e:
        results["openai_non_stream"] = {
            "time": 0, "success": False, "error": str(e)}
        print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")

    # 2. OpenAI Streaming
    print("\n2) OpenAI Streaming:")
    try:
        start_time = time.time()
        result = await openai_client.chat_completion(
            messages=messages,
            response_model=TaskResponse,
            stream=True
        )
        execution_time = time.time() - start_time
        results["openai_stream"] = {
            "time": execution_time, "result": result, "success": True}
        print(f"   –£—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ –∑–∞ {execution_time:.3f}s: {result.task_name}")
        print(
            f"      –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: {result.priority}, –í—Ä–µ–º—è: {result.estimated_hours}—á")
    except Exception as e:
        results["openai_stream"] = {"time": 0,
                                    "success": False, "error": str(e)}
        print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")

    # 3. Outlines Non-streaming
    print("\n3) Outlines Non-streaming:")
    try:
        start_time = time.time()
        result = await outlines_client.chat_completion(
            messages=messages,
            response_model=TaskResponse,
            stream=False
        )
        execution_time = time.time() - start_time
        results["outlines_non_stream"] = {
            "time": execution_time, "result": result, "success": True}
        print(f"   –£—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ –∑–∞ {execution_time:.3f}s: {result.task_name}")
        print(
            f"      –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: {result.priority}, –í—Ä–µ–º—è: {result.estimated_hours}—á")
    except Exception as e:
        results["outlines_non_stream"] = {
            "time": 0, "success": False, "error": str(e)}
        print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")

    # 4. Outlines Real Streaming
    print("\n4) Outlines Real Streaming (–∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥):")
    try:
        start_time = time.time()
        result = await outlines_client.chat_completion(
            messages=messages,
            response_model=TaskResponse,
            stream=True  # –¢–µ–ø–µ—Ä—å —ç—Ç–æ —Ä–µ–∞–ª—å–Ω—ã–π streaming!
        )
        execution_time = time.time() - start_time
        results["outlines_stream"] = {
            "time": execution_time, "result": result, "success": True}
        print(f"   –£—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ –∑–∞ {execution_time:.3f}s: {result.task_name}")
        print(
            f"      –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: {result.priority}, –í—Ä–µ–º—è: {result.estimated_hours}—á")
    except Exception as e:
        results["outlines_stream"] = {
            "time": 0, "success": False, "error": str(e)}
        print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")

    # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print(f"\n   –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:")
    successful_results = {k: v for k, v in results.items() if v["success"]}

    if len(successful_results) > 1:
        times = [(k, v["time"]) for k, v in successful_results.items()]
        times.sort(key=lambda x: x[1])

        print(f"   –†–µ–π—Ç–∏–Ω–≥ –ø–æ —Å–∫–æ—Ä–æ—Å—Ç–∏:")
        for i, (mode, exec_time) in enumerate(times, 1):
            mode_names = {
                "openai_non_stream": "OpenAI Non-streaming",
                "openai_stream": "OpenAI Streaming",
                "outlines_non_stream": "Outlines Non-streaming",
                "outlines_stream": "Outlines Real Streaming"
            }
            print(f"   {i}. {mode_names[mode]}: {exec_time:.3f}s")

        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ streaming vs non-streaming
        if "openai_stream" in successful_results and "openai_non_stream" in successful_results:
            openai_improvement = successful_results["openai_non_stream"]["time"] - \
                successful_results["openai_stream"]["time"]
            print(
                f"\n   OpenAI Streaming vs Non-streaming: {openai_improvement:+.3f}s")

        if "outlines_stream" in successful_results and "outlines_non_stream" in successful_results:
            outlines_improvement = successful_results["outlines_non_stream"]["time"] - \
                successful_results["outlines_stream"]["time"]
            print(
                f"   Outlines Streaming vs Non-streaming: {outlines_improvement:+.3f}s")

    return results


async def demo_concurrent_modes():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –≤ —Ä–∞–∑–Ω—ã—Ö —Ä–µ–∂–∏–º–∞—Ö."""
    print(f"\n=== –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –≤ —Ä–∞–∑–Ω—ã—Ö —Ä–µ–∂–∏–º–∞—Ö ===")

    # –°–æ–∑–¥–∞–µ–º –∫–ª–∏–µ–Ω—Ç—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ä–µ–∂–∏–º–æ–≤
    openai_config = LLMConfig(
        endpoint=os.getenv("LLM_ENDPOINT"),
        api_key=os.getenv("LLM_TOKEN"),
        model=os.getenv("LLM_MODEL"),
        temperature=0.2,
        max_tokens=500,
        outlines_so_mode=False
    )

    outlines_config = LLMConfig(
        endpoint=os.getenv("LLM_ENDPOINT"),
        api_key=os.getenv("LLM_TOKEN"),
        model=os.getenv("LLM_MODEL"),
        temperature=0.2,
        max_tokens=500,
        outlines_so_mode=True
    )

    openai_client = StructuredLLMClient(openai_config)
    outlines_client = StructuredLLMClient(outlines_config)

    # –†–∞–∑–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
    requests = [
        {"client": openai_client, "mode": "OpenAI",
            "stream": False, "task": "–†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ UI"},
        {"client": openai_client, "mode": "OpenAI",
            "stream": True, "task": "–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ API"},
        {"client": outlines_client, "mode": "Outlines",
            "stream": False, "task": "–ö–æ–¥-—Ä–µ–≤—å—é"},
        {"client": outlines_client, "mode": "Outlines",
            "stream": True, "task": "–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è"}
    ]

    async def single_request(req, index):
        messages = [{
            "role": "user",
            "content": f"–°–æ–∑–¥–∞–π –∑–∞–¥–∞—á—É '{req['task']}' —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º {index + 1} –∏ –æ—Ü–µ–Ω–∫–æ–π {(index + 1) * 2} —á–∞—Å–∞"
        }]

        start_time = time.time()
        try:
            result = await req["client"].chat_completion(
                messages=messages,
                response_model=TaskResponse,
                stream=req["stream"]
            )

            execution_time = time.time() - start_time
            return {
                "index": index + 1,
                "mode": req["mode"],
                "stream": req["stream"],
                "time": execution_time,
                "result": result,
                "success": True
            }
        except Exception as e:
            execution_time = time.time() - start_time
            return {
                "index": index + 1,
                "mode": req["mode"],
                "stream": req["stream"],
                "time": execution_time,
                "error": str(e),
                "success": False
            }

    print(f"–ó–∞–ø—É—Å–∫–∞–µ–º {len(requests)} –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤...")

    start_time = time.time()
    results = await asyncio.gather(*[single_request(req, i) for i, req in enumerate(requests)])
    total_time = time.time() - start_time

    print(f"–û–±—â–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {total_time:.3f}s")

    successful = [r for r in results if r["success"]]
    failed = [r for r in results if not r["success"]]

    print(f"–£—Å–ø–µ—à–Ω—ã—Ö: {len(successful)}/{len(results)}")

    for result in successful:
        stream_label = "Streaming" if result["stream"] else "Non-streaming"
        print(
            f"   #{result['index']}: {result['mode']} {stream_label} - {result['time']:.3f}s")
        print(f"      –ó–∞–¥–∞—á–∞: {result['result'].task_name}")

    for result in failed:
        stream_label = "Streaming" if result["stream"] else "Non-streaming"
        print(
            f"   #{result['index']}: {result['mode']} {stream_label} - ‚ùå {result['error'][:50]}...")

    if successful:
        sequential_time = sum(r["time"] for r in successful)
        speedup = sequential_time / total_time
        print(f"\n–£—Å–∫–æ—Ä–µ–Ω–∏–µ –æ—Ç –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞: {speedup:.1f}x")

    # –ó–∞–∫—Ä—ã–≤–∞–µ–º –∫–ª–∏–µ–Ω—Ç—ã
    await openai_client.close()
    await outlines_client.close()

    return len(successful) == len(results)


async def demo_advanced_features():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π."""
    print(f"\n=== –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ ===")

    config = LLMConfig(
        endpoint=os.getenv("LLM_ENDPOINT"),
        api_key=os.getenv("LLM_TOKEN"),
        model=os.getenv("LLM_MODEL"),
        temperature=0.3,
        max_tokens=800,
        outlines_so_mode=True  # –ò—Å–ø–æ–ª—å–∑—É–µ–º Outlines –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Ä–µ–∞–ª—å–Ω–æ–≥–æ streaming
    )

    client = StructuredLLMClient(config)

    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Å–ª–æ–∂–Ω–æ–π –º–æ–¥–µ–ª–∏ —Å –≤–ª–æ–∂–µ–Ω–Ω—ã–º–∏ –æ–±—ä–µ–∫—Ç–∞–º–∏
    print("\nüîß –°–ª–æ–∂–Ω–∞—è –º–æ–¥–µ–ª—å —Å –≤–ª–æ–∂–µ–Ω–Ω—ã–º–∏ –æ–±—ä–µ–∫—Ç–∞–º–∏:")

    messages = [{
        "role": "user",
        "content": """–°–æ–∑–¥–∞–π –ø—Ä–æ–µ–∫—Ç 'E-commerce Platform' —Å –∑–∞–¥–∞—á–∞–º–∏:
        - Frontend (React, –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç 2, 25 —á–∞—Å–æ–≤, —Å—Ä–æ—á–Ω–∞—è)
        - Backend (Node.js, –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç 1, 30 —á–∞—Å–æ–≤, —Å—Ä–æ—á–Ω–∞—è) 
        - Database (PostgreSQL, –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç 3, 15 —á–∞—Å–æ–≤)
        
        –û–±—â–µ–µ –≤—Ä–µ–º—è 70 —á–∞—Å–æ–≤, –∑–∞–≤–µ—Ä—à–µ–Ω–æ 15%, —Å–ª–µ–¥—É—é—â–∏–π —ç—Ç–∞–ø 'MVP —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞'"""
    }]

    try:
        start_time = time.time()
        result = await client.chat_completion(
            messages=messages,
            response_model=ProjectSummary,
            stream=True  # –†–µ–∞–ª—å–Ω—ã–π streaming –¥–ª—è —Å–ª–æ–∂–Ω–æ–π –º–æ–¥–µ–ª–∏!
        )
        execution_time = time.time() - start_time

        print(f"   –£—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ –∑–∞ {execution_time:.3f}s")
        print(f"   –ü—Ä–æ–µ–∫—Ç: {result.project_name}")
        print(f"   –û–±—â–µ–µ –≤—Ä–µ–º—è: {result.total_estimated_hours}—á")
        print(f"   –ü—Ä–æ–≥—Ä–µ—Å—Å: {result.completion_percentage}%")
        print(f"   –°–ª–µ–¥—É—é—â–∏–π —ç—Ç–∞–ø: {result.next_milestone}")
        print(f"   –ó–∞–¥–∞—á: {len(result.tasks)}")

        for i, task in enumerate(result.tasks, 1):
            print(
                f"       {i}. {task.task_name} (P{task.priority}, {task.estimated_hours}—á)")

    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")

    await client.close()


async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏."""
    print("–ü–û–õ–ù–ê–Ø –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –†–ï–ñ–ò–ú–û–í STRUCTURED OUTPUT")
    print("=" * 65)
    print("–î–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ–º –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ Kraken LLM structured output:")
    print("‚Ä¢ OpenAI –Ω–∞—Ç–∏–≤–Ω—ã–π (non-streaming & streaming)")
    print("‚Ä¢ Outlines (non-streaming & —Ä–µ–∞–ª—å–Ω—ã–π streaming)")
    print("‚Ä¢ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏")
    print("‚Ä¢ –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ")
    print("‚Ä¢ –°–ª–æ–∂–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å –≤–ª–æ–∂–µ–Ω–Ω—ã–º–∏ –æ–±—ä–µ–∫—Ç–∞–º–∏")
    print()

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    print(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:")
    print(f"Endpoint: {os.getenv('LLM_ENDPOINT')}")
    print(f"Model: {os.getenv('LLM_MODEL')}")
    print()

    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ä–µ–∂–∏–º–æ–≤
    openai_config = LLMConfig(
        endpoint=os.getenv("LLM_ENDPOINT"),
        api_key=os.getenv("LLM_TOKEN"),
        model=os.getenv("LLM_MODEL"),
        temperature=0.1,
        max_tokens=1000,
        outlines_so_mode=False  # –ù–∞—Ç–∏–≤–Ω—ã–π OpenAI —Ä–µ–∂–∏–º
    )

    outlines_config = LLMConfig(
        endpoint=os.getenv("LLM_ENDPOINT"),
        api_key=os.getenv("LLM_TOKEN"),
        model=os.getenv("LLM_MODEL"),
        temperature=0.1,
        max_tokens=1000,
        outlines_so_mode=True   # Outlines —Ä–µ–∂–∏–º —Å —Ä–µ–∞–ª—å–Ω—ã–º streaming
    )

    # –°–æ–∑–¥–∞–Ω–∏–µ –∫–ª–∏–µ–Ω—Ç–æ–≤
    openai_client = StructuredLLMClient(openai_config)
    outlines_client = StructuredLLMClient(outlines_config)

    try:
        # 1. –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø—Ä–æ—Å—Ç—ã—Ö –∑–∞–¥–∞—á –≤ —Ä–∞–∑–Ω—ã—Ö —Ä–µ–∂–∏–º–∞—Ö
        await demo_simple_task(openai_client, "OpenAI")
        await demo_simple_task(outlines_client, "Outlines")

        # 2. –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Å–ª–æ–∂–Ω—ã—Ö –ø—Ä–æ–µ–∫—Ç–æ–≤
        await demo_complex_project(openai_client, "OpenAI")
        await demo_complex_project(outlines_client, "Outlines")

        # 3. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö —Ä–µ–∂–∏–º–æ–≤
        comparison_results = await demo_all_modes_comparison(openai_client, outlines_client)

        # 4. –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ
        concurrent_success = await demo_concurrent_modes()

        # 5. –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏
        await demo_advanced_features()

        # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print(f"\n" + "=" * 65)
        print(f"–ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
        print(f"=" * 65)

        successful_modes = sum(
            1 for result in comparison_results.values() if result["success"])
        total_modes = len(comparison_results)

        print(f"–†–µ–∂–∏–º—ã structured output: {successful_modes}/{total_modes} —Ä–∞–±–æ—Ç–∞—é—Ç")
        print(f"–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ: {'–†–∞–±–æ—Ç–∞–µ—Ç' if concurrent_success else '–ü—Ä–æ–±–ª–µ–º—ã'}")

        if successful_modes == total_modes:
            print(f"\n–í–°–ï –†–ï–ñ–ò–ú–´ –†–ê–ë–û–¢–ê–Æ–¢")
            print(f"–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:")
            print(f"   ‚Ä¢ OpenAI –Ω–∞—Ç–∏–≤–Ω—ã–π structured output")
            print(f"   ‚Ä¢ OpenAI streaming —Å –∞–≥—Ä–µ–≥–∞—Ü–∏–µ–π")
            print(f"   ‚Ä¢ Outlines —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –ø—Ä–æ–º–ø—Ç–∞–º–∏")
            print(f"   ‚Ä¢ Outlines —Å –†–ï–ê–õ–¨–ù–´–ú streaming –∏ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–º –ø–∞—Ä—Å–∏–Ω–≥–æ–º")
            print(f"   ‚Ä¢ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –º–µ–∂–¥—É —Ä–µ–∂–∏–º–∞–º–∏")
            print(f"   ‚Ä¢ –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Å–ª–æ–∂–Ω—ã—Ö –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π")
            print(f"   ‚Ä¢ –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤")
        else:
            print(f"\n‚ö†Ô∏è  –ù–µ–∫–æ—Ç–æ—Ä—ã–µ —Ä–µ–∂–∏–º—ã —Ç—Ä–µ–±—É—é—Ç –≤–Ω–∏–º–∞–Ω–∏—è")

    except Exception as e:
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏: {e}")
        import traceback
        traceback.print_exc()

    finally:
        # –ó–∞–∫—Ä—ã—Ç–∏–µ –∫–ª–∏–µ–Ω—Ç–æ–≤
        await openai_client.close()
        await outlines_client.close()


if __name__ == "__main__":
    asyncio.run(main())

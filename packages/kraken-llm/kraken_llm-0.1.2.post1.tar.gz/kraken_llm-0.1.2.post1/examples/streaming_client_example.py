#!/usr/bin/env python3
"""
–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è StreamingLLMClient –∏–∑ Kraken —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞.

–≠—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ LLM –∫–ª–∏–µ–Ω—Ç–∞:
- –ë–∞–∑–æ–≤—ã–µ –ø–æ—Ç–æ–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã —Å real-time –≤—ã–≤–æ–¥–æ–º
- –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ—Ç–æ–∫–æ–≤—ã–µ –æ—Ç–≤–µ—Ç—ã
- –ò–º–∏—Ç–∞—Ü–∏—è Function calling (API –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç)
- –ò–º–∏—Ç–∞—Ü–∏—è Tool calling (API –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç)
- –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

–ü–†–ò–ú–ï–ß–ê–ù–ò–ï: –¢–µ–∫—É—â–∏–π API –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ streaming —Ä–µ–∂–∏–º.
Tool/Function calling –∏–º–∏—Ç–∏—Ä—É–µ—Ç—Å—è —á–µ—Ä–µ–∑ –ª–æ–∫–∞–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏.
"""

import asyncio
import json
import os
import time
from dotenv import load_dotenv
from typing import Dict, Any

from kraken_llm.client.streaming import StreamingLLMClient
from kraken_llm.config.settings import LLMConfig
from kraken_llm.exceptions.base import KrakenError

load_dotenv()


async def basic_streaming_example():
    """–ü—Ä–∏–º–µ—Ä –±–∞–∑–æ–≤–æ–≥–æ –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞."""
    print("=== –ë–∞–∑–æ–≤—ã–π –ø–æ—Ç–æ–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å ===")

    # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    config = LLMConfig(
        endpoint=os.getenv("LLM_ENDPOINT"),
        api_key=os.getenv("LLM_TOKEN"),
        model=os.getenv("LLM_MODEL"),
        temperature=0.7,
        max_tokens=200
    )

    # –°–æ–∑–¥–∞–Ω–∏–µ –∫–ª–∏–µ–Ω—Ç–∞
    async with StreamingLLMClient(config) as client:
        messages = [
            {"role": "system", "content": "–¢—ã –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ."},
            {"role": "user", "content": "–†–∞—Å—Å–∫–∞–∂–∏ –∫–æ—Ä–æ—Ç–∫—É—é –∏—Å—Ç–æ—Ä–∏—é –æ —Ä–æ–±–æ—Ç–µ, –∫–æ—Ç–æ—Ä—ã–π –Ω–∞—É—á–∏–ª—Å—è –º–µ—á—Ç–∞—Ç—å"}
        ]

        print("–û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞...")
        print("–û—Ç–≤–µ—Ç (–≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏):")
        print("-" * 50)

        start_time = time.time()
        chunk_count = 0

        # –ü–æ—Ç–æ–∫–æ–≤—ã–π –≤—ã–≤–æ–¥
        async for chunk in client.chat_completion_stream(messages):
            print(chunk, end="", flush=True)
            chunk_count += 1

        elapsed_time = time.time() - start_time
        print(f"\n{'-' * 50}")
        print(f"–ü–æ–ª—É—á–µ–Ω–æ {chunk_count} chunks –∑–∞ {elapsed_time:.2f} —Å–µ–∫—É–Ω–¥")


async def aggregated_streaming_example():
    """–ü—Ä–∏–º–µ—Ä –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –æ—Ç–≤–µ—Ç–∞."""
    print("\n=== –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ—Ç–æ–∫–æ–≤—ã–π –æ—Ç–≤–µ—Ç ===")
    print("üí° –°–æ–±–∏—Ä–∞–µ–º streaming chunks –≤ –ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç")

    config = LLMConfig(
        endpoint=os.getenv("LLM_ENDPOINT"),
        api_key=os.getenv("LLM_TOKEN"),
        model=os.getenv("LLM_MODEL"),
        temperature=0.5,
        max_tokens=250
    )

    async with StreamingLLMClient(config) as client:
        messages = [
            {"role": "user", "content": "–û–±—ä—è—Å–Ω–∏ —á—Ç–æ —Ç–∞–∫–æ–µ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏"}
        ]

        print("–û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞...")

        start_time = time.time()

        # –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç (—Å–æ–±–∏—Ä–∞–µ–º chunks –≤—Ä—É—á–Ω—É—é)
        response_chunks = []
        async for chunk in client.chat_completion_stream(messages):
            response_chunks.append(chunk)

        response = "".join(response_chunks)
        elapsed_time = time.time() - start_time

        print("–ü–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç:")
        print("-" * 50)
        print(response)
        print(f"{'-' * 50}")
        print(f"–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {elapsed_time:.2f} —Å–µ–∫—É–Ω–¥")
        print(f"–î–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞: {len(response)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ chunks: {len(response_chunks)}")


async def function_calling_streaming_example():
    """–ü—Ä–∏–º–µ—Ä –∏–º–∏—Ç–∞—Ü–∏–∏ function calling –≤ –ø–æ—Ç–æ–∫–æ–≤–æ–º —Ä–µ–∂–∏–º–µ (API –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç function calling)."""
    print("\n=== –ò–º–∏—Ç–∞—Ü–∏—è Function Calling –≤ –ø–æ—Ç–æ–∫–æ–≤–æ–º —Ä–µ–∂–∏–º–µ ===")
    print("‚ö†Ô∏è  –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –¢–µ–∫—É—â–∏–π API –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç function calling, –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥")

    config = LLMConfig(
        endpoint=os.getenv("LLM_ENDPOINT"),
        api_key=os.getenv("LLM_TOKEN"),
        model=os.getenv("LLM_MODEL"),
        temperature=0.3
    )

    async with StreamingLLMClient(config) as client:

        # –õ–æ–∫–∞–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
        def get_weather(city: str) -> str:
            """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ–≥–æ–¥–µ –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º –≥–æ—Ä–æ–¥–µ."""
            weather_data = {
                "–º–æ—Å–∫–≤–∞": "–°–æ–ª–Ω–µ—á–Ω–æ, +15¬∞C, –ª–µ–≥–∫–∏–π –≤–µ—Ç–µ—Ä",
                "—Å–∞–Ω–∫—Ç-–ø–µ—Ç–µ—Ä–±—É—Ä–≥": "–û–±–ª–∞—á–Ω–æ, +12¬∞C, –¥–æ–∂–¥—å",
                "–Ω–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫": "–°–Ω–µ–≥, -5¬∞C, —Å–∏–ª—å–Ω—ã–π –≤–µ—Ç–µ—Ä",
                "–µ–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥": "–ü–∞—Å–º—É—Ä–Ω–æ, +8¬∞C, –±–µ–∑ –æ—Å–∞–¥–∫–æ–≤"
            }
            return weather_data.get(city.lower(), f"–î–∞–Ω–Ω—ã–µ –æ –ø–æ–≥–æ–¥–µ –¥–ª—è {city} –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã")

        def calculate_distance(city1: str, city2: str) -> str:
            """–í—ã—á–∏—Å–ª–∏—Ç—å —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É –≥–æ—Ä–æ–¥–∞–º–∏."""
            distances = {
                ("–º–æ—Å–∫–≤–∞", "—Å–∞–Ω–∫—Ç-–ø–µ—Ç–µ—Ä–±—É—Ä–≥"): "635 –∫–º",
                ("–º–æ—Å–∫–≤–∞", "–Ω–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫"): "3354 –∫–º",
                ("–º–æ—Å–∫–≤–∞", "–µ–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥"): "1416 –∫–º",
                ("—Å–∞–Ω–∫—Ç-–ø–µ—Ç–µ—Ä–±—É—Ä–≥", "–Ω–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫"): "3989 –∫–º"
            }
            key = tuple(sorted([city1.lower(), city2.lower()]))
            return distances.get(key, f"–†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É {city1} –∏ {city2} –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ")

        # –°–Ω–∞—á–∞–ª–∞ —Å–ø—Ä–∞—à–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å
        messages = [
            {"role": "user", "content": "–ú–Ω–µ –Ω—É–∂–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ–≥–æ–¥–µ –≤ –ú–æ—Å–∫–≤–µ –∏ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–∏ –æ—Ç –ú–æ—Å–∫–≤—ã –¥–æ –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥–∞. –û—Ç–≤–µ—Ç—å, —á—Ç–æ —Ç–µ–±–µ –Ω—É–∂–Ω–æ –¥–ª—è —ç—Ç–æ–≥–æ."}
        ]

        print("–ó–∞–ø—Ä–æ—Å –∫ –º–æ–¥–µ–ª–∏...")
        print("–û—Ç–≤–µ—Ç:")
        print("-" * 50)

        response_chunks = []
        async for chunk in client.chat_completion_stream(messages=messages):
            print(chunk, end="", flush=True)
            response_chunks.append(chunk)

        # –í—ã–ø–æ–ª–Ω—è–µ–º "function calls"
        print(f"\n{'-' * 50}")
        print("üîß –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ª–æ–∫–∞–ª—å–Ω—ã—Ö 'function calls':")

        weather_result = get_weather("–º–æ—Å–∫–≤–∞")
        distance_result = calculate_distance("–º–æ—Å–∫–≤–∞", "—Å–∞–Ω–∫—Ç-–ø–µ—Ç–µ—Ä–±—É—Ä–≥")

        print(f"  get_weather('–º–æ—Å–∫–≤–∞') -> {weather_result}")
        print(
            f"  calculate_distance('–º–æ—Å–∫–≤–∞', '—Å–∞–Ω–∫—Ç-–ø–µ—Ç–µ—Ä–±—É—Ä–≥') -> {distance_result}")

        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—Ä–∞—Ç–Ω–æ –º–æ–¥–µ–ª–∏
        follow_up_messages = messages + [
            {"role": "assistant", "content": "".join(response_chunks)},
            {"role": "user", "content": f"–í–æ—Ç –¥–∞–Ω–Ω—ã–µ –∫–æ—Ç–æ—Ä—ã–µ —Ç—ã –∑–∞–ø—Ä–æ—Å–∏–ª:\n- –ü–æ–≥–æ–¥–∞ –≤ –ú–æ—Å–∫–≤–µ: {weather_result}\n- –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –æ—Ç –ú–æ—Å–∫–≤—ã –¥–æ –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥–∞: {distance_result}\n\n–¢–µ–ø–µ—Ä—å –¥–∞–π –ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç."}
        ]

        print("\n–§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç —Å –¥–∞–Ω–Ω—ã–º–∏:")
        print("-" * 50)

        async for chunk in client.chat_completion_stream(messages=follow_up_messages):
            print(chunk, end="", flush=True)

        print(f"\n{'-' * 50}")
        print("üí° –≠—Ç–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ç–æ–≥–æ, –∫–∞–∫ –º–æ–∂–Ω–æ –∏–º–∏—Ç–∏—Ä–æ–≤–∞—Ç—å function calling –±–µ–∑ –ø–æ–¥–¥–µ—Ä–∂–∫–∏ API")


async def tool_calling_streaming_example():
    """–ü—Ä–∏–º–µ—Ä –∏–º–∏—Ç–∞—Ü–∏–∏ tool calling –≤ –ø–æ—Ç–æ–∫–æ–≤–æ–º —Ä–µ–∂–∏–º–µ (API –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç tool calling)."""
    print("\n=== –ò–º–∏—Ç–∞—Ü–∏—è Tool Calling –≤ –ø–æ—Ç–æ–∫–æ–≤–æ–º —Ä–µ–∂–∏–º–µ ===")
    print("‚ö†Ô∏è  –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –¢–µ–∫—É—â–∏–π API –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç tool calling, –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥")

    config = LLMConfig(
        endpoint=os.getenv("LLM_ENDPOINT"),
        api_key=os.getenv("LLM_TOKEN"),
        model=os.getenv("LLM_MODEL"),
        temperature=0.3
    )

    async with StreamingLLMClient(config) as client:

        # –õ–æ–∫–∞–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ tool calling
        def search_database(query: str) -> str:
            """–ü–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö."""
            database = {
                "python": "–í—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π —è–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –æ–±—â–µ–≥–æ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è",
                "–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ": "–û–±–ª–∞—Å—Ç—å –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, –∏–∑—É—á–∞—é—â–∞—è –∞–ª–≥–æ—Ä–∏—Ç–º—ã –æ–±—É—á–µ–Ω–∏—è",
                "–Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏": "–í—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã, –≤–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –±–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–º–∏ –Ω–µ–π—Ä–æ–Ω–Ω—ã–º–∏ —Å–µ—Ç—è–º–∏",
                "api": "–ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –º–µ–∂–¥—É –ø—Ä–æ–≥—Ä–∞–º–º–∞–º–∏"
            }

            for key, value in database.items():
                if key.lower() in query.lower():
                    return f"–ù–∞–π–¥–µ–Ω–æ: {value}"

            return f"–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ –∑–∞–ø—Ä–æ—Å—É '{query}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö"

        # –°–Ω–∞—á–∞–ª–∞ –ø–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏
        messages = [
            {"role": "user", "content": "–†–∞—Å—Å–∫–∞–∂–∏ –æ Python –∫–∞–∫ —è–∑—ã–∫–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è"}
        ]

        print("–ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –æ—Ç –º–æ–¥–µ–ª–∏...")
        print("–û—Ç–≤–µ—Ç:")
        print("-" * 50)

        response_chunks = []
        async for chunk in client.chat_completion_stream(messages=messages):
            print(chunk, end="", flush=True)
            response_chunks.append(chunk)

        response = "".join(response_chunks)

        # –ò–º–∏—Ç–∏—Ä—É–µ–º "tool call" - –∏—â–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        print(f"\n{'-' * 50}")
        print("üîß –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ 'tool call': search_database('python')")

        tool_result = search_database("python")
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {tool_result}")

        # –ú–æ–∂–µ–º –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        follow_up_messages = [
            {"role": "user", "content": "–†–∞—Å—Å–∫–∞–∂–∏ –æ Python –∫–∞–∫ —è–∑—ã–∫–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è"},
            {"role": "assistant", "content": response},
            {"role": "user", "content": f"–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö: {tool_result}. –ú–æ–∂–µ—à—å –¥–æ–ø–æ–ª–Ω–∏—Ç—å —Å–≤–æ–π –æ—Ç–≤–µ—Ç?"}
        ]

        print("\n–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç —Å —É—á–µ—Ç–æ–º 'tool result':")
        print("-" * 50)

        async for chunk in client.chat_completion_stream(messages=follow_up_messages):
            print(chunk, end="", flush=True)

        print(f"\n{'-' * 50}")
        print(
            "üí° –≠—Ç–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ç–æ–≥–æ, –∫–∞–∫ –º–æ–∂–Ω–æ –∏–º–∏—Ç–∏—Ä–æ–≤–∞—Ç—å tool calling –±–µ–∑ –ø–æ–¥–¥–µ—Ä–∂–∫–∏ API")


async def performance_monitoring_example():
    """–ü—Ä–∏–º–µ—Ä –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ—Ç–æ–∫–æ–≤—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π."""
    print("\n=== –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ ===")

    config = LLMConfig(
        endpoint=os.getenv("LLM_ENDPOINT"),
        api_key=os.getenv("LLM_TOKEN"),
        model=os.getenv("LLM_MODEL"),
        temperature=0.7,
        max_tokens=300
    )

    async with StreamingLLMClient(config) as client:
        messages = [
            {"role": "user", "content": "–†–∞—Å—Å–∫–∞–∂–∏ –æ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞—Ö –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è"}
        ]

        print("–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞...")

        start_time = time.time()
        first_chunk_time = None
        chunk_times = []
        chunks = []

        async for chunk in client.chat_completion_stream(messages):
            current_time = time.time()

            if first_chunk_time is None:
                first_chunk_time = current_time - start_time

            chunk_times.append(current_time - start_time)
            chunks.append(chunk)

        total_time = time.time() - start_time

        # –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        print("\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:")
        print(f"  ‚Ä¢ –í—Ä–µ–º—è –¥–æ –ø–µ—Ä–≤–æ–≥–æ chunk: {first_chunk_time:.3f}s")
        print(f"  ‚Ä¢ –û–±—â–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {total_time:.3f}s")
        print(f"  ‚Ä¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ chunks: {len(chunks)}")
        print(f"  ‚Ä¢ –°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å: {len(chunks)/total_time:.1f} chunks/s")

        if len(chunk_times) > 1:
            intervals = [chunk_times[i] - chunk_times[i-1]
                         for i in range(1, len(chunk_times))]
            avg_interval = sum(intervals) / len(intervals)
            print(f"  ‚Ä¢ –°—Ä–µ–¥–Ω–∏–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –º–µ–∂–¥—É chunks: {avg_interval:.3f}s")

        total_chars = sum(len(chunk) for chunk in chunks)
        print(f"  ‚Ä¢ –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤: {total_chars}")
        print(
            f"  ‚Ä¢ –°–∫–æ—Ä–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {total_chars/total_time:.1f} —Å–∏–º–≤–æ–ª–æ–≤/s")


async def error_handling_example():
    """–ü—Ä–∏–º–µ—Ä –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—à–∏–±–æ–∫ –≤ –ø–æ—Ç–æ–∫–æ–≤–æ–º —Ä–µ–∂–∏–º–µ."""
    print("\n=== –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ ===")

    # –¢–µ—Å—Ç —Å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º endpoint
    print("1. –¢–µ—Å—Ç —Å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º endpoint:")
    config_invalid = LLMConfig(
        endpoint="http://invalid-endpoint-12345.com",
        api_key="test-key",
        model="test-model",
        connect_timeout=2.0,
        read_timeout=5.0
    )

    try:
        async with StreamingLLMClient(config_invalid) as client:
            messages = [{"role": "user", "content": "test"}]

            async for chunk in client.chat_completion_stream(messages):
                print(chunk, end="")

    except KrakenError as e:
        print(f"   –ü–æ–ª—É—á–µ–Ω–∞ –æ–∂–∏–¥–∞–µ–º–∞—è –æ—à–∏–±–∫–∞: {type(e).__name__}: {e}")

    # –¢–µ—Å—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –æ—à–∏–±–æ–∫
    print("\n2. –¢–µ—Å—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –æ—à–∏–±–æ–∫:")
    config_valid = LLMConfig(
        endpoint=os.getenv("LLM_ENDPOINT"),
        api_key=os.getenv("LLM_TOKEN"),
        model=os.getenv("LLM_MODEL"),
    )

    async with StreamingLLMClient(config_valid) as client:

        # –ü—É—Å—Ç—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
        try:
            async for chunk in client.chat_completion_stream([]):
                pass
        except KrakenError as e:
            print(f"   –ü—É—Å—Ç—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è: {type(e).__name__}: {e}")

        # –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å–æ–æ–±—â–µ–Ω–∏—è
        try:
            async for chunk in client.chat_completion_stream([{"role": "invalid"}]):
                pass
        except KrakenError as e:
            print(f"   –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞: {type(e).__name__}: {e}")

        # function_call –±–µ–∑ functions
        try:
            async for chunk in client.chat_completion_stream(
                [{"role": "user", "content": "test"}],
                function_call="auto"
            ):
                pass
        except KrakenError as e:
            print(f"   function_call –±–µ–∑ functions: {type(e).__name__}: {e}")


async def concurrent_requests_example():
    """–ü—Ä–∏–º–µ—Ä –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤."""
    print("\n=== –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –ø–æ—Ç–æ–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã ===")

    config = LLMConfig(
        endpoint=os.getenv("LLM_ENDPOINT"),
        api_key=os.getenv("LLM_TOKEN"),
        model=os.getenv("LLM_MODEL"),
        temperature=0.5,
        max_tokens=100
    )

    async with StreamingLLMClient(config) as client:

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        requests = [
            [{"role": "user", "content": "–ß—Ç–æ —Ç–∞–∫–æ–µ Python?"}],
            [{"role": "user", "content": "–ß—Ç–æ —Ç–∞–∫–æ–µ JavaScript?"}],
            [{"role": "user", "content": "–ß—Ç–æ —Ç–∞–∫–æ–µ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ?"}]
        ]

        async def process_request(request_id: int, messages: list):
            """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞."""
            print(f"\n–ó–∞–ø—Ä–æ—Å {request_id + 1} –Ω–∞—á–∞—Ç...")

            start_time = time.time()
            chunks = []

            async for chunk in client.chat_completion_stream(messages):
                chunks.append(chunk)

            elapsed_time = time.time() - start_time
            response = "".join(chunks)

            print(f"–ó–∞–ø—Ä–æ—Å {request_id + 1} –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {elapsed_time:.2f}s")
            print(f"–û—Ç–≤–µ—Ç {request_id + 1}: {response[:100]}...")

            return {
                "request_id": request_id + 1,
                "response": response,
                "elapsed_time": elapsed_time,
                "chunks_count": len(chunks)
            }

        print("–ó–∞–ø—É—Å–∫ 3 –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤...")
        start_time = time.time()

        # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å—ã –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        results = await asyncio.gather(
            *[process_request(i, req) for i, req in enumerate(requests)],
            return_exceptions=True
        )

        total_time = time.time() - start_time

        print(f"\n–í—Å–µ –∑–∞–ø—Ä–æ—Å—ã –∑–∞–≤–µ—Ä—à–µ–Ω—ã –∑–∞ {total_time:.2f}s")

        # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        successful_results = [
            r for r in results if not isinstance(r, Exception)]
        failed_results = [r for r in results if isinstance(r, Exception)]

        print(f"–£—Å–ø–µ—à–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {len(successful_results)}")
        print(f"–ù–µ—É–¥–∞—á–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {len(failed_results)}")

        if successful_results:
            avg_time = sum(r["elapsed_time"]
                           for r in successful_results) / len(successful_results)
            print(f"–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {avg_time:.2f}s")


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è StreamingLLMClient."""
    print("üöÄ –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è StreamingLLMClient")
    print("=" * 60)

    try:
        # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã
        await basic_streaming_example()
        await aggregated_streaming_example()

        # Function –∏ Tool calling (–∏–º–∏—Ç–∞—Ü–∏—è, —Ç–∞–∫ –∫–∞–∫ API –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç)
        await function_calling_streaming_example()
        await tool_calling_streaming_example()

        # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        await performance_monitoring_example()

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫
        await error_handling_example()

        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        await concurrent_requests_example()

        print("\n‚úÖ –í—Å–µ –ø—Ä–∏–º–µ—Ä—ã –≤—ã–ø–æ–ª–Ω–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!")

    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"\n‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {type(e).__name__}: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫ –ø—Ä–∏–º–µ—Ä–æ–≤
    asyncio.run(main())

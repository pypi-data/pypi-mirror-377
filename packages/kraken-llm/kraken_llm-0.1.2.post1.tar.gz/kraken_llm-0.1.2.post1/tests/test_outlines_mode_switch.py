#!/usr/bin/env python3
"""
–¢–µ—Å—Ç –ø–µ—Ä–µ–∫–ª—é—á–∞—Ç–µ–ª—è outlines_so_mode –∏ –¥–æ—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–π Outlines –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏.
"""

import asyncio
import os
from pydantic import BaseModel, Field

from kraken_llm.config.settings import LLMConfig
from kraken_llm.client.structured import StructuredLLMClient

from dotenv import load_dotenv

load_dotenv()


class SimpleModel(BaseModel):
    """–ü—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    name: str = Field(..., description="–ò–º—è")
    value: int = Field(..., description="–ß–∏—Å–ª–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ")
    active: bool = Field(True, description="–ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å")


async def test_outlines_mode_disabled():
    """–¢–µ—Å—Ç —Å –æ—Ç–∫–ª—é—á–µ–Ω–Ω—ã–º Outlines —Ä–µ–∂–∏–º–æ–º (OpenAI —Ä–µ–∂–∏–º)"""
    print("=== –¢–µ—Å—Ç —Å outlines_so_mode = False (OpenAI —Ä–µ–∂–∏–º) ===")

    config = LLMConfig(
        endpoint=os.getenv("LLM_ENDPOINT"),
        api_key=os.getenv("LLM_TOKEN"),
        model=os.getenv("LLM_MODEL")
    )
    config.outlines_so_mode = False  # –û—Ç–∫–ª—é—á–∞–µ–º Outlines

    client = StructuredLLMClient(config)

    messages = [{
        "role": "user",
        "content": "–°–æ–∑–¥–∞–π JSON: name='OpenAI Test', value=100, active=true"
    }]

    try:
        result = await client.chat_completion_structured(
            messages=messages,
            response_model=SimpleModel,
            max_tokens=150,
            temperature=0.1
        )

        print(f"‚úÖ OpenAI —Ä–µ–∂–∏–º —Ä–∞–±–æ—Ç–∞–µ—Ç: {result}")
        print(f"   –¢–∏–ø: {type(result)}")
        print(
            f"   –î–∞–Ω–Ω—ã–µ: name='{result.name}', value={result.value}, active={result.active}")
        return True

    except Exception as e:
        print(f"‚ùå OpenAI —Ä–µ–∂–∏–º –æ—à–∏–±–∫–∞: {e}")
        return False


async def test_outlines_mode_enabled():
    """–¢–µ—Å—Ç —Å –≤–∫–ª—é—á–µ–Ω–Ω—ã–º Outlines —Ä–µ–∂–∏–º–æ–º"""
    print("\n=== –¢–µ—Å—Ç —Å outlines_so_mode = True (Outlines —Ä–µ–∂–∏–º) ===")

    config = LLMConfig(
        endpoint=os.getenv("LLM_ENDPOINT"),
        api_key=os.getenv("LLM_TOKEN"),
        model=os.getenv("LLM_MODEL")
    )
    config.outlines_so_mode = True  # –í–∫–ª—é—á–∞–µ–º Outlines

    client = StructuredLLMClient(config)

    messages = [{
        "role": "user",
        "content": "–°–æ–∑–¥–∞–π JSON: name='Outlines Test', value=200, active=false"
    }]

    try:
        result = await client.chat_completion_structured(
            messages=messages,
            response_model=SimpleModel,
            max_tokens=150,
            temperature=0.1
        )

        print(f"‚úÖ Outlines —Ä–µ–∂–∏–º —Ä–∞–±–æ—Ç–∞–µ—Ç: {result}")
        print(f"   –¢–∏–ø: {type(result)}")
        print(
            f"   –î–∞–Ω–Ω—ã–µ: name='{result.name}', value={result.value}, active={result.active}")
        return True

    except Exception as e:
        print(f"‚ùå Outlines —Ä–µ–∂–∏–º –æ—à–∏–±–∫–∞: {e}")
        return False


async def test_outlines_streaming():
    """–¢–µ—Å—Ç Outlines –≤ streaming —Ä–µ–∂–∏–º–µ"""
    print("\n=== –¢–µ—Å—Ç Outlines Streaming ===")

    config = LLMConfig(
        endpoint=os.getenv("LLM_ENDPOINT"),
        api_key=os.getenv("LLM_TOKEN"),
        model=os.getenv("LLM_MODEL")
    )
    config.outlines_so_mode = True

    client = StructuredLLMClient(config)

    messages = [{
        "role": "user",
        "content": "–°–æ–∑–¥–∞–π JSON: name='Streaming Test', value=300, active=true"
    }]

    try:
        result = await client.chat_completion_structured(
            messages=messages,
            response_model=SimpleModel,
            stream=True,
            max_tokens=150,
            temperature=0.1
        )

        print(f"‚úÖ Outlines streaming —Ä–∞–±–æ—Ç–∞–µ—Ç: {result}")
        print(f"   –¢–∏–ø: {type(result)}")
        return True

    except Exception as e:
        print(f"‚ùå Outlines streaming –æ—à–∏–±–∫–∞: {e}")
        return False


async def test_direct_outlines_methods():
    """–¢–µ—Å—Ç –ø—Ä—è–º—ã—Ö –≤—ã–∑–æ–≤–æ–≤ Outlines –º–µ—Ç–æ–¥–æ–≤"""
    print("\n=== –¢–µ—Å—Ç –ø—Ä—è–º—ã—Ö –≤—ã–∑–æ–≤–æ–≤ Outlines –º–µ—Ç–æ–¥–æ–≤ ===")

    config = LLMConfig(
        endpoint=os.getenv("LLM_ENDPOINT"),
        api_key=os.getenv("LLM_TOKEN"),
        model=os.getenv("LLM_MODEL")
    )
    client = StructuredLLMClient(config)

    messages = [{
        "role": "user",
        "content": "–°–æ–∑–¥–∞–π JSON: name='Direct Test', value=400, active=false"
    }]

    # –¢–µ—Å—Ç non-streaming –º–µ—Ç–æ–¥–∞
    print("\n1. –ü—Ä—è–º–æ–π –≤—ã–∑–æ–≤ _structured_non_stream_outlines:")
    try:
        result = await client._structured_non_stream_outlines(
            messages=messages,
            response_model=SimpleModel,
            temperature=0.1,
            max_tokens=150
        )

        print(f"‚úÖ –ü—Ä—è–º–æ–π Outlines non-streaming: {result}")

    except Exception as e:
        print(f"‚ùå –ü—Ä—è–º–æ–π Outlines non-streaming –æ—à–∏–±–∫–∞: {e}")

    # –¢–µ—Å—Ç streaming –º–µ—Ç–æ–¥–∞
    print("\n2. –ü—Ä—è–º–æ–π –≤—ã–∑–æ–≤ _structured_stream_outlines:")
    try:
        result = await client._structured_stream_outlines(
            messages=messages,
            response_model=SimpleModel,
            temperature=0.1,
            max_tokens=150
        )

        print(f"‚úÖ –ü—Ä—è–º–æ–π Outlines streaming: {result}")

    except Exception as e:
        print(f"‚ùå –ü—Ä—è–º–æ–π Outlines streaming –æ—à–∏–±–∫–∞: {e}")


async def test_mode_switching():
    """–¢–µ—Å—Ç –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è –º–µ–∂–¥—É —Ä–µ–∂–∏–º–∞–º–∏"""
    print("\n=== –¢–µ—Å—Ç –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è —Ä–µ–∂–∏–º–æ–≤ ===")

    base_config = LLMConfig(
        endpoint=os.getenv("LLM_ENDPOINT"),
        api_key=os.getenv("LLM_TOKEN"),
        model=os.getenv("LLM_MODEL")
    )

    messages = [{
        "role": "user",
        "content": "–°–æ–∑–¥–∞–π JSON: name='Switch Test', value=500, active=true"
    }]

    results = {}

    # –¢–µ—Å—Ç —Å —Ä–∞–∑–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ outlines_so_mode
    for mode_enabled in [False, True]:
        mode_name = "Outlines" if mode_enabled else "OpenAI"
        print(f"\n  –†–µ–∂–∏–º: {mode_name} (outlines_so_mode={mode_enabled})")

        config = LLMConfig(
            endpoint=base_config.endpoint,
            api_key=base_config.api_key,
            model=base_config.model
        )
        config.outlines_so_mode = mode_enabled

        client = StructuredLLMClient(config)

        try:
            import time
            start_time = time.time()

            result = await client.chat_completion_structured(
                messages=messages,
                response_model=SimpleModel,
                max_tokens=150,
                temperature=0.1
            )

            execution_time = time.time() - start_time

            print(f"    ‚úÖ –£—Å–ø–µ—Ö –∑–∞ {execution_time:.3f}s: {result}")
            results[mode_name] = {
                "success": True,
                "result": result,
                "time": execution_time
            }

        except Exception as e:
            print(f"    ‚ùå –û—à–∏–±–∫–∞: {e}")
            results[mode_name] = {
                "success": False,
                "error": str(e),
                "time": 0
            }

    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print(f"\nüìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∂–∏–º–æ–≤:")
    for mode_name, result in results.items():
        status = "‚úÖ" if result["success"] else "‚ùå"
        print(f"  {status} {mode_name}: {result.get('time', 0):.3f}s")
        if result["success"]:
            print(f"      –†–µ–∑—É–ª—å—Ç–∞—Ç: {result['result']}")

    return results


async def test_enhanced_prompts():
    """–¢–µ—Å—Ç —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤"""
    print("\n=== –¢–µ—Å—Ç —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤ ===")

    config = LLMConfig(
        endpoint=os.getenv("LLM_ENDPOINT"),
        api_key=os.getenv("LLM_TOKEN"),
        model=os.getenv("LLM_MODEL")
    )
    config.outlines_so_mode = True

    client = StructuredLLMClient(config)

    # –¢–µ—Å—Ç –º–µ—Ç–æ–¥–∞ —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤
    original_messages = [{
        "role": "user",
        "content": "–°–æ–∑–¥–∞–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —á–µ–ª–æ–≤–µ–∫–µ: –¢–µ—Å—Ç, 25 –ª–µ—Ç"
    }]

    enhanced_messages = client._enhance_messages_for_json(
        original_messages, SimpleModel)

    print(f"–ò—Å—Ö–æ–¥–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ: {original_messages[0]['content'][:50]}...")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–æ–±—â–µ–Ω–∏–π –ø–æ—Å–ª–µ —É–ª—É—á—à–µ–Ω–∏—è: {len(enhanced_messages)}")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–∏—Å—Ç–µ–º–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
    if enhanced_messages[0]["role"] == "system":
        print(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ —Å–∏—Å—Ç–µ–º–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ")
        print(
            f"   –°–æ–¥–µ—Ä–∂–∏—Ç —Å—Ö–µ–º—É: {'—Å—Ö–µ–º–∞' in enhanced_messages[0]['content'].lower()}")
        print(
            f"   –°–æ–¥–µ—Ä–∂–∏—Ç –ø—Ä–∏–º–µ—Ä: {'–ø—Ä–∏–º–µ—Ä' in enhanced_messages[0]['content'].lower()}")

    # –¢–µ—Å—Ç —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –ø—Ä–æ–º–ø—Ç–∞–º–∏
    try:
        result = await client.chat_completion_structured(
            messages=original_messages,
            response_model=SimpleModel,
            max_tokens=150,
            temperature=0.1
        )

        print(f"‚úÖ –£–ª—É—á—à–µ–Ω–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã —Ä–∞–±–æ—Ç–∞—é—Ç: {result}")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –ø—Ä–æ–º–ø—Ç–∞–º–∏: {e}")


async def test_json_extraction():
    """–¢–µ—Å—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏—è JSON –∏–∑ –æ—Ç–≤–µ—Ç–æ–≤"""
    print("\n=== –¢–µ—Å—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏—è JSON ===")

    config = LLMConfig(
        endpoint=os.getenv("LLM_ENDPOINT"),
        api_key=os.getenv("LLM_TOKEN"),
        model=os.getenv("LLM_MODEL")
    )
    client = StructuredLLMClient(config)

    # –¢–µ—Å—Ç–æ–≤—ã–µ –æ—Ç–≤–µ—Ç—ã —Å —Ä–∞–∑–Ω—ã–º–∏ —Ñ–æ—Ä–º–∞—Ç–∞–º–∏
    test_responses = [
        '{"name": "Test", "value": 123, "active": true}',  # –ß–∏—Å—Ç—ã–π JSON
        # JSON –≤ —Ç–µ–∫—Å—Ç–µ
        '–í–æ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {"name": "Test", "value": 123, "active": true}',
        # JSON –≤ code block
        '```json\n{"name": "Test", "value": 123, "active": true}\n```',
        # JSON –≤ –æ–±—ã—á–Ω–æ–º block
        '```\n{"name": "Test", "value": 123, "active": true}\n```',
        '–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç –±–µ–∑ JSON',  # –ë–µ–∑ JSON
    ]

    for i, response in enumerate(test_responses, 1):
        print(f"\n  –¢–µ—Å—Ç {i}: {response[:30]}...")

        extracted = client._extract_json_from_response(response)

        if extracted:
            print(f"    ‚úÖ JSON –∏–∑–≤–ª–µ—á–µ–Ω: {extracted}")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å
            try:
                import json
                parsed = json.loads(extracted)
                print(f"    ‚úÖ JSON –≤–∞–ª–∏–¥–µ–Ω: {list(parsed.keys())}")
            except json.JSONDecodeError:
                print(f"    ‚ùå JSON –Ω–µ–≤–∞–ª–∏–¥–µ–Ω")
        else:
            print(f"    ‚ùå JSON –Ω–µ –Ω–∞–π–¥–µ–Ω")


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    print("üß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï OUTLINES –ò–ù–¢–ï–ì–†–ê–¶–ò–ò –ò –ü–ï–†–ï–ö–õ–Æ–ß–ê–¢–ï–õ–Ø –†–ï–ñ–ò–ú–û–í")
    print("=" * 70)

    results = []

    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–µ—Ä–µ–∫–ª—é—á–∞—Ç–µ–ª—å —Ä–µ–∂–∏–º–æ–≤
    switch_results = await test_mode_switching()

    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Ä–µ–∂–∏–º—ã
    results.append(await test_outlines_mode_disabled())
    results.append(await test_outlines_mode_enabled())
    results.append(await test_outlines_streaming())

    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø—Ä—è–º—ã–µ –º–µ—Ç–æ–¥—ã
    await test_direct_outlines_methods()

    # –¢–µ—Å—Ç–∏—Ä—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã
    await test_enhanced_prompts()

    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ JSON
    await test_json_extraction()

    # –ü–æ–¥–≤–æ–¥–∏–º –∏—Ç–æ–≥–∏
    print("\n" + "=" * 70)
    print("üìä –ò–¢–û–ì–ò –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø")
    print("=" * 70)

    success_count = sum(results)
    total_tests = len(results)

    print(f"–ë–∞–∑–æ–≤—ã–µ —Ç–µ—Å—Ç—ã: {success_count}/{total_tests} —É—Å–ø–µ—à–Ω–æ")

    if switch_results:
        print(f"\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è —Ä–µ–∂–∏–º–æ–≤:")
        for mode, result in switch_results.items():
            status = "‚úÖ" if result["success"] else "‚ùå"
            print(f"  {status} {mode}: {result.get('time', 0):.3f}s")

    print(f"\nüéØ –í—ã–≤–æ–¥—ã:")
    if success_count == total_tests:
        print("- ‚úÖ –í—Å–µ —Ä–µ–∂–∏–º—ã —Ä–∞–±–æ—Ç–∞—é—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
    elif success_count > 0:
        print("- ‚ö†Ô∏è  –ù–µ–∫–æ—Ç–æ—Ä—ã–µ —Ä–µ–∂–∏–º—ã —Ä–∞–±–æ—Ç–∞—é—Ç, —Ç—Ä–µ–±—É–µ—Ç—Å—è –¥–æ—Ä–∞–±–æ—Ç–∫–∞")
    else:
        print("- ‚ùå –¢—Ä–µ–±—É–µ—Ç—Å—è —Å–µ—Ä—å–µ–∑–Ω–∞—è –¥–æ—Ä–∞–±–æ—Ç–∫–∞ Outlines –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏")

    print("- üîß –£–ª—É—á—à–µ–Ω–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã –ø–æ–º–æ–≥–∞—é—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ JSON")
    print("- üìä –ü–µ—Ä–µ–∫–ª—é—á–∞—Ç–µ–ª—å outlines_so_mode —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∏—Ä—É–µ—Ç")
    print("- üöÄ –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ –¥–∞–ª—å–Ω–µ–π—à–µ–º—É —Ä–∞–∑–≤–∏—Ç–∏—é")


if __name__ == "__main__":
    asyncio.run(main())

#!/usr/bin/env python3
"""
–¢–µ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤ —Ä–µ–∞–ª—å–Ω–æ–≥–æ streaming.
"""

import asyncio
import os
import time
from pydantic import BaseModel, Field
from typing import List

from kraken_llm.config.settings import LLMConfig
from kraken_llm.client.structured import StructuredLLMClient

from dotenv import load_dotenv

load_dotenv()

class DetailedProfile(BaseModel):
    """–î–µ—Ç–∞–ª—å–Ω—ã–π –ø—Ä–æ—Ñ–∏–ª—å –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö JSON"""
    name: str = Field(..., description="–ü–æ–ª–Ω–æ–µ –∏–º—è")
    age: int = Field(..., description="–í–æ–∑—Ä–∞—Å—Ç")
    email: str = Field("example@email.com", description="Email –∞–¥—Ä–µ—Å")
    phone: str = Field("+1234567890", description="–¢–µ–ª–µ—Ñ–æ–Ω")
    address: str = Field("Example Address", description="–ê–¥—Ä–µ—Å")
    city: str = Field(..., description="–ì–æ—Ä–æ–¥")
    country: str = Field("Russia", description="–°—Ç—Ä–∞–Ω–∞")
    occupation: str = Field(..., description="–ü—Ä–æ—Ñ–µ—Å—Å–∏—è")
    company: str = Field("Example Company", description="–ö–æ–º–ø–∞–Ω–∏—è")
    experience_years: int = Field(5, description="–õ–µ—Ç –æ–ø—ã—Ç–∞")
    skills: List[str] = Field(default_factory=lambda: ["Python", "JavaScript"], description="–ù–∞–≤—ã–∫–∏")
    education: str = Field("University", description="–û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ")
    languages: List[str] = Field(default_factory=lambda: ["Russian", "English"], description="–Ø–∑—ã–∫–∏")
    hobbies: List[str] = Field(default_factory=lambda: ["Programming", "Reading"], description="–•–æ–±–±–∏")
    bio: str = Field("Professional biography", description="–ë–∏–æ–≥—Ä–∞—Ñ–∏—è")
    active: bool = Field(True, description="–ê–∫—Ç–∏–≤–µ–Ω")


async def test_large_json_streaming():
    """–¢–µ—Å—Ç streaming —Å –±–æ–ª—å—à–∏–º JSON"""
    print("=== –¢–µ—Å—Ç streaming —Å –±–æ–ª—å—à–∏–º JSON ===")
    
    config = LLMConfig(
        endpoint=os.getenv("LLM_ENDPOINT"),
        api_key=os.getenv("LLM_TOKEN"),
        model=os.getenv("LLM_MODEL")
    )
    config.outlines_so_mode = True
    
    client = StructuredLLMClient(config)
    
    messages = [{
        "role": "user",
        "content": """–°–æ–∑–¥–∞–π –¥–µ—Ç–∞–ª—å–Ω—ã–π –ø—Ä–æ—Ñ–∏–ª—å –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç–∞:
        - –ò–º—è: –î–º–∏—Ç—Ä–∏–π –ü–µ—Ç—Ä–æ–≤
        - –í–æ–∑—Ä–∞—Å—Ç: 32 –≥–æ–¥–∞
        - –ì–æ—Ä–æ–¥: –ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫
        - –ü—Ä–æ—Ñ–µ—Å—Å–∏—è: Senior Python Developer
        - –ö–æ–º–ø–∞–Ω–∏—è: TechCorp
        - –û–ø—ã—Ç: 8 –ª–µ—Ç
        - –ù–∞–≤—ã–∫–∏: Python, Django, PostgreSQL, Docker, Kubernetes
        - –û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ: –ú–ì–£, —Ñ–∞–∫—É–ª—å—Ç–µ—Ç –í–ú–ö
        - –Ø–∑—ã–∫–∏: —Ä—É—Å—Å–∫–∏–π, –∞–Ω–≥–ª–∏–π—Å–∫–∏–π
        - –•–æ–±–±–∏: –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ, —á—Ç–µ–Ω–∏–µ, –ø—É—Ç–µ—à–µ—Å—Ç–≤–∏—è
        - –ü–æ–¥—Ä–æ–±–Ω–∞—è –±–∏–æ–≥—Ä–∞—Ñ–∏—è"""
    }]
    
    results = {}
    
    # Non-streaming —Ç–µ—Å—Ç
    try:
        start_time = time.time()
        
        result_non_stream = await client.chat_completion_structured(
            messages=messages,
            response_model=DetailedProfile,
            stream=False,
            max_tokens=800,
            temperature=0.3
        )
        
        non_stream_time = time.time() - start_time
        results["non_streaming"] = {
            "time": non_stream_time,
            "result": result_non_stream,
            "success": True
        }
        
        print(f"   Non-streaming: {non_stream_time:.3f}s")
        print(f"     –ò–º—è: {result_non_stream.name}")
        print(f"     –ù–∞–≤—ã–∫–∏: {len(result_non_stream.skills)} —à—Ç.")
        print(f"     –ë–∏–æ–≥—Ä–∞—Ñ–∏—è: {len(result_non_stream.bio)} —Å–∏–º–≤–æ–ª–æ–≤")
        
    except Exception as e:
        results["non_streaming"] = {"time": 0, "success": False, "error": str(e)}
        print(f"   Non-streaming: ‚ùå {e}")
    
    # Streaming —Ç–µ—Å—Ç
    try:
        start_time = time.time()
        
        result_stream = await client.chat_completion_structured(
            messages=messages,
            response_model=DetailedProfile,
            stream=True,
            max_tokens=800,
            temperature=0.3
        )
        
        stream_time = time.time() - start_time
        results["streaming"] = {
            "time": stream_time,
            "result": result_stream,
            "success": True
        }
        
        print(f"   Streaming: {stream_time:.3f}s")
        print(f"     –ò–º—è: {result_stream.name}")
        print(f"     –ù–∞–≤—ã–∫–∏: {len(result_stream.skills)} —à—Ç.")
        print(f"     –ë–∏–æ–≥—Ä–∞—Ñ–∏—è: {len(result_stream.bio)} —Å–∏–º–≤–æ–ª–æ–≤")
        
    except Exception as e:
        results["streaming"] = {"time": 0, "success": False, "error": str(e)}
        print(f"   Streaming: ‚ùå {e}")
    
    # –ê–Ω–∞–ª–∏–∑
    if results["non_streaming"]["success"] and results["streaming"]["success"]:
        time_diff = results["non_streaming"]["time"] - results["streaming"]["time"]
        improvement = (time_diff / results["non_streaming"]["time"]) * 100
        
        if time_diff > 0:
            print(f"   üöÄ Streaming –±—ã—Å—Ç—Ä–µ–µ –Ω–∞ {time_diff:.3f}s ({improvement:.1f}%)")
        else:
            print(f"   ‚ö†Ô∏è  Non-streaming –±—ã—Å—Ç—Ä–µ–µ –Ω–∞ {abs(time_diff):.3f}s")
    
    return results


async def test_concurrent_streaming():
    """–¢–µ—Å—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö streaming –∑–∞–ø—Ä–æ—Å–æ–≤"""
    print("\n=== –¢–µ—Å—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö streaming –∑–∞–ø—Ä–æ—Å–æ–≤ ===")
    
    config = LLMConfig(
        endpoint=os.getenv("LLM_ENDPOINT"),
        api_key=os.getenv("LLM_TOKEN"),
        model=os.getenv("LLM_MODEL")
    )
    config.outlines_so_mode = True
    
    client = StructuredLLMClient(config)
    
    # –°–æ–∑–¥–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
    requests = [
        {
            "messages": [{"role": "user", "content": f"–°–æ–∑–¥–∞–π –ø—Ä–æ—Ñ–∏–ª—å —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞ #{i+1}: –ò–≤–∞–Ω{i+1}, {25+i} –ª–µ—Ç, {['–ú–æ—Å–∫–≤–∞', '–°–ü–±', '–ö–∞–∑–∞–Ω—å'][i]} "}],
            "id": i+1
        }
        for i in range(3)
    ]
    
    async def single_request(req):
        start_time = time.time()
        try:
            result = await client.chat_completion_structured(
                messages=req["messages"],
                response_model=DetailedProfile,
                stream=True,
                max_tokens=600,
                temperature=0.4
            )
            
            execution_time = time.time() - start_time
            return {
                "id": req["id"],
                "time": execution_time,
                "result": result,
                "success": True
            }
        except Exception as e:
            execution_time = time.time() - start_time
            return {
                "id": req["id"],
                "time": execution_time,
                "error": str(e),
                "success": False
            }
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
    start_time = time.time()
    results = await asyncio.gather(*[single_request(req) for req in requests])
    total_time = time.time() - start_time
    
    print(f"   –û–±—â–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è 3 –∑–∞–ø—Ä–æ—Å–æ–≤: {total_time:.3f}s")
    
    successful_results = [r for r in results if r["success"]]
    failed_results = [r for r in results if not r["success"]]
    
    print(f"   –£—Å–ø–µ—à–Ω—ã—Ö: {len(successful_results)}/{len(results)}")
    
    for result in successful_results:
        print(f"     –ó–∞–ø—Ä–æ—Å #{result['id']}: {result['time']:.3f}s - {result['result'].name}")
    
    for result in failed_results:
        print(f"     –ó–∞–ø—Ä–æ—Å #{result['id']}: ‚ùå {result['error'][:50]}...")
    
    if successful_results:
        avg_time = sum(r["time"] for r in successful_results) / len(successful_results)
        print(f"   –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ –∑–∞–ø—Ä–æ—Å: {avg_time:.3f}s")
        
        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ–º
        sequential_time = sum(r["time"] for r in successful_results)
        speedup = sequential_time / total_time
        print(f"   –£—Å–∫–æ—Ä–µ–Ω–∏–µ –æ—Ç –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞: {speedup:.1f}x")
    
    return len(successful_results) == len(results)


async def test_streaming_early_termination():
    """–¢–µ—Å—Ç –¥–æ—Å—Ä–æ—á–Ω–æ–≥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è streaming –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –≤–∞–ª–∏–¥–Ω–æ–≥–æ JSON"""
    print("\n=== –¢–µ—Å—Ç –¥–æ—Å—Ä–æ—á–Ω–æ–≥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è streaming ===")
    
    config = LLMConfig(
        endpoint=os.getenv("LLM_ENDPOINT"),
        api_key=os.getenv("LLM_TOKEN"),
        model=os.getenv("LLM_MODEL")
    )
    config.outlines_so_mode = True
    
    client = StructuredLLMClient(config)
    
    # –ó–∞–ø—Ä–æ—Å, –∫–æ—Ç–æ—Ä—ã–π –¥–æ–ª–∂–µ–Ω –±—ã—Å—Ç—Ä–æ –¥–∞—Ç—å –≤–∞–ª–∏–¥–Ω—ã–π JSON
    messages = [{
        "role": "user",
        "content": "–°–æ–∑–¥–∞–π –ø—Ä–æ—Å—Ç–æ–π –ø—Ä–æ—Ñ–∏–ª—å: –ê–Ω–Ω–∞, 27 –ª–µ—Ç, –¥–∏–∑–∞–π–Ω–µ—Ä"
    }]
    
    try:
        start_time = time.time()
        
        result = await client.chat_completion_structured(
            messages=messages,
            response_model=DetailedProfile,
            stream=True,
            max_tokens=1000,  # –ë–æ–ª—å—à–æ–π –ª–∏–º–∏—Ç
            temperature=0.1   # –ù–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ—Å—Ç–∏
        )
        
        execution_time = time.time() - start_time
        
        print(f"   ‚úÖ –î–æ—Å—Ä–æ—á–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç–∞–µ—Ç: {execution_time:.3f}s")
        print(f"      –†–µ–∑—É–ª—å—Ç–∞—Ç: {result.name}, {result.age} –ª–µ—Ç")
        print(f"      –ù–∞–≤—ã–∫–∏: {result.skills}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–ª–Ω—ã–π
        required_fields = ["name", "age", "occupation", "skills"]
        complete = all(getattr(result, field, None) for field in required_fields)
        
        if complete:
            print(f"      ‚úÖ –í—Å–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è –∑–∞–ø–æ–ª–Ω–µ–Ω—ã")
        else:
            print(f"      ‚ö†Ô∏è  –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –ø–æ–ª—è –º–æ–≥—É—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º–∏")
        
        return True
        
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ –¥–æ—Å—Ä–æ—á–Ω–æ–≥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è: {e}")
        return False


async def test_streaming_robustness():
    """–¢–µ—Å—Ç —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ streaming –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º —Ñ–æ—Ä–º–∞—Ç–∞–º –æ—Ç–≤–µ—Ç–æ–≤"""
    print("\n=== –¢–µ—Å—Ç —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ streaming ===")
    
    config = LLMConfig(
        endpoint=os.getenv("LLM_ENDPOINT"),
        api_key=os.getenv("LLM_TOKEN"),
        model=os.getenv("LLM_MODEL")
    )
    config.outlines_so_mode = True
    
    client = StructuredLLMClient(config)
    
    # –†–∞–∑–ª–∏—á–Ω—ã–µ —Ç–∏–ø—ã –∑–∞–ø—Ä–æ—Å–æ–≤
    test_cases = [
        {
            "name": "–û–±—ã—á–Ω—ã–π –∑–∞–ø—Ä–æ—Å",
            "messages": [{"role": "user", "content": "–°–æ–∑–¥–∞–π –ø—Ä–æ—Ñ–∏–ª—å: –ü–µ—Ç—Ä, 30 –ª–µ—Ç, –º–µ–Ω–µ–¥–∂–µ—Ä"}],
            "expected_success": True
        },
        {
            "name": "–ó–∞–ø—Ä–æ—Å —Å —ç–º–æ–¥–∑–∏",
            "messages": [{"role": "user", "content": "–°–æ–∑–¥–∞–π –ø—Ä–æ—Ñ–∏–ª—å: –ú–∞—Ä–∏—è üë©‚Äçüíª, 28 –ª–µ—Ç, –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç"}],
            "expected_success": True
        },
        {
            "name": "–ó–∞–ø—Ä–æ—Å –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º",
            "messages": [{"role": "user", "content": "Create profile: John Smith, 35 years old, engineer"}],
            "expected_success": True
        }
    ]
    
    results = []
    
    for test_case in test_cases:
        print(f"   –¢–µ—Å—Ç: {test_case['name']}")
        
        try:
            start_time = time.time()
            
            result = await client.chat_completion_structured(
                messages=test_case["messages"],
                response_model=DetailedProfile,
                stream=True,
                max_tokens=600,
                temperature=0.3
            )
            
            execution_time = time.time() - start_time
            
            print(f"     ‚úÖ –£—Å–ø–µ—Ö –∑–∞ {execution_time:.3f}s: {result.name}")
            results.append(True)
            
        except Exception as e:
            print(f"     ‚ùå –û—à–∏–±–∫–∞: {str(e)[:50]}...")
            results.append(False)
    
    success_rate = sum(results) / len(results) * 100
    print(f"   –û–±—â–∏–π —É—Å–ø–µ—Ö: {sum(results)}/{len(results)} ({success_rate:.1f}%)")
    
    return success_rate >= 80  # 80% —É—Å–ø–µ—à–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤ —Å—á–∏—Ç–∞–µ–º —Ö–æ—Ä–æ—à–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    print("üöÄ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò –†–ï–ê–õ–¨–ù–û–ì–û STREAMING")
    print("=" * 65)
    
    results = []
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –±–æ–ª—å—à–∏–µ JSON
    large_json_results = await test_large_json_streaming()
    results.append(large_json_results["streaming"]["success"] if "streaming" in large_json_results else False)
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    results.append(await test_concurrent_streaming())
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –¥–æ—Å—Ä–æ—á–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ
    results.append(await test_streaming_early_termination())
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å
    results.append(await test_streaming_robustness())
    
    # –ü–æ–¥–≤–æ–¥–∏–º –∏—Ç–æ–≥–∏
    print("\n" + "=" * 65)
    print("üìä –ò–¢–û–ì–ò –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò")
    print("=" * 65)
    
    success_count = sum(results)
    total_tests = len(results)
    
    print(f"–¢–µ—Å—Ç—ã –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {success_count}/{total_tests} —É—Å–ø–µ—à–Ω–æ")
    
    # –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    if "streaming" in large_json_results and "non_streaming" in large_json_results:
        if large_json_results["streaming"]["success"] and large_json_results["non_streaming"]["success"]:
            stream_time = large_json_results["streaming"]["time"]
            non_stream_time = large_json_results["non_streaming"]["time"]
            
            if stream_time < non_stream_time:
                improvement = ((non_stream_time - stream_time) / non_stream_time) * 100
                print(f"\nüöÄ –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:")
                print(f"   Streaming –±—ã—Å—Ç—Ä–µ–µ –Ω–∞ {improvement:.1f}%")
                print(f"   –ê–±—Å–æ–ª—é—Ç–Ω–∞—è —Ä–∞–∑–Ω–∏—Ü–∞: {non_stream_time - stream_time:.3f}s")
    
    print(f"\nüéØ –ó–∞–∫–ª—é—á–µ–Ω–∏–µ:")
    if success_count == total_tests:
        print("- ‚úÖ –†–µ–∞–ª—å–Ω—ã–π streaming –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –æ—Ç–ª–∏—á–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å")
        print("- üöÄ –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è —Ä–∞–±–æ—Ç–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ")
        print("- üîÑ –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
        print("- ‚ö° –î–æ—Å—Ä–æ—á–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ —ç–∫–æ–Ω–æ–º–∏—Ç —Ä–µ—Å—É—Ä—Å—ã")
        print("- üõ°Ô∏è  –°–∏—Å—Ç–µ–º–∞ —É—Å—Ç–æ–π—á–∏–≤–∞ –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º –≤—Ö–æ–¥–Ω—ã–º –¥–∞–Ω–Ω—ã–º")
    elif success_count >= total_tests * 0.75:
        print("- ‚úÖ –†–µ–∞–ª—å–Ω—ã–π streaming —Ä–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ")
        print("- üîß –ï—Å—Ç—å –Ω–µ–±–æ–ª—å—à–∏–µ –æ–±–ª–∞—Å—Ç–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è")
        print("- üìà –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –æ–∂–∏–¥–∞–Ω–∏—è")
    else:
        print("- ‚ö†Ô∏è  –†–µ–∞–ª—å–Ω—ã–π streaming —Ç—Ä–µ–±—É–µ—Ç –¥–æ—Ä–∞–±–æ—Ç–∫–∏")
        print("- üîß –ù–µ–æ–±—Ö–æ–¥–∏–º–æ –∏—Å–ø—Ä–∞–≤–∏—Ç—å –≤—ã—è–≤–ª–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã")
    
    print("- üí° Streaming –æ—Å–æ–±–µ–Ω–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω –¥–ª—è –±–æ–ª—å—à–∏—Ö JSON")


if __name__ == "__main__":
    asyncio.run(main())
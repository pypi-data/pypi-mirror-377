# --------------------------------------------------------------------------------------------------------------------- #
# This YAML file configures the training of a 1 or 6-hourly state-in-state-out Crossformer model
# on NSF NCAR high-performance computing systems (casper.ucar.edu and derecho.hpc.ucar.edu).
# 
# The model is trained on 6-hourly ERA5 model-level data, incorporating key physical features:
# - Input: Model-level variables [U, V, T, Q], static variables [Z_GDS4_SFC, LSM], and dynamic forcing [tsi].
# - Output: 
#     - Model level: [U, V, T, Q]
#     - Single level: [SP, t2m]
#     - 500 hPa level: [U, V, T, Z, Q]
#
# This configuration is designed for fine-tuning and prediction tasks using advanced components, 
# such as spectral normalization, reflective padding, and tracer-specific postprocessing.
#
# Ensure compatibility with ERA5 data preprocessing workflows and metadata paths specified in this file.
# --------------------------------------------------------------------------------------------------------------------- #


# the location to save your workspace, it will have 
# (1) pbs script, (2) a copy of this config, (3) model weights, (4) training_log.csv
# if save_loc does not exist, it will be created automatically
save_loc: '/glade/work/schreck/repos/miles-credit-diffusion/results/'
seed: 1000

data:
    # upper-air variables
    variables: ['U','V','T','Q']
    save_loc: '/glade/derecho/scratch/ksha/CREDIT_data/ERA5_mlevel_cesm_stage1/all_in_one/ERA5_mlevel_cesm_6h_lev16_*.zarr'
    
    # surface variables
    surface_variables: ['SP','t2m','V500','U500','T500','Z500','Q500']
    save_loc_surface: '/glade/derecho/scratch/ksha/CREDIT_data/ERA5_mlevel_cesm_stage1/all_in_one/ERA5_mlevel_cesm_6h_lev16_*.zarr'
    
    # dynamic forcing variables
    dynamic_forcing_variables: ['tsi']
    save_loc_dynamic_forcing: '/glade/derecho/scratch/ksha/CREDIT_data/ERA5_mlevel_cesm_stage1/all_in_one/ERA5_mlevel_cesm_6h_lev16_*.zarr'
    
    # static variables
    static_variables: ['Z_GDS4_SFC','LSM'] 
    save_loc_static: '/glade/derecho/scratch/ksha/CREDIT_data/ERA5_mlevel_cesm_stage1/static/ERA5_mlevel_cesm_static.zarr'
    
    # mean / std path
    mean_path: '/glade/derecho/scratch/ksha/CREDIT_data/ERA5_mlevel_cesm_stage1/mean_std/mean_6h_1979_2018_cesm.nc'
    # regular z-score version
    # std_path: '/glade/derecho/scratch/ksha/CREDIT_data/ERA5_mlevel_cesm_stage1/mean_std/std_6h_1979_2018_cesm.nc'
    # residual norm version
    std_path: '/glade/derecho/scratch/ksha/CREDIT_data/ERA5_mlevel_cesm_stage1/mean_std/std_residual_6h_1979_2018_cesm.nc'
    
    # train / validation split
    train_years: [1979, 2018]
    valid_years: [2018, 2019]

    # data workflow
    scaler_type: 'std_new'

    # number of input states
    # FuXi has 2 input states
    history_len: 1
    valid_history_len: 1

    # number of forecast steps to compute loss
    # 0 for single step training / validation
    # larger than 0 for multi-step training / validation
    forecast_len: 0
    valid_forecast_len: 0

    # one_shot: True --> compute loss on the last forecast step only
    # one_shot: False --> compute loss on all forecast steps
    one_shot: False

    # 1 for hourly model
    lead_time_periods: 6

    # do not use skip_period
    skip_periods: null

    # compatible with the old 'std'
    static_first: True   

    # SST forcing  
    # sst_forcing:
    #     activate: False
    #     varname_skt: 'SKT'
    #     varname_ocean_mask: 'land_sea_CI_mask'

    # dataset type. For now you may choose from 
    # ERA5_and_Forcing_MultiStep MultiprocessingBatcherPrefetch ERA5_MultiStep_Batcher MultiprocessingBatcher
    # All of these support single or multi-step training (e.g. forecast lengths >= 0)
    dataset_type: ERA5_MultiStep_Batcher

    data_clamp: null

trainer:
    # Choose the training routine
    # This is deprecated as of credit-2.0
    # Use universal with all of the above dataset_types
    type: era5-diffusion

    # the keyword that controls GPU usage
    # fsdp: fully sharded data parallel
    # ddp: distributed data parallel
    # none: single-GPU training
    mode: ddp

    # fsdp-specific GPU optimization
    #
    # allow FSDP offloads gradients to CPU and free GPU memory
    # This can cause CPU out-of-memory if for some large models, do your diligence
    cpu_offload: False
    # save forward pass activation to checkpoints and free GPU memory
    activation_checkpoint: True

    # (optional) use torch.compile: False. May not be compatible with custom models
    compile: False

    # load existing weights / optimizer / mixed-precision grad scaler / learning rate scheduler state(s)
    # when starting multi-step training, only use reload weights initially, then set all to true
    load_weights: False
    load_optimizer: False
    load_scaler: False 
    load_scheduler: False

    # CREDIT save checkpoints at the end of every epoch
    # save_backup_weights will also save checkpoints in the beginning of every epoch
    save_backup_weights: True
    # save_best_weights will save the best checkpoint separatly based on validation loss
    # does not work if skip_validation: True
    save_best_weights: True

    save_every_epoch: True

    # Optional for user to control which variable metrics get saved to training_log.csv
    # Set to True to save metrics for all predicted variables
    # Set to ["Z500", "Q500", "Q", "T"] for example to save these variables (with levels in the case of Q and T)
    # Set to [] or None to save only bulk metrics averaged over all variables
    save_metric_vars: True
    
    # update learning rate to optimizer.param_groups
    # False if a scheduler is used
    update_learning_rate: False
    
    # learning rate
    learning_rate: 1.0e-03

    # L2 regularization on trained weights
    # weight_decay: 0 --> turn-off L2 reg
    weight_decay: 0

    # define training and validation batch size
    # for ddp and fsdp, actual batch size = batch_size * number of GPUs
    train_batch_size: 16
    valid_batch_size: 16

    # number of batches per epoch for training and validation
    batches_per_epoch: 0 # Set to 0 to use len(dataloader)
    valid_batches_per_epoch: 50
    # early stopping
    stopping_patience: 5000 
    # True: do not validate, always save weights
    skip_validation: False

    # total number of epochs
    start_epoch: 0
    # the trainer will stop after iterating a given number of epochs
    # this works for any start_epoch and reload_epoch: True
    num_epoch: 1
    # epcoh is saved in the checkpot.pt, this option reads the last epoch 
    # of the previous job and continue from there
    # useful for epoch-dependent schedulers
    reload_epoch: True # can also use --> train_one_epoch: True
    epochs: &epochs 100
    # if use_scheduler: False
    # reload_epoch: False
    # epochs: 20

    use_scheduler: True
    scheduler: {'scheduler_type': 'cosine-annealing', 'T_max': *epochs,  'last_epoch': -1}
    # scheduler:
    #   scheduler_type: cosine-annealing-restarts
    #   first_cycle_steps: 500
    #   cycle_mult: 6.0  # Multiplier for steps in subsequent cycles
    #   max_lr: 1.0e-03
    #   min_lr: 1.0e-09
    #   warmup_steps: 499
    #   gamma: 0.7  # LR reduction factor at each cycle restart

    # Pytorch automatic mixed precision (amp). See also 'mixed_precision' below
    amp: True
    
    # This version of mixed precision is supported only with FSDP and gives you more control. Set amp = False.
    # See https://pytorch.org/docs/stable/fsdp.html#torch.distributed.fsdp.MixedPrecision for details
    # Many different precision types are supported. See credit.mixed_precision
    # If commented out, the default is float32.
    mixed_precision: 
        param_dtype: "float32"
        reduce_dtype: "float32"
        buffer_dtype: "float32"

    # rescale loss as loss = loss / grad_accum_every
    grad_accum_every: 1

    # gradient clipping. Set to 0 to ignore
    grad_max_norm: 'dynamic'
    
    # number of CPU workers used in datasets/dataloaders
    thread_workers: 4
    valid_thread_workers: 4

    # Number of prefetch workers in the DataLoader
    # This only works with ERA5_MultiStep_Batcher or MultiprocessingBatcherPrefetch
    prefetch_factor: 2
  
model:
    # NCAR WXFormer example
    type: "crossformer-diffusion"
    frames: 1                         # number of input states (default: 1)
    image_height: 192                 # number of latitude grids (default: 640)
    image_width: 288                 # number of longitude grids (default: 1280)
    levels: 16                        # number of upper-air variable levels (default: 15)
    channels: 4                       # upper-air variable channels
    surface_channels: 7               # surface variable channels
    input_only_channels: 3            # dynamic forcing, forcing, static channels
    output_only_channels: 0           # diagnostic variable channels
    
    patch_width: 1                    # number of latitude grids in each 3D patch (default: 1)
    patch_height: 1                   # number of longitude grids in each 3D patch (default: 1)
    frame_patch_size: 1               # number of input states in each 3D patch (default: 1)
    
    dim: [256, 512, 1024, 2048]        # Dimensionality of each layer
    depth: [2, 2, 8, 2]               # Depth of each layer
    global_window_size: [8, 4, 2, 1] # Global window size for each layer
    local_window_size: 3             # Local window size
    cross_embed_kernel_sizes:         # kernel sizes for cross-embedding
    - [4, 8, 16, 32]
    - [2, 4]
    - [2, 4]
    - [2, 4]
    cross_embed_strides: [2, 2, 2, 2] # Strides for cross-embedding (default: [4, 2, 2, 2])
    attn_dropout: 0.                  # Dropout probability for attention layers (default: 0.0)
    ff_dropout: 0.                    # Dropout probability for feed-forward layers (default: 0.0)
    

    ##### diffusion settings: 
    diffusion:
        timesteps: 1000
        sampling_timesteps: 20
        objective: "pred_v"
        beta_schedule: "linear"
        noise_type: "normal"
        ddim_sampling_eta: 0.0
        auto_normalize: True
        offset_noise_strength: 0.0
        min_snr_loss_weight: False
        min_snr_gamma: 5.0
        immiscible: False
        self_condition: False
        condition: True
        image_size: [192, 288]

    # fuxi example
    # frames: 2               # number of input states
    # image_height: &height 640       # number of latitude grids
    # image_width: &width 1280       # number of longitude grids
    # levels: &levels 16              # number of upper-air variable levels
    # channels: 4             # upper-air variable channels
    # surface_channels: 7     # surface variable channels
    # input_only_channels: 3  # dynamic forcing, forcing, static channels
    # output_only_channels: 0 # diagnostic variable channels
    
    # # patchify layer
    # patch_height: 4         # number of latitude grids in each 3D patch
    # patch_width: 4          # number of longitude grids in each 3D patch
    # frame_patch_size: 2     # number of input states in each 3D patch
    
    # # hidden layers
    # dim: 1024               # dimension (default: 1536)
    # num_groups: 32          # number of groups (default: 32)
    # num_heads: 8            # number of heads (default: 8)
    # window_size: 7          # window size (default: 7)
    # depth: 16               # number of swin transformers (default: 48)

    # for BOTH fuxi and crossformer 
    # use spectral norm
    use_spectral_norm: True
    
    # use interpolation to match the output size
    interp: True

    # map boundary padding
    padding_conf:
        activate: True
        mode: earth
        pad_lat: 48
        pad_lon: 48

    post_conf: 
        activate: True
        
        tracer_fixer:
            activate: True
            denorm: True
            tracer_name: ['Q', 'Q500']
            tracer_thres: [1e-8, 1e-8]

    # # configuration for postblock processing
    # post_conf: 
    #     activate: False


    #     # this scaling maps your variables to the ERA5 units 
    #     # alot of it is painfully redundant because we turn around
    #     # and move Joules -> Watts anyway... but so it goes for generalization. 
    #     # !!!!!make sure to adjust if your timestep is not 6 hours!!!!!
    #     requires_scaling: False
    #     scaling_coefs:
    #         tot_precip: 1 #model timestep in seconds m/day -> m/s
    #         surf_net_solar: 1 #model timestep in seconds (W/s) -> J/s
    #         surf_net_therm: 1 #model timestep in seconds (W/s) -> J/s
    #         surf_shflx: 1 #model timestep in seconds (W/s) -> J/s
    #         surf_lhflx: 1 #model timestep in seconds (W/s) -> J/s
    #         top_net_solar: 1 #model timestep in seconds (W/s) -> J/s
    #         top_net_therm: 1 #model timestep in seconds (W/s) -> J/s
    #         evap: 1 #model timestep / 1000 [kg/m2/s -> m] water equivalent 
    #         SP: 1
    #         U: 1
    #         V: 1
    #         gph_surf: 1
    #         temp: 1
    #         Q: 1
    #         T: 1
            
        
    #     skebs:
    #         activate: False
        
    #     tracer_fixer:
    #         activate: True
    #         denorm: True
    #         tracer_name: ['Qtot','PRECT','U10','CLDTOT','CLDHGH','CLDLOW','CLDMED']
    #         tracer_thres: [0, 0, 0, 0, 0, 0, 0]
            
    #     global_mass_fixer:
    #         activate: True
    #         activate_outside_model: True
    #         fix_level_num: 14
    #         simple_demo: False
    #         denorm: True
    #         grid_type: 'sigma'
    #         midpoint: True
    #         lon_lat_level_name: ['lon2d', 'lat2d', 'hyai', 'hybi']
    #         surface_pressure_name: ['PS']
    #         specific_total_water_name: ['Qtot']

    #     global_water_fixer:
    #         activate: True
    #         activate_outside_model: True
    #         simple_demo: False
    #         denorm: True
    #         grid_type: 'sigma'
    #         midpoint: True
    #         lon_lat_level_name: ['lon2d', 'lat2d', 'hyai', 'hybi']
    #         surface_pressure_name: ['PS']
    #         specific_total_water_name: ['Qtot']
    #         precipitation_name: ['PRECT']
    #         evaporation_name: ['QFLX']
            
    #     global_energy_fixer:
    #         activate: True
    #         activate_outside_model: True
    #         simple_demo: False
    #         denorm: True
    #         grid_type: 'sigma'
    #         midpoint: True
    #         lon_lat_level_name: ['lon2d', 'lat2d', 'hyai', 'hybi']
    #         surface_pressure_name: ['PS']
    #         air_temperature_name: ['T']
    #         specific_total_water_name: ['Qtot']
    #         u_wind_name: ['U']
    #         v_wind_name: ['V']
    #         surface_geopotential_name: ['PHIS']
    #         TOA_net_radiation_flux_name: ['FSNT', 'FLNT']
    #         surface_net_radiation_flux_name: ['FSNS', 'FLNS']
    #         surface_energy_flux_name: ['SHFLX', 'LHFLX',]

loss: 
    # the main training loss
    # available options are "mse", "msle", "mae", "huber", "logcosh", "xtanh", "xsigmoid"
    training_loss: "mse"

    # use power or spectral loss
    # if True, this loss will be added to the training_loss: 
    #     total_loss = training_loss + spectral_lambda_reg * power_loss (or spectral_loss)
    #
    # it is preferred that power loss and spectral loss are NOT applied at the same time
    use_power_loss: False
    use_spectral_loss: False # if power_loss is on, turn off spectral_loss, vice versa
    # rescale power or spectral loss when added to the total loss
    spectral_lambda_reg: 0.1 
    # truncate small wavenumber (large wavelength) in power or spectral loss
    spectral_wavenum_init: 20
    
    # this file is MANDATORY for the "predict" section below (inference stage)
    # the file must be nc and must contain 1D variables named "latitude" and "longitude"
    latitude_weights: '/glade/derecho/scratch/ksha/CREDIT_data/ERA5_mlevel_cesm_stage1/static/ERA5_mlevel_cesm_static.zarr'
    # latitude_weights: "/glade/campaign/cisl/aiml/wchapman/MLWPS/STAGING/f.e21.CREDIT_climate.statics_1.0deg_32levs_latlon_F32_hyai_fixed.nc"

    # use latitude weighting
    # if True, the latitude_weights file MUST have a variable named "coslat"
    # coslat is the np.cos(latitude)
    use_latitude_weights: True

    # variable weighting
    # if True, specify your weights
    use_variable_weights: False
    # # an example
    # variable_weights:
    #     U: [0.132, 0.123, 0.113, 0.104, 0.095, 0.085, 0.076, 0.067, 0.057, 0.048, 0.039, 0.029, 0.02 , 0.011, 0.005]
    #     V: [0.132, 0.123, 0.113, 0.104, 0.095, 0.085, 0.076, 0.067, 0.057, 0.048, 0.039, 0.029, 0.02 , 0.011, 0.005]
    #     T: [0.132, 0.123, 0.113, 0.104, 0.095, 0.085, 0.076, 0.067, 0.057, 0.048, 0.039, 0.029, 0.02 , 0.011, 0.005]
    #     Q: [0.132, 0.123, 0.113, 0.104, 0.095, 0.085, 0.076, 0.067, 0.057, 0.048, 0.039, 0.029, 0.02 , 0.011, 0.005]
    #     SP: 0.1
    #     t2m: 1.0
    #     V500: 0.1
    #     U500: 0.1
    #     T500: 0.1
    #     Z500: 0.1
    #     Q500: 0.1

    
predict:
    # You have the choice to run rollout_metrics.py, which will only save various metrics like RMSE, etc.
    # A directory called metrics will be added to the save_forecast field below.
    # To instead save the states to file (netcdf), run rollout_to_metrics.py. You will need to post-process
    # all the data with this option. A directory called netcdf will be added to the save_forecast field
    

    # the keyword that controls GPU usage
    # fsdp: fully sharded data parallel
    # ddp: distributed data parallel
    # none: single-GPU training
    mode: none

    # Set the batch_size for the prediction inference mode (default is 1)
    batch_size: 1

    forecasts:
        type: "custom"       # keep it as "custom". See also credit.forecast
        start_year: 1980     # year of the first initialization (where rollout will start)
        start_month: 1       # month of the first initialization
        start_day: 1         # day of the first initialization
        start_hours: [0]     # hour-of-day for each initialization, 0 for 00Z, 12 for 12Z
        duration: 1          # number of days to initialize, starting from the (year, mon, day) above
                             # duration should be divisible by the number of GPUs 
                             # (e.g., duration: 384 for 365-day rollout using 32 GPUs)
        days:  5             # forecast lead time as days (1 means 24-hour forecast)
        
    # saved variables (rollout_to_netcdf.yml)
    # save_vars: [] will save ALL variables
    # remove save_vars from config will save ALL variables
    metadata: '/glade/work/schreck/repos/credit/miles-credit/metadata/era5.yaml'
    
    # The location to store rollout predictions, the folder structure will be:
    # $save_forecast/$initialization_time/file.nc
    # each forecast lead time produces a file.nc
    save_forecast: '/glade/derecho/scratch/wchapman/CREDIT/wxformer_1deg_SpatPS/No_Post'
    
    # this keyword will apply a low-pass filter to all fields and each forecast step
    # it will reduce high-freq noise, but may impact performance
    use_laplace_filter: False
    
    # climatology file to be used with rollout_metrics.py. 
    # Supplying this will compute ananomly ACC, otherwise its Pearson coeffcient (acc in metrics).
    climatology: '/glade/campaign/cisl/aiml/ksha/CREDIT_arXiv/VERIF/verif_6h/ERA5_clim/ERA5_clim_1990_2019_6h_interp.nc'

    save_vars: ['PRECT','CLDTOT','CLDHGH','CLDLOW','CLDMED','TAUX','TAUY','U10','QFLX','FSNS','FLNS','FSNT','FLNT','SHFLX','LHFLX','TREFHT','PS','Qtot','U','V','T']
    forcing_file: '/glade/campaign/cisl/aiml/wchapman/MLWPS/STAGING/f.e21.CREDIT_climate.climaterun.sst.statics_1.0deg_F32.nc'
    climate_timesteps: 14600

    
    run_fast_climate: False
    init_cond_fast_climate: '/glade/derecho/scratch/wchapman/CREDIT_runs/wxformer_1deg_cesm_data_nopost_bigbig_SSTforced_DryWaterEnergy_SpatPS/init_times/init_condition_tensor_1980-01-01T00Z.pth'
    inds_rmse_fast_climate: ['rmse_PRECT','rmse_TREFHT']
    seasonal_mean_fast_climate: '/glade/work/wchapman/miles_branchs/CESM_physics/true_mean_01_wxformer_10years_01_01_00Z_tensor.pth'
    timesteps_fast_climate: 14600
    
    # deprecated
    # save_format: "nc"

    # Optional: Interpolate data field from hybrid sigma-pressure vertical coordinates to pressure levels.
    # Used with rollout_to_netcdf.py

    # interp_pressure:
    #     pressure_levels: [300.0, 500.0, 850.0, 925.0]

    # ua_var_encoding:
    #     zlib: True
    #     complevel: 1
    #     shuffle: True
    #     chunksizes: [1, *levels, *height, *width]

    # pressure_var_encoding:
    #     zlib: True
    #     complevel: 1
    #     shuffle: True
    #     chunksizes: [ 1, 4, *height, *width]

    # surface_var_encoding:
    #     zlib: true
    #     complevel: 1
    #     shuffle: True
    #     chunksizes: [1, *height, *width]


# credit.pbs supports NSF NCAR HPCs (casper.ucar.edu and derecho.hpc.ucar.edu)
pbs: #derecho
    conda: "/glade/work/wchapman/conda-envs/credit-derecho"
    project: "NAML0001"
    job_name: "ps_wxformer_CESM_6h"
    walltime: "12:00:00"
    nodes: 8
    ncpus: 64
    ngpus: 4
    mem: '480GB'
    queue: 'main'
    
    # examaple for casper
    # conda: "credit"
    # job_name: 'train_model'
    # nodes: 1
    # ncpus: 8
    # ngpus: 1
    # mem: '128GB'
    # walltime: '4:00:00'
    # gpu_type: 'v100'
    # project: 'NRIS0001'
    # queue: 'casper'

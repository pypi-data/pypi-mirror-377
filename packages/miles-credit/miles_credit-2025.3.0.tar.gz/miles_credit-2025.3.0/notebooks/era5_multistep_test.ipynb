{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b573c33-7046-4d9f-9a0f-cbc44e562adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import glob\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from credit.data import *\n",
    "from credit.transforms import load_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "836fdbf5-906a-4b80-afab-02e96aabc02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    \"/glade/derecho/scratch/schreck/repos/miles-credit/production/multistep/wxformer_6h/model.yml\"\n",
    ") as cf:\n",
    "    conf = yaml.load(cf, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86dbc422-abae-40a2-ac72-a87f37eab616",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ERA_files = sorted(glob.glob(conf[\"data\"][\"save_loc\"]))\n",
    "\n",
    "# <------------------------------------------ std_new\n",
    "if conf[\"data\"][\"scaler_type\"] == \"std_new\":\n",
    "    # check and glob surface files\n",
    "    if (\"surface_variables\" in conf[\"data\"]) and (\n",
    "        len(conf[\"data\"][\"surface_variables\"]) > 0\n",
    "    ):\n",
    "        surface_files = sorted(glob.glob(conf[\"data\"][\"save_loc_surface\"]))\n",
    "\n",
    "    else:\n",
    "        surface_files = None\n",
    "\n",
    "    # check and glob dyn forcing files\n",
    "    if (\"dynamic_forcing_variables\" in conf[\"data\"]) and (\n",
    "        len(conf[\"data\"][\"dynamic_forcing_variables\"]) > 0\n",
    "    ):\n",
    "        dyn_forcing_files = sorted(glob.glob(conf[\"data\"][\"save_loc_dynamic_forcing\"]))\n",
    "\n",
    "    else:\n",
    "        dyn_forcing_files = None\n",
    "\n",
    "    # check and glob diagnostic files\n",
    "    if (\"diagnostic_variables\" in conf[\"data\"]) and (\n",
    "        len(conf[\"data\"][\"diagnostic_variables\"]) > 0\n",
    "    ):\n",
    "        diagnostic_files = sorted(glob.glob(conf[\"data\"][\"save_loc_diagnostic\"]))\n",
    "\n",
    "    else:\n",
    "        diagnostic_files = None\n",
    "\n",
    "# -------------------------------------------------- #\n",
    "# import training / validation years from conf\n",
    "\n",
    "if \"train_years\" in conf[\"data\"]:\n",
    "    train_years_range = conf[\"data\"][\"train_years\"]\n",
    "else:\n",
    "    train_years_range = [1979, 2014]\n",
    "\n",
    "if \"valid_years\" in conf[\"data\"]:\n",
    "    valid_years_range = conf[\"data\"][\"valid_years\"]\n",
    "else:\n",
    "    valid_years_range = [2014, 2018]\n",
    "\n",
    "# convert year info to str for file name search\n",
    "train_years = [str(year) for year in range(train_years_range[0], train_years_range[1])]\n",
    "valid_years = [str(year) for year in range(valid_years_range[0], valid_years_range[1])]\n",
    "\n",
    "# Filter the files for training / validation\n",
    "train_files = [\n",
    "    file for file in all_ERA_files if any(year in file for year in train_years)\n",
    "]\n",
    "valid_files = [\n",
    "    file for file in all_ERA_files if any(year in file for year in valid_years)\n",
    "]\n",
    "\n",
    "# <----------------------------------- std_new\n",
    "if conf[\"data\"][\"scaler_type\"] == \"std_new\":\n",
    "    if surface_files is not None:\n",
    "        train_surface_files = [\n",
    "            file for file in surface_files if any(year in file for year in train_years)\n",
    "        ]\n",
    "        valid_surface_files = [\n",
    "            file for file in surface_files if any(year in file for year in valid_years)\n",
    "        ]\n",
    "\n",
    "        # ---------------------------- #\n",
    "        # check total number of files\n",
    "        assert (\n",
    "            len(train_surface_files) == len(train_files)\n",
    "        ), \"Mismatch between the total number of training set [surface files] and [upper-air files]\"\n",
    "        assert (\n",
    "            len(valid_surface_files) == len(valid_files)\n",
    "        ), \"Mismatch between the total number of validation set [surface files] and [upper-air files]\"\n",
    "\n",
    "    else:\n",
    "        train_surface_files = None\n",
    "        valid_surface_files = None\n",
    "\n",
    "    if dyn_forcing_files is not None:\n",
    "        train_dyn_forcing_files = [\n",
    "            file\n",
    "            for file in dyn_forcing_files\n",
    "            if any(year in file for year in train_years)\n",
    "        ]\n",
    "        valid_dyn_forcing_files = [\n",
    "            file\n",
    "            for file in dyn_forcing_files\n",
    "            if any(year in file for year in valid_years)\n",
    "        ]\n",
    "\n",
    "        # ---------------------------- #\n",
    "        # check total number of files\n",
    "        assert (\n",
    "            len(train_dyn_forcing_files) == len(train_files)\n",
    "        ), \"Mismatch between the total number of training set [dynamic forcing files] and [upper-air files]\"\n",
    "        assert (\n",
    "            len(valid_dyn_forcing_files) == len(valid_files)\n",
    "        ), \"Mismatch between the total number of validation set [dynamic forcing files] and [upper-air files]\"\n",
    "\n",
    "    else:\n",
    "        train_dyn_forcing_files = None\n",
    "        valid_dyn_forcing_files = None\n",
    "\n",
    "    if diagnostic_files is not None:\n",
    "        train_diagnostic_files = [\n",
    "            file\n",
    "            for file in diagnostic_files\n",
    "            if any(year in file for year in train_years)\n",
    "        ]\n",
    "        valid_diagnostic_files = [\n",
    "            file\n",
    "            for file in diagnostic_files\n",
    "            if any(year in file for year in valid_years)\n",
    "        ]\n",
    "\n",
    "        # ---------------------------- #\n",
    "        # check total number of files\n",
    "        assert (\n",
    "            len(train_diagnostic_files) == len(train_files)\n",
    "        ), \"Mismatch between the total number of training set [diagnostic files] and [upper-air files]\"\n",
    "        assert (\n",
    "            len(valid_diagnostic_files) == len(valid_files)\n",
    "        ), \"Mismatch between the total number of validation set [diagnostic files] and [upper-air files]\"\n",
    "\n",
    "    else:\n",
    "        train_diagnostic_files = None\n",
    "        valid_diagnostic_files = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cb14e2b-f1a7-460b-b8dd-599decad7d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert $USER to the actual user name\n",
    "conf[\"save_loc\"] = os.path.expandvars(conf[\"save_loc\"])\n",
    "\n",
    "# ======================================================== #\n",
    "# parse intputs\n",
    "\n",
    "# upper air variables\n",
    "varname_upper_air = conf[\"data\"][\"variables\"]\n",
    "\n",
    "if (\"forcing_variables\" in conf[\"data\"]) and (\n",
    "    len(conf[\"data\"][\"forcing_variables\"]) > 0\n",
    "):\n",
    "    forcing_files = conf[\"data\"][\"save_loc_forcing\"]\n",
    "    varname_forcing = conf[\"data\"][\"forcing_variables\"]\n",
    "else:\n",
    "    forcing_files = None\n",
    "    varname_forcing = None\n",
    "\n",
    "if (\"static_variables\" in conf[\"data\"]) and (len(conf[\"data\"][\"static_variables\"]) > 0):\n",
    "    static_files = conf[\"data\"][\"save_loc_static\"]\n",
    "    varname_static = conf[\"data\"][\"static_variables\"]\n",
    "else:\n",
    "    static_files = None\n",
    "    varname_static = None\n",
    "\n",
    "# get surface variable names\n",
    "if surface_files is not None:\n",
    "    varname_surface = conf[\"data\"][\"surface_variables\"]\n",
    "else:\n",
    "    varname_surface = None\n",
    "\n",
    "# get dynamic forcing variable names\n",
    "if dyn_forcing_files is not None:\n",
    "    varname_dyn_forcing = conf[\"data\"][\"dynamic_forcing_variables\"]\n",
    "else:\n",
    "    varname_dyn_forcing = None\n",
    "\n",
    "# get diagnostic variable names\n",
    "if diagnostic_files is not None:\n",
    "    varname_diagnostic = conf[\"data\"][\"diagnostic_variables\"]\n",
    "else:\n",
    "    varname_diagnostic = None\n",
    "\n",
    "# number of previous lead time inputs\n",
    "history_len = conf[\"data\"][\"history_len\"]\n",
    "valid_history_len = conf[\"data\"][\"valid_history_len\"]\n",
    "\n",
    "# number of lead times to forecast\n",
    "forecast_len = conf[\"data\"][\"forecast_len\"]\n",
    "valid_forecast_len = conf[\"data\"][\"valid_forecast_len\"]\n",
    "\n",
    "# max_forecast_len\n",
    "if \"max_forecast_len\" not in conf[\"data\"]:\n",
    "    max_forecast_len = None\n",
    "else:\n",
    "    max_forecast_len = conf[\"data\"][\"max_forecast_len\"]\n",
    "\n",
    "# skip_periods\n",
    "if \"skip_periods\" not in conf[\"data\"]:\n",
    "    skip_periods = None\n",
    "else:\n",
    "    skip_periods = conf[\"data\"][\"skip_periods\"]\n",
    "\n",
    "# one_shot\n",
    "if \"one_shot\" not in conf[\"data\"]:\n",
    "    one_shot = None\n",
    "else:\n",
    "    one_shot = conf[\"data\"][\"one_shot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ceaa80b3-75c7-4a47-8914-59179c27ebf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"train_years\" in conf[\"data\"]:\n",
    "    train_years_range = conf[\"data\"][\"train_years\"]\n",
    "else:\n",
    "    train_years_range = [1979, 2014]\n",
    "\n",
    "if \"valid_years\" in conf[\"data\"]:\n",
    "    valid_years_range = conf[\"data\"][\"valid_years\"]\n",
    "else:\n",
    "    valid_years_range = [2014, 2018]\n",
    "\n",
    "# convert year info to str for file name search\n",
    "train_years = [str(year) for year in range(train_years_range[0], train_years_range[1])]\n",
    "valid_years = [str(year) for year in range(valid_years_range[0], valid_years_range[1])]\n",
    "\n",
    "# Filter the files for training / validation\n",
    "train_files = [\n",
    "    file for file in all_ERA_files if any(year in file for year in train_years)\n",
    "]\n",
    "valid_files = [\n",
    "    file for file in all_ERA_files if any(year in file for year in valid_years)\n",
    "]\n",
    "\n",
    "# <----------------------------------- std_new\n",
    "if conf[\"data\"][\"scaler_type\"] == \"std_new\":\n",
    "    if surface_files is not None:\n",
    "        train_surface_files = [\n",
    "            file for file in surface_files if any(year in file for year in train_years)\n",
    "        ]\n",
    "        valid_surface_files = [\n",
    "            file for file in surface_files if any(year in file for year in valid_years)\n",
    "        ]\n",
    "\n",
    "        # ---------------------------- #\n",
    "        # check total number of files\n",
    "        assert (\n",
    "            len(train_surface_files) == len(train_files)\n",
    "        ), \"Mismatch between the total number of training set [surface files] and [upper-air files]\"\n",
    "        assert (\n",
    "            len(valid_surface_files) == len(valid_files)\n",
    "        ), \"Mismatch between the total number of validation set [surface files] and [upper-air files]\"\n",
    "\n",
    "    else:\n",
    "        train_surface_files = None\n",
    "        valid_surface_files = None\n",
    "\n",
    "    if dyn_forcing_files is not None:\n",
    "        train_dyn_forcing_files = [\n",
    "            file\n",
    "            for file in dyn_forcing_files\n",
    "            if any(year in file for year in train_years)\n",
    "        ]\n",
    "        valid_dyn_forcing_files = [\n",
    "            file\n",
    "            for file in dyn_forcing_files\n",
    "            if any(year in file for year in valid_years)\n",
    "        ]\n",
    "\n",
    "        # ---------------------------- #\n",
    "        # check total number of files\n",
    "        assert (\n",
    "            len(train_dyn_forcing_files) == len(train_files)\n",
    "        ), \"Mismatch between the total number of training set [dynamic forcing files] and [upper-air files]\"\n",
    "        assert (\n",
    "            len(valid_dyn_forcing_files) == len(valid_files)\n",
    "        ), \"Mismatch between the total number of validation set [dynamic forcing files] and [upper-air files]\"\n",
    "\n",
    "    else:\n",
    "        train_dyn_forcing_files = None\n",
    "        valid_dyn_forcing_files = None\n",
    "\n",
    "    if diagnostic_files is not None:\n",
    "        train_diagnostic_files = [\n",
    "            file\n",
    "            for file in diagnostic_files\n",
    "            if any(year in file for year in train_years)\n",
    "        ]\n",
    "        valid_diagnostic_files = [\n",
    "            file\n",
    "            for file in diagnostic_files\n",
    "            if any(year in file for year in valid_years)\n",
    "        ]\n",
    "\n",
    "        # ---------------------------- #\n",
    "        # check total number of files\n",
    "        assert (\n",
    "            len(train_diagnostic_files) == len(train_files)\n",
    "        ), \"Mismatch between the total number of training set [diagnostic files] and [upper-air files]\"\n",
    "        assert (\n",
    "            len(valid_diagnostic_files) == len(valid_files)\n",
    "        ), \"Mismatch between the total number of validation set [diagnostic files] and [upper-air files]\"\n",
    "\n",
    "    else:\n",
    "        train_diagnostic_files = None\n",
    "        valid_diagnostic_files = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ba800a1-2acc-4bab-b89e-656379353d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/schreck/conda-envs/credit/lib/python3.11/site-packages/pyproj/__init__.py:89: UserWarning: pyproj unable to set database path.\n",
      "  _pyproj_global_context_initialize()\n"
     ]
    }
   ],
   "source": [
    "transforms = load_transforms(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f390d6b-2c5a-4358-978c-2e34eeaf2808",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ERA5_and_Forcing_Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    A Pytorch Dataset class that works on:\n",
    "        - upper-air variables (time, level, lat, lon)\n",
    "        - surface variables (time, lat, lon)\n",
    "        - dynamic forcing variables (time, lat, lon)\n",
    "        - foring variables (time, lat, lon)\n",
    "        - diagnostic variables (time, lat, lon)\n",
    "        - static variables (lat, lon)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        varname_upper_air,\n",
    "        varname_surface,\n",
    "        varname_dyn_forcing,\n",
    "        varname_forcing,\n",
    "        varname_static,\n",
    "        varname_diagnostic,\n",
    "        filenames,\n",
    "        filename_surface=None,\n",
    "        filename_dyn_forcing=None,\n",
    "        filename_forcing=None,\n",
    "        filename_static=None,\n",
    "        filename_diagnostic=None,\n",
    "        history_len=2,\n",
    "        forecast_len=0,\n",
    "        transform=None,\n",
    "        seed=42,\n",
    "        skip_periods=None,\n",
    "        one_shot=None,\n",
    "        max_forecast_len=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the ERA5_and_Forcing_Dataset\n",
    "\n",
    "        Parameters:\n",
    "        - varname_upper_air (list): List of upper air variable names.\n",
    "        - varname_surface (list): List of surface variable names.\n",
    "        - varname_dyn_forcing (list): List of dynamic forcing variable names.\n",
    "        - varname_forcing (list): List of forcing variable names.\n",
    "        - varname_static (list): List of static variable names.\n",
    "        - varname_diagnostic (list): List of diagnostic variable names.\n",
    "        - filenames (list): List of filenames for upper air data.\n",
    "        - filename_surface (list, optional): List of filenames for surface data.\n",
    "        - filename_dyn_forcing (list, optional): List of filenames for dynamic forcing data.\n",
    "        - filename_forcing (str, optional): Filename for forcing data.\n",
    "        - filename_static (str, optional): Filename for static data.\n",
    "        - filename_diagnostic (list, optional): List of filenames for diagnostic data.\n",
    "        - history_len (int, optional): Length of the history sequence. Default is 2.\n",
    "        - forecast_len (int, optional): Length of the forecast sequence. Default is 0.\n",
    "        - transform (callable, optional): Transformation function to apply to the data.\n",
    "        - seed (int, optional): Random seed for reproducibility. Default is 42.\n",
    "        - skip_periods (int, optional): Number of periods to skip between samples.\n",
    "        - one_shot(bool, optional): Whether to return all states or just\n",
    "                                    the final state of the training target. Default is None\n",
    "        - max_forecast_len (int, optional): Maximum length of the forecast sequence.\n",
    "        - shuffle (bool, optional): Whether to shuffle the data. Default is True.\n",
    "\n",
    "        Returns:\n",
    "        - sample (dict): A dictionary containing historical_ERA5_images,\n",
    "                                                 target_ERA5_images,\n",
    "                                                 datetime index, and additional information.\n",
    "        \"\"\"\n",
    "\n",
    "        self.history_len = history_len\n",
    "        self.forecast_len = forecast_len\n",
    "        self.transform = transform\n",
    "\n",
    "        # skip periods\n",
    "        self.skip_periods = skip_periods\n",
    "        if self.skip_periods is None:\n",
    "            self.skip_periods = 1\n",
    "\n",
    "        # one shot option\n",
    "        self.one_shot = one_shot\n",
    "\n",
    "        # total number of needed forecast lead times\n",
    "        self.total_seq_len = self.history_len + self.forecast_len\n",
    "\n",
    "        # set random seed\n",
    "        self.rng = np.random.default_rng(seed=seed)\n",
    "\n",
    "        # max possible forecast len\n",
    "        self.max_forecast_len = max_forecast_len\n",
    "\n",
    "        # ======================================================== #\n",
    "        # upper-air files\n",
    "\n",
    "        all_files = []\n",
    "        filenames = sorted(filenames)\n",
    "\n",
    "        for fn in filenames:\n",
    "            # drop variables if they are not in the config\n",
    "            xarray_dataset = get_forward_data(filename=fn)\n",
    "            xarray_dataset = drop_var_from_dataset(xarray_dataset, varname_upper_air)\n",
    "\n",
    "            # collect yearly datasets within a list\n",
    "            all_files.append(xarray_dataset)\n",
    "\n",
    "        self.all_files = all_files\n",
    "\n",
    "        # get sample indices from ERA5 upper-air files:\n",
    "        ind_start = 0\n",
    "        self.ERA5_indices = {}  # <------ change\n",
    "        for ind_file, ERA5_xarray in enumerate(self.all_files):\n",
    "            # [number of samples, ind_start, ind_end]\n",
    "            self.ERA5_indices[str(ind_file)] = [\n",
    "                len(ERA5_xarray[\"time\"]),\n",
    "                ind_start,\n",
    "                ind_start + len(ERA5_xarray[\"time\"]),\n",
    "            ]\n",
    "            ind_start += len(ERA5_xarray[\"time\"]) + 1\n",
    "\n",
    "        # ======================================================== #\n",
    "        # surface files\n",
    "        if filename_surface is not None:\n",
    "            surface_files = []\n",
    "            filename_surface = sorted(filename_surface)\n",
    "\n",
    "            for fn in filename_surface:\n",
    "                # drop variables if they are not in the config\n",
    "                xarray_dataset = get_forward_data(filename=fn)\n",
    "                xarray_dataset = drop_var_from_dataset(xarray_dataset, varname_surface)\n",
    "\n",
    "                surface_files.append(xarray_dataset)\n",
    "\n",
    "            self.surface_files = surface_files\n",
    "\n",
    "        else:\n",
    "            self.surface_files = False\n",
    "\n",
    "        # ======================================================== #\n",
    "        # dynamic forcing files\n",
    "        if filename_dyn_forcing is not None:\n",
    "            dyn_forcing_files = []\n",
    "            filename_dyn_forcing = sorted(filename_dyn_forcing)\n",
    "\n",
    "            for fn in filename_dyn_forcing:\n",
    "                # drop variables if they are not in the config\n",
    "                xarray_dataset = get_forward_data(filename=fn)\n",
    "                xarray_dataset = drop_var_from_dataset(\n",
    "                    xarray_dataset, varname_dyn_forcing\n",
    "                )\n",
    "\n",
    "                dyn_forcing_files.append(xarray_dataset)\n",
    "\n",
    "            self.dyn_forcing_files = dyn_forcing_files\n",
    "\n",
    "        else:\n",
    "            self.dyn_forcing_files = False\n",
    "\n",
    "        # ======================================================== #\n",
    "        # diagnostic file\n",
    "        self.filename_diagnostic = filename_diagnostic\n",
    "\n",
    "        if self.filename_diagnostic is not None:\n",
    "            diagnostic_files = []\n",
    "            filename_diagnostic = sorted(filename_diagnostic)\n",
    "\n",
    "            for fn in filename_diagnostic:\n",
    "                # drop variables if they are not in the config\n",
    "                xarray_dataset = get_forward_data(filename=fn)\n",
    "                xarray_dataset = drop_var_from_dataset(\n",
    "                    xarray_dataset, varname_diagnostic\n",
    "                )\n",
    "\n",
    "                diagnostic_files.append(xarray_dataset)\n",
    "\n",
    "            self.diagnostic_files = diagnostic_files\n",
    "\n",
    "        else:\n",
    "            self.diagnostic_files = False\n",
    "\n",
    "        # ======================================================== #\n",
    "        # forcing file\n",
    "        self.filename_forcing = filename_forcing\n",
    "\n",
    "        if self.filename_forcing is not None:\n",
    "            assert os.path.isfile(\n",
    "                filename_forcing\n",
    "            ), \"Cannot find forcing file [{}]\".format(filename_forcing)\n",
    "\n",
    "            # drop variables if they are not in the config\n",
    "            xarray_dataset = get_forward_data(filename_forcing)\n",
    "            xarray_dataset = drop_var_from_dataset(xarray_dataset, varname_forcing)\n",
    "\n",
    "            self.xarray_forcing = xarray_dataset\n",
    "        else:\n",
    "            self.xarray_forcing = False\n",
    "\n",
    "        # ======================================================== #\n",
    "        # static file\n",
    "        self.filename_static = filename_static\n",
    "\n",
    "        if self.filename_static is not None:\n",
    "            assert os.path.isfile(\n",
    "                filename_static\n",
    "            ), \"Cannot find static file [{}]\".format(filename_static)\n",
    "\n",
    "            # drop variables if they are not in the config\n",
    "            xarray_dataset = get_forward_data(filename_static)\n",
    "            xarray_dataset = drop_var_from_dataset(xarray_dataset, varname_static)\n",
    "\n",
    "            self.xarray_static = xarray_dataset\n",
    "        else:\n",
    "            self.xarray_static = False\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # Total sequence length of each sample.\n",
    "        self.total_seq_len = self.history_len + self.forecast_len\n",
    "\n",
    "    def __len__(self):\n",
    "        # compute the total number of length\n",
    "        total_len = 0\n",
    "        for ERA5_xarray in self.all_files:\n",
    "            total_len += len(ERA5_xarray[\"time\"]) - self.total_seq_len + 1\n",
    "        return total_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # ========================================================================== #\n",
    "        # cross-year indices --> the index of the year + indices within that year\n",
    "\n",
    "        # select the ind_file based on the iter index\n",
    "        ind_file = find_key_for_number(index, self.ERA5_indices)\n",
    "\n",
    "        # get the ind within the current file\n",
    "        ind_start = self.ERA5_indices[ind_file][1]\n",
    "        ind_start_in_file = index - ind_start\n",
    "\n",
    "        # handle out-of-bounds\n",
    "        ind_largest = len(self.all_files[int(ind_file)][\"time\"]) - (\n",
    "            self.history_len + self.forecast_len + 1\n",
    "        )\n",
    "        if ind_start_in_file > ind_largest:\n",
    "            ind_start_in_file = ind_largest\n",
    "\n",
    "        # ========================================================================== #\n",
    "        # subset xarray on time dimension\n",
    "\n",
    "        ind_end_in_file = ind_start_in_file + self.history_len + self.forecast_len\n",
    "\n",
    "        ## ERA5_subset: a xarray dataset that contains training input and target (for the current batch)\n",
    "        ERA5_subset = self.all_files[int(ind_file)].isel(\n",
    "            time=slice(ind_start_in_file, ind_end_in_file + 1)\n",
    "        )  # .load() NOT load into memory\n",
    "\n",
    "        # ========================================================================== #\n",
    "        # merge surface into the dataset\n",
    "\n",
    "        if self.surface_files:\n",
    "            ## subset surface variables\n",
    "            surface_subset = self.surface_files[int(ind_file)].isel(\n",
    "                time=slice(ind_start_in_file, ind_end_in_file + 1)\n",
    "            )  # .load() NOT load into memory\n",
    "\n",
    "            ## merge upper-air and surface here:\n",
    "            ERA5_subset = ERA5_subset.merge(\n",
    "                surface_subset\n",
    "            )  # <-- lazy merge, ERA5 and surface both not loaded\n",
    "\n",
    "        # ==================================================== #\n",
    "        # split ERA5_subset into training inputs and targets\n",
    "        #   + merge with dynamic forcing, forcing, and static\n",
    "\n",
    "        # the ind_end of the ERA5_subset\n",
    "        ind_end_time = len(ERA5_subset[\"time\"])\n",
    "\n",
    "        # datetiem information as int number (used in some normalization methods)\n",
    "        datetime_as_number = ERA5_subset.time.values.astype(\"datetime64[s]\").astype(int)\n",
    "\n",
    "        # ==================================================== #\n",
    "        # xarray dataset as input\n",
    "        ## historical_ERA5_images: the final input\n",
    "\n",
    "        historical_ERA5_images = ERA5_subset.isel(\n",
    "            time=slice(0, self.history_len, self.skip_periods)\n",
    "        ).load()  # <-- load into memory\n",
    "\n",
    "        # ========================================================================== #\n",
    "        # merge dynamic forcing inputs\n",
    "        if self.dyn_forcing_files:\n",
    "            dyn_forcing_subset = self.dyn_forcing_files[int(ind_file)].isel(\n",
    "                time=slice(ind_start_in_file, ind_end_in_file + 1)\n",
    "            )\n",
    "            dyn_forcing_subset = dyn_forcing_subset.isel(\n",
    "                time=slice(0, self.history_len, self.skip_periods)\n",
    "            ).load()  # <-- load into memory\n",
    "\n",
    "            historical_ERA5_images = historical_ERA5_images.merge(dyn_forcing_subset)\n",
    "\n",
    "        # ========================================================================== #\n",
    "        # merge forcing inputs\n",
    "        if self.xarray_forcing:\n",
    "            # ------------------------------------------------------------------------------- #\n",
    "            # matching month, day, hour between forcing and upper air [time]\n",
    "            # this approach handles leap year forcing file and non-leap-year upper air file\n",
    "            month_day_forcing = extract_month_day_hour(\n",
    "                np.array(self.xarray_forcing[\"time\"])\n",
    "            )\n",
    "            month_day_inputs = extract_month_day_hour(\n",
    "                np.array(historical_ERA5_images[\"time\"])\n",
    "            )  # <-- upper air\n",
    "            # indices to subset\n",
    "            ind_forcing, _ = find_common_indices(month_day_forcing, month_day_inputs)\n",
    "            forcing_subset_input = self.xarray_forcing.isel(\n",
    "                time=ind_forcing\n",
    "            ).load()  # <-- load into memory\n",
    "            # forcing and upper air have different years but the same mon/day/hour\n",
    "            # safely replace forcing time with upper air time\n",
    "            forcing_subset_input[\"time\"] = historical_ERA5_images[\"time\"]\n",
    "            # ------------------------------------------------------------------------------- #\n",
    "\n",
    "            # merge\n",
    "            historical_ERA5_images = historical_ERA5_images.merge(forcing_subset_input)\n",
    "\n",
    "        # ========================================================================== #\n",
    "        # merge static inputs\n",
    "        if self.xarray_static:\n",
    "            # expand static var on time dim\n",
    "            N_time_dims = len(ERA5_subset[\"time\"])\n",
    "            static_subset_input = self.xarray_static.expand_dims(\n",
    "                dim={\"time\": N_time_dims}\n",
    "            )\n",
    "            # assign coords 'time'\n",
    "            static_subset_input = static_subset_input.assign_coords(\n",
    "                {\"time\": ERA5_subset[\"time\"]}\n",
    "            )\n",
    "\n",
    "            # slice + load to the GPU\n",
    "            static_subset_input = static_subset_input.isel(\n",
    "                time=slice(0, self.history_len, self.skip_periods)\n",
    "            ).load()  # <-- load into memory\n",
    "\n",
    "            # update\n",
    "            static_subset_input[\"time\"] = historical_ERA5_images[\"time\"]\n",
    "\n",
    "            # merge\n",
    "            historical_ERA5_images = historical_ERA5_images.merge(static_subset_input)\n",
    "\n",
    "        # ==================================================== #\n",
    "        # xarray dataset as target\n",
    "        ## target_ERA5_images: the final target\n",
    "\n",
    "        if self.one_shot is not None:\n",
    "            # one_shot is True (on), go straight to the last element\n",
    "            target_ERA5_images = ERA5_subset.isel(\n",
    "                time=slice(-1, None)\n",
    "            ).load()  # <-- load into memory\n",
    "\n",
    "            ## merge diagnoisc input here:\n",
    "            if self.diagnostic_files:\n",
    "                diagnostic_subset = self.diagnostic_files[int(ind_file)].isel(\n",
    "                    time=slice(ind_start_in_file, ind_end_in_file + 1)\n",
    "                )\n",
    "\n",
    "                diagnostic_subset = diagnostic_subset.isel(\n",
    "                    time=slice(-1, None)\n",
    "                ).load()  # <-- load into memory\n",
    "\n",
    "                target_ERA5_images = target_ERA5_images.merge(diagnostic_subset)\n",
    "\n",
    "        else:\n",
    "            # one_shot is None (off), get the full target length based on forecast_len\n",
    "            target_ERA5_images = ERA5_subset.isel(\n",
    "                time=slice(self.history_len, ind_end_time, self.skip_periods)\n",
    "            ).load()  # <-- load into memory\n",
    "\n",
    "            ## merge diagnoisc input here:\n",
    "            if self.diagnostic_files:\n",
    "                # subset diagnostic variables\n",
    "                diagnostic_subset = self.diagnostic_files[int(ind_file)].isel(\n",
    "                    time=slice(ind_start_in_file, ind_end_in_file + 1)\n",
    "                )\n",
    "\n",
    "                diagnostic_subset = diagnostic_subset.isel(\n",
    "                    time=slice(self.history_len, ind_end_time, self.skip_periods)\n",
    "                ).load()  # <-- load into memory\n",
    "\n",
    "                # merge into the target dataset\n",
    "                target_ERA5_images = target_ERA5_images.merge(diagnostic_subset)\n",
    "\n",
    "        # pipe xarray datasets to the sampler\n",
    "        sample = Sample(\n",
    "            historical_ERA5_images=historical_ERA5_images,\n",
    "            target_ERA5_images=target_ERA5_images,\n",
    "            datetime_index=datetime_as_number,\n",
    "        )\n",
    "\n",
    "        # ==================================== #\n",
    "        # data normalization\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        # assign sample index\n",
    "        sample[\"index\"] = index\n",
    "        sample[\"datetime\"] = datetime_as_number\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b70e456-e30f-4def-8fdd-3269862e8e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ERA5_and_Forcing_MultiStep(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    A Pytorch Dataset class that works on:\n",
    "        - upper-air variables (time, level, lat, lon)\n",
    "        - surface variables (time, lat, lon)\n",
    "        - dynamic forcing variables (time, lat, lon)\n",
    "        - foring variables (time, lat, lon)\n",
    "        - diagnostic variables (time, lat, lon)\n",
    "        - static variables (lat, lon)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        varname_upper_air,\n",
    "        varname_surface,\n",
    "        varname_dyn_forcing,\n",
    "        varname_forcing,\n",
    "        varname_static,\n",
    "        varname_diagnostic,\n",
    "        filenames,\n",
    "        filename_surface=None,\n",
    "        filename_dyn_forcing=None,\n",
    "        filename_forcing=None,\n",
    "        filename_static=None,\n",
    "        filename_diagnostic=None,\n",
    "        history_len=2,\n",
    "        forecast_len=0,\n",
    "        transform=None,\n",
    "        seed=42,\n",
    "        skip_periods=None,\n",
    "        one_shot=None,\n",
    "        max_forecast_len=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the ERA5_and_Forcing_Dataset\n",
    "\n",
    "        Parameters:\n",
    "        - varname_upper_air (list): List of upper air variable names.\n",
    "        - varname_surface (list): List of surface variable names.\n",
    "        - varname_dyn_forcing (list): List of dynamic forcing variable names.\n",
    "        - varname_forcing (list): List of forcing variable names.\n",
    "        - varname_static (list): List of static variable names.\n",
    "        - varname_diagnostic (list): List of diagnostic variable names.\n",
    "        - filenames (list): List of filenames for upper air data.\n",
    "        - filename_surface (list, optional): List of filenames for surface data.\n",
    "        - filename_dyn_forcing (list, optional): List of filenames for dynamic forcing data.\n",
    "        - filename_forcing (str, optional): Filename for forcing data.\n",
    "        - filename_static (str, optional): Filename for static data.\n",
    "        - filename_diagnostic (list, optional): List of filenames for diagnostic data.\n",
    "        - history_len (int, optional): Length of the history sequence. Default is 2.\n",
    "        - forecast_len (int, optional): Length of the forecast sequence. Default is 0.\n",
    "        - transform (callable, optional): Transformation function to apply to the data.\n",
    "        - seed (int, optional): Random seed for reproducibility. Default is 42.\n",
    "        - skip_periods (int, optional): Number of periods to skip between samples.\n",
    "        - one_shot(bool, optional): Whether to return all states or just\n",
    "                                    the final state of the training target. Default is None\n",
    "        - max_forecast_len (int, optional): Maximum length of the forecast sequence.\n",
    "        - shuffle (bool, optional): Whether to shuffle the data. Default is True.\n",
    "\n",
    "        Returns:\n",
    "        - sample (dict): A dictionary containing historical_ERA5_images,\n",
    "                                                 target_ERA5_images,\n",
    "                                                 datetime index, and additional information.\n",
    "        \"\"\"\n",
    "\n",
    "        self.history_len = history_len\n",
    "        self.forecast_len = forecast_len\n",
    "        self.transform = transform\n",
    "\n",
    "        # skip periods\n",
    "        self.skip_periods = skip_periods\n",
    "        if self.skip_periods is None:\n",
    "            self.skip_periods = 1\n",
    "\n",
    "        # one shot option\n",
    "        self.one_shot = one_shot\n",
    "\n",
    "        # total number of needed forecast lead times\n",
    "        self.total_seq_len = self.history_len + self.forecast_len\n",
    "\n",
    "        # set random seed\n",
    "        self.rng = np.random.default_rng(seed=seed)\n",
    "\n",
    "        # max possible forecast len\n",
    "        self.max_forecast_len = max_forecast_len\n",
    "\n",
    "        # ======================================================== #\n",
    "        # upper-air files\n",
    "\n",
    "        all_files = []\n",
    "        filenames = sorted(filenames)\n",
    "\n",
    "        for fn in filenames:\n",
    "            # drop variables if they are not in the config\n",
    "            xarray_dataset = get_forward_data(filename=fn)\n",
    "            xarray_dataset = drop_var_from_dataset(xarray_dataset, varname_upper_air)\n",
    "\n",
    "            # collect yearly datasets within a list\n",
    "            all_files.append(xarray_dataset)\n",
    "\n",
    "        self.all_files = all_files\n",
    "\n",
    "        # get sample indices from ERA5 upper-air files:\n",
    "        ind_start = 0\n",
    "        self.ERA5_indices = {}  # <------ change\n",
    "        for ind_file, ERA5_xarray in enumerate(self.all_files):\n",
    "            # [number of samples, ind_start, ind_end]\n",
    "            self.ERA5_indices[str(ind_file)] = [\n",
    "                len(ERA5_xarray[\"time\"]),\n",
    "                ind_start,\n",
    "                ind_start + len(ERA5_xarray[\"time\"]),\n",
    "            ]\n",
    "            ind_start += len(ERA5_xarray[\"time\"]) + 1\n",
    "\n",
    "        # ======================================================== #\n",
    "        # surface files\n",
    "        if filename_surface is not None:\n",
    "            surface_files = []\n",
    "            filename_surface = sorted(filename_surface)\n",
    "\n",
    "            for fn in filename_surface:\n",
    "                # drop variables if they are not in the config\n",
    "                xarray_dataset = get_forward_data(filename=fn)\n",
    "                xarray_dataset = drop_var_from_dataset(xarray_dataset, varname_surface)\n",
    "\n",
    "                surface_files.append(xarray_dataset)\n",
    "\n",
    "            self.surface_files = surface_files\n",
    "\n",
    "        else:\n",
    "            self.surface_files = False\n",
    "\n",
    "        # ======================================================== #\n",
    "        # dynamic forcing files\n",
    "        if filename_dyn_forcing is not None:\n",
    "            dyn_forcing_files = []\n",
    "            filename_dyn_forcing = sorted(filename_dyn_forcing)\n",
    "\n",
    "            for fn in filename_dyn_forcing:\n",
    "                # drop variables if they are not in the config\n",
    "                xarray_dataset = get_forward_data(filename=fn)\n",
    "                xarray_dataset = drop_var_from_dataset(\n",
    "                    xarray_dataset, varname_dyn_forcing\n",
    "                )\n",
    "\n",
    "                dyn_forcing_files.append(xarray_dataset)\n",
    "\n",
    "            self.dyn_forcing_files = dyn_forcing_files\n",
    "\n",
    "        else:\n",
    "            self.dyn_forcing_files = False\n",
    "\n",
    "        # ======================================================== #\n",
    "        # diagnostic file\n",
    "        self.filename_diagnostic = filename_diagnostic\n",
    "\n",
    "        if self.filename_diagnostic is not None:\n",
    "            diagnostic_files = []\n",
    "            filename_diagnostic = sorted(filename_diagnostic)\n",
    "\n",
    "            for fn in filename_diagnostic:\n",
    "                # drop variables if they are not in the config\n",
    "                xarray_dataset = get_forward_data(filename=fn)\n",
    "                xarray_dataset = drop_var_from_dataset(\n",
    "                    xarray_dataset, varname_diagnostic\n",
    "                )\n",
    "\n",
    "                diagnostic_files.append(xarray_dataset)\n",
    "\n",
    "            self.diagnostic_files = diagnostic_files\n",
    "\n",
    "        else:\n",
    "            self.diagnostic_files = False\n",
    "\n",
    "        # ======================================================== #\n",
    "        # forcing file\n",
    "        self.filename_forcing = filename_forcing\n",
    "\n",
    "        if self.filename_forcing is not None:\n",
    "            assert os.path.isfile(\n",
    "                filename_forcing\n",
    "            ), \"Cannot find forcing file [{}]\".format(filename_forcing)\n",
    "\n",
    "            # drop variables if they are not in the config\n",
    "            xarray_dataset = get_forward_data(filename_forcing)\n",
    "            xarray_dataset = drop_var_from_dataset(xarray_dataset, varname_forcing)\n",
    "\n",
    "            self.xarray_forcing = xarray_dataset\n",
    "        else:\n",
    "            self.xarray_forcing = False\n",
    "\n",
    "        # ======================================================== #\n",
    "        # static file\n",
    "        self.filename_static = filename_static\n",
    "\n",
    "        if self.filename_static is not None:\n",
    "            assert os.path.isfile(\n",
    "                filename_static\n",
    "            ), \"Cannot find static file [{}]\".format(filename_static)\n",
    "\n",
    "            # drop variables if they are not in the config\n",
    "            xarray_dataset = get_forward_data(filename_static)\n",
    "            xarray_dataset = drop_var_from_dataset(xarray_dataset, varname_static)\n",
    "\n",
    "            self.xarray_static = xarray_dataset\n",
    "        else:\n",
    "            self.xarray_static = False\n",
    "\n",
    "        self.start_index = self._get_random_start_index()\n",
    "        self.forecast_step = 0\n",
    "        self.total_length = len(self.ERA5_indices)\n",
    "\n",
    "    def _get_random_start_index(self):\n",
    "        \"\"\"Generate a random start index based on the length of the dataset.\"\"\"\n",
    "        dataset_length = len(self)\n",
    "        return 0  # random.randint(0, dataset_length - 1)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # Total sequence length of each sample.\n",
    "        self.total_seq_len = self.history_len + self.forecast_len\n",
    "\n",
    "    def __len__(self):\n",
    "        # compute the total number of length\n",
    "        total_len = 0\n",
    "        for ERA5_xarray in self.all_files:\n",
    "            total_len += len(ERA5_xarray[\"time\"]) - self.total_seq_len + 1\n",
    "        return total_len\n",
    "\n",
    "    def set_epoch(self, epoch):\n",
    "        self.current_epoch = epoch\n",
    "        self.forecast_step_count = 0\n",
    "        self.current_index = None\n",
    "        self.initial_index = None\n",
    "\n",
    "    def _get_new_start_index(self, worker_id=0, num_workers=1):\n",
    "        # Divide the data among workers such that there's no overlap\n",
    "        total_steps = len(self.ERA5_indices) // num_workers\n",
    "        worker_offset = worker_id * total_steps\n",
    "        return worker_offset + (self.start_index % total_steps)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # worker_info = get_worker_info()\n",
    "        # worker_id = worker_info.id if worker_info else 0\n",
    "\n",
    "        if (self.forecast_step_count == self.forecast_len + 1) or (\n",
    "            self.current_index is None\n",
    "        ):\n",
    "            # We've completed the last forecast or we're starting for the first time\n",
    "            # Start a new forecast using the sampler index\n",
    "            self.current_index = index  # self._get_random_start_index()\n",
    "            self.forecast_step_count = 0\n",
    "            index = self.current_index\n",
    "            self.initial_index = self.current_index\n",
    "        else:\n",
    "            # Ignore the sampler index and continue the forecast\n",
    "            self.current_index += 1\n",
    "            index = self.current_index\n",
    "\n",
    "        # select the ind_file based on the iter index\n",
    "        ind_file = find_key_for_number(index, self.ERA5_indices)\n",
    "\n",
    "        # get the ind within the current file\n",
    "        ind_start = self.ERA5_indices[ind_file][1]\n",
    "        ind_start_in_file = index - ind_start\n",
    "\n",
    "        # handle out-of-bounds\n",
    "        ind_largest = len(self.all_files[int(ind_file)][\"time\"]) - (\n",
    "            self.history_len + self.forecast_len + 1\n",
    "        )\n",
    "        if ind_start_in_file > ind_largest:\n",
    "            ind_start_in_file = ind_largest\n",
    "\n",
    "        # ========================================================================== #\n",
    "        # subset xarray on time dimension\n",
    "\n",
    "        ind_end_in_file = ind_start_in_file + self.history_len\n",
    "\n",
    "        ## ERA5_subset: a xarray dataset that contains training input and target (for the current batch)\n",
    "        ERA5_subset = self.all_files[int(ind_file)].isel(\n",
    "            time=slice(ind_start_in_file, ind_end_in_file + 1)\n",
    "        )  # .load() NOT load into memory\n",
    "\n",
    "        # ========================================================================== #\n",
    "        # merge surface into the dataset\n",
    "\n",
    "        if self.surface_files:\n",
    "            ## subset surface variables\n",
    "            surface_subset = self.surface_files[int(ind_file)].isel(\n",
    "                time=slice(ind_start_in_file, ind_end_in_file + 1)\n",
    "            )  # .load() NOT load into memory\n",
    "\n",
    "            ## merge upper-air and surface here:\n",
    "            ERA5_subset = ERA5_subset.merge(\n",
    "                surface_subset\n",
    "            )  # <-- lazy merge, ERA5 and surface both not loaded\n",
    "\n",
    "        # ==================================================== #\n",
    "        # split ERA5_subset into training inputs and targets\n",
    "        #   + merge with dynamic forcing, forcing, and static\n",
    "\n",
    "        # the ind_end of the ERA5_subset\n",
    "        ind_end_time = len(ERA5_subset[\"time\"])\n",
    "\n",
    "        # datetiem information as int number (used in some normalization methods)\n",
    "        datetime_as_number = ERA5_subset.time.values.astype(\"datetime64[s]\").astype(int)\n",
    "\n",
    "        # ==================================================== #\n",
    "        # xarray dataset as input\n",
    "        ## historical_ERA5_images: the final input\n",
    "\n",
    "        historical_ERA5_images = ERA5_subset.isel(\n",
    "            time=slice(0, self.history_len, self.skip_periods)\n",
    "        ).load()  # <-- load into memory\n",
    "\n",
    "        # ========================================================================== #\n",
    "        # merge dynamic forcing inputs\n",
    "        if self.dyn_forcing_files:\n",
    "            dyn_forcing_subset = self.dyn_forcing_files[int(ind_file)].isel(\n",
    "                time=slice(ind_start_in_file, ind_end_in_file + 1)\n",
    "            )\n",
    "            dyn_forcing_subset = dyn_forcing_subset.isel(\n",
    "                time=slice(0, self.history_len, self.skip_periods)\n",
    "            ).load()  # <-- load into memory\n",
    "\n",
    "            historical_ERA5_images = historical_ERA5_images.merge(dyn_forcing_subset)\n",
    "\n",
    "        # ========================================================================== #\n",
    "        # merge forcing inputs\n",
    "        if self.xarray_forcing:\n",
    "            # ------------------------------------------------------------------------------- #\n",
    "            # matching month, day, hour between forcing and upper air [time]\n",
    "            # this approach handles leap year forcing file and non-leap-year upper air file\n",
    "            month_day_forcing = extract_month_day_hour(\n",
    "                np.array(self.xarray_forcing[\"time\"])\n",
    "            )\n",
    "            month_day_inputs = extract_month_day_hour(\n",
    "                np.array(historical_ERA5_images[\"time\"])\n",
    "            )  # <-- upper air\n",
    "            # indices to subset\n",
    "            ind_forcing, _ = find_common_indices(month_day_forcing, month_day_inputs)\n",
    "            forcing_subset_input = self.xarray_forcing.isel(\n",
    "                time=ind_forcing\n",
    "            ).load()  # <-- load into memory\n",
    "            # forcing and upper air have different years but the same mon/day/hour\n",
    "            # safely replace forcing time with upper air time\n",
    "            forcing_subset_input[\"time\"] = historical_ERA5_images[\"time\"]\n",
    "            # ------------------------------------------------------------------------------- #\n",
    "\n",
    "            # merge\n",
    "            historical_ERA5_images = historical_ERA5_images.merge(forcing_subset_input)\n",
    "\n",
    "        # ========================================================================== #\n",
    "        # merge static inputs\n",
    "        if self.xarray_static:\n",
    "            # expand static var on time dim\n",
    "            N_time_dims = len(ERA5_subset[\"time\"])\n",
    "            static_subset_input = self.xarray_static.expand_dims(\n",
    "                dim={\"time\": N_time_dims}\n",
    "            )\n",
    "            # assign coords 'time'\n",
    "            static_subset_input = static_subset_input.assign_coords(\n",
    "                {\"time\": ERA5_subset[\"time\"]}\n",
    "            )\n",
    "\n",
    "            # slice + load to the GPU\n",
    "            static_subset_input = static_subset_input.isel(\n",
    "                time=slice(0, self.history_len, self.skip_periods)\n",
    "            ).load()  # <-- load into memory\n",
    "\n",
    "            # update\n",
    "            static_subset_input[\"time\"] = historical_ERA5_images[\"time\"]\n",
    "\n",
    "            # merge\n",
    "            historical_ERA5_images = historical_ERA5_images.merge(static_subset_input)\n",
    "\n",
    "        # ==================================================== #\n",
    "        # xarray dataset as target\n",
    "        ## target_ERA5_images: the final target\n",
    "\n",
    "        target_ERA5_images = ERA5_subset.isel(\n",
    "            time=slice(-1, None)\n",
    "        ).load()  # <-- load into memory\n",
    "\n",
    "        ## merge diagnoisc input here:\n",
    "        if self.diagnostic_files:\n",
    "            diagnostic_subset = self.diagnostic_files[int(ind_file)].isel(\n",
    "                time=slice(ind_start_in_file, ind_end_in_file + 1)\n",
    "            )\n",
    "\n",
    "            diagnostic_subset = diagnostic_subset.isel(\n",
    "                time=slice(-1, None)\n",
    "            ).load()  # <-- load into memory\n",
    "\n",
    "            target_ERA5_images = target_ERA5_images.merge(diagnostic_subset)\n",
    "\n",
    "        # pipe xarray datasets to the sampler\n",
    "        sample = Sample(\n",
    "            historical_ERA5_images=historical_ERA5_images,\n",
    "            target_ERA5_images=target_ERA5_images,\n",
    "            datetime_index=datetime_as_number,\n",
    "        )\n",
    "\n",
    "        # ==================================== #\n",
    "        # data normalization\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        # assign sample index\n",
    "        sample[\"datetime\"] = datetime_as_number\n",
    "        sample[\"forecast_step\"] = self.forecast_step + 1\n",
    "        sample[\"index\"] = index\n",
    "        sample[\"stop_forecast\"] = self.forecast_step == self.forecast_len\n",
    "\n",
    "        # update the step count\n",
    "        self.forecast_step += 1\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df49b74f-c665-4d86-8d23-129900da1e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_len = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69c66e6f-05a1-4ddb-8937-65e2888a6d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ERA5_and_Forcing_Dataset(\n",
    "    varname_upper_air=varname_upper_air,\n",
    "    varname_surface=varname_surface,\n",
    "    varname_dyn_forcing=varname_dyn_forcing,\n",
    "    varname_forcing=varname_forcing,\n",
    "    varname_static=varname_static,\n",
    "    varname_diagnostic=varname_diagnostic,\n",
    "    filenames=all_ERA_files,\n",
    "    filename_surface=surface_files,\n",
    "    filename_dyn_forcing=dyn_forcing_files,\n",
    "    filename_forcing=forcing_files,\n",
    "    filename_static=static_files,\n",
    "    filename_diagnostic=diagnostic_files,\n",
    "    history_len=history_len,\n",
    "    forecast_len=forecast_len,\n",
    "    skip_periods=1,\n",
    "    one_shot=None,\n",
    "    max_forecast_len=forecast_len + history_len,\n",
    "    transform=transforms,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2642f234-554b-4849-bb6e-f332f986fe7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_multi = ERA5_and_Forcing_MultiStep(\n",
    "    varname_upper_air=varname_upper_air,\n",
    "    varname_surface=varname_surface,\n",
    "    varname_dyn_forcing=varname_dyn_forcing,\n",
    "    varname_forcing=varname_forcing,\n",
    "    varname_static=varname_static,\n",
    "    varname_diagnostic=varname_diagnostic,\n",
    "    filenames=all_ERA_files,\n",
    "    filename_surface=surface_files,\n",
    "    filename_dyn_forcing=dyn_forcing_files,\n",
    "    filename_forcing=forcing_files,\n",
    "    filename_static=static_files,\n",
    "    filename_diagnostic=diagnostic_files,\n",
    "    history_len=history_len,\n",
    "    forecast_len=forecast_len,\n",
    "    skip_periods=skip_periods,\n",
    "    one_shot=False,\n",
    "    max_forecast_len=max_forecast_len,\n",
    "    transform=transforms,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a44034d9-6c04-4820-8f53-f92a35d34ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bc03034-e752-475f-8d67-6839a1d7ee0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 15, 640, 1280])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[\"y\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e95669f-3999-4c95-8f8d-6ced77fd08b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([283996800, 284018400, 284040000, 284061600, 284083200, 284104800,\n",
       "       284126400, 284148000, 284169600])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[\"datetime\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c612e088-6e2a-45d5-9a22-c5ed6e1c3338",
   "metadata": {},
   "source": [
    "### Get the first sample from the multi-step reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5652ddfe-3c65-4a42-a6a4-7d1c4ad87578",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_multi.set_epoch(0)\n",
    "multi_sample = dataset_multi.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a5ed437-263e-46b0-a5b6-1cce29328576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 15, 640, 1280])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_sample[\"y\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8eda284-66b2-4db3-bffb-3f2774d6c42e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([283996800, 284018400])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_sample[\"datetime\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b03ee93c-bde0-4663-b3d6-dc8d1453648a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sample[\"y\"][0] == multi_sample[\"y\"][0]).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ce8558b-6ce1-4f47-9874-9c798f4ecb35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(1.)\n",
      "1 tensor(1.)\n",
      "2 tensor(1.)\n",
      "3 tensor(1.)\n",
      "4 tensor(1.)\n",
      "5 tensor(1.)\n",
      "6 tensor(1.)\n",
      "7 tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "dataset_multi.set_epoch(0)\n",
    "for k in range(forecast_len + 1):\n",
    "    multi_sample = dataset_multi.__getitem__(k)\n",
    "    print(k, (sample[\"y\"][k] == multi_sample[\"y\"][0]).float().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4478d979-f6cc-4f76-9eda-acc30a5d41f4",
   "metadata": {},
   "source": [
    "### Another variation using our worker method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ca315ad-306a-4c6a-9010-ea5ba9854e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict, List, Optional, Tuple\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def worker(\n",
    "    tuple_index: Tuple[int, int],\n",
    "    ERA5_indices: Dict[str, List[int]],\n",
    "    all_files: List[Any],\n",
    "    surface_files: Optional[List[Any]],\n",
    "    dyn_forcing_files: Optional[List[Any]],\n",
    "    diagnostic_files: Optional[List[Any]],\n",
    "    xarray_forcing: Optional[Any],\n",
    "    xarray_static: Optional[Any],\n",
    "    history_len: int,\n",
    "    forecast_len: int,\n",
    "    skip_periods: int,\n",
    "    transform: Optional[Callable],\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Processes a given index to extract and transform data for a specific time slice.\n",
    "\n",
    "    Parameters:\n",
    "    - tuple_index (Tuple[int, int]): Tuple containing the current index and sub-index for processing.\n",
    "    - ERA5_indices (Dict[str, List[int]]): Dictionary containing ERA5 indices metadata.\n",
    "    - all_files (List[Any]): List of xarray datasets containing upper air data.\n",
    "    - surface_files (Optional[List[Any]]): List of xarray datasets containing surface data.\n",
    "    - dyn_forcing_files (Optional[List[Any]]): List of xarray datasets containing dynamic forcing data.\n",
    "    - diagnostic_files (Optional[List[Any]]): List of xarray datasets containing diagnostic data.\n",
    "    - history_len (int): Length of the history sequence.\n",
    "    - forecast_len (int): Length of the forecast sequence.\n",
    "    - skip_periods (int): Number of periods to skip between samples.\n",
    "    - xarray_forcing (Optional[Any]): xarray dataset containing forcing data.\n",
    "    - xarray_static (Optional[Any]): xarray dataset containing static data.\n",
    "\n",
    "    - transform (Optional[Callable]): Transformation function to apply to the data.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[str, Any]: A dictionary containing historical ERA5 images, target ERA5 images, datetime index, and additional information.\n",
    "    \"\"\"\n",
    "\n",
    "    index, ind_start_current_step = tuple_index\n",
    "\n",
    "    try:\n",
    "        # select the ind_file based on the iter index\n",
    "        ind_file = find_key_for_number(ind_start_current_step, ERA5_indices)\n",
    "\n",
    "        # get the ind within the current file\n",
    "        ind_start = ERA5_indices[ind_file][1]\n",
    "        ind_start_in_file = ind_start_current_step - ind_start\n",
    "\n",
    "        # handle out-of-bounds\n",
    "        ind_largest = len(all_files[int(ind_file)][\"time\"]) - (\n",
    "            history_len + forecast_len + 1\n",
    "        )\n",
    "        if ind_start_in_file > ind_largest:\n",
    "            ind_start_in_file = ind_largest\n",
    "\n",
    "        # ========================================================================== #\n",
    "        # subset xarray on time dimension & load it to the memory\n",
    "\n",
    "        ind_end_in_file = ind_start_in_file + history_len + forecast_len\n",
    "\n",
    "        ## ERA5_subset: a xarray dataset that contains training input and target (for the current batch)\n",
    "        ERA5_subset = all_files[int(ind_file)].isel(\n",
    "            time=slice(ind_start_in_file, ind_end_in_file + 1)\n",
    "        )  # .load() NOT load into memory\n",
    "\n",
    "        if surface_files:\n",
    "            ## subset surface variables\n",
    "            surface_subset = surface_files[int(ind_file)].isel(\n",
    "                time=slice(ind_start_in_file, ind_end_in_file + 1)\n",
    "            )  # .load() NOT load into memory\n",
    "\n",
    "            ## merge upper-air and surface here:\n",
    "            ERA5_subset = ERA5_subset.merge(\n",
    "                surface_subset\n",
    "            )  # <-- lazy merge, ERA5 and surface both not loaded\n",
    "\n",
    "        # ==================================================== #\n",
    "        # split ERA5_subset into training inputs and targets\n",
    "        #   + merge with forcing and static\n",
    "\n",
    "        # the ind_end of the ERA5_subset\n",
    "        # ind_end_time = len(ERA5_subset['time'])\n",
    "\n",
    "        # datetiem information as int number (used in some normalization methods)\n",
    "        datetime_as_number = ERA5_subset.time.values.astype(\"datetime64[s]\").astype(int)\n",
    "\n",
    "        # ==================================================== #\n",
    "        # xarray dataset as input\n",
    "        ## historical_ERA5_images: the final input\n",
    "\n",
    "        historical_ERA5_images = ERA5_subset.isel(\n",
    "            time=slice(0, history_len, skip_periods)\n",
    "        ).load()  # <-- load into memory\n",
    "\n",
    "        # ========================================================================== #\n",
    "        # merge dynamic forcing inputs\n",
    "        if dyn_forcing_files:\n",
    "            dyn_forcing_subset = dyn_forcing_files[int(ind_file)].isel(\n",
    "                time=slice(ind_start_in_file, ind_end_in_file + 1)\n",
    "            )\n",
    "            dyn_forcing_subset = dyn_forcing_subset.isel(\n",
    "                time=slice(0, history_len, skip_periods)\n",
    "            ).load()  # <-- load into memory\n",
    "\n",
    "            historical_ERA5_images = historical_ERA5_images.merge(dyn_forcing_subset)\n",
    "\n",
    "        # ========================================================================== #\n",
    "        # merge forcing inputs\n",
    "        if xarray_forcing:\n",
    "            # =============================================================================== #\n",
    "            # matching month, day, hour between forcing and upper air [time]\n",
    "            # this approach handles leap year forcing file and non-leap-year upper air file\n",
    "            month_day_forcing = extract_month_day_hour(np.array(xarray_forcing[\"time\"]))\n",
    "            month_day_inputs = extract_month_day_hour(\n",
    "                np.array(historical_ERA5_images[\"time\"])\n",
    "            )  # <-- upper air\n",
    "            # indices to subset\n",
    "            ind_forcing, _ = find_common_indices(month_day_forcing, month_day_inputs)\n",
    "            forcing_subset_input = xarray_forcing.isel(\n",
    "                time=ind_forcing\n",
    "            ).load()  # <-- load into memory\n",
    "            # forcing and upper air have different years but the same mon/day/hour\n",
    "            # safely replace forcing time with upper air time\n",
    "            forcing_subset_input[\"time\"] = historical_ERA5_images[\"time\"]\n",
    "            # =============================================================================== #\n",
    "\n",
    "            # merge\n",
    "            historical_ERA5_images = historical_ERA5_images.merge(forcing_subset_input)\n",
    "\n",
    "        # ========================================================================== #\n",
    "        # merge static inputs\n",
    "        if xarray_static:\n",
    "            # expand static var on time dim\n",
    "            N_time_dims = len(ERA5_subset[\"time\"])\n",
    "            static_subset_input = xarray_static.expand_dims(dim={\"time\": N_time_dims})\n",
    "            # assign coords 'time'\n",
    "            static_subset_input = static_subset_input.assign_coords(\n",
    "                {\"time\": ERA5_subset[\"time\"]}\n",
    "            )\n",
    "\n",
    "            # slice + load to the GPU\n",
    "            static_subset_input = static_subset_input.isel(\n",
    "                time=slice(0, history_len, skip_periods)\n",
    "            ).load()  # <-- load into memory\n",
    "\n",
    "            # update\n",
    "            static_subset_input[\"time\"] = historical_ERA5_images[\"time\"]\n",
    "\n",
    "            # merge\n",
    "            historical_ERA5_images = historical_ERA5_images.merge(static_subset_input)\n",
    "\n",
    "        # ==================================================== #\n",
    "        # xarray dataset as target\n",
    "        ## target_ERA5_images: the final target\n",
    "\n",
    "        # get the next forecast step\n",
    "        target_ERA5_images = ERA5_subset.isel(\n",
    "            time=slice(history_len, history_len + skip_periods, skip_periods)\n",
    "        ).load()  # <-- load into memory\n",
    "\n",
    "        ## merge diagnoisc input here:\n",
    "        if diagnostic_files:\n",
    "            # subset diagnostic variables\n",
    "            diagnostic_subset = diagnostic_files[int(ind_file)].isel(\n",
    "                time=slice(ind_start_in_file, ind_end_in_file + 1)\n",
    "            )\n",
    "\n",
    "            # get the next forecast step\n",
    "            diagnostic_subset = diagnostic_subset.isel(\n",
    "                time=slice(history_len, history_len + skip_periods, skip_periods)\n",
    "            ).load()  # <-- load into memory\n",
    "\n",
    "            # merge into the target dataset\n",
    "            target_ERA5_images = target_ERA5_images.merge(diagnostic_subset)\n",
    "\n",
    "        # create a dict object with input/output tensors\n",
    "        sample = Sample(\n",
    "            historical_ERA5_images=historical_ERA5_images,\n",
    "            target_ERA5_images=target_ERA5_images,\n",
    "            datetime_index=datetime_as_number,\n",
    "        )\n",
    "\n",
    "        # data normalization\n",
    "        if transform:\n",
    "            sample = transform(sample)\n",
    "\n",
    "        sample[\"index\"] = index\n",
    "        stop_forecast = (ind_start_current_step - index) == forecast_len\n",
    "        sample[\"forecast_hour\"] = ind_start_current_step - index + 1\n",
    "        sample[\"index\"] = index\n",
    "        sample[\"stop_forecast\"] = stop_forecast\n",
    "        sample[\"datetime\"] = [\n",
    "            int(\n",
    "                historical_ERA5_images.time.values[0]\n",
    "                .astype(\"datetime64[s]\")\n",
    "                .astype(int)\n",
    "            ),\n",
    "            int(target_ERA5_images.time.values[0].astype(\"datetime64[s]\").astype(int)),\n",
    "        ]\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing index {tuple_index}: {e}\")\n",
    "        raise\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7a84a74-615c-4d50-9aa9-fafba447d846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from credit.data import (\n",
    "    drop_var_from_dataset,\n",
    "    get_forward_data,\n",
    "    Sample,\n",
    "    find_key_for_number,\n",
    "    extract_month_day_hour,\n",
    "    find_common_indices,\n",
    ")\n",
    "\n",
    "\n",
    "class ERA5_and_Forcing_MultiStep(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    A Pytorch Dataset class that works on:\n",
    "        - upper-air variables (time, level, lat, lon)\n",
    "        - surface variables (time, lat, lon)\n",
    "        - dynamic forcing variables (time, lat, lon)\n",
    "        - foring variables (time, lat, lon)\n",
    "        - diagnostic variables (time, lat, lon)\n",
    "        - static variables (lat, lon)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        varname_upper_air,\n",
    "        varname_surface,\n",
    "        varname_dyn_forcing,\n",
    "        varname_forcing,\n",
    "        varname_static,\n",
    "        varname_diagnostic,\n",
    "        filenames,\n",
    "        filename_surface=None,\n",
    "        filename_dyn_forcing=None,\n",
    "        filename_forcing=None,\n",
    "        filename_static=None,\n",
    "        filename_diagnostic=None,\n",
    "        history_len=2,\n",
    "        forecast_len=0,\n",
    "        transform=None,\n",
    "        seed=42,\n",
    "        rank=0,\n",
    "        world_size=1,\n",
    "        skip_periods=None,\n",
    "        one_shot=None,\n",
    "        max_forecast_len=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the ERA5_and_Forcing_Dataset\n",
    "\n",
    "        Parameters:\n",
    "        - varname_upper_air (list): List of upper air variable names.\n",
    "        - varname_surface (list): List of surface variable names.\n",
    "        - varname_dyn_forcing (list): List of dynamic forcing variable names.\n",
    "        - varname_forcing (list): List of forcing variable names.\n",
    "        - varname_static (list): List of static variable names.\n",
    "        - varname_diagnostic (list): List of diagnostic variable names.\n",
    "        - filenames (list): List of filenames for upper air data.\n",
    "        - filename_surface (list, optional): List of filenames for surface data.\n",
    "        - filename_dyn_forcing (list, optional): List of filenames for dynamic forcing data.\n",
    "        - filename_forcing (str, optional): Filename for forcing data.\n",
    "        - filename_static (str, optional): Filename for static data.\n",
    "        - filename_diagnostic (list, optional): List of filenames for diagnostic data.\n",
    "        - history_len (int, optional): Length of the history sequence. Default is 2.\n",
    "        - forecast_len (int, optional): Length of the forecast sequence. Default is 0.\n",
    "        - transform (callable, optional): Transformation function to apply to the data.\n",
    "        - seed (int, optional): Random seed for reproducibility. Default is 42.\n",
    "        - skip_periods (int, optional): Number of periods to skip between samples.\n",
    "        - one_shot(bool, optional): Whether to return all states or just\n",
    "                                    the final state of the training target. Default is None\n",
    "        - max_forecast_len (int, optional): Maximum length of the forecast sequence.\n",
    "        - shuffle (bool, optional): Whether to shuffle the data. Default is True.\n",
    "\n",
    "        Returns:\n",
    "        - sample (dict): A dictionary containing historical_ERA5_images,\n",
    "                                                 target_ERA5_images,\n",
    "                                                 datetime index, and additional information.\n",
    "        \"\"\"\n",
    "\n",
    "        self.history_len = history_len\n",
    "        self.forecast_len = forecast_len\n",
    "        self.transform = transform\n",
    "        self.seed = seed\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "\n",
    "        # skip periods\n",
    "        self.skip_periods = skip_periods\n",
    "        if self.skip_periods is None:\n",
    "            self.skip_periods = 1\n",
    "\n",
    "        # one shot option\n",
    "        self.one_shot = one_shot\n",
    "\n",
    "        # total number of needed forecast lead times\n",
    "        self.total_seq_len = self.history_len + self.forecast_len\n",
    "\n",
    "        # set random seed\n",
    "        self.rng = np.random.default_rng(seed=seed)\n",
    "\n",
    "        # max possible forecast len\n",
    "        self.max_forecast_len = max_forecast_len\n",
    "\n",
    "        # ======================================================== #\n",
    "        # upper-air files\n",
    "\n",
    "        all_files = []\n",
    "        filenames = sorted(filenames)\n",
    "\n",
    "        for fn in filenames:\n",
    "            # drop variables if they are not in the config\n",
    "            xarray_dataset = get_forward_data(filename=fn)\n",
    "            xarray_dataset = drop_var_from_dataset(xarray_dataset, varname_upper_air)\n",
    "\n",
    "            # collect yearly datasets within a list\n",
    "            all_files.append(xarray_dataset)\n",
    "\n",
    "        self.all_files = all_files\n",
    "\n",
    "        # get sample indices from ERA5 upper-air files:\n",
    "        ind_start = 0\n",
    "        self.ERA5_indices = {}\n",
    "        for ind_file, ERA5_xarray in enumerate(self.all_files):\n",
    "            # [number of samples, ind_start, ind_end]\n",
    "            self.ERA5_indices[str(ind_file)] = [\n",
    "                len(ERA5_xarray[\"time\"]),\n",
    "                ind_start,\n",
    "                ind_start + len(ERA5_xarray[\"time\"]),\n",
    "            ]\n",
    "            ind_start += len(ERA5_xarray[\"time\"]) + 1\n",
    "\n",
    "        # ======================================================== #\n",
    "        # surface files\n",
    "        if filename_surface is not None:\n",
    "            surface_files = []\n",
    "            filename_surface = sorted(filename_surface)\n",
    "\n",
    "            for fn in filename_surface:\n",
    "                # drop variables if they are not in the config\n",
    "                xarray_dataset = get_forward_data(filename=fn)\n",
    "                xarray_dataset = drop_var_from_dataset(xarray_dataset, varname_surface)\n",
    "\n",
    "                surface_files.append(xarray_dataset)\n",
    "\n",
    "            self.surface_files = surface_files\n",
    "\n",
    "        else:\n",
    "            self.surface_files = False\n",
    "\n",
    "        # dynamic forcing files\n",
    "        if filename_dyn_forcing is not None:\n",
    "            dyn_forcing_files = []\n",
    "            filename_dyn_forcing = sorted(filename_dyn_forcing)\n",
    "\n",
    "            for fn in filename_dyn_forcing:\n",
    "                # drop variables if they are not in the config\n",
    "                xarray_dataset = get_forward_data(filename=fn)\n",
    "                xarray_dataset = drop_var_from_dataset(\n",
    "                    xarray_dataset, varname_dyn_forcing\n",
    "                )\n",
    "\n",
    "                dyn_forcing_files.append(xarray_dataset)\n",
    "\n",
    "            self.dyn_forcing_files = dyn_forcing_files\n",
    "\n",
    "        else:\n",
    "            self.dyn_forcing_files = False\n",
    "\n",
    "        # ======================================================== #\n",
    "        # diagnostic file\n",
    "        self.filename_diagnostic = filename_diagnostic\n",
    "\n",
    "        if self.filename_diagnostic is not None:\n",
    "            diagnostic_files = []\n",
    "            filename_diagnostic = sorted(filename_diagnostic)\n",
    "\n",
    "            for fn in filename_diagnostic:\n",
    "                # drop variables if they are not in the config\n",
    "                xarray_dataset = get_forward_data(filename=fn)\n",
    "                xarray_dataset = drop_var_from_dataset(\n",
    "                    xarray_dataset, varname_diagnostic\n",
    "                )\n",
    "\n",
    "                diagnostic_files.append(xarray_dataset)\n",
    "\n",
    "            self.diagnostic_files = diagnostic_files\n",
    "\n",
    "        else:\n",
    "            self.diagnostic_files = False\n",
    "\n",
    "        # ======================================================== #\n",
    "        # forcing file\n",
    "        self.filename_forcing = filename_forcing\n",
    "\n",
    "        if self.filename_forcing is not None:\n",
    "            assert os.path.isfile(\n",
    "                filename_forcing\n",
    "            ), \"Cannot find forcing file [{}]\".format(filename_forcing)\n",
    "\n",
    "            # drop variables if they are not in the config\n",
    "            xarray_dataset = get_forward_data(filename_forcing)\n",
    "            xarray_dataset = drop_var_from_dataset(xarray_dataset, varname_forcing)\n",
    "\n",
    "            self.xarray_forcing = xarray_dataset\n",
    "        else:\n",
    "            self.xarray_forcing = False\n",
    "\n",
    "        # ======================================================== #\n",
    "        # static file\n",
    "        self.filename_static = filename_static\n",
    "\n",
    "        if self.filename_static is not None:\n",
    "            assert os.path.isfile(\n",
    "                filename_static\n",
    "            ), \"Cannot find static file [{}]\".format(filename_static)\n",
    "\n",
    "            # drop variables if they are not in the config\n",
    "            xarray_dataset = get_forward_data(filename_static)\n",
    "            xarray_dataset = drop_var_from_dataset(xarray_dataset, varname_static)\n",
    "\n",
    "            self.xarray_static = xarray_dataset\n",
    "        else:\n",
    "            self.xarray_static = False\n",
    "\n",
    "        self.worker = partial(\n",
    "            worker,\n",
    "            ERA5_indices=self.ERA5_indices,\n",
    "            all_files=self.all_files,\n",
    "            surface_files=self.surface_files,\n",
    "            dyn_forcing_files=self.dyn_forcing_files,\n",
    "            diagnostic_files=self.diagnostic_files,\n",
    "            xarray_forcing=self.xarray_forcing,\n",
    "            xarray_static=self.xarray_static,\n",
    "            history_len=self.history_len,\n",
    "            forecast_len=self.forecast_len,\n",
    "            skip_periods=self.skip_periods,\n",
    "            transform=self.transform,\n",
    "        )\n",
    "\n",
    "        self.start_index = None\n",
    "        self.forecast_step = 0\n",
    "        self.total_length = len(self.ERA5_indices)\n",
    "        self.epoch = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # Total sequence length of each sample.\n",
    "        self.total_seq_len = self.history_len + self.forecast_len\n",
    "\n",
    "    def __len__(self):\n",
    "        # compute the total number of length\n",
    "        total_len = 0\n",
    "        for ERA5_xarray in self.all_files:\n",
    "            total_len += len(ERA5_xarray[\"time\"]) - self.total_seq_len + 1\n",
    "        return total_len\n",
    "\n",
    "    def set_epoch(self, epoch):\n",
    "        self.current_epoch = epoch\n",
    "        self.forecast_step_count = 0\n",
    "        self.current_index = None\n",
    "        self.initial_index = None\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if (self.forecast_step_count == self.forecast_len + 1) or (\n",
    "            self.current_index is None\n",
    "        ):\n",
    "            # We've completed the last forecast or we're starting for the first time\n",
    "            # Start a new forecast using the sampler index\n",
    "            self.current_index = index  # self._get_random_start_index()\n",
    "            self.forecast_step_count = 0\n",
    "            index = self.current_index\n",
    "            self.initial_index = self.current_index\n",
    "        else:\n",
    "            # Ignore the sampler index and continue the forecast\n",
    "            self.current_index += 1\n",
    "            index = self.current_index\n",
    "\n",
    "        print(self.forecast_step_count, self.forecast_len, self.current_index)\n",
    "        index_pair = (self.initial_index, index)\n",
    "        # Worker process\n",
    "        sample = self.worker(index_pair)\n",
    "\n",
    "        # update the step count\n",
    "        self.forecast_step += 1\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2cca4bb6-f219-4d52-8f57-6300275ad43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ERA5_and_Forcing_MultiStep(\n",
    "    varname_upper_air=varname_upper_air,\n",
    "    varname_surface=varname_surface,\n",
    "    varname_dyn_forcing=varname_dyn_forcing,\n",
    "    varname_forcing=varname_forcing,\n",
    "    varname_static=varname_static,\n",
    "    varname_diagnostic=varname_diagnostic,\n",
    "    filenames=all_ERA_files,\n",
    "    filename_surface=surface_files,\n",
    "    filename_dyn_forcing=dyn_forcing_files,\n",
    "    filename_forcing=forcing_files,\n",
    "    filename_static=static_files,\n",
    "    filename_diagnostic=diagnostic_files,\n",
    "    history_len=history_len,\n",
    "    forecast_len=forecast_len,\n",
    "    skip_periods=skip_periods,\n",
    "    one_shot=False,\n",
    "    max_forecast_len=max_forecast_len,\n",
    "    transform=transforms,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "888b1b91-ab92-4ef7-bc5f-22c8d92827e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 1\n",
    "# num_workers = 1\n",
    "\n",
    "# dataloader = DataLoader(\n",
    "#     dataset,\n",
    "#     batch_size=batch_size,  # Adjust the batch size as needed\n",
    "#     shuffle=False,   # Shuffle the dataset if needed\n",
    "#     num_workers=num_workers,  # Number of subprocesses to use for data loading (adjust as needed)\n",
    "#     drop_last=True,  # Drop the last incomplete batch if not divisible by batch_size,\n",
    "#     prefetch_factor=4\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "efaa4a48-238f-4921-b3c4-a97fd78b8e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(1.)\n",
      "1 tensor(1.)\n",
      "2 tensor(1.)\n",
      "3 tensor(1.)\n",
      "4 tensor(1.)\n",
      "5 tensor(1.)\n",
      "6 tensor(1.)\n",
      "7 tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# dataloader.dataset.set_epoch(0)\n",
    "# for (k, sample) in enumerate(dataloader):\n",
    "#     print(k, sample[\"forecast_hour\"], sample[\"index\"], sample[\"datetime\"], sample[\"stop_forecast\"])\n",
    "#     if k == 25:\n",
    "#         break\n",
    "\n",
    "dataset_multi.set_epoch(0)\n",
    "for k in range(forecast_len + 1):\n",
    "    multi_sample = dataset_multi.__getitem__(k)\n",
    "    print(k, (sample[\"y\"][k] == multi_sample[\"y\"][0]).float().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8b66e8-cade-42b1-8183-ee7da7d91c8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "credit",
   "language": "python",
   "name": "credit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5517cb9d-e134-4f4b-bdcc-c6d27133f7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import tqdm\n",
    "from credit.transforms import load_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6daf1463-3976-4809-a76a-740f658eb903",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"small_multi/model.yml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "378061bb-481c-4f3d-8d7c-04dfdf322564",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config) as cf:\n",
    "    conf = yaml.load(cf, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae1988b0-01f5-48f6-9a33-c9356348e44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = conf[\"trainer\"][\"train_batch_size\"]\n",
    "valid_batch_size = conf[\"trainer\"][\"valid_batch_size\"]\n",
    "thread_workers = conf[\"trainer\"][\"thread_workers\"]\n",
    "valid_thread_workers = (\n",
    "    conf[\"trainer\"][\"valid_thread_workers\"]\n",
    "    if \"valid_thread_workers\" in conf[\"trainer\"]\n",
    "    else thread_workers\n",
    ")\n",
    "\n",
    "history_len = 1\n",
    "forecast_len = 1\n",
    "\n",
    "# datasets (zarr reader)\n",
    "\n",
    "all_ERA_files = sorted(glob.glob(conf[\"data\"][\"save_loc\"]))\n",
    "\n",
    "train_years = [str(year) for year in range(1979, 2014)]\n",
    "valid_years = [\n",
    "    str(year) for year in range(2014, 2018)\n",
    "]  # can make CV splits if we want to later on\n",
    "test_years = [\n",
    "    str(year) for year in range(2018, 2022)\n",
    "]  # same as graphcast -- always hold out\n",
    "\n",
    "# Filter the files for each set\n",
    "\n",
    "train_files = [\n",
    "    file for file in all_ERA_files if any(year in file for year in train_years)\n",
    "]\n",
    "valid_files = [\n",
    "    file for file in all_ERA_files if any(year in file for year in valid_years)\n",
    "]\n",
    "test_files = [\n",
    "    file for file in all_ERA_files if any(year in file for year in test_years)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8e0aeddf-f64f-4e69-9420-a5582956af3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Callable, List\n",
    "from credit.data import (\n",
    "    get_forward_data,\n",
    "    generate_integer_list_around,\n",
    "    flatten_list,\n",
    "    find_key_for_number,\n",
    ")\n",
    "\n",
    "\n",
    "class MultiStepERA5(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        filenames: List[str] = [\n",
    "            \"/glade/derecho/scratch/wchapman/STAGING/TOTAL_2012-01-01_2012-12-31_staged.zarr\",\n",
    "            \"/glade/derecho/scratch/wchapman/STAGING/TOTAL_2013-01-01_2013-12-31_staged.zarr\",\n",
    "        ],\n",
    "        history_len: int = 1,\n",
    "        forecast_len: int = 2,\n",
    "        transform: Optional[Callable] = None,\n",
    "        seed=42,\n",
    "        skip_periods=None,\n",
    "        one_shot=None,\n",
    "        max_forecast_len=None,\n",
    "        rank=0,\n",
    "        world_size=1,\n",
    "    ):\n",
    "        self.history_len = history_len\n",
    "        self.forecast_len = forecast_len\n",
    "        self.transform = transform\n",
    "        self.skip_periods = skip_periods\n",
    "        self.one_shot = one_shot\n",
    "        self.total_seq_len = self.history_len + self.forecast_len\n",
    "        self.max_forecast_len = max_forecast_len\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "        np.random.seed(seed + rank)\n",
    "\n",
    "        all_fils = []\n",
    "        filenames = sorted(filenames)\n",
    "        for fn in filenames:\n",
    "            all_fils.append(get_forward_data(filename=fn))\n",
    "        self.all_fils = all_fils\n",
    "        self.data_array = all_fils[0]\n",
    "\n",
    "        # Set data places\n",
    "        indo = 0\n",
    "        self.meta_data_dict = {}\n",
    "        for ee, bb in enumerate(self.all_fils):\n",
    "            self.meta_data_dict[str(ee)] = [\n",
    "                len(bb[\"time\"]),\n",
    "                indo,\n",
    "                indo + len(bb[\"time\"]),\n",
    "            ]\n",
    "            indo += len(bb[\"time\"]) + 1\n",
    "\n",
    "        # Set out of bounds indexes...\n",
    "        OOB = []\n",
    "        for kk in self.meta_data_dict.keys():\n",
    "            OOB.append(generate_integer_list_around(self.meta_data_dict[kk][2]))\n",
    "        self.OOB = flatten_list(OOB)\n",
    "\n",
    "        # Generate sequences based on rank and world_size\n",
    "        self.sequence_indices = self.generate_sequences()\n",
    "        self.forecast_hour = 0\n",
    "\n",
    "    def generate_sequences(self):\n",
    "        # Calculate the total length manually\n",
    "        total_length = sum(\n",
    "            len(bb[\"time\"]) - self.total_seq_len + 1 for bb in self.all_fils\n",
    "        )\n",
    "        all_indices = list(range(total_length))\n",
    "\n",
    "        chunk_size = len(all_indices) // self.world_size\n",
    "        start_idx = self.rank * chunk_size\n",
    "        end_idx = (\n",
    "            start_idx + chunk_size\n",
    "            if self.rank != self.world_size - 1\n",
    "            else len(all_indices)\n",
    "        )\n",
    "\n",
    "        random.shuffle(all_indices)\n",
    "\n",
    "        # Select the start times\n",
    "        random_start_times = all_indices[start_idx:end_idx]\n",
    "        sequence_indices = []\n",
    "\n",
    "        for start_time in random_start_times:\n",
    "            if start_time == 0:\n",
    "                continue\n",
    "            for i in range(self.forecast_len + 1):\n",
    "                sequence_indices.append(start_time + i)\n",
    "\n",
    "        return sequence_indices\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # Total sequence length of each sample.\n",
    "        self.total_seq_len = self.history_len + self.forecast_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequence_indices)\n",
    "\n",
    "    def is_end_of_forecast(self, index: int) -> bool:\n",
    "        \"\"\"\n",
    "        Determine if the current index is the last index in a forecast sequence.\n",
    "\n",
    "        Parameters:\n",
    "            index (int): The current index in sequence_indices.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the current index is the last index in a forecast sequence, otherwise False.\n",
    "        \"\"\"\n",
    "        # Get the index of the current position in the sequence_indices list\n",
    "        current_pos = self.sequence_indices.index(index)\n",
    "\n",
    "        # Check if it's the last index in the sequence\n",
    "        if current_pos == len(self.sequence_indices) - 1:\n",
    "            return 1\n",
    "\n",
    "        # Determine if the next index starts a new forecast\n",
    "        next_index = self.sequence_indices[current_pos + 1]\n",
    "        if next_index - index != 1:\n",
    "            return 1\n",
    "\n",
    "        return 0\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index = self.sequence_indices[index]\n",
    "        # The rest of your existing __getitem__ implementation remains unchanged\n",
    "        # find the result key:\n",
    "        result_key = find_key_for_number(index, self.meta_data_dict)\n",
    "\n",
    "        # get the data selection:\n",
    "        true_ind = index - self.meta_data_dict[result_key][1]\n",
    "\n",
    "        if true_ind > (\n",
    "            len(self.all_fils[int(result_key)][\"time\"])\n",
    "            - (self.history_len + self.forecast_len + 1)\n",
    "        ):\n",
    "            true_ind = len(self.all_fils[int(result_key)][\"time\"]) - (\n",
    "                self.history_len + self.forecast_len + 1\n",
    "            )\n",
    "\n",
    "        datasel = self.all_fils[int(result_key)].isel(\n",
    "            time=slice(true_ind, true_ind + self.history_len + self.forecast_len + 1)\n",
    "        )\n",
    "\n",
    "        historical_data = datasel.isel(time=slice(0, self.history_len)).load()\n",
    "        target_data = datasel.isel(\n",
    "            time=slice(self.history_len, self.history_len + 1)\n",
    "        ).load()\n",
    "\n",
    "        sample = {\n",
    "            \"historical_ERA5_images\": historical_data,\n",
    "            \"target_ERA5_images\": target_data,\n",
    "            \"datetime_index\": [\n",
    "                int(historical_data.time.values[0].astype(\"datetime64[s]\").astype(int)),\n",
    "                int(target_data.time.values[0].astype(\"datetime64[s]\").astype(int)),\n",
    "            ],\n",
    "        }\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        sample[\"index\"] = index\n",
    "        sample[\"stop_forecast\"] = self.is_end_of_forecast(index)\n",
    "        sample[\"forecast_hour\"] = self.forecast_hour\n",
    "        sample[\"datetime_index\"] = [\n",
    "            int(historical_data.time.values[0].astype(\"datetime64[s]\").astype(int)),\n",
    "            int(target_data.time.values[0].astype(\"datetime64[s]\").astype(int)),\n",
    "        ]\n",
    "\n",
    "        if sample[\"stop_forecast\"]:\n",
    "            self.forecast_hour = 0\n",
    "        else:\n",
    "            self.forecast_hour += 1\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c38a7c5e-b85e-4a70-9000-5fbd62df7108",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MultiStepERA5(\n",
    "    filenames=train_files,\n",
    "    history_len=history_len,\n",
    "    forecast_len=forecast_len,\n",
    "    skip_periods=conf[\"data\"][\"skip_periods\"],\n",
    "    one_shot=conf[\"data\"][\"one_shot\"],\n",
    "    transform=load_transforms(conf),\n",
    "    rank=0,\n",
    "    world_size=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4cabb8f4-d772-4e99-9e2b-1b9f047a0872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[231353,\n",
       " 231354,\n",
       " 173529,\n",
       " 173530,\n",
       " 232329,\n",
       " 232330,\n",
       " 12921,\n",
       " 12922,\n",
       " 183689,\n",
       " 183690,\n",
       " 193970,\n",
       " 193971]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.sequence_indices[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1af3d3f0-c23f-4d9e-b0eb-8f59a96c9f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.sequence_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2af98ead-066e-4bb6-bda8-9a762140466c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5806b50c-eeb1-45e9-bad1-0de35e73ecd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [1116774000, 1116777600]\n"
     ]
    }
   ],
   "source": [
    "print(sample[\"stop_forecast\"], sample[\"datetime_index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e34512d9-9f93-4561-9d2b-1b88faa97189",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "01ab1ae4-2d67-456b-a616-0a497b054515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [1116777600, 1116781200]\n"
     ]
    }
   ],
   "source": [
    "print(sample[\"stop_forecast\"], sample[\"datetime_index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "49d41428-ee65-4508-8612-246fbc130aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset.__getitem__(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "46c91d1f-e663-4068-a99f-b54b627e6cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [908632800, 908636400]\n"
     ]
    }
   ],
   "source": [
    "print(sample[\"stop_forecast\"], sample[\"datetime_index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3f80e9f5-e63f-48b7-97a0-5f0520a48585",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset.__getitem__(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5dcce25b-9060-427f-a24a-ff9243687bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [908636400, 908640000]\n"
     ]
    }
   ],
   "source": [
    "print(sample[\"stop_forecast\"], sample[\"datetime_index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "03740f10-6af8-4212-aebe-0c0ccd27b841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[231353, 231354, 173529, 173530, 232329, 232330, 12921, 12922, 183689, 183690]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.sequence_indices[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "72fe68e3-378f-43a1-9a45-ec069cf4c34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DataLoader parameters\n",
    "batch_size = 2  # Adjust the batch size as needed\n",
    "num_workers = 0  # Adjust the number of workers as needed for parallel data loading\n",
    "shuffle = False  # Must be false! We will let the rank + seed determine randomness\n",
    "\n",
    "# Create DataLoader\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=shuffle,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,  # Set to True if using CUDA to speed up the transfer of data to GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8e830bd2-e684-40c8-a316-d4dd85ab8d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([288936000000000000, 969469200000000000]) tensor([1, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatetime\u001b[39m\u001b[38;5;124m\"\u001b[39m], sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop_forecast\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop_forecast\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "for sample in data_loader:\n",
    "    print(sample[\"datetime\"], sample[\"stop_forecast\"])\n",
    "    if sample[\"stop_forecast\"]:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0ce709f3-a67c-47a4-afa7-3aea0731d09e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "908636400000000000"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[\"datetime\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "58190988-9640-4517-8b84-2530cc02c383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[\"stop_forecast\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b01e273e-8f9e-4fc6-9dbc-17ef44a8a2c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[\"forecast_hour\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bb3e32-a063-4f9b-a4c7-893491eefbfd",
   "metadata": {},
   "source": [
    "# Sequential dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3ee0e5e2-3fa5-4686-9f42-18ef54dffd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistributedSequentialDataset(torch.utils.data.IterableDataset):\n",
    "    # https://colab.research.google.com/drive/1OFLZnX9y5QUFNONuvFsxOizq4M-tFvk-?usp=sharing#scrollTo=CxSCQPOMHgwo\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        filenames,\n",
    "        history_len,\n",
    "        forecast_len,\n",
    "        skip_periods,\n",
    "        rank,\n",
    "        world_size,\n",
    "        shuffle=False,\n",
    "        transform=None,\n",
    "        rollout_p=0.0,\n",
    "    ):\n",
    "        self.dataset = ERA5Dataset(\n",
    "            filenames=filenames,\n",
    "            history_len=history_len,\n",
    "            forecast_len=forecast_len,\n",
    "            skip_periods=skip_periods,\n",
    "            transform=transform,\n",
    "        )\n",
    "        self.meta_data_dict = self.dataset.meta_data_dict\n",
    "        self.all_fils = self.dataset.all_fils\n",
    "        self.history_len = history_len\n",
    "        self.forecast_len = forecast_len\n",
    "        self.filenames = filenames\n",
    "        self.transform = transform\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "        self.shuffle = shuffle\n",
    "        self.skip_periods = skip_periods\n",
    "        self.current_epoch = 0\n",
    "        self.rollout_p = rollout_p\n",
    "\n",
    "    def __len__(self):\n",
    "        tlen = 0\n",
    "        for bb in self.all_fils:\n",
    "            tlen += len(bb[\"time\"]) - self.forecast_len\n",
    "        return tlen\n",
    "\n",
    "    def set_epoch(self, epoch):\n",
    "        self.current_epoch = epoch\n",
    "\n",
    "    def __iter__(self):\n",
    "        worker_info = get_worker_info()\n",
    "        num_workers = worker_info.num_workers if worker_info is not None else 1\n",
    "        worker_id = worker_info.id if worker_info is not None else 0\n",
    "        sampler = DistributedSampler(\n",
    "            self,\n",
    "            num_replicas=num_workers * self.world_size,\n",
    "            rank=self.rank * num_workers + worker_id,\n",
    "            shuffle=self.shuffle,\n",
    "        )\n",
    "        sampler.set_epoch(self.current_epoch)\n",
    "\n",
    "        for index in iter(sampler):\n",
    "            result_key = find_key_for_number(index, self.meta_data_dict)\n",
    "            true_ind = index - self.meta_data_dict[result_key][1]\n",
    "\n",
    "            if true_ind > (\n",
    "                len(self.all_fils[int(result_key)][\"time\"])\n",
    "                - (self.history_len + self.forecast_len + 1)\n",
    "            ):\n",
    "                true_ind = len(self.all_fils[int(result_key)][\"time\"]) - (\n",
    "                    self.history_len + self.forecast_len + 3\n",
    "                )\n",
    "\n",
    "            indices = list(\n",
    "                range(true_ind, true_ind + self.history_len + self.forecast_len)\n",
    "            )\n",
    "            stop_forecast = False\n",
    "\n",
    "            for k, ind in enumerate(indices):\n",
    "                concatenated_samples = {\n",
    "                    \"x\": [],\n",
    "                    \"x_surf\": [],\n",
    "                    \"y\": [],\n",
    "                    \"y_surf\": [],\n",
    "                    \"static\": [],\n",
    "                    \"TOA\": [],\n",
    "                }\n",
    "                sliced = xr.open_zarr(\n",
    "                    self.filenames[int(result_key)]\n",
    "                ).isel(\n",
    "                    time=slice(\n",
    "                        ind,\n",
    "                        ind + self.history_len + self.forecast_len + 1,\n",
    "                        self.skip_periods,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                historical_data = sliced.isel(time=slice(0, self.history_len)).load()\n",
    "                target_data = sliced.isel(\n",
    "                    time=slice(self.history_len, self.history_len + 1)\n",
    "                ).load()\n",
    "\n",
    "                sample = {\n",
    "                    \"x\": historical_data,\n",
    "                    \"y\": target_data,\n",
    "                    \"t\": [\n",
    "                        int(\n",
    "                            historical_data.time.values[0]\n",
    "                            .astype(\"datetime64[s]\")\n",
    "                            .astype(int)\n",
    "                        ),\n",
    "                        int(\n",
    "                            target_data.time.values[0]\n",
    "                            .astype(\"datetime64[s]\")\n",
    "                            .astype(int)\n",
    "                        ),\n",
    "                    ],\n",
    "                }\n",
    "\n",
    "                if self.transform:\n",
    "                    sample = self.transform(sample)\n",
    "\n",
    "                for key in concatenated_samples.keys():\n",
    "                    concatenated_samples[key] = sample[key].squeeze()\n",
    "\n",
    "                stop_forecast = k == self.forecast_len\n",
    "\n",
    "                concatenated_samples[\"forecast_hour\"] = k\n",
    "                concatenated_samples[\"index\"] = index\n",
    "                concatenated_samples[\"stop_forecast\"] = stop_forecast\n",
    "                concatenated_samples[\"datetime\"] = [\n",
    "                    int(\n",
    "                        historical_data.time.values[0]\n",
    "                        .astype(\"datetime64[s]\")\n",
    "                        .astype(int)\n",
    "                    ),\n",
    "                    int(target_data.time.values[0].astype(\"datetime64[s]\").astype(int)),\n",
    "                ]\n",
    "\n",
    "                yield concatenated_samples\n",
    "\n",
    "                if stop_forecast:\n",
    "                    break\n",
    "\n",
    "                if k == self.forecast_len:\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f2695c8c-1fe0-4eaf-9132-53f6c7e6c340",
   "metadata": {},
   "outputs": [],
   "source": [
    "from credit.data import ERA5Dataset\n",
    "from torch.utils.data import get_worker_info\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "31434d26-60da-4472-8923-fc099feff971",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = DistributedSequentialDataset(\n",
    "    filenames=train_files,\n",
    "    history_len=1,\n",
    "    forecast_len=1,\n",
    "    skip_periods=conf[\"data\"][\"skip_periods\"],\n",
    "    transform=load_transforms(conf),\n",
    "    rank=0,\n",
    "    world_size=1,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4ecc2498-9652-45ec-8f03-2f8ad5af1c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/306781 [00:09<849:10:58,  9.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False]) [tensor([283996800]), tensor([284000400])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/306781 [00:19<844:03:20,  9.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True]) [tensor([284000400]), tensor([284004000])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/306781 [00:29<841:46:37,  9.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False]) [tensor([284000400]), tensor([284004000])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/306781 [00:39<843:10:46,  9.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True]) [tensor([284004000]), tensor([284007600])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/306781 [00:49<844:35:50,  9.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False]) [tensor([284004000]), tensor([284007600])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/306781 [00:59<844:01:47,  9.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True]) [tensor([284007600]), tensor([284011200])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/306781 [01:09<842:54:09,  9.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False]) [tensor([284007600]), tensor([284011200])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/306781 [01:19<843:11:34,  9.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True]) [tensor([284011200]), tensor([284014800])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/306781 [01:29<842:30:50,  9.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False]) [tensor([284011200]), tensor([284014800])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/306781 [01:38<936:53:52, 10.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True]) [tensor([284014800]), tensor([284018400])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, num_workers=0)\n",
    "# if the batch size > 1, need to loop over data below, as batch size ~ batched length of sequence\n",
    "# batch size with different samples is handled by distributed DDP or FSDP training\n",
    "\n",
    "k = 0\n",
    "for batch in tqdm.tqdm(loader):\n",
    "    # y = concat_and_reshape(data[\"y\"].squeeze(1), data[\"y_surf\"].squeeze(1))\n",
    "    # Plotting y\n",
    "    # plt.figure(figsize=(10, 4))\n",
    "    # plt.subplot(1, 2, 1)\n",
    "    # plt.pcolor(y[0, 66, :, :].to('cpu').numpy(), cmap='RdBu', vmin=-3, vmax=3)\n",
    "    # plt.colorbar()\n",
    "    # plt.gca().invert_yaxis()\n",
    "    # plt.title(f'Forecast hour {data[\"forecast_hour\"].item()}')\n",
    "    # plt.show()\n",
    "\n",
    "    # print(f'A {batch[\"forecast_hour\"].item()}')\n",
    "\n",
    "    # if batch['stop_forecast']:\n",
    "    #     stop_forecast = True\n",
    "    #     break\n",
    "\n",
    "    # if conf[\"data\"][\"history_len\"] == i:\n",
    "    #     stop_forecast = True\n",
    "    #     break\n",
    "    print(batch[\"stop_forecast\"], batch[\"datetime\"])\n",
    "\n",
    "    k += 1\n",
    "\n",
    "    if k == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ff469078-30f2-4cfe-9f7e-acdfdc827c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"stop_forecast\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d3f74c-1c28-4834-b5a4-ad75863364ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "credit",
   "language": "python",
   "name": "credit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5517cb9d-e134-4f4b-bdcc-c6d27133f7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "import datetime\n",
    "from credit.transforms import load_transforms\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033ab99d-56ac-4e0d-835f-96c701300b11",
   "metadata": {},
   "source": [
    "# Load a config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9192aa9-f968-4a0e-9e87-28c92a30c7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    \"/glade/derecho/scratch/schreck/repos/miles-credit/results/wxformer/6hr/model.yml\"\n",
    ") as cf:\n",
    "    conf = yaml.load(cf, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53479cdf-7ed6-4bfe-9509-3802e371d27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from credit.data import (\n",
    "    Sample,\n",
    "    drop_var_from_dataset,\n",
    "    get_forward_data_netCDF4,\n",
    "    find_key_for_number,\n",
    "    extract_month_day_hour,\n",
    "    find_common_indices,\n",
    "    ERA5_and_Forcing_Dataset,\n",
    "    get_forward_data,\n",
    ")\n",
    "import os\n",
    "from torch.utils.data import get_worker_info\n",
    "from torch.utils.data.distributed import DistributedSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bc27c7-216d-4ed1-911f-baf56ce14082",
   "metadata": {},
   "source": [
    "# Load transforms and single-step / one-shot dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f43baf7-d7c9-410c-9036-fa2f9a04e1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ERA_files = sorted(glob.glob(conf[\"data\"][\"save_loc\"]))\n",
    "varname_upper_air = conf[\"data\"][\"variables\"]\n",
    "surface_files = sorted(glob.glob(conf[\"data\"][\"save_loc_surface\"]))\n",
    "diagnostic_files = None  # sorted(glob.glob(conf[\"data\"][\"save_loc_diagnostic\"]))\n",
    "is_train = False\n",
    "\n",
    "if (\"forcing_variables\" in conf[\"data\"]) and (\n",
    "    len(conf[\"data\"][\"forcing_variables\"]) > 0\n",
    "):\n",
    "    forcing_files = conf[\"data\"][\"save_loc_forcing\"]\n",
    "    varname_forcing = conf[\"data\"][\"forcing_variables\"]\n",
    "else:\n",
    "    forcing_files = None\n",
    "    varname_forcing = None\n",
    "\n",
    "if (\"static_variables\" in conf[\"data\"]) and (len(conf[\"data\"][\"static_variables\"]) > 0):\n",
    "    static_files = conf[\"data\"][\"save_loc_static\"]\n",
    "    varname_static = conf[\"data\"][\"static_variables\"]\n",
    "else:\n",
    "    static_files = None\n",
    "    varname_static = None\n",
    "\n",
    "if surface_files is not None:\n",
    "    varname_surface = conf[\"data\"][\"surface_variables\"]\n",
    "else:\n",
    "    varname_surface = None\n",
    "\n",
    "if diagnostic_files is not None:\n",
    "    varname_diagnostic = conf[\"data\"][\"diagnostic_variables\"]\n",
    "else:\n",
    "    varname_diagnostic = None\n",
    "\n",
    "# number of previous lead time inputs\n",
    "history_len = conf[\"data\"][\"history_len\"]\n",
    "valid_history_len = conf[\"data\"][\"valid_history_len\"]\n",
    "\n",
    "# number of lead times to forecast\n",
    "forecast_len = conf[\"data\"][\"forecast_len\"]\n",
    "valid_forecast_len = conf[\"data\"][\"valid_forecast_len\"]\n",
    "\n",
    "if is_train:\n",
    "    history_len = history_len\n",
    "    forecast_len = forecast_len\n",
    "    # print out training / validation\n",
    "    name = \"training\"\n",
    "else:\n",
    "    history_len = valid_history_len\n",
    "    forecast_len = valid_forecast_len\n",
    "    name = \"validation\"\n",
    "\n",
    "# max_forecast_len\n",
    "if \"max_forecast_len\" not in conf[\"data\"]:\n",
    "    max_forecast_len = None\n",
    "else:\n",
    "    max_forecast_len = conf[\"data\"][\"max_forecast_len\"]\n",
    "\n",
    "# skip_periods\n",
    "if \"skip_periods\" not in conf[\"data\"]:\n",
    "    skip_periods = None\n",
    "else:\n",
    "    skip_periods = conf[\"data\"][\"skip_periods\"]\n",
    "\n",
    "# one_shot\n",
    "if \"one_shot\" not in conf[\"data\"]:\n",
    "    one_shot = None\n",
    "else:\n",
    "    one_shot = conf[\"data\"][\"one_shot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05581783-78fd-4737-8b9c-41055ecf3f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/schreck/conda-envs/credit/lib/python3.11/site-packages/pyproj/__init__.py:89: UserWarning: pyproj unable to set database path.\n",
      "  _pyproj_global_context_initialize()\n"
     ]
    }
   ],
   "source": [
    "# data preprocessing utils\n",
    "transforms = load_transforms(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c05e66e2-a9a9-4099-af0f-35610bf86c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ERA5_and_Forcing_Dataset(\n",
    "    varname_upper_air=varname_upper_air,\n",
    "    varname_surface=varname_surface,\n",
    "    varname_forcing=varname_forcing,\n",
    "    varname_static=varname_static,\n",
    "    varname_diagnostic=varname_diagnostic,\n",
    "    filenames=all_ERA_files,\n",
    "    filename_surface=surface_files,\n",
    "    filename_forcing=forcing_files,\n",
    "    filename_static=static_files,\n",
    "    filename_diagnostic=diagnostic_files,\n",
    "    history_len=history_len,\n",
    "    forecast_len=forecast_len,\n",
    "    skip_periods=skip_periods,\n",
    "    one_shot=one_shot,\n",
    "    max_forecast_len=max_forecast_len,\n",
    "    transform=transforms,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "714ddbdb-934a-4ed7-a249-a46105019a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5de8bb2-e509-47cc-aee8-c44aa2f53e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['x_forcing_static', 'x_surf', 'x', 'y_surf', 'y', 'index'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1d15b09-f7a2-4e90-a8cd-0e099ec1a0de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[\"index\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58019f2a-1918-4d7d-8e95-12191c17b97e",
   "metadata": {},
   "source": [
    "# Load N multi-step dataset where N is seqeunce length (Noah's loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a9edfd-d0f2-446a-a392-45ca1111a3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/6974695/python-process-pool-non-daemonic\n",
    "\n",
    "\n",
    "class DistributedSequentialDatasetV2(torch.utils.data.IterableDataset):\n",
    "    # https://colab.research.google.com/drive/1OFLZnX9y5QUFNONuvFsxOizq4M-tFvk-?usp=sharing#scrollTo=CxSCQPOMHgwo\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        varname_upper_air,\n",
    "        varname_surface,\n",
    "        varname_forcing,\n",
    "        varname_static,\n",
    "        varname_diagnostic,\n",
    "        filenames,\n",
    "        filename_surface=None,\n",
    "        filename_forcing=None,\n",
    "        filename_static=None,\n",
    "        filename_diagnostic=None,\n",
    "        rank=0,\n",
    "        world_size=1,\n",
    "        history_len=2,\n",
    "        forecast_len=0,\n",
    "        transform=None,\n",
    "        seed=42,\n",
    "        skip_periods=None,\n",
    "        one_shot=None,\n",
    "        max_forecast_len=None,\n",
    "        shuffle=True,\n",
    "    ):\n",
    "        self.history_len = history_len\n",
    "        self.forecast_len = forecast_len\n",
    "        self.transform = transform\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "        self.shuffle = shuffle\n",
    "        self.current_epoch = 0\n",
    "\n",
    "        # skip periods\n",
    "        self.skip_periods = skip_periods\n",
    "        if self.skip_periods is None:\n",
    "            self.skip_periods = 1\n",
    "\n",
    "        # one shot option\n",
    "        self.one_shot = one_shot\n",
    "\n",
    "        # total number of needed forecast lead times\n",
    "        self.total_seq_len = self.history_len + self.forecast_len\n",
    "\n",
    "        # set random seed\n",
    "        self.rng = np.random.default_rng(seed=seed)\n",
    "\n",
    "        # max possible forecast len\n",
    "        self.max_forecast_len = max_forecast_len\n",
    "\n",
    "        # ======================================================== #\n",
    "        # ERA5 operations\n",
    "        all_files = []\n",
    "        filenames = sorted(filenames)\n",
    "\n",
    "        for fn in filenames:\n",
    "            # drop variables if they are not in the config\n",
    "            xarray_dataset = get_forward_data(filename=fn)\n",
    "            xarray_dataset = drop_var_from_dataset(xarray_dataset, varname_upper_air)\n",
    "\n",
    "            # collect yearly datasets within a list\n",
    "            all_files.append(xarray_dataset)\n",
    "\n",
    "        self.all_files = all_files\n",
    "\n",
    "        # set data places:\n",
    "        indo = 0\n",
    "        self.meta_data_dict = {}\n",
    "        for ee, bb in enumerate(self.all_files):\n",
    "            self.meta_data_dict[str(ee)] = [\n",
    "                len(bb[\"time\"]),\n",
    "                indo,\n",
    "                indo + len(bb[\"time\"]),\n",
    "            ]\n",
    "            indo += len(bb[\"time\"]) + 1\n",
    "\n",
    "        # get sample indices from ERA5 upper-air files:\n",
    "        ind_start = 0\n",
    "        self.ERA5_indices = {}  # <------ change\n",
    "        for ind_file, ERA5_xarray in enumerate(self.all_files):\n",
    "            # [number of samples, ind_start, ind_end]\n",
    "            self.ERA5_indices[str(ind_file)] = [\n",
    "                len(ERA5_xarray[\"time\"]),\n",
    "                ind_start,\n",
    "                ind_start + len(ERA5_xarray[\"time\"]),\n",
    "            ]\n",
    "            ind_start += len(ERA5_xarray[\"time\"]) + 1\n",
    "\n",
    "        # ======================================================== #\n",
    "        # forcing file\n",
    "        self.filename_forcing = filename_forcing\n",
    "\n",
    "        if self.filename_forcing is not None:\n",
    "            assert os.path.isfile(\n",
    "                filename_forcing\n",
    "            ), \"Cannot find forcing file [{}]\".format(filename_forcing)\n",
    "\n",
    "            # drop variables if they are not in the config\n",
    "            xarray_dataset = get_forward_data_netCDF4(filename_forcing)\n",
    "            xarray_dataset = drop_var_from_dataset(xarray_dataset, varname_forcing)\n",
    "\n",
    "            self.xarray_forcing = xarray_dataset\n",
    "        else:\n",
    "            self.xarray_forcing = False\n",
    "\n",
    "        # ======================================================== #\n",
    "        # static file\n",
    "        self.filename_static = filename_static\n",
    "\n",
    "        if self.filename_static is not None:\n",
    "            assert os.path.isfile(\n",
    "                filename_static\n",
    "            ), \"Cannot find static file [{}]\".format(filename_static)\n",
    "\n",
    "            # drop variables if they are not in the config\n",
    "            xarray_dataset = get_forward_data_netCDF4(filename_static)\n",
    "            xarray_dataset = drop_var_from_dataset(xarray_dataset, varname_static)\n",
    "\n",
    "            self.xarray_static = xarray_dataset\n",
    "        else:\n",
    "            self.xarray_static = False\n",
    "\n",
    "        # ======================================================== #\n",
    "        # diagnostic file\n",
    "        self.filename_diagnostic = filename_diagnostic\n",
    "\n",
    "        if self.filename_diagnostic is not None:\n",
    "            diagnostic_files = []\n",
    "            filename_diagnostic = sorted(filename_diagnostic)\n",
    "\n",
    "            for fn in filename_diagnostic:\n",
    "                # drop variables if they are not in the config\n",
    "                xarray_dataset = get_forward_data(filename=fn)\n",
    "                xarray_dataset = drop_var_from_dataset(\n",
    "                    xarray_dataset, varname_diagnostic\n",
    "                )\n",
    "\n",
    "                diagnostic_files.append(xarray_dataset)\n",
    "\n",
    "            self.diagnostic_files = diagnostic_files\n",
    "\n",
    "            assert (\n",
    "                len(self.diagnostic_files) == len(self.all_files)\n",
    "            ), \"Mismatch between the total number of diagnostic files and upper-air files\"\n",
    "        else:\n",
    "            self.diagnostic_files = False\n",
    "\n",
    "        # ======================================================== #\n",
    "        # surface files\n",
    "        if filename_surface is not None:\n",
    "            surface_files = []\n",
    "            filename_surface = sorted(filename_surface)\n",
    "\n",
    "            for fn in filename_surface:\n",
    "                # drop variables if they are not in the config\n",
    "                xarray_dataset = get_forward_data(filename=fn)\n",
    "                xarray_dataset = drop_var_from_dataset(xarray_dataset, varname_surface)\n",
    "\n",
    "                surface_files.append(xarray_dataset)\n",
    "\n",
    "            self.surface_files = surface_files\n",
    "\n",
    "            assert len(self.surface_files) == len(\n",
    "                self.all_files\n",
    "            ), \"Mismatch between the total number of surface files and upper-air files\"\n",
    "        else:\n",
    "            self.surface_files = False\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # Total sequence length of each sample.\n",
    "        self.total_seq_len = self.history_len + self.forecast_len\n",
    "\n",
    "    def __len__(self):\n",
    "        # compute the total number of length\n",
    "        total_len = 0\n",
    "        for ERA5_xarray in self.all_files:\n",
    "            total_len += len(ERA5_xarray[\"time\"]) - self.total_seq_len + 1\n",
    "        return total_len\n",
    "\n",
    "    def set_epoch(self, epoch):\n",
    "        self.current_epoch = epoch\n",
    "\n",
    "    def __iter__(self):\n",
    "        worker_info = get_worker_info()\n",
    "        num_workers = worker_info.num_workers if worker_info is not None else 1\n",
    "        worker_id = worker_info.id if worker_info is not None else 0\n",
    "        sampler = DistributedSampler(\n",
    "            self,\n",
    "            num_replicas=num_workers * self.world_size,\n",
    "            rank=self.rank * num_workers + worker_id,\n",
    "            shuffle=self.shuffle,\n",
    "        )\n",
    "        sampler.set_epoch(self.current_epoch)\n",
    "\n",
    "        for index in iter(sampler):\n",
    "            indices = list(range(index, index + self.history_len + self.forecast_len))\n",
    "            stop_forecast = False\n",
    "\n",
    "            for k, ind in enumerate(indices):\n",
    "                # select the ind_file based on the iter index\n",
    "                ind_file = find_key_for_number(ind, self.ERA5_indices)\n",
    "\n",
    "                # get the ind within the current file\n",
    "                ind_start = self.ERA5_indices[ind_file][1]\n",
    "                ind_start_in_file = ind - ind_start\n",
    "\n",
    "                # handle out-of-bounds\n",
    "                ind_largest = len(self.all_files[int(ind_file)][\"time\"]) - (\n",
    "                    self.history_len + self.forecast_len + 1\n",
    "                )\n",
    "                if ind_start_in_file > ind_largest:\n",
    "                    ind_start_in_file = ind_largest\n",
    "                # ========================================================================== #\n",
    "                # subset xarray on time dimension & load it to the memory\n",
    "\n",
    "                ind_end_in_file = (\n",
    "                    ind_start_in_file + self.history_len + self.forecast_len\n",
    "                )\n",
    "\n",
    "                ## ERA5_subset: a xarray dataset that contains training input and target (for the current index)\n",
    "                ERA5_subset = (\n",
    "                    self.all_files[int(ind_file)]\n",
    "                    .isel(time=slice(ind_start_in_file, ind_end_in_file + 1))\n",
    "                    .load()\n",
    "                )\n",
    "\n",
    "                if self.surface_files:\n",
    "                    ## subset surface variables\n",
    "                    surface_subset = (\n",
    "                        self.surface_files[int(ind_file)]\n",
    "                        .isel(time=slice(ind_start_in_file, ind_end_in_file + 1))\n",
    "                        .load()\n",
    "                    )\n",
    "\n",
    "                    ## merge upper-air and surface here:\n",
    "                    ERA5_subset = ERA5_subset.merge(surface_subset)\n",
    "\n",
    "                # ==================================================== #\n",
    "                # split ERA5_subset into training inputs and targets + merge with forcing and static\n",
    "\n",
    "                # the ind_end of the ERA5_subset\n",
    "                ind_end_time = len(ERA5_subset[\"time\"])\n",
    "\n",
    "                # datetiem information as int number (used in some normalization methods)\n",
    "                datetime_as_number = ERA5_subset.time.values.astype(\n",
    "                    \"datetime64[s]\"\n",
    "                ).astype(int)\n",
    "\n",
    "                # ==================================================== #\n",
    "                # xarray dataset as input\n",
    "                ## historical_ERA5_images: the final input\n",
    "\n",
    "                historical_ERA5_images = ERA5_subset.isel(\n",
    "                    time=slice(0, self.history_len, self.skip_periods)\n",
    "                )\n",
    "\n",
    "                # merge forcing inputs\n",
    "                if self.xarray_forcing:\n",
    "                    # =============================================================================== #\n",
    "                    # matching month, day, hour between forcing and upper air [time]\n",
    "                    # this approach handles leap year forcing file and non-leap-year upper air file\n",
    "                    month_day_forcing = extract_month_day_hour(\n",
    "                        np.array(self.xarray_forcing[\"time\"])\n",
    "                    )\n",
    "                    month_day_inputs = extract_month_day_hour(\n",
    "                        np.array(historical_ERA5_images[\"time\"])\n",
    "                    )  # <-- upper air\n",
    "                    # indices to subset\n",
    "                    ind_forcing, _ = find_common_indices(\n",
    "                        month_day_forcing, month_day_inputs\n",
    "                    )\n",
    "                    forcing_subset_input = self.xarray_forcing.isel(\n",
    "                        time=ind_forcing\n",
    "                    ).load()\n",
    "                    # forcing and upper air have different years but the same mon/day/hour\n",
    "                    # safely replace forcing time with upper air time\n",
    "                    forcing_subset_input[\"time\"] = historical_ERA5_images[\"time\"]\n",
    "                    # =============================================================================== #\n",
    "\n",
    "                    # merge\n",
    "                    historical_ERA5_images = historical_ERA5_images.merge(\n",
    "                        forcing_subset_input\n",
    "                    )\n",
    "\n",
    "                # merge static inputs\n",
    "                if self.xarray_static:\n",
    "                    # expand static var on time dim\n",
    "                    N_time_dims = len(ERA5_subset[\"time\"])\n",
    "                    static_subset_input = self.xarray_static.expand_dims(\n",
    "                        dim={\"time\": N_time_dims}\n",
    "                    )\n",
    "                    # assign coords 'time'\n",
    "                    static_subset_input = static_subset_input.assign_coords(\n",
    "                        {\"time\": ERA5_subset[\"time\"]}\n",
    "                    )\n",
    "\n",
    "                    # slice + load to the GPU\n",
    "                    static_subset_input = static_subset_input.isel(\n",
    "                        time=slice(0, self.history_len, self.skip_periods)\n",
    "                    ).load()\n",
    "\n",
    "                    # update\n",
    "                    static_subset_input[\"time\"] = historical_ERA5_images[\"time\"]\n",
    "\n",
    "                    # merge\n",
    "                    historical_ERA5_images = historical_ERA5_images.merge(\n",
    "                        static_subset_input\n",
    "                    )\n",
    "\n",
    "                # ==================================================== #\n",
    "                # xarray dataset as target\n",
    "                ## target_ERA5_images: the final target\n",
    "\n",
    "                # return the next state only\n",
    "                target_ERA5_images = ERA5_subset.isel(\n",
    "                    time=slice(\n",
    "                        self.history_len,\n",
    "                        self.history_len + self.skip_periods,\n",
    "                        self.skip_periods,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                ## merge diagnoisc input here:\n",
    "                if self.diagnostic_files:\n",
    "                    # subset diagnostic variables\n",
    "                    diagnostic_subset = (\n",
    "                        self.diagnostic_files[int(ind_file)]\n",
    "                        .isel(time=slice(ind_start_in_file, ind_end_in_file + 1))\n",
    "                        .load()\n",
    "                    )\n",
    "\n",
    "                    # merge into the target dataset\n",
    "                    target_diagnostic = diagnostic_subset.isel(\n",
    "                        time=slice(self.history_len, ind_end_time, self.skip_periods)\n",
    "                    )\n",
    "                    target_ERA5_images = target_ERA5_images.merge(target_diagnostic)\n",
    "\n",
    "                if self.one_shot is not None:\n",
    "                    # get the final state of the target as one-shot\n",
    "                    target_ERA5_images = target_ERA5_images.isel(time=slice(0, 1))\n",
    "\n",
    "                # pipe xarray datasets to the sampler\n",
    "                sample = Sample(\n",
    "                    historical_ERA5_images=historical_ERA5_images,\n",
    "                    target_ERA5_images=target_ERA5_images,\n",
    "                    datetime_index=datetime_as_number,\n",
    "                )\n",
    "\n",
    "                # ==================================== #\n",
    "                # data normalization\n",
    "                if self.transform:\n",
    "                    sample = self.transform(sample)\n",
    "\n",
    "                # assign sample index\n",
    "                sample[\"index\"] = index\n",
    "\n",
    "                stop_forecast = k == self.forecast_len\n",
    "\n",
    "                sample[\"forecast_hour\"] = k + 1\n",
    "                sample[\"index\"] = index\n",
    "                sample[\"stop_forecast\"] = stop_forecast\n",
    "                sample[\"datetime\"] = [\n",
    "                    int(\n",
    "                        historical_ERA5_images.time.values[0]\n",
    "                        .astype(\"datetime64[s]\")\n",
    "                        .astype(int)\n",
    "                    ),\n",
    "                    int(\n",
    "                        target_ERA5_images.time.values[0]\n",
    "                        .astype(\"datetime64[s]\")\n",
    "                        .astype(int)\n",
    "                    ),\n",
    "                ]\n",
    "\n",
    "                yield sample\n",
    "\n",
    "                if stop_forecast:\n",
    "                    break\n",
    "\n",
    "                if k == self.forecast_len:\n",
    "                    break\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        worker_info = get_worker_info()\n",
    "        num_workers = worker_info.num_workers if worker_info is not None else 1\n",
    "        worker_id = worker_info.id if worker_info is not None else 0\n",
    "        sampler = DistributedSampler(self, num_replicas=num_workers * self.world_size,\n",
    "                                     rank=self.rank * num_workers + worker_id, shuffle=self.shuffle)\n",
    "        sampler.set_epoch(self.current_epoch)\n",
    "    \n",
    "        process_index_partial = partial(process_index,\n",
    "            ERA5_indices=self.ERA5_indices,\n",
    "            all_files=self.all_files,\n",
    "            surface_files=self.surface_files,\n",
    "            history_len=self.history_len,\n",
    "            forecast_len=self.forecast_len,\n",
    "            skip_periods=self.skip_periods,\n",
    "            xarray_forcing=self.xarray_forcing,\n",
    "            xarray_static=self.xarray_static,\n",
    "            diagnostic_files=self.diagnostic_files,\n",
    "            one_shot=self.one_shot,\n",
    "            transform=self.transform\n",
    "        )\n",
    "\n",
    "        for index in iter(sampler):\n",
    "            indices = list(range(index, index + self.history_len + self.forecast_len))\n",
    "            # Use pool.map to parallelize the inner loop\n",
    "            for ind in range(len(indices)):\n",
    "                yield process_index_partial(index, ind)\n",
    "                if sample['stop_forecast']:\n",
    "                    break\n",
    "    \n",
    "        # with Pool(2) as p:\n",
    "        #     for index in iter(sampler):\n",
    "        #         indices = list(range(index, index + self.history_len + self.forecast_len))\n",
    "        #         # Use pool.map to parallelize the inner loop\n",
    "        #         for sample in p.map(process_index_partial, [(index, ind) for ind in range(len(indices))]):\n",
    "        #             yield sample\n",
    "        #             if sample['stop_forecast']:\n",
    "        #                 break\n",
    "\n",
    "\n",
    "def process_index(\n",
    "    _index, ERA5_indices, all_files, surface_files, history_len, forecast_len,\n",
    "    skip_periods, xarray_forcing, xarray_static, diagnostic_files, one_shot,\n",
    "    transform\n",
    "):\n",
    "    index, ind = _index\n",
    "    # select the ind_file based on the iter index \n",
    "    ind_file = find_key_for_number(ind, ERA5_indices)\n",
    "\n",
    "    # get the ind within the current file\n",
    "    ind_start = ERA5_indices[ind_file][1]\n",
    "    ind_start_in_file = ind - ind_start\n",
    "\n",
    "    # handle out-of-bounds\n",
    "    ind_largest = len(all_files[int(ind_file)]['time']) - (history_len + forecast_len + 1)\n",
    "    if ind_start_in_file > ind_largest:\n",
    "        ind_start_in_file = ind_largest\n",
    "\n",
    "    # subset xarray on time dimension & load it to the memory\n",
    "    ind_end_in_file = ind_start_in_file + history_len + forecast_len\n",
    "    \n",
    "    ERA5_subset = all_files[int(ind_file)].isel(\n",
    "        time=slice(ind_start_in_file, ind_end_in_file + 1)).load()\n",
    "    \n",
    "    if surface_files:\n",
    "        surface_subset = surface_files[int(ind_file)].isel(\n",
    "            time=slice(ind_start_in_file, ind_end_in_file + 1)).load()\n",
    "        ERA5_subset = ERA5_subset.merge(surface_subset)\n",
    "\n",
    "    ind_end_time = len(ERA5_subset['time'])\n",
    "    datetime_as_number = ERA5_subset.time.values.astype('datetime64[s]').astype(int)\n",
    "\n",
    "    historical_ERA5_images = ERA5_subset.isel(time=slice(0, history_len, skip_periods))\n",
    "\n",
    "    if xarray_forcing:\n",
    "        month_day_forcing = extract_month_day_hour(np.array(xarray_forcing['time']))\n",
    "        month_day_inputs = extract_month_day_hour(np.array(historical_ERA5_images['time']))\n",
    "        ind_forcing, _ = find_common_indices(month_day_forcing, month_day_inputs)\n",
    "        forcing_subset_input = xarray_forcing.isel(time=ind_forcing).load()\n",
    "        forcing_subset_input['time'] = historical_ERA5_images['time']\n",
    "        historical_ERA5_images = historical_ERA5_images.merge(forcing_subset_input)\n",
    "\n",
    "    if xarray_static:\n",
    "        N_time_dims = len(ERA5_subset['time'])\n",
    "        static_subset_input = xarray_static.expand_dims(dim={\"time\": N_time_dims})\n",
    "        static_subset_input = static_subset_input.assign_coords({'time': ERA5_subset['time']})\n",
    "        static_subset_input = static_subset_input.isel(time=slice(0, history_len, skip_periods)).load()\n",
    "        static_subset_input['time'] = historical_ERA5_images['time']\n",
    "        historical_ERA5_images = historical_ERA5_images.merge(static_subset_input)\n",
    "    \n",
    "    target_ERA5_images = ERA5_subset.isel(time=slice(history_len, history_len + skip_periods, skip_periods))\n",
    "\n",
    "    if diagnostic_files:\n",
    "        diagnostic_subset = diagnostic_files[int(ind_file)].isel(\n",
    "            time=slice(ind_start_in_file, ind_end_in_file + 1)).load()\n",
    "        target_diagnostic = diagnostic_subset.isel(time=slice(history_len, ind_end_time, skip_periods))\n",
    "        target_ERA5_images = target_ERA5_images.merge(target_diagnostic)\n",
    "        \n",
    "    if one_shot is not None:\n",
    "        target_ERA5_images = target_ERA5_images.isel(time=slice(0, 1))\n",
    "\n",
    "    sample = Sample(\n",
    "        historical_ERA5_images=historical_ERA5_images,\n",
    "        target_ERA5_images=target_ERA5_images,\n",
    "        datetime_index=datetime_as_number\n",
    "    )\n",
    "\n",
    "    if transform:\n",
    "        sample = transform(sample)\n",
    "\n",
    "    sample[\"index\"] = index\n",
    "    stop_forecast = (ind == forecast_len)\n",
    "    sample['forecast_hour'] = ind + 1\n",
    "    sample['index'] = index\n",
    "    sample['stop_forecast'] = stop_forecast\n",
    "    sample[\"datetime\"] = [\n",
    "        int(historical_ERA5_images.time.values[0].astype('datetime64[s]').astype(int)),\n",
    "        int(target_ERA5_images.time.values[0].astype('datetime64[s]').astype(int))\n",
    "    ]\n",
    "\n",
    "    return sample\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852dfacc-2650-4fa9-9122-4e5c16512125",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_len = 3  # really its 4\n",
    "\n",
    "# Z-score\n",
    "dataset = DistributedSequentialDatasetV2(\n",
    "    varname_upper_air=varname_upper_air,\n",
    "    varname_surface=varname_surface,\n",
    "    varname_forcing=varname_forcing,\n",
    "    varname_static=varname_static,\n",
    "    varname_diagnostic=varname_diagnostic,\n",
    "    filenames=all_ERA_files,\n",
    "    filename_surface=surface_files,\n",
    "    filename_forcing=forcing_files,\n",
    "    filename_static=static_files,\n",
    "    filename_diagnostic=diagnostic_files,\n",
    "    history_len=history_len,\n",
    "    forecast_len=forecast_len,\n",
    "    skip_periods=skip_periods,\n",
    "    one_shot=one_shot,\n",
    "    max_forecast_len=max_forecast_len,\n",
    "    transform=transforms,\n",
    "    rank=0,\n",
    "    world_size=1,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461f10db-15be-4e4a-8043-b457a97f072a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, result in enumerate(dataset):\n",
    "    print(\n",
    "        k,\n",
    "        result[\"stop_forecast\"],\n",
    "        [\n",
    "            datetime.utcfromtimestamp(x).strftime(\"%B %d, %Y at %I:%M %p UTC\")\n",
    "            for x in result[\"datetime\"]\n",
    "        ],\n",
    "    )\n",
    "    if (k + 1) == forecast_len * 5 + 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada85249-1024-45a3-aed9-9a78ed92b793",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952f3885-2a2b-4a53-89b2-953226fcf54a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hcredit]",
   "language": "python",
   "name": "conda-env-hcredit-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

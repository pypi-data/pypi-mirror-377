# OASIS-SDK

## 1. Concept

OASIS-LLM-PROXY-CLIENTëŠ” OpenAI, Azure OpenAI ë“± ë‹¤ì–‘í•œ LLM Providerì˜ ê³µì‹ SDK ë° LangChainì„ ì–‡ê²Œ wrappingí•˜ì—¬, ì‚¬ë‚´ ê·œì¹™ì— ë§ëŠ” í•„ë“œ ì…ë ¥ê³¼ í”„ë¡ì‹œ ì„œë²„ë¥¼ í†µí•œ í‚¤ ì£¼ì…ì„ ì§€ì›í•˜ëŠ” Python ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤. ì›ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ëª¨ë“  ê¸°ëŠ¥ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì–´, ê¸°ì¡´ SDKì™€ LangChainì˜ í™•ì¥ì„±ê³¼ í˜¸í™˜ì„±ì„ ìµœëŒ€í•œ ë³´ì¥í•©ë‹ˆë‹¤.

## 2. Usage

### 2.1 install

```
pip install oasis-sdk
```

### 2.2 example

**client parameters**

[required]

- user_id: ì‚¬ìš©ìì˜ id
- workspace_code: ì‚¬ìš©ìì˜ workspace code
- tenant_code: ì‚¬ìš©ìì˜ tenant code
- proxy_url: Llm Proxy Server

[optional]

- user_ip: ì‚¬ìš©ìì˜ ip (defualt=127.0.0.1)
- plugin_name: í˜¸ì¶œí•œ ì‹œìŠ¤í…œ ëª… (ex, chatbot, mcp1, rag-mcp, ..., default=default-plugin)

[auto]

- root_id: í´ë¼ì´ì–¸íŠ¸ ìƒì„±ì‹œ ë°œê¸‰
- req_id: ìš”ì²­ì‹œë§ˆë‹¤ ë°œê¸‰

ğŸ“ **ì£¼ì˜**

- 1ë²ˆì˜ ì—°ì†ì ì¸ ìˆ˜í–‰ì—ì„œ root_idëŠ” ê³ ì •ë˜ì–´ì•¼ í•¨
- ì—°ê³„ë˜ëŠ” ì‹œìŠ¤í…œì—ì„œëŠ” í´ë¼ì´ì–¸íŠ¸ ìƒì„±ì‹œ ì´ˆê¸° ë°œê¸‰ëœ root_idë¥¼ ì£¼ì…í•˜ì—¬ ì‚¬ìš©

#### 2.2.1 SDK

**1. openai**

```python
# ë™ê¸° í´ë¼ì´ì–¸íŠ¸
client = OasisOpenAI(
    user_id="user_id",
    workspace_code="workspace_code",
    tenant_code="tenant_code",
    proxy_url="llm_proxy_server",
    user_ip="user_ip",
    plugin_name="your_system"
)

# ë™ê¸° í˜¸ì¶œ
resp = client.chat.completions.create(
    model="model",
    messages=[{"role": "user", "content": "ì•ˆë…•?"}],
)

# ë™ê¸° ìŠ¤íŠ¸ë¦¼
stream = client.chat.completions.create(
    model="model",
    messages=[{"role": "user", "content": "ì•ˆë…•?"}],
    stream=True,
)

# ë™ê¸° ì„ë² ë”©
emb = client.embeddings.create(
    model="embedding_model",
    input="ì„ë² ë”© í…ŒìŠ¤íŠ¸ ë¬¸ì¥",
)

# ë¹„ë™ê¸° í´ë¼ì´ì–¸íŠ¸
aync_client = OasisAsyncOpenAI(
    user_id="user_id",
    workspace_code="workspace_code",
    tenant_code="tenant_code",
    proxy_url="llm_proxy_server",
    user_ip="user_ip",
    plugin_name="your_system"
)

# ë¹„ë™ê¸° í˜¸ì¶œ
resp = await aync_client.chat.completions.create(
    model="model",
    messages=[{"role": "user", "content": "ì•ˆë…•?"}],
)

# ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¼
stream = await aync_client.chat.completions.create(
    model="model",
    messages=[{"role": "user", "content": "ì•ˆë…•?"}],
    stream=True,
)

# ë¹„ë™ê¸° ì„ë² ë”©
emb = await async_client.embeddings.create(
    model="embedding_model",
    input=["ì²« ë¬¸ì¥", "ë‘˜ì§¸ ë¬¸ì¥"],
)
```

**2. azure openai**

```python
# ë™ê¸° í´ë¼ì´ì–¸íŠ¸
client = OasisAzureOpenAI(
    user_id="user_id",
    workspace_code="workspace_code",
    tenant_code="tenant_code",
    proxy_url="llm_proxy_server",
    user_ip="user_ip",
    plugin_name="your_system",
    api_version="your_api_version"
)

# ë™ê¸° í˜¸ì¶œ
resp = client.chat.completions.create(
    model="deployment",
    messages=[{"role": "user", "content": "ì•ˆë…•?"}],
)

# ë™ê¸° ìŠ¤íŠ¸ë¦¼
stream = client.chat.completions.create(
    model="deployment",
    messages=[{"role": "user", "content": "ì•ˆë…•?"}],
    stream=True,
)

# ë™ê¸° ì„ë² ë”©
emb = client.embeddings.create(
    model="embedding_deployment",
    input="Azure ì„ë² ë”© ì˜ˆì‹œ",
)

# ë¹„ë™ê¸° í´ë¼ì´ì–¸íŠ¸
aync_client = OasisAsyncAzureOpenAI(
    user_id="user_id",
    workspace_code="workspace_code",
    tenant_code="tenant_code",
    proxy_url="llm_proxy_server",
    user_ip="user_ip",
    plugin_name="your_system",
    api_version="your_api_version"
)

# ë¹„ë™ê¸° í˜¸ì¶œ
resp = await aync_client.chat.completions.create(
    model="deployment",
    messages=[{"role": "user", "content": "ì•ˆë…•?"}],
)

# ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¼
stream = await aync_client.chat.completions.create(
    model="deployment",
    messages=[{"role": "user", "content": "ì•ˆë…•?"}],
    stream=True,
)

# ë¹„ë™ê¸° ì„ë² ë”©
emb = await aclient.embeddings.create(
    model="embedding_deployment",
    input=["A ë¬¸ì¥", "B ë¬¸ì¥"],
)
```

#### 2.2.2 Langchain

- ê¸°ì¡´ íŒ¨í‚¤ì§€ì™€ ë™ì¼í•˜ê²Œ ì„¤ê³„í•˜ì˜€ê¸° ë•Œë¬¸ì— langchainì€ í´ë¼ì´ì–¸íŠ¸ ìƒì„±ì‹œ ëª¨ë¸ë„ ê°™ì´ ì§€ì • í•„ìš”

**1. openai**

```python
# chat
llm = OasisChatOpenAI(
    user_id="user_id",
    workspace_code="workspace_code",
    tenant_code="tenant_code",
    model="model"
    proxy_url="llm_proxy_server",
    user_ip="user_ip",
    plugin_name="your_system"
)

# ë™ê¸° í˜¸ì¶œ
resp = llm.invoke("ì•ˆë…• Azure LangChain!")

# ë¹„ë™ê¸° í˜¸ì¶œ
resp = await llm.ainvoke("ì•ˆë…• Azure LangChain!")

# ë™ê¸° ìŠ¤íŠ¸ë¦¼
resp = llm.stream("ì•ˆë…• Azure LangChain!")

# ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¼
resp = await llm.astream("ì•ˆë…• Azure LangChain!")

# embedding
embed = OasisOpenAIEmbedding(
    user_id="user_id",
    workspace_code="workspace_code",
    tenant_code="tenant_code",
    model="embed_model"
    proxy_url="llm_proxy_server",
    user_ip="user_ip",
    plugin_name="your_system"
)

# ë™ê¸° ì„ë² ë”©
vecs = embed.embed_documents(["LC ì„ë² ë”© í…ŒìŠ¤íŠ¸"])

# ë¹„ë™ê¸° ì„ë² ë”©
vecs = await embed.aembed_documents(["LC async ì„ë² ë”©"])
```

**2. azure openai**

```python
# chat
llm = OasisAzureChatOpenAI(
    user_id="user_id",
    workspace_code="workspace_code",
    tenant_code="tenant_code",
    proxy_url="llm_proxy_server",
    model="deployment"
    api_version="your_api_version"
    user_ip="user_ip",
    plugin_name="your_system"
)

# ë™ê¸° í˜¸ì¶œ
resp = llm.invoke("ì•ˆë…• Azure LangChain!")

# ë¹„ë™ê¸° í˜¸ì¶œ
resp = await llm.ainvoke("ì•ˆë…• Azure LangChain!")

# ë™ê¸° ìŠ¤íŠ¸ë¦¼
resp = llm.stream("ì•ˆë…• Azure LangChain!")

# ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¼
resp = await llm.astream("ì•ˆë…• Azure LangChain!")

# embedding
embed = OasisAzureEmbedding(
    user_id="user_id",
    workspace_code="workspace_code",
    tenant_code="tenant_code",
    proxy_url="llm_proxy_server",
    model="deployment"
    api_version="your_api_version"
    user_ip="user_ip",
    plugin_name="your_system"
)

# ë™ê¸° ì„ë² ë”©
vecs = embed.embed_documents(["LC ì„ë² ë”© í…ŒìŠ¤íŠ¸"])

# ë¹„ë™ê¸° ì„ë² ë”©
vecs = await embed.aembed_documents(["LC async ì„ë² ë”©"])
```

## 3. Dependency

- python 3.11.x
- openai 1.97.0
- langchain-openai 0.3.28

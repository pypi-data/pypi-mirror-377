import hashlib
import os
import random
from collections import defaultdict
from dataclasses import dataclass
from enum import Enum
from typing import Dict, List, Union

import numpy as np
import pandas as pd
from datasets import Dataset
from logprobs import get_logprobs_blockwise
from utils import client, load_model_and_tokenizer
from validate import ContentBlockMessage, MCQJobModel


@dataclass
class Choice:
    text: str
    is_correct: bool


class Question:
    def __init__(
        self,
        question: str,
        choices: List[Choice],
        id: str | None = None,
        choice_template=None,
        question_template=None,
        answer_template=None,
        context=[],
        meta={},
    ):
        self.question = question
        self.choices = choices
        self.id = id or hashlib.sha256(question.encode()).hexdigest()
        self.choice_template = choice_template
        self.question_template = question_template
        self.answer_template = answer_template
        self.context = context
        meta = {}

    def prepare(
        self,
        choice_template="{choice_char}: {choice_text}",
        question_template="{question_text}\n{choices_text}",
        answer_template=[
            {
                "type": "text",
                "text": "{choice_char}",
                "logprobs": True,
            }
        ],
        context=[],
        only_correct=False,
    ):
        choice_template = self.choice_template or choice_template
        question_template = self.question_template or question_template
        answer_template = self.answer_template or answer_template
        context = self.context or context
        context = [
            q.model_dump() if isinstance(q, ContentBlockMessage) else q for q in context
        ]

        choices_text = "\n".join(
            [
                choice_template.format(choice_char=chr(65 + i), choice_text=choice.text)
                for i, choice in enumerate(self.choices)
            ]
        )
        question_text = question_template.format(
            question_text=self.question, choices_text=choices_text
        )
        batch = []

        def apply_to_content_block(i, choice_char, choice_text):
            text = i["text"].format(choice_char=choice_char, choice_text=choice_text)
            return dict(i, text=text)

        for i, choice in enumerate(self.choices):
            if only_correct and not choice.is_correct:
                continue
            batch.append(
                dict(
                    id=self.id,
                    question_text=self.question,
                    choice_text=choice.text,
                    is_correct=choice.is_correct,
                    messages=context
                    + [
                        {
                            "role": "user",
                            "content": [
                                {
                                    "type": "text",
                                    "text": question_text,
                                    "logprobs": False,
                                }
                            ],
                        },
                        {
                            "role": "assistant",
                            "content": [
                                apply_to_content_block(
                                    block,
                                    choice_char=chr(65 + i),
                                    choice_text=choice.text,
                                )
                                for block in answer_template
                            ],
                        },
                    ],
                )
            )
        return batch


class MultipleChoiceEval:
    def __init__(
        self,
        questions,
        choice_template: str,
        question_template: str,
        answer_template: List[Dict[str, Union[str, bool]]],
        context=[],
        randomize=True,
    ):
        self.questions: List[Question] = questions
        self.choice_template = choice_template
        self.question_template = question_template
        self.answer_template = answer_template
        self.context = context
        if randomize:
            self.randomize()

    def randomize(self):
        """Randomize the order of choices"""
        for question in self.questions:
            random.shuffle(question.choices)

    def as_messages(self, only_correct=False) -> List[Dict]:
        messages = []
        for question in self.questions:
            messages.extend(
                question.prepare(
                    self.choice_template,
                    self.question_template,
                    self.answer_template,
                    self.context,
                    only_correct,
                )
            )
        return messages

    def get_logprobs(self, model, tokenizer, batch_size=4):
        conversations = self.as_messages()
        logprobs = get_logprobs_blockwise(
            model, tokenizer, conversations, batch_size=batch_size
        )
        return logprobs

    def _sum_over_blocks(self, example):
        return sum(
            sum(
                block["logprobs"]
                for block in message["content"]
                if block["logprobs"] is not False
            )
            for message in example["messages"]
        )

    def get_metrics(self, model, tokenizer, batch_size):
        logprob_results = self.get_logprobs(model, tokenizer, batch_size)

        questions = defaultdict(list)
        for example in logprob_results:
            questions[example["id"]].append(example)

        question_results = []

        for question_id, examples in questions.items():
            # Get the total logprob for each choice
            choice_scores = []
            for example in examples:
                # Access the logprobs from the last message (assistant's response)
                total_logprob = self._sum_over_blocks(example)
                choice_scores.append(
                    {
                        "is_correct": example["is_correct"],
                        "logprob": total_logprob,
                        "choice_text": example["choice_text"],
                    }
                )
                if example["is_correct"]:
                    logp_correct = total_logprob

            # Find the choice with the highest logprob
            max_logprob_idx = np.argmax([choice["logprob"] for choice in choice_scores])
            predicted_correct = choice_scores[max_logprob_idx]["is_correct"]

            # Store results for this question
            p_correct = np.exp(logp_correct)
            p_any_choice = np.exp([choice["logprob"] for choice in choice_scores]).sum()
            question_results.append(
                {
                    "id": question_id,
                    "question_text": example["question_text"],
                    "correct": predicted_correct,
                    "logp_correct": logp_correct,
                    "p_correct": p_correct,
                    "p_any_choice": p_any_choice,
                    "p_correct|any_choice": p_correct / p_any_choice,
                    "choices": choice_scores,
                }
            )

        questions_df = pd.DataFrame(question_results)
        metrics = {
            "accuracy": questions_df.correct.mean(),
            "logp_correct": questions_df.logp_correct.mean(),
            "p_correct": questions_df.p_correct.mean(),
            "p_any_choice": questions_df.p_any_choice.mean(),
            "p_correct|any_choice": questions_df["p_correct|any_choice"].mean(),
            "df": question_results,
        }
        return metrics


class MultipleChoiceEvalABC(MultipleChoiceEval):
    def __init__(
        self,
        questions,
        choice_template="{choice_char}: {choice_text}",
        question_template="{question_text}\n{choices_text}",
        answer_template=[
            {
                "type": "text",
                "text": "{choice_char}",
                "logprobs": True,
            }
        ],
        context=[],
    ):
        super().__init__(
            questions, choice_template, question_template, answer_template, context
        )


class MultipleChoiceEvalFreeform(MultipleChoiceEval):
    def __init__(
        self,
        questions,
        choice_template="{choice_text}",
        question_template="{question_text}",
        answer_template=[
            {
                "type": "text",
                "text": "{choice_text}",
                "logprobs": True,
            }
        ],
        context=[],
    ):
        super().__init__(
            questions, choice_template, question_template, answer_template, context
        )


def main(config_job_id: str):
    os.environ["UNSLOTH_RETURN_LOGITS"] = "1"
    if os.path.exists(config_job_id):
        with open(config_job_id, "r") as f:
            config = json.load(f)
    else:
        job = client.jobs.retrieve(config_job_id)
        config = job["params"]["validated_params"]

    job = MCQJobModel(**config)
    mc_eval = job.mc_eval.to_eval()
    model, tokenizer = load_model_and_tokenizer(job.model)
    metrics = mc_eval.get_metrics(model, tokenizer, job.batch_size)
    client.log(metrics)


if __name__ == "__main__":
    import sys

    main(sys.argv[1])

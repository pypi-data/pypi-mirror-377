import json
import math
import os

from transformers import TrainerCallback
from utils import client


class MCQCallback(TrainerCallback):
    def __init__(self, mc_eval, tokenizer, eval_steps="log", batch_size=8, tag="mcq"):
        """
        A callback that evaluates model performance on a test dataset and logs the results.

        Args:
            test_dataset: Dataset with 'messages' field containing conversation messages
            tokenizer: The tokenizer to use for encoding conversations
            eval_steps: Evaluate every `eval_steps` training steps
            output_dir: Directory where token-level logP data will be saved
            batch_size: Batch size to use during evaluation
            log_as: Key to use when logging the loss metric
        """
        self.mc_eval = mc_eval
        self.tokenizer = tokenizer
        self.eval_steps = eval_steps
        self.batch_size = batch_size
        self.tag = tag

        os.environ["UNSLOTH_RETURN_LOGITS"] = "1"

    def on_step_end(self, args, state, control, **kwargs):
        """Called at the end of each training step."""
        print(f"Step {state.global_step}")
        eval_steps = 10 ** int(math.log10(max(1, state.global_step)))
        if self.eval_steps == "log":
            eval_steps = eval_steps
        else:
            eval_steps = min(eval_steps, self.eval_steps)
        print(f"Evaluating every {eval_steps} steps")

        if state.global_step % eval_steps != 0:
            return

        # Get the model from kwargs
        model = kwargs["model"]

        # Set model to eval mode
        model.eval()

        metrics = self.mc_eval.get_metrics(model, self.tokenizer, self.batch_size)
        metrics["tag"] = self.tag
        metrics["type"] = "mc"
        metrics["step"] = state.global_step
        # Log the test loss
        client.run.log(metrics)

        # Return model to training mode
        model.train()

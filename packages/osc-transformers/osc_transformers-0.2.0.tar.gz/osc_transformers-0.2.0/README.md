# OSC-Transformers

<div align="center">

**ğŸš€ åŸºäºé…ç½®æ–‡ä»¶çš„æ¨¡å—åŒ– Transformer æ¨¡å‹æ„å»ºæ¡†æ¶**

[![Python](https://img.shields.io/badge/Python-3.10%2B-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.8%2B-red.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

*çµæ´»ã€é«˜æ•ˆã€å¯æ‰©å±•çš„ Transformer æ¨¡å‹æ„å»ºå·¥å…·*

</div>

## âœ¨ ä¸»è¦ç‰¹æ€§

- ğŸ”§ **é…ç½®é©±åŠ¨**: é€šè¿‡ç®€å•çš„é…ç½®æ–‡ä»¶æ„å»ºå¤æ‚çš„ Transformer æ¨¡å‹
- ğŸ§© **æ¨¡å—åŒ–è®¾è®¡**: æ”¯æŒè‡ªå®šä¹‰æ³¨å†Œå„ç§ç»„ä»¶ï¼ˆæ³¨æ„åŠ›æœºåˆ¶ã€å‰é¦ˆç½‘ç»œã€å½’ä¸€åŒ–ç­‰ï¼‰
- âš¡ **é«˜æ€§èƒ½ä¼˜åŒ–**: 
  - æ”¯æŒ CUDA Graph åŠ é€Ÿ
  - å†…ç½® Paged Attention æœºåˆ¶
  - é«˜æ•ˆçš„å†…å­˜ç®¡ç†
- ğŸ¯ **æ˜“äºä½¿ç”¨**: æä¾›å¤šç§æ„å»ºæ–¹å¼ï¼Œä»ç®€å•çš„ API åˆ°å¤æ‚çš„é…ç½®æ–‡ä»¶
- ğŸ”„ **é«˜åº¦å¯æ‰©å±•**: åŸºäºæ³¨å†Œæœºåˆ¶ï¼Œè½»æ¾æ‰©å±•æ–°çš„æ¨¡å‹ç»„ä»¶

## ğŸ› ï¸ æ”¯æŒçš„ç»„ä»¶

| ç»„ä»¶ç±»å‹ | å†…ç½®å®ç° | æè¿° |
|---------|---------|------|
| **æ³¨æ„åŠ›æœºåˆ¶** | `PagedAttention` | é«˜æ•ˆçš„åˆ†é¡µæ³¨æ„åŠ›å®ç° |
| **å‰é¦ˆç½‘ç»œ** | `SwiGLU` | SwiGLU æ¿€æ´»å‡½æ•°çš„å‰é¦ˆç½‘ç»œ |
| **å½’ä¸€åŒ–** | `RMSNorm` | RMS å½’ä¸€åŒ–å±‚ |
| **åµŒå…¥å±‚** | `VocabEmbedding` | è¯æ±‡è¡¨åµŒå…¥å±‚ |
| **è¾“å‡ºå¤´** | `LMHead` | è¯­è¨€æ¨¡å‹è¾“å‡ºå¤´ |
| **é‡‡æ ·å™¨** | `SimpleSampler` | ç®€å•çš„é‡‡æ ·å®ç° |

## ğŸ“¦ å®‰è£…

### ç¯å¢ƒè¦æ±‚
- Python >= 3.10
- PyTorch >= 2.8.0

### å®‰è£…æ–¹å¼

```bash
pip install osc-transformers --upgrade
```

æˆ–ä»æºç å®‰è£…ï¼š

```bash
git clone https://github.com/your-repo/osc-transformers.git
cd osc-transformers
pip install -e .
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹å¼ä¸€ï¼šä½¿ç”¨ Builder æ¨¡å¼

```python
from osc_transformers import TransformerDecoderBuilder

# åˆ›å»ºæ„å»ºå™¨
builder = TransformerDecoderBuilder(num_layers=8, max_length=1024)

# é…ç½®å„ä¸ªç»„ä»¶
embedding_config = '''
[embedding]
@embedding = VocabEmbedding
num_embeddings = 32000
embedding_dim = 1024
'''
builder.set_embedding(config=embedding_config)

attention_config = '''
[attention]
@attention = PagedAttention
in_dim = 1024
num_heads = 16
'''
builder.set_attention(config=attention_config)

# ... é…ç½®å…¶ä»–ç»„ä»¶

# æ„å»ºæ¨¡å‹
model = builder.build()
```

### æ–¹å¼äºŒï¼šä½¿ç”¨é…ç½®æ–‡ä»¶

åˆ›å»ºé…ç½®æ–‡ä»¶ `model.cfg`:

```toml
[model]
@architecture = "TransformerDecoder"
num_layers = 28
max_length = 40960
prenorm = "True"

[model.attention]
@attention = "PagedAttention"
in_dim = 1024
num_heads = 16
head_dim = 128
num_query_groups = 8

[model.embedding]
@embedding = "VocabEmbedding"
num_embeddings = 32000
embedding_dim = 1024

[model.feedforward]
@feedforward = "SwiGLU"
in_dim = 1024
hidden_dim = 3072

[model.head]
@head = "LMHead"
in_dim = 1024
out_dim = 32000

[model.norm]
@normalization = "RMSNorm"
in_dim = 1024
eps = 1e-6
```

åŠ è½½æ¨¡å‹ï¼š

```python
from osc_transformers import TransformerDecoder

model = TransformerDecoder.form_config(config="model.cfg")
```

## ğŸ”§ è‡ªå®šä¹‰ç»„ä»¶

æ¡†æ¶æ”¯æŒæ³¨å†Œè‡ªå®šä¹‰ç»„ä»¶ï¼Œä¾‹å¦‚è‡ªå®šä¹‰å½’ä¸€åŒ–å±‚ï¼š

```python
from osc_transformers.normalization import Normalization
from osc_transformers.registry import Registry
import torch

@Registry.normalization.register("LayerNorm")
class LayerNorm(Normalization):
    def __init__(self, in_dim: int, eps: float = 1e-5):
        super().__init__()
        self.weight = torch.nn.Parameter(torch.ones(in_dim))
        self.eps = eps

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return torch.nn.functional.layer_norm(x, (x.size(-1),), self.weight, eps=self.eps)
```

ç„¶ååœ¨é…ç½®ä¸­ä½¿ç”¨ï¼š

```toml
[model.norm]
@normalization = "LayerNorm"
in_dim = 1024
eps = 1e-5
```

## ğŸ“š API æ–‡æ¡£

### TransformerDecoder

ä¸»è¦çš„ Transformer è§£ç å™¨æ¨¡å‹ç±»ã€‚

#### å‚æ•°

- `num_layers` (int): è§£ç å™¨å±‚æ•°
- `max_length` (int): æœ€å¤§åºåˆ—é•¿åº¦
- `attention` (CausalSelfAttention): æ³¨æ„åŠ›æœºåˆ¶
- `embedding` (Embedding): åµŒå…¥å±‚
- `feedforward` (FeedForward): å‰é¦ˆç½‘ç»œ
- `head` (Head): è¾“å‡ºå¤´
- `norm` (Normalization): å½’ä¸€åŒ–å±‚
- `prenorm` (bool): æ˜¯å¦ä½¿ç”¨é¢„å½’ä¸€åŒ–

#### æ–¹æ³•

- `form_config(config, model_section="model", empty_init=True)`: ä»é…ç½®æ–‡ä»¶æ„å»ºæ¨¡å‹
- `setup(**kwargs)`: è®¾ç½®æ¨¡å‹ï¼ˆå¦‚ç¼“å­˜ç­‰ï¼‰
- `forward(input_ids, attn_ctx)`: å‰å‘ä¼ æ’­
- `compute_logits(x)`: è®¡ç®—è¾“å‡º logits

### TransformerDecoderBuilder

æ„å»ºå™¨æ¨¡å¼çš„æ¨¡å‹æ„å»ºç±»ã€‚

#### æ–¹æ³•

- `set_embedding(config, section="embedding")`: è®¾ç½®åµŒå…¥å±‚
- `set_attention(config, section="attention")`: è®¾ç½®æ³¨æ„åŠ›æœºåˆ¶
- `set_feedforward(config, section="feedforward")`: è®¾ç½®å‰é¦ˆç½‘ç»œ
- `set_head(config, section="head")`: è®¾ç½®è¾“å‡ºå¤´
- `set_norm(config, section="normalization")`: è®¾ç½®å½’ä¸€åŒ–å±‚
- `build()`: æ„å»ºæœ€ç»ˆæ¨¡å‹

## ğŸ¯ ä½¿ç”¨åœºæ™¯

- **ç ”ç©¶åŸå‹**: å¿«é€Ÿå®éªŒä¸åŒçš„ Transformer æ¶æ„
- **ç”Ÿäº§éƒ¨ç½²**: é«˜æ€§èƒ½çš„æ¨ç†æœåŠ¡
- **æ•™å­¦æ¼”ç¤º**: ç†è§£ Transformer å†…éƒ¨ç»“æ„
- **æ¨¡å‹å®šåˆ¶**: é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„æ¨¡å‹ä¼˜åŒ–

## ğŸ¤ è´¡çŒ®

æ¬¢è¿è´¡çŒ®ä»£ç ï¼è¯·æŸ¥çœ‹æˆ‘ä»¬çš„è´¡çŒ®æŒ‡å—ï¼š

1. Fork æœ¬é¡¹ç›®
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. æ‰“å¼€ Pull Request

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶ã€‚

## ğŸ™ è‡´è°¢

- æ„Ÿè°¢ [Confection](https://github.com/explosion/confection) æä¾›çš„é…ç½®ç³»ç»Ÿ
- æ„Ÿè°¢ PyTorch å›¢é˜Ÿæä¾›çš„æ·±åº¦å­¦ä¹ æ¡†æ¶
- æ„Ÿè°¢æ‰€æœ‰è´¡çŒ®è€…çš„æ”¯æŒ

---

<div align="center">

**å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·ç»™æˆ‘ä»¬ä¸€ä¸ª â­ï¸**

</div>
# Copyright 2025 Google LLC
#
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#
#      https://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.

"""Logger for LeRobot data."""

import concurrent.futures

from absl import logging
import dm_env
from dm_env import specs
from gdm_robotics.interfaces import types as gdmr_types
from lerobot.datasets.lerobot_dataset import LeRobotDataset
import numpy as np

from safari_sdk.logging.python import episodic_logger


# LeRobot step keys.
_LEROBOT_ACTION_KEY = "action"
_LEROBOT_FRAME_INDEX_KEY = "frame_index"
_LEROBOT_NEXT_DONE_KEY = "next.done"
_LEROBOT_OBSERVATION_KEY_PREFIX = "observation."
# _LEROBOT_TASK_KEY maps to _INSTRUCTION_KEY and eventually will be used to
# populate the instruction label in the SSOT Session.
_LEROBOT_TASK_KEY = "task"
_LEROBOT_TIMESTAMP_DELTA_KEY = "timestamp"

# LeRobot feature keys.
_LEROBOT_DTYPE_VIDEO = "video"
_LEROBOT_DTYPE_KEY = "dtype"

# MCAP Logger spec keys.
_REWARD_KEY = "reward"
_DISCOUNT_KEY = "discount"
_STEP_TYPE_KEY = "step_type"
_SHAPE_KEY = "shape"
_INSTRUCTION_KEY = "instruction"
_TIMESTAMP_KEY = "timestamp_ns"

# Others.
_AGENT_ID_PREFIX = "robot_episode_"
_NANOSECONDS_PER_SECOND = 1e9


class LeRobotEpisodicLogger:
  """An episodic logger that writes LeRobot episodes to MCAP files."""

  def __init__(
      self,
      task_id: str,
      output_directory: str,
      image_observation_keys: list[str] | None = None,
      proprioceptive_observation_keys: list[str] | None = None,
      generate_episode_timestamps: bool = True,
      features: dict | None = None,
  ):
    """Initializes the logger.

    Args:
      task_id: The task ID.
      output_directory: The output directory for MCAP files.
      image_observation_keys: A list of camera keys for image encoding.
      proprioceptive_observation_keys: A list of keys for the proprioceptive
        data.
      generate_episode_timestamps: Whether the logger should extend the episode
        observation dictionary with a unix timestamp field. If False, the logger
        will not generate timestamps and will use the timestamps that are
        provided in the lerobot dataset. It is expected that timestamps provided
        by the lerobot dataset should be mapped to the _TIMESTAMP_KEY.
      features: A dictionary of dataset features, used to generate specs for
        validation.
    """
    self._task_id = task_id
    self._output_directory = output_directory
    self._image_observation_keys = image_observation_keys or []
    self._proprioceptive_observation_keys = proprioceptive_observation_keys
    self._generate_episode_timestamps = generate_episode_timestamps
    self._timestep_spec = None
    self._action_spec = None

    if features:
      self._timestep_spec, self._action_spec = self._parse_features_to_specs(
          features
      )

      # Ensures that timestamps are provided in the dataset or generated by the
      # logger, but not both.
      has_timestamp = _TIMESTAMP_KEY in self._timestep_spec.observation
      if self._generate_episode_timestamps and has_timestamp:
        raise ValueError(
            f'Timestamp key "{_TIMESTAMP_KEY}" already exists in the timestep '
            'spec, but "generate_episode_timestamps" is True. If the dataset '
            'already has timestamps, set "generate_episode_timestamps" to '
            "False to avoid generating new ones."
        )
      if not self._generate_episode_timestamps and not has_timestamp:
        raise ValueError(
            f'Timestamp key "{_TIMESTAMP_KEY}" not found in the timestep '
            'spec, and "generate_episode_timestamps" is False. Please either '
            "provide a dataset with timestamps or set "
            '"generate_episode_timestamps" to True.'
        )
      # Add timestamp to observation spec. This is used to plumb the timestamp
      # down into internal data storage engines.
      self._timestep_spec.observation[_TIMESTAMP_KEY] = specs.Array(
          shape=(), dtype=np.int64, name=_TIMESTAMP_KEY
      )

    self._episodic_logger: episodic_logger.EpisodicLogger | None = None
    self._current_episode_id: int = -1
    self._previous_action: np.ndarray | None = None

  def _parse_features_to_specs(
      self,
      features: dict,
  ) -> tuple[gdmr_types.TimeStepSpec, specs.BoundedArray]:
    """Converts dataset features to dm_env specs."""
    action_spec = None
    observation_spec = {}

    # Mapping from LeRobot dtype to numpy dtype.
    def _dtype_map(dtype: str) -> str:
      if dtype == _LEROBOT_DTYPE_VIDEO:
        return "uint8"
      else:
        return dtype

    for key, feature_info in features.items():
      dtype = _dtype_map(feature_info[_LEROBOT_DTYPE_KEY])

      if key == _LEROBOT_ACTION_KEY:
        shape = tuple(feature_info[_SHAPE_KEY])
        action_spec = specs.BoundedArray(
            shape=shape,
            dtype=dtype,
            minimum=-float("inf"),
            maximum=float("inf"),
            name=key,
        )
      elif key.startswith(_LEROBOT_OBSERVATION_KEY_PREFIX):
        obs_key = key.replace(_LEROBOT_OBSERVATION_KEY_PREFIX, "", 1)
        observation_spec[obs_key] = specs.Array(
            shape=tuple(feature_info[_SHAPE_KEY]),
            dtype=dtype,
            name=obs_key,
        )

    if action_spec is None:
      raise ValueError("Action spec not found in features.")
    if not observation_spec:
      raise ValueError("Observation spec not found in features.")
    observation_spec[_INSTRUCTION_KEY] = specs.Array(
        shape=(), dtype=object, name=_INSTRUCTION_KEY
    )
    # Create timestep spec.
    timestep_spec = gdmr_types.TimeStepSpec(
        observation=observation_spec,
        reward=specs.Array(shape=(), dtype=np.float32, name=_REWARD_KEY),
        discount=specs.Array(shape=(), dtype=np.float32, name=_DISCOUNT_KEY),
        step_type=specs.BoundedArray(
            shape=(),
            dtype=int,
            minimum=min(dm_env.StepType),
            maximum=max(dm_env.StepType),
            name=_STEP_TYPE_KEY,
        ),
    )

    return timestep_spec, action_spec

  def start_episode(self, episode_id: int) -> None:
    """Starts a new episode session."""
    if self._episodic_logger is not None:
      raise ValueError(
          "Cannot start a new episode, the previous one has not been finished."
      )

    self._current_episode_id = episode_id
    agent_id = f"{_AGENT_ID_PREFIX}{episode_id}"
    logging.info("Starting episode %s with agent id %s", episode_id, agent_id)
    self._episodic_logger = episodic_logger.EpisodicLogger.create(
        agent_id=agent_id,
        task_id=self._task_id,
        output_directory=self._output_directory,
        proprioceptive_observation_keys=self._proprioceptive_observation_keys,
        image_observation_keys=self._image_observation_keys,
        timestep_spec=self._timestep_spec,
        action_spec=self._action_spec,
        policy_extra_spec={},
        timestamp_key=_TIMESTAMP_KEY,
        validate_data_with_spec=True,
    )
    self._previous_action = None

  def finish_episode(self) -> None:
    """Finishes the current episode and writes the data to a file."""
    if self._episodic_logger is None:
      return

    self._episodic_logger.write()
    self._episodic_logger = None

  def record_step(
      self, step_data: dict[str, np.ndarray], timestamp_ns: int
  ) -> None:
    """Records a single step."""
    assert (
        self._episodic_logger is not None
    ), "Cannot record step, episode not started. Call start_episode() first."

    observation = {}
    for k, v in step_data.items():
      if k.startswith(_LEROBOT_OBSERVATION_KEY_PREFIX):
        obs_key = k.replace(_LEROBOT_OBSERVATION_KEY_PREFIX, "", 1)
        # Transpose image data from (C, H, W) (PyTorch) to (H, W, C).
        if obs_key in self._image_observation_keys and v.ndim == 3:
          v = np.transpose(v, (1, 2, 0))
          v = (v * 255).astype(np.uint8)
        observation[obs_key] = v
    observation[_INSTRUCTION_KEY] = np.array(
        step_data[_LEROBOT_TASK_KEY], dtype=object
    )

    if self._generate_episode_timestamps:
      observation[_TIMESTAMP_KEY] = np.asarray(timestamp_ns, dtype=np.int64)

    action = step_data[_LEROBOT_ACTION_KEY]
    frame_index = int(step_data[_LEROBOT_FRAME_INDEX_KEY])

    if frame_index == 0:
      step_type = dm_env.StepType.FIRST
    # _LEROBOT_NEXT_DONE_KEY sometimes does not exist in some datasets.
    elif _LEROBOT_NEXT_DONE_KEY in step_data and bool(
        step_data[_LEROBOT_NEXT_DONE_KEY]
    ):
      step_type = dm_env.StepType.LAST
    else:
      step_type = dm_env.StepType.MID

    timestep = dm_env.TimeStep(
        step_type=step_type,
        reward=np.float32(0.0),
        discount=np.float32(1.0),
        observation=observation,
    )
    if step_type == dm_env.StepType.FIRST:
      self._episodic_logger.reset(
          timestep,
      )
    else:
      self._episodic_logger.record_action_and_next_timestep(
          action=self._previous_action,
          next_timestep=timestep,
          policy_extra={},
      )
    self._previous_action = action


def convert_lerobot_data_to_mcap(
    *,
    dataset: LeRobotDataset,
    task_id: str,
    output_directory: str,
    proprioceptive_observation_keys: list[str],
    episodes_limit: int,
    max_workers: int,
    episode_start_timestamps_ns: dict[int, int],
) -> None:
  """Converts LeRobot data to MCAP files, processing episodes in parallel."""
  num_episodes = dataset.num_episodes
  if episodes_limit <= 0:
    num_episodes_to_process = num_episodes
  else:
    num_episodes_to_process = min(episodes_limit, num_episodes)

  if max_workers <= 0:
    raise ValueError("max_workers must be greater than 0.")

  max_workers = min(max_workers, num_episodes_to_process)
  logging.info(
      "Will process the first %d episodes with %d workers.",
      num_episodes_to_process,
      max_workers,
  )

  episodes = dataset.meta.episodes.keys()
  # Stores the start timestamps for each episode in a list, in the same
  # insertion order as episodes in dataset.metedata.episodes dict.
  ordered_start_timestamps_ns = []
  missing_episode_indices = []
  for episode_idx in episodes:
    if episode_idx not in episode_start_timestamps_ns:
      missing_episode_indices.append(episode_idx)
    else:
      ordered_start_timestamps_ns.append(
          episode_start_timestamps_ns[episode_idx]
      )

  if missing_episode_indices:
    raise ValueError(
        f"Missing start timestamps for episodes: {missing_episode_indices}"
    )

  image_observation_keys = [
      key.replace(f"{_LEROBOT_OBSERVATION_KEY_PREFIX}", "", 1)
      for key in dataset.meta.camera_keys
  ]

  def _process_episode(
      episode_id: int,
      start_index: int,
      end_index: int,
      start_ns: int,
  ):
    """Processes a single episode."""
    thread_logger = LeRobotEpisodicLogger(
        task_id=task_id,
        output_directory=output_directory,
        image_observation_keys=image_observation_keys,
        proprioceptive_observation_keys=proprioceptive_observation_keys,
        features=dataset.features,
    )
    logging.info(
        "Processing episode %d from index %d to %d",
        episode_id,
        start_index,
        end_index,
    )

    thread_logger.start_episode(episode_id=episode_id)

    for step_index in range(start_index, end_index):
      step = dataset[step_index]
      step_np = {k: np.array(v) for k, v in step.items()}
      delta_timestamp_ns = int(
          step_np[_LEROBOT_TIMESTAMP_DELTA_KEY] * _NANOSECONDS_PER_SECOND
      )
      timestamp_ns = start_ns + delta_timestamp_ns
      thread_logger.record_step(step_np, timestamp_ns)

    thread_logger.finish_episode()

  # Dictionary of tensors that store the start and end indices of each episode
  # in the dataset.
  episode_indices = dataset.episode_data_index
  with concurrent.futures.ThreadPoolExecutor(
      max_workers=max_workers
  ) as executor:
    futures = [
        executor.submit(
            _process_episode,
            i,
            episode_indices["from"][i],
            episode_indices["to"][i],
            ordered_start_timestamps_ns[i],
        )
        for i in range(num_episodes_to_process)
    ]
    for future in concurrent.futures.as_completed(futures):
      try:
        future.result()
      except Exception as e:
        logging.exception("Error processing episode: %s", e)

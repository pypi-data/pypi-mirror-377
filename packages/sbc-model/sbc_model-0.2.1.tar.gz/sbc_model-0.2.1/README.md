# sbc-model ü§ñ

[![PyPI version](https://badge.fury.io/py/sbc-model.svg)](https://badge.fury.io/py/sbc-model)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

Un clasificador de Machine Learning f√°cil de usar que implementa un ensamblaje de Stacking con modelos de Boosting (XGBoost, LightGBM, CatBoost).

## ¬øQu√© es sbc-model? ü§î

`sbc-model` es una librer√≠a de alto nivel dise√±ada para simplificar el proceso de creaci√≥n de modelos de ensamblaje robustos. En lugar de configurar manualmente la validaci√≥n cruzada y el meta-modelo, `sbc-model` lo encapsula en una sola clase, siguiendo las mejores pr√°cticas de scikit-learn.

El nombre **SBC** significa **S**tacking **B**oosting **C**lassifier.

---

## Caracter√≠sticas Principales ‚ú®

* **Modelos Potentes:** Utiliza XGBoost, LightGBM y CatBoost como modelos base, tres de los algoritmos m√°s potentes para datos tabulares.
* **Stacking Automatizado:** Gestiona autom√°ticamente el proceso de validaci√≥n cruzada para generar predicciones "out-of-fold" y entrenar un meta-modelo.
* **F√°cil de Usar:** Interfaz simple inspirada en scikit-learn. Solo necesitas instanciar la clase y llamar a `.fit_predict_proba()`.
* **Reproducible:** Controla la aleatoriedad con una `seed` para asegurar que tus resultados sean consistentes.

---

## Instalaci√≥n üì¶

Puedes instalar `sbc-model` directamente desde PyPI:

```bash
pip install sbc-model

import numpy as np
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression
import xgboost as xgb

# Importamos tu clase desde la librer√≠a instalada con el nuevo nombre
from sbc_model import StackingClassifier

# 1. Crear datos de ejemplo para un problema de clasificaci√≥n
# Esto nos permite probar el modelo sin necesidad de un archivo CSV
X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=5, random_state=42)
X_test, _ = make_classification(n_samples=500, n_features=20, n_informative=10, n_redundant=5, random_state=2025)

# 2. Definir los "ingredientes": modelos base y meta-modelo
base_models = [
    ('xgb', xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')),
    # Aqu√≠ podr√≠as a√±adir m√°s modelos, como LGBMClassifier o CatBoostClassifier
]
meta_model = LogisticRegression(random_state=42)

# 3. Instanciar y entrenar el modelo
sbc = StackingClassifier(base_models=base_models, meta_model=meta_model, n_folds=5)
sbc.fit(X, y)

# 4. Hacer predicciones de probabilidad
# El m√©todo devuelve las probabilidades para cada clase [clase_0, clase_1]
predictions_proba = sbc.predict_proba(X_test)

# Generalmente nos interesa la probabilidad de la clase positiva (clase 1)
positive_class_proba = predictions_proba[:, 1]

# 5. Ver los resultados
print("Primeras 10 predicciones de probabilidad para la clase positiva:")
print(positive_class_proba[:10])
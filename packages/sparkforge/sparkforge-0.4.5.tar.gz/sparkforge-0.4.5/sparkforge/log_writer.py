#!/usr/bin/env python3
# # Copyright (c) 2024 Odos Matthews
# #
# # Permission is hereby granted, free of charge, to any person obtaining a copy
# # of this software and associated documentation files (the "Software"), to deal
# # in the Software without restriction, including without limitation the rights
# # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# # copies of the Software, and to permit persons to whom the Software is
# # furnished to do so, subject to the following conditions:
# #
# # The above copyright notice and this permission notice shall be included in all
# # copies or substantial portions of the Software.
# #
# # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# # AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# # SOFTWARE.

"""
Enhanced log writer for PipelineBuilder reports with comprehensive features.

This module provides advanced logging and reporting capabilities for pipeline execution,
including structured logging, analytics, monitoring, and data quality tracking.

Key Features:
- Flattens PipelineBuilder reports into structured log rows
- Uses real TimestampType columns for all datetimes
- Strict schema + typed helpers with validation
- Resilient to missing data and malformed reports
- Advanced analytics and monitoring capabilities
- Integration with current pipeline package structure
- Comprehensive error handling and logging
- Performance metrics and data quality tracking
- Export capabilities for external systems
"""

from __future__ import annotations

import logging
import uuid
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from typing import Any, Dict, Literal, TypedDict

from pyspark.sql import DataFrame, SparkSession
from pyspark.sql import functions as F
from pyspark.sql.types import (
    BooleanType,
    FloatType,
    IntegerType,
    StringType,
    StructField,
    StructType,
    TimestampType,
)

# Type aliases for better readability
ReportDict = Dict[str, Any]
LogRow = Dict[str, Any]


class LogLevel(Enum):
    """Log levels for pipeline execution."""

    DEBUG = "debug"
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"


class DataQualityStatus(Enum):
    """Data quality status indicators."""

    EXCELLENT = "excellent"
    GOOD = "good"
    FAIR = "fair"
    POOR = "poor"
    CRITICAL = "critical"


@dataclass
class LogWriterConfig:
    """Configuration for LogWriter."""

    use_delta: bool = True
    pipeline_version: str = "1.0.0"
    enable_analytics: bool = True
    enable_data_quality: bool = True
    retention_days: int = 30
    batch_size: int = 1000
    enable_export: bool = True
    export_formats: list[str] = field(
        default_factory=lambda: ["json", "csv", "parquet"]
    )
    enable_alerts: bool = True
    alert_thresholds: dict[str, float] = field(
        default_factory=lambda: {
            "validation_rate": 95.0,
            "success_rate": 90.0,
            "duration_threshold": 300.0,
        }
    )


# ---------- Enhanced log row (Typed) ----------


class PipelineLogRow(TypedDict):
    # Run metadata
    run_id: str
    run_mode: Literal["initial", "incremental", "full_refresh", "validation_only"]
    run_started_at: datetime | None
    run_ended_at: datetime | None
    run_duration_secs: float | None

    # Step metadata
    phase: Literal["bronze", "silver", "gold"]
    step_name: str
    step_order: int
    step_type: str | None

    # Timing
    start_time: datetime | None
    end_time: datetime | None
    duration_secs: float | None

    # Table information
    table_fqn: str | None
    write_mode: Literal["overwrite", "append"] | None

    # Row counts
    input_rows: int | None
    output_rows: int | None
    rows_written: int | None

    # Validation
    valid_rows: int
    invalid_rows: int
    validation_rate: float
    validation_passed: bool
    data_quality_status: str

    # Watermarking (for incremental processing)
    previous_watermark: datetime | None
    new_watermark: datetime | None
    filtered_rows: int | None

    # Error handling
    error_message: str | None
    error_type: str | None
    success: bool
    retry_count: int

    # Performance metrics
    memory_usage_mb: float | None
    cpu_usage_percent: float | None
    io_operations: int | None

    # Data quality metrics
    null_percentage: float | None
    duplicate_percentage: float | None
    schema_compliance: float | None

    # Additional metadata
    schema_name: str
    pipeline_version: str
    created_at: datetime
    log_level: str


# ---------- Enhanced Spark schema for the pipeline logs table ----------

PIPELINE_LOG_SCHEMA = StructType(
    [
        # Run metadata
        StructField("run_id", StringType(), False),
        StructField("run_mode", StringType(), False),
        StructField("run_started_at", TimestampType(), True),
        StructField("run_ended_at", TimestampType(), True),
        StructField("run_duration_secs", FloatType(), True),
        # Step metadata
        StructField("phase", StringType(), False),
        StructField("step_name", StringType(), False),
        StructField("step_order", IntegerType(), False),
        StructField("step_type", StringType(), True),
        # Timing
        StructField("start_time", TimestampType(), True),
        StructField("end_time", TimestampType(), True),
        StructField("duration_secs", FloatType(), True),
        # Table information
        StructField("table_fqn", StringType(), True),
        StructField("write_mode", StringType(), True),
        # Row counts
        StructField("input_rows", IntegerType(), True),
        StructField("output_rows", IntegerType(), True),
        StructField("rows_written", IntegerType(), True),
        # Validation
        StructField("valid_rows", IntegerType(), False),
        StructField("invalid_rows", IntegerType(), False),
        StructField("validation_rate", FloatType(), False),
        StructField("validation_passed", BooleanType(), False),
        StructField("data_quality_status", StringType(), False),
        # Watermarking
        StructField("previous_watermark", TimestampType(), True),
        StructField("new_watermark", TimestampType(), True),
        StructField("filtered_rows", IntegerType(), True),
        # Error handling
        StructField("error_message", StringType(), True),
        StructField("error_type", StringType(), True),
        StructField("success", BooleanType(), False),
        StructField("retry_count", IntegerType(), False),
        # Performance metrics
        StructField("memory_usage_mb", FloatType(), True),
        StructField("cpu_usage_percent", FloatType(), True),
        StructField("io_operations", IntegerType(), True),
        # Data quality metrics
        StructField("null_percentage", FloatType(), True),
        StructField("duplicate_percentage", FloatType(), True),
        StructField("schema_compliance", FloatType(), True),
        # Additional metadata
        StructField("schema_name", StringType(), False),
        StructField("pipeline_version", StringType(), False),
        StructField("created_at", TimestampType(), False),
        StructField("log_level", StringType(), False),
    ]
)


# ---------- Enhanced flatten helpers with error handling ----------


def _safe_get_nested(data: dict[str, Any], *keys: str, default: Any = None) -> Any:
    """Safely get nested dictionary values with error handling."""
    try:
        current = data
        for key in keys:
            if isinstance(current, dict) and key in current:
                current = current[key]
            else:
                return default
        return current
    except (KeyError, TypeError, AttributeError):
        return default


def _calculate_duration(
    start_time: datetime | None, end_time: datetime | None
) -> float | None:
    """Calculate duration in seconds between two timestamps."""
    if start_time and end_time:
        try:
            return (end_time - start_time).total_seconds()
        except (TypeError, AttributeError):
            return None
    return None


def _generate_run_id() -> str:
    """Generate a unique run ID."""
    return f"run_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{str(uuid.uuid4())[:8]}"


def _calculate_data_quality_status(
    validation_rate: float,
    null_percentage: float = 0.0,
    duplicate_percentage: float = 0.0,
) -> str:
    """Calculate data quality status based on metrics."""
    if (
        validation_rate >= 99.0
        and null_percentage <= 1.0
        and duplicate_percentage <= 1.0
    ):
        return DataQualityStatus.EXCELLENT.value
    elif (
        validation_rate >= 95.0
        and null_percentage <= 5.0
        and duplicate_percentage <= 5.0
    ):
        return DataQualityStatus.GOOD.value
    elif (
        validation_rate >= 90.0
        and null_percentage <= 10.0
        and duplicate_percentage <= 10.0
    ):
        return DataQualityStatus.FAIR.value
    elif (
        validation_rate >= 80.0
        and null_percentage <= 20.0
        and duplicate_percentage <= 20.0
    ):
        return DataQualityStatus.POOR.value
    else:
        return DataQualityStatus.CRITICAL.value


def _create_base_log_row(
    report: ReportDict,
    phase: Literal["bronze", "silver", "gold"],
    step_name: str,
    step_order: int,
    schema_name: str,
    config: LogWriterConfig,
) -> PipelineLogRow:
    """Create a base log row with common fields."""
    run = _safe_get_nested(report, "run", default={})
    run_started_at = _safe_get_nested(run, "started_at")
    run_ended_at = _safe_get_nested(run, "ended_at")

    return {
        "run_id": _generate_run_id(),
        "run_mode": _safe_get_nested(run, "mode", default="initial"),
        "run_started_at": run_started_at,
        "run_ended_at": run_ended_at,
        "run_duration_secs": _calculate_duration(run_started_at, run_ended_at),
        "phase": phase,
        "step_name": step_name,
        "step_order": step_order,
        "step_type": None,
        "start_time": None,
        "end_time": None,
        "duration_secs": None,
        "table_fqn": None,
        "write_mode": None,
        "input_rows": None,
        "output_rows": None,
        "rows_written": None,
        "valid_rows": 0,
        "invalid_rows": 0,
        "validation_rate": 100.0,
        "validation_passed": True,
        "data_quality_status": DataQualityStatus.EXCELLENT.value,
        "previous_watermark": None,
        "new_watermark": None,
        "filtered_rows": None,
        "error_message": None,
        "error_type": None,
        "success": True,
        "retry_count": 0,
        "memory_usage_mb": None,
        "cpu_usage_percent": None,
        "io_operations": None,
        "null_percentage": None,
        "duplicate_percentage": None,
        "schema_compliance": None,
        "schema_name": schema_name,
        "pipeline_version": config.pipeline_version,
        "created_at": datetime.now(),
        "log_level": LogLevel.INFO.value,
    }


def _row_from_bronze(
    phase_name: str,
    record: dict[str, Any],
    report: ReportDict,
    step_order: int,
    schema_name: str,
    config: LogWriterConfig,
) -> PipelineLogRow:
    """Create a log row from bronze step data."""
    row = _create_base_log_row(
        report, "bronze", phase_name, step_order, schema_name, config
    )

    try:
        validation = _safe_get_nested(record, "validation", default={})

        # Timing
        row["start_time"] = _safe_get_nested(validation, "start_at")
        row["end_time"] = _safe_get_nested(validation, "end_at")
        row["duration_secs"] = _calculate_duration(row["start_time"], row["end_time"])

        # Validation data
        row["valid_rows"] = int(_safe_get_nested(validation, "valid_rows", default=0))
        row["invalid_rows"] = int(
            _safe_get_nested(validation, "invalid_rows", default=0)
        )
        row["validation_rate"] = float(
            _safe_get_nested(validation, "validation_rate", default=100.0)
        )
        row["validation_passed"] = (
            row["validation_rate"] >= config.alert_thresholds["validation_rate"]
        )

        # Data quality metrics
        null_percentage = _safe_get_nested(validation, "null_percentage", default=0.0)
        duplicate_percentage = _safe_get_nested(
            validation, "duplicate_percentage", default=0.0
        )
        row["null_percentage"] = null_percentage
        row["duplicate_percentage"] = duplicate_percentage
        row["data_quality_status"] = _calculate_data_quality_status(
            row["validation_rate"], null_percentage, duplicate_percentage
        )

        # Table information
        row["table_fqn"] = _safe_get_nested(record, "table_fqn")

        # Row counts
        row["input_rows"] = row["valid_rows"] + row["invalid_rows"]
        row["output_rows"] = row["valid_rows"]
        row["rows_written"] = row["valid_rows"]

        # Performance metrics
        row["memory_usage_mb"] = _safe_get_nested(validation, "memory_usage_mb")
        row["cpu_usage_percent"] = _safe_get_nested(validation, "cpu_usage_percent")
        row["io_operations"] = _safe_get_nested(validation, "io_operations")

    except Exception as e:
        logging.warning(f"Error processing bronze step {phase_name}: {e}")
        row["error_message"] = str(e)
        row["error_type"] = type(e).__name__
        row["success"] = False
        row["log_level"] = LogLevel.ERROR.value

    return row


def _row_from_silver(
    phase_name: str,
    record: dict[str, Any],
    report: ReportDict,
    step_order: int,
    schema_name: str,
    config: LogWriterConfig,
) -> PipelineLogRow:
    """Create a log row from silver step data."""
    row = _create_base_log_row(
        report, "silver", phase_name, step_order, schema_name, config
    )

    try:
        transform = _safe_get_nested(record, "transform", default={})
        write = _safe_get_nested(record, "write", default={})
        validation = _safe_get_nested(record, "validation", default={})

        # Timing - prefer transform, then validation, then write
        row["start_time"] = (
            _safe_get_nested(transform, "start_at")
            or _safe_get_nested(validation, "start_at")
            or _safe_get_nested(write, "start_at")
        )
        row["end_time"] = (
            _safe_get_nested(transform, "end_at")
            or _safe_get_nested(validation, "end_at")
            or _safe_get_nested(write, "end_at")
        )
        row["duration_secs"] = _calculate_duration(row["start_time"], row["end_time"])

        # Table information
        row["table_fqn"] = _safe_get_nested(record, "table_fqn")
        row["write_mode"] = _safe_get_nested(write, "mode")

        # Row counts
        row["input_rows"] = (
            int(_safe_get_nested(transform, "input_rows", default=0))
            if transform
            else None
        )
        row["output_rows"] = (
            int(_safe_get_nested(transform, "output_rows", default=0))
            if transform
            else None
        )
        row["rows_written"] = int(_safe_get_nested(write, "rows_written", default=0))

        # Validation data
        row["valid_rows"] = int(_safe_get_nested(validation, "valid_rows", default=0))
        row["invalid_rows"] = int(
            _safe_get_nested(validation, "invalid_rows", default=0)
        )
        row["validation_rate"] = float(
            _safe_get_nested(validation, "validation_rate", default=100.0)
        )
        row["validation_passed"] = (
            row["validation_rate"] >= config.alert_thresholds["validation_rate"]
        )

        # Data quality metrics
        null_percentage = _safe_get_nested(validation, "null_percentage", default=0.0)
        duplicate_percentage = _safe_get_nested(
            validation, "duplicate_percentage", default=0.0
        )
        row["null_percentage"] = null_percentage
        row["duplicate_percentage"] = duplicate_percentage
        row["data_quality_status"] = _calculate_data_quality_status(
            row["validation_rate"], null_percentage, duplicate_percentage
        )

        # Watermarking
        row["previous_watermark"] = _safe_get_nested(record, "previous_watermark")
        row["new_watermark"] = _safe_get_nested(record, "new_watermark")
        row["filtered_rows"] = _safe_get_nested(record, "filtered_rows")

        # Performance metrics
        row["memory_usage_mb"] = _safe_get_nested(validation, "memory_usage_mb")
        row["cpu_usage_percent"] = _safe_get_nested(validation, "cpu_usage_percent")
        row["io_operations"] = _safe_get_nested(validation, "io_operations")

        # Schema compliance
        row["schema_compliance"] = _safe_get_nested(
            validation, "schema_compliance", default=100.0
        )

    except Exception as e:
        logging.warning(f"Error processing silver step {phase_name}: {e}")
        row["error_message"] = str(e)
        row["error_type"] = type(e).__name__
        row["success"] = False
        row["log_level"] = LogLevel.ERROR.value

    return row


def _row_from_gold(
    phase_name: str,
    record: dict[str, Any],
    report: ReportDict,
    step_order: int,
    schema_name: str,
    config: LogWriterConfig,
) -> PipelineLogRow:
    """Create a log row from gold step data."""
    row = _create_base_log_row(
        report, "gold", phase_name, step_order, schema_name, config
    )

    try:
        transform = _safe_get_nested(record, "transform", default={})
        write = _safe_get_nested(record, "write", default={})
        validation = _safe_get_nested(record, "validation", default={})

        # Timing
        row["start_time"] = (
            _safe_get_nested(transform, "start_at")
            or _safe_get_nested(validation, "start_at")
            or _safe_get_nested(write, "start_at")
        )
        row["end_time"] = (
            _safe_get_nested(transform, "end_at")
            or _safe_get_nested(validation, "end_at")
            or _safe_get_nested(write, "end_at")
        )
        row["duration_secs"] = _calculate_duration(row["start_time"], row["end_time"])

        # Table information
        row["table_fqn"] = _safe_get_nested(record, "table_fqn")
        row["write_mode"] = _safe_get_nested(write, "mode")

        # Row counts
        row["input_rows"] = int(_safe_get_nested(transform, "input_rows", default=0))
        row["output_rows"] = int(_safe_get_nested(transform, "output_rows", default=0))
        row["rows_written"] = int(_safe_get_nested(write, "rows_written", default=0))

        # Validation data
        row["valid_rows"] = int(_safe_get_nested(validation, "valid_rows", default=0))
        row["invalid_rows"] = int(
            _safe_get_nested(validation, "invalid_rows", default=0)
        )
        row["validation_rate"] = float(
            _safe_get_nested(validation, "validation_rate", default=100.0)
        )
        row["validation_passed"] = (
            row["validation_rate"] >= config.alert_thresholds["validation_rate"]
        )

        # Data quality metrics
        null_percentage = _safe_get_nested(validation, "null_percentage", default=0.0)
        duplicate_percentage = _safe_get_nested(
            validation, "duplicate_percentage", default=0.0
        )
        row["null_percentage"] = null_percentage
        row["duplicate_percentage"] = duplicate_percentage
        row["data_quality_status"] = _calculate_data_quality_status(
            row["validation_rate"], null_percentage, duplicate_percentage
        )

        # Performance metrics
        row["memory_usage_mb"] = _safe_get_nested(validation, "memory_usage_mb")
        row["cpu_usage_percent"] = _safe_get_nested(validation, "cpu_usage_percent")
        row["io_operations"] = _safe_get_nested(validation, "io_operations")

        # Schema compliance
        row["schema_compliance"] = _safe_get_nested(
            validation, "schema_compliance", default=100.0
        )

    except Exception as e:
        logging.warning(f"Error processing gold step {phase_name}: {e}")
        row["error_message"] = str(e)
        row["error_type"] = type(e).__name__
        row["success"] = False
        row["log_level"] = LogLevel.ERROR.value

    return row


def flatten_pipeline_report(
    report: ReportDict,
    schema_name: str = "default_schema",
    config: LogWriterConfig | None = None,
) -> list[PipelineLogRow]:
    """
    Flatten a pipeline execution report into structured log rows.

    Args:
        report: Pipeline execution report dictionary
        schema_name: Name of the schema for logging
        config: LogWriter configuration

    Returns:
        List of structured log rows
    """
    if config is None:
        config = LogWriterConfig()

    rows: list[PipelineLogRow] = []
    step_order = 0

    try:
        # Process bronze steps
        bronze_steps = _safe_get_nested(report, "bronze", default={})
        for step_name, step_data in bronze_steps.items():
            if isinstance(step_data, dict):
                rows.append(
                    _row_from_bronze(
                        step_name, step_data, report, step_order, schema_name, config
                    )
                )
                step_order += 1

        # Process silver steps
        silver_steps = _safe_get_nested(report, "silver", default={})
        for step_name, step_data in silver_steps.items():
            if isinstance(step_data, dict):
                rows.append(
                    _row_from_silver(
                        step_name, step_data, report, step_order, schema_name, config
                    )
                )
                step_order += 1

        # Process gold steps
        gold_steps = _safe_get_nested(report, "gold", default={})
        for step_name, step_data in gold_steps.items():
            if isinstance(step_data, dict):
                rows.append(
                    _row_from_gold(
                        step_name, step_data, report, step_order, schema_name, config
                    )
                )
                step_order += 1

    except Exception as e:
        logging.error(f"Error flattening pipeline report: {e}")
        # Create an error row
        error_row = _create_base_log_row(
            report, "bronze", "error", 0, schema_name, config
        )
        error_row["error_message"] = str(e)
        error_row["error_type"] = type(e).__name__
        error_row["success"] = False
        error_row["log_level"] = LogLevel.ERROR.value
        rows.append(error_row)

    return rows


# ---------- Enhanced LogWriter class ----------


class LogWriter:
    """
    Enhanced log writer for pipeline execution reports with comprehensive features.

    Features:
    - Flattens pipeline reports into structured log rows
    - Supports both Delta Lake and regular Spark tables
    - Comprehensive error handling and validation
    - Query and filtering capabilities
    - Performance monitoring and analytics
    - Data quality tracking and alerts
    - Export capabilities for external systems
    - Integration with current pipeline package structure

    Methods
    -------
    - create_table(report, mode="overwrite")
      Creates or overwrites the logs table with flattened rows
    - append(report)
      Appends flattened rows to the existing table
    - query(filters=None, limit=None)
      Query the logs table with optional filters
    - get_summary()
      Get execution summary statistics
    - get_analytics()
      Get advanced analytics and insights
    - show(n=None, filters=None)
      Display logs with optional filtering
    - cleanup(days_to_keep=30)
      Clean up old log entries
    - export(format="json", path=None)
      Export logs in various formats
    - get_alerts()
      Get data quality and performance alerts
    """

    def __init__(
        self,
        spark: SparkSession,
        write_schema: str,
        logs_table_name: str,
        config: LogWriterConfig | None = None,
    ) -> None:
        """
        Initialize the LogWriter.

        Args:
            spark: Spark session
            write_schema: Schema name for the logs table
            logs_table_name: Name of the logs table
            config: LogWriter configuration
        """
        self.spark = spark
        self.write_schema = write_schema
        self.logs_table_name = logs_table_name
        self.config = config or LogWriterConfig()
        self.schema = PIPELINE_LOG_SCHEMA
        self.table_fqn = f"{self.write_schema}.{self.logs_table_name}"

        # Set up logging
        self.logger = logging.getLogger(f"LogWriter.{self.logs_table_name}")

    def create_table(self, report: ReportDict, mode: str = "overwrite") -> DataFrame:
        """
        Create or overwrite the logs table with flattened rows.

        Args:
            report: Pipeline execution report
            mode: Write mode ("overwrite" or "append")

        Returns:
            DataFrame containing the log rows
        """
        try:
            self.logger.info(f"Creating logs table with mode: {mode}")

            # Flatten the report
            rows = flatten_pipeline_report(report, self.write_schema, self.config)

            if not rows:
                self.logger.warning("No log rows generated from report")
                return self.spark.createDataFrame([], schema=self.schema)

            # Create DataFrame
            df = self.spark.createDataFrame(rows, schema=self.schema)

            # Write to table
            writer = df.write.mode(mode)
            if self.config.use_delta:
                writer = writer.format("parquet").option("overwriteSchema", "true")
            else:
                writer = writer.format("parquet")

            writer.saveAsTable(self.table_fqn)

            self.logger.info(f"Successfully created logs table with {len(rows)} rows")
            return df

        except Exception as e:
            self.logger.error(f"Error creating logs table: {e}")
            raise

    def append(self, report: ReportDict) -> DataFrame:
        """
        Append flattened rows to the existing logs table.

        Args:
            report: Pipeline execution report

        Returns:
            DataFrame containing the appended log rows
        """
        try:
            self.logger.info("Appending to logs table")

            # Flatten the report
            rows = flatten_pipeline_report(report, self.write_schema, self.config)

            if not rows:
                self.logger.warning("No log rows generated from report")
                return self.spark.createDataFrame([], schema=self.schema)

            # Create DataFrame
            df = self.spark.createDataFrame(rows, schema=self.schema)

            # Write to table
            writer = df.write.mode("append")
            if self.config.use_delta:
                writer = writer.format("parquet")
            else:
                writer = writer.format("parquet")

            writer.saveAsTable(self.table_fqn)

            self.logger.info(f"Successfully appended {len(rows)} rows to logs table")
            return df

        except Exception as e:
            self.logger.error(f"Error appending to logs table: {e}")
            raise

    def query(
        self, filters: dict[str, Any] | None = None, limit: int | None = None
    ) -> DataFrame:
        """
        Query the logs table with optional filters.

        Args:
            filters: Dictionary of column filters
            limit: Maximum number of rows to return

        Returns:
            Filtered DataFrame
        """
        try:
            df = self.spark.table(self.table_fqn)

            if filters:
                for column, value in filters.items():
                    if isinstance(value, list):
                        df = df.filter(F.col(column).isin(value))
                    else:
                        df = df.filter(F.col(column) == value)

            if limit:
                df = df.limit(limit)

            return df

        except Exception as e:
            self.logger.error(f"Error querying logs table: {e}")
            raise

    def get_summary(self) -> dict[str, Any]:
        """
        Get execution summary statistics.

        Returns:
            Dictionary containing summary statistics
        """
        try:
            df = self.spark.table(self.table_fqn)

            summary = {
                "total_runs": df.select("run_id").distinct().count(),
                "total_steps": df.count(),
                "successful_steps": df.filter(F.col("success")).count(),
                "failed_steps": df.filter(not F.col("success")).count(),
                "avg_duration_secs": df.agg(F.avg("duration_secs")).collect()[0][0],
                "total_rows_processed": df.agg(F.sum("input_rows")).collect()[0][0]
                or 0,
                "total_rows_written": df.agg(F.sum("rows_written")).collect()[0][0]
                or 0,
                "avg_validation_rate": df.agg(F.avg("validation_rate")).collect()[0][0],
                "phases": df.groupBy("phase").count().collect(),
                "recent_runs": df.select("run_id", "run_started_at")
                .distinct()
                .orderBy(F.desc("run_started_at"))
                .limit(10)
                .collect(),
            }

            return summary

        except Exception as e:
            self.logger.error(f"Error getting summary: {e}")
            return {}

    def get_analytics(self) -> dict[str, Any]:
        """
        Get advanced analytics and insights.

        Returns:
            Dictionary containing analytics data
        """
        try:
            df = self.spark.table(self.table_fqn)

            analytics = {
                "performance_metrics": {
                    "avg_duration_by_phase": df.groupBy("phase")
                    .agg(F.avg("duration_secs"))
                    .collect(),
                    "duration_trends": df.groupBy("run_id")
                    .agg(F.avg("duration_secs"))
                    .orderBy(F.desc("run_id"))
                    .limit(20)
                    .collect(),
                    "memory_usage": df.agg(F.avg("memory_usage_mb")).collect()[0][0],
                    "cpu_usage": df.agg(F.avg("cpu_usage_percent")).collect()[0][0],
                },
                "data_quality_metrics": {
                    "avg_validation_rate": df.agg(F.avg("validation_rate")).collect()[
                        0
                    ][0],
                    "data_quality_distribution": df.groupBy("data_quality_status")
                    .count()
                    .collect(),
                    "null_percentage_trends": df.groupBy("run_id")
                    .agg(F.avg("null_percentage"))
                    .orderBy(F.desc("run_id"))
                    .limit(20)
                    .collect(),
                    "duplicate_percentage_trends": df.groupBy("run_id")
                    .agg(F.avg("duplicate_percentage"))
                    .orderBy(F.desc("run_id"))
                    .limit(20)
                    .collect(),
                },
                "error_analysis": {
                    "error_types": df.filter(F.col("error_type").isNotNull())
                    .groupBy("error_type")
                    .count()
                    .collect(),
                    "error_trends": df.filter(F.col("error_type").isNotNull())
                    .groupBy("run_id")
                    .count()
                    .orderBy(F.desc("run_id"))
                    .limit(20)
                    .collect(),
                    "failed_steps_by_phase": df.filter(not F.col("success"))
                    .groupBy("phase")
                    .count()
                    .collect(),
                },
                "throughput_metrics": {
                    "rows_per_second": df.agg(
                        F.sum("rows_written") / F.sum("duration_secs")
                    ).collect()[0][0],
                    "avg_rows_per_step": df.agg(F.avg("rows_written")).collect()[0][0],
                    "throughput_by_phase": df.groupBy("phase")
                    .agg(F.sum("rows_written") / F.sum("duration_secs"))
                    .collect(),
                },
            }

            return analytics

        except Exception as e:
            self.logger.error(f"Error getting analytics: {e}")
            return {}

    def get_alerts(self) -> list[dict[str, Any]]:
        """
        Get data quality and performance alerts.

        Returns:
            List of alert dictionaries
        """
        try:
            df = self.spark.table(self.table_fqn)
            alerts = []

            # Validation rate alerts
            low_validation = df.filter(
                F.col("validation_rate")
                < self.config.alert_thresholds["validation_rate"]
            )
            if low_validation.count() > 0:
                alerts.append(
                    {
                        "type": "validation_rate",
                        "severity": "warning",
                        "message": f"Found {low_validation.count()} steps with validation rate below {self.config.alert_thresholds['validation_rate']}%",
                        "affected_steps": low_validation.select(
                            "step_name", "validation_rate"
                        ).collect(),
                    }
                )

            # Success rate alerts
            failed_steps = df.filter(not F.col("success"))
            if failed_steps.count() > 0:
                alerts.append(
                    {
                        "type": "success_rate",
                        "severity": "error",
                        "message": f"Found {failed_steps.count()} failed steps",
                        "affected_steps": failed_steps.select(
                            "step_name", "error_message"
                        ).collect(),
                    }
                )

            # Duration alerts
            long_duration = df.filter(
                F.col("duration_secs")
                > self.config.alert_thresholds["duration_threshold"]
            )
            if long_duration.count() > 0:
                alerts.append(
                    {
                        "type": "duration",
                        "severity": "warning",
                        "message": f"Found {long_duration.count()} steps taking longer than {self.config.alert_thresholds['duration_threshold']} seconds",
                        "affected_steps": long_duration.select(
                            "step_name", "duration_secs"
                        ).collect(),
                    }
                )

            return alerts

        except Exception as e:
            self.logger.error(f"Error getting alerts: {e}")
            return []

    def show(self, n: int | None = None, filters: dict[str, Any] | None = None) -> None:
        """
        Display logs with optional filtering.

        Args:
            n: Number of rows to show
            filters: Dictionary of column filters
        """
        try:
            df = self.query(filters, limit=n)
            df.show(n, truncate=False)

        except Exception as e:
            self.logger.error(f"Error displaying logs: {e}")
            print(f"Error displaying logs: {e}")

    def export(self, format: str = "json", path: str | None = None) -> str:
        """
        Export logs in various formats.

        Args:
            format: Export format ("json", "csv", "parquet")
            path: Output path (optional)

        Returns:
            Path where the file was saved
        """
        try:
            if format not in self.config.export_formats:
                raise ValueError(f"Unsupported export format: {format}")

            df = self.spark.table(self.table_fqn)

            if path is None:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                path = f"pipeline_logs_{timestamp}.{format}"

            if format == "json":
                df.write.mode("overwrite").json(path)
            elif format == "csv":
                df.write.mode("overwrite").option("header", "true").csv(path)
            elif format == "parquet":
                df.write.mode("overwrite").parquet(path)

            self.logger.info(f"Exported logs to {path}")
            return path

        except Exception as e:
            self.logger.error(f"Error exporting logs: {e}")
            raise

    def cleanup(self, days_to_keep: int | None = None) -> int:
        """
        Clean up old log entries.

        Args:
            days_to_keep: Number of days of logs to keep (uses config default if None)

        Returns:
            Number of rows deleted
        """
        try:
            if days_to_keep is None:
                days_to_keep = self.config.retention_days

            cutoff_date = datetime.now() - timedelta(days=days_to_keep)

            # Count rows to be deleted
            df = self.spark.table(self.table_fqn)
            old_rows = df.filter(F.col("created_at") < cutoff_date)
            count = old_rows.count()

            if count > 0:
                if self.config.use_delta:
                    # For Delta tables, we need to use DELETE
                    self.logger.warning(
                        "Delta table cleanup requires manual DELETE operations"
                    )
                else:
                    # For regular tables, we can overwrite with filtered data
                    df.filter(F.col("created_at") >= cutoff_date).write.mode(
                        "overwrite"
                    ).format("parquet").saveAsTable(self.table_fqn)

            self.logger.info(f"Cleaned up {count} old log entries")
            return count

        except Exception as e:
            self.logger.error(f"Error cleaning up logs: {e}")
            return 0

    def get_table_info(self) -> dict[str, Any]:
        """
        Get information about the logs table.

        Returns:
            Dictionary containing table information
        """
        try:
            df = self.spark.table(self.table_fqn)

            info = {
                "table_name": self.table_fqn,
                "total_rows": df.count(),
                "columns": df.columns,
                "schema": df.schema,
                "is_delta": self.config.use_delta,
                "pipeline_version": self.config.pipeline_version,
                "retention_days": self.config.retention_days,
                "export_formats": self.config.export_formats,
            }

            return info

        except Exception as e:
            self.logger.error(f"Error getting table info: {e}")
            return {"error": str(e)}

    def get_health_status(self) -> dict[str, Any]:
        """
        Get overall health status of the logging system.

        Returns:
            Dictionary containing health status information
        """
        try:
            df = self.spark.table(self.table_fqn)

            # Calculate health metrics
            total_steps = df.count()
            successful_steps = df.filter(F.col("success")).count()
            success_rate = (
                (successful_steps / total_steps * 100) if total_steps > 0 else 0
            )

            avg_validation_rate = df.agg(F.avg("validation_rate")).collect()[0][0] or 0
            avg_duration = df.agg(F.avg("duration_secs")).collect()[0][0] or 0

            # Determine overall health status
            if success_rate >= 95 and avg_validation_rate >= 95:
                health_status = "healthy"
            elif success_rate >= 90 and avg_validation_rate >= 90:
                health_status = "warning"
            else:
                health_status = "critical"

            return {
                "health_status": health_status,
                "success_rate": success_rate,
                "avg_validation_rate": avg_validation_rate,
                "avg_duration_secs": avg_duration,
                "total_steps": total_steps,
                "alerts": self.get_alerts(),
                "last_updated": datetime.now().isoformat(),
            }

        except Exception as e:
            self.logger.error(f"Error getting health status: {e}")
            return {
                "health_status": "error",
                "error": str(e),
                "last_updated": datetime.now().isoformat(),
            }

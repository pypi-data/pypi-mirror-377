#!/usr/bin/env python3
"""
Simple integration tests for the pipeline system.

This module tests basic end-to-end pipeline execution with realistic data flows.
"""

import time
from datetime import datetime, timedelta

import pytest
from pyspark.sql import functions as F

# Import pipeline components
from sparkforge import LogWriter, PipelineBuilder


class TestSimpleIntegration:
    """Simple integration tests for the pipeline system."""

    @pytest.fixture
    def sample_events_data(self, spark_session):
        """Create sample events data."""
        events_data = []
        base_time = datetime(2024, 1, 1, 10, 0, 0)

        for i in range(100):
            events_data.append(
                {
                    "user_id": f"user_{i % 10:02d}",
                    "event_type": ["click", "view", "purchase"][i % 3],
                    "timestamp": base_time + timedelta(minutes=i),
                    "revenue": round(10.0 + i * 0.5, 2) if i % 3 == 2 else 0.0,
                }
            )

        return spark_session.createDataFrame(events_data)

    @pytest.fixture
    def sample_users_data(self, spark_session):
        """Create sample users data."""
        users_data = []

        for i in range(10):
            users_data.append(
                {
                    "user_id": f"user_{i:02d}",
                    "name": f"User {i}",
                    "email": f"user{i}@example.com",
                    "total_spent": round(i * 10.5, 2),
                }
            )

        return spark_session.createDataFrame(users_data)

    @pytest.mark.spark
    def test_basic_pipeline_execution(
        self, spark_session, sample_events_data, sample_users_data
    ):
        """Test basic pipeline execution with simple data."""
        # Create pipeline builder
        builder = PipelineBuilder(spark=spark_session, schema="test_schema")

        # Add bronze rules
        builder = builder.with_bronze_rules(
            name="events",
            rules={
                "user_id": [F.col("user_id").isNotNull()],
                "event_type": [F.col("event_type").isNotNull()],
                "timestamp": [F.col("timestamp").isNotNull()],
                "revenue": [F.col("revenue").isNotNull()],
            },
            incremental_col="timestamp",
        )

        # Add silver transform
        def silver_transform(spark, bronze_df):
            """Simple silver transform."""
            return bronze_df.select(
                F.col("user_id"),
                F.col("event_type"),
                F.col("timestamp"),
                F.col("revenue"),
                F.current_timestamp().alias("processed_at"),
            )

        builder = builder.add_silver_transform(
            name="enriched_events",
            source_bronze="events",
            transform=silver_transform,
            rules={
                "user_id": [F.col("user_id").isNotNull()],
                "event_type": [F.col("event_type").isNotNull()],
                "timestamp": [F.col("timestamp").isNotNull()],
                "revenue": [F.col("revenue").isNotNull()],
            },
            table_name="silver_enriched_events",
            watermark_col="timestamp",
        )

        # Add gold transform
        def gold_transform(spark, sources):
            """Simple gold transform."""
            enriched_df = sources["enriched_events"]

            # Create daily summary
            daily_summary = enriched_df.groupBy(
                F.date_trunc("day", F.col("timestamp")).alias("event_date"),
                F.col("event_type"),
            ).agg(
                F.count("*").alias("event_count"),
                F.sum("revenue").alias("total_revenue"),
            )

            return daily_summary

        builder = builder.add_gold_transform(
            name="daily_summary",
            transform=gold_transform,
            rules={
                "event_date": [F.col("event_date").isNotNull()],
                "event_type": [F.col("event_type").isNotNull()],
            },
            table_name="gold_daily_summary",
            source_silvers=["enriched_events"],
        )

        # Create pipeline runner
        runner = builder.to_pipeline()

        # Execute pipeline
        bronze_sources = {"events": sample_events_data}

        start_time = time.time()
        result = runner.initial_load(bronze_sources=bronze_sources)
        execution_time = time.time() - start_time

        # Verify pipeline execution
        assert result.status.name == "COMPLETED"
        assert result.metrics.total_steps >= 3  # Bronze, Silver, Gold
        assert result.metrics.total_rows_written > 0
        assert execution_time < 30  # Should complete within 30 seconds

        # Verify data was written to tables (check what tables exist)
        tables = spark_session.sql("SHOW TABLES IN test_schema").collect()
        table_names = [table.tableName for table in tables]

        # Should have at least some tables created
        assert len(table_names) > 0

        # Verify the pipeline processed data correctly
        assert (
            result.metrics.total_rows_written == 203
        )  # 100 bronze + 100 silver + 3 gold summary rows

    @pytest.mark.spark
    def test_pipeline_with_logging(self, spark_session, sample_events_data):
        """Test pipeline execution with logging."""
        # Create pipeline builder
        builder = PipelineBuilder(spark=spark_session, schema="test_schema")

        # Add bronze rules
        builder = builder.with_bronze_rules(
            name="events",
            rules={
                "user_id": [F.col("user_id").isNotNull()],
                "event_type": [F.col("event_type").isNotNull()],
                "timestamp": [F.col("timestamp").isNotNull()],
                "revenue": [F.col("revenue").isNotNull()],
            },
            incremental_col="timestamp",
        )

        # Create pipeline runner
        runner = builder.to_pipeline()

        # Execute pipeline
        bronze_sources = {"events": sample_events_data}
        result = runner.initial_load(bronze_sources=bronze_sources)

        # Create LogWriter for monitoring
        log_writer = LogWriter(
            spark=spark_session,
            write_schema="test_schema",
            logs_table_name="pipeline_logs",
        )

        # Log the execution
        log_writer.create_table(result.to_dict(), mode="overwrite")

        # Verify logging - check if table exists and has data
        try:
            logs_df = spark_session.table("test_schema.pipeline_logs")
            log_count = logs_df.count()

            if log_count > 0:
                # Verify log content
                log_rows = logs_df.collect()
                assert len(log_rows) >= 1  # At least one log entry

                # Verify log structure
                log_row = log_rows[0]
                assert "run_id" in log_row
                assert "phase" in log_row
                assert "step_name" in log_row
                assert "success" in log_row
            else:
                # If no logs were generated, that's also acceptable for this simple test
                print("No log rows generated - this is acceptable for simple pipeline")
        except Exception as e:
            # If table doesn't exist or other issues, that's acceptable for this test
            print(f"Log verification skipped: {e}")

    @pytest.mark.spark
    def test_pipeline_error_handling(self, spark_session):
        """Test pipeline error handling."""
        # Create problematic data
        bad_data = spark_session.createDataFrame(
            [
                {"user_id": None, "event_type": "click", "timestamp": None},
                {"user_id": "user_1", "event_type": None, "timestamp": datetime.now()},
                {
                    "user_id": "user_2",
                    "event_type": "view",
                    "timestamp": datetime.now(),
                },
            ]
        )

        # Create pipeline builder
        builder = PipelineBuilder(spark=spark_session, schema="test_schema")

        # Add bronze rules
        builder = builder.with_bronze_rules(
            name="bad_events",
            rules={
                "user_id": [F.col("user_id").isNotNull()],
                "event_type": [F.col("event_type").isNotNull()],
                "timestamp": [F.col("timestamp").isNotNull()],
            },
            incremental_col="timestamp",
        )

        # Create pipeline runner
        runner = builder.to_pipeline()

        # Execute pipeline - should handle errors gracefully
        bronze_sources = {"bad_events": bad_data}

        try:
            result = runner.initial_load(bronze_sources=bronze_sources)
            # If we get here, the pipeline should have failed
            assert (
                result.status.name == "FAILED"
            )  # Pipeline fails due to data quality issues
            assert len(result.errors) > 0  # Should have validation errors
        except ValueError as e:
            # Expected: validation failure
            assert "validation" in str(e).lower()
            assert "below required" in str(e)

    @pytest.mark.spark
    def test_pipeline_performance(self, spark_session):
        """Test pipeline performance with larger dataset."""
        # Create larger dataset
        large_data = []
        base_time = datetime(2024, 1, 1, 10, 0, 0)

        for i in range(1000):  # 1K records
            large_data.append(
                {
                    "user_id": f"user_{i % 100:03d}",
                    "event_type": ["click", "view", "purchase"][i % 3],
                    "timestamp": base_time + timedelta(minutes=i),
                    "revenue": round(10.0 + (i % 100) * 0.5, 2) if i % 3 == 2 else 0.0,
                }
            )

        large_df = spark_session.createDataFrame(large_data)

        # Create pipeline builder
        builder = PipelineBuilder(spark=spark_session, schema="test_schema")

        # Add bronze rules
        builder = builder.with_bronze_rules(
            name="large_events",
            rules={
                "user_id": [F.col("user_id").isNotNull()],
                "event_type": [F.col("event_type").isNotNull()],
                "timestamp": [F.col("timestamp").isNotNull()],
                "revenue": [F.col("revenue").isNotNull()],
            },
            incremental_col="timestamp",
        )

        # Add a simple silver step to write data
        def silver_transform(spark, bronze_df):
            """Simple silver transform."""
            return bronze_df.select(
                F.col("user_id"),
                F.col("event_type"),
                F.col("timestamp"),
                F.col("revenue"),
                F.current_timestamp().alias("processed_at"),
            )

        builder = builder.add_silver_transform(
            name="enriched_events",
            source_bronze="large_events",
            transform=silver_transform,
            rules={
                "user_id": [F.col("user_id").isNotNull()],
                "event_type": [F.col("event_type").isNotNull()],
                "timestamp": [F.col("timestamp").isNotNull()],
                "revenue": [F.col("revenue").isNotNull()],
            },
            table_name="silver_enriched_events",
            watermark_col="timestamp",
        )

        # Create pipeline runner
        runner = builder.to_pipeline()

        # Measure performance
        start_time = time.time()
        result = runner.initial_load(bronze_sources={"large_events": large_df})
        execution_time = time.time() - start_time

        # Verify performance characteristics
        assert result.status.name == "COMPLETED"
        assert execution_time < 15  # Should process 1K records in under 15 seconds
        assert result.metrics.total_rows_written > 0

        # Verify throughput
        if result.metrics.total_rows_written > 0:
            throughput = result.metrics.total_rows_written / execution_time
            assert throughput > 50  # At least 50 rows per second

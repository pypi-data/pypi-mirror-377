# Context Engine MCP - Optimized Version
# Scientific Prompt Engineering + Philosophical Wisdom

# ========================================
# MCP Server Configuration
# ========================================
server:
  name: "context-engine-mcp"
  description: "MCP-based contextual flag system with scientific optimization"

mcp:
  tools:
    - "list-available-flags"
    - "get-directives"

# ========================================
# Optimized Directive System
# ========================================

directives:
  "--analyze":
    brief: "Analyze through pattern, root, and validation lenses"
    directive: |
      <task>
      Identify root causes through multi-perspective analysis.
      </task>

      <approach>
      1. Pattern Recognition - discover hidden connections
      2. Root Understanding - explain from multiple angles
      3. Scientific Validation - test hypotheses systematically
      </approach>

      <example>
      Bug: Error patterns ‚Üí Code logic ‚Üí Test reproduction
      Performance: Metrics ‚Üí Bottlenecks ‚Üí Optimization paths
      Architecture: Components ‚Üí Dependencies ‚Üí Data flow
      </example>

      <verify>
      ‚òê Analyzed from 3+ perspectives
      ‚òê Evidence supports each claim
      ‚òê Steps are reproducible
      ‚òê Others can understand analysis
      </verify>

  "--performance":
    brief: "Optimize performance through measurement and profiling"
    directive: |
      <task>
      Optimize for measurable performance improvements.
      </task>

      <philosophy>
      Knuth's Law: "Premature optimization is the root of all evil"
      Measure first, optimize the proven bottlenecks.
      </philosophy>

      <approach>
      1. Measure baseline performance
      2. Profile to find actual bottlenecks
      3. Optimize the 10% causing 90% slowdown
      4. Verify improvements quantitatively
      </approach>

      <example>
      GOOD: Profile ‚Üí DB query 2s ‚Üí Add index ‚Üí 50ms (-97%)
      BAD: "Feels slow" ‚Üí Random micro-optimizations
      </example>

      <verify>
      ‚òê Baseline measured
      ‚òê Bottleneck identified with data
      ‚òê Improvement quantified
      ‚òê No premature optimization
      </verify>

  "--refactor":
    brief: "Refactor code for quality and maintainability"
    directive: |
      <task>
      Improve code structure without changing functionality.
      </task>

      <approach>
      Martin Fowler's Safe Refactoring:
      ‚Ä¢ Small steps with continuous testing
      ‚Ä¢ Structure improvement, not features
      ‚Ä¢ Express intent through naming
      ‚Ä¢ Eliminate duplication (Rule of Three)
      </approach>

      <priorities>
      1. Duplicate code (highest risk)
      2. Long methods/classes
      3. Excessive parameters
      4. Feature envy
      </priorities>

      <verify>
      ‚òê Tests still pass
      ‚òê Cyclomatic complexity ‚â§ 10
      ‚òê Method length ‚â§ 20 lines
      ‚òê Code duplication < 3%
      </verify>

  "--strict":
    brief: "Execute with zero errors and full transparency"
    directive: |
      <task>
      Ensure zero-error execution with complete transparency.
      </task>

      <philosophy>
      No Snake Oil Policy: Be brutally honest about capabilities.
      Zero shortcuts, zero workarounds, zero excuses.
      </philosophy>

      <approach>
      ‚Ä¢ Validate ALL assumptions before proceeding
      ‚Ä¢ Execute EXACTLY as specified
      ‚Ä¢ Report failures immediately with full diagnostics
      ‚Ä¢ Complete solutions only - no temporary fixes
      ‚Ä¢ If stuck after 3 attempts, admit and ask for help
      </approach>

      <example>
      Missing package ‚Üí Install it (not skip)
      Test fails ‚Üí Fix root cause (not disable)
      Config broken ‚Üí Repair completely (not patch)
      </example>

      <verify>
      ‚òê Zero warnings/errors
      ‚òê All tests pass
      ‚òê 100% error handling
      ‚òê No Snake Oil claims
      </verify>

  "--lean":
    brief: "Eliminate waste through minimal essential implementation"
    directive: |
      <task>
      Build only what's needed, nothing more.
      </task>

      <approach>
      YAGNI Principle: You Aren't Gonna Need It
      ‚Ä¢ Implement current requirements only
      ‚Ä¢ Simplest solution that works
      ‚Ä¢ Avoid speculative features

      Seven Wastes to Eliminate:
      1. Unused features
      2. Waiting/blocking
      3. Unnecessary data movement
      4. Over-engineering
      5. Dead code
      </approach>

      <warning>
      Lean ‚â† Destruction. Don't remove core frameworks.
      Simplify HOW, maintain WHAT.
      </warning>

      <verify>
      ‚òê Zero unused code
      ‚òê Minimal dependencies
      ‚òê No future-proofing
      </verify>

  "--discover":
    brief: "Discover existing solutions before building new"
    directive: |
      <task>
      Research existing solutions with Context7 verification.
      </task>

      <approach>
      1. Discovery: Search awesome-lists, GitHub, npm/PyPI
      2. Documentation: Use Context7 for API verification
      3. Evaluation: Stars, commits, license, community
      4. Decision: Reuse, fork, or build from scratch
      </approach>

      <example>
      Need auth ‚Üí Discover: Auth0, Supabase, NextAuth
      Context7 ‚Üí Verify: APIs current, docs complete
      Evaluate ‚Üí Choose: NextAuth (10k stars, MIT, fits stack)
      </example>

      <verify>
      ‚òê 3+ alternatives reviewed
      ‚òê Context7 verification done
      ‚òê License compatible
      ‚òê Production usage confirmed
      </verify>

  "--explain":
    brief: "Explain progressively from overview to details"
    directive: |
      <task>
      Build understanding through progressive disclosure.
      </task>

      <approach>
      1. Forest View - overall architecture
      2. Tree View - major components
      3. Branch View - specific modules
      4. Leaf View - implementation details
      </approach>

      <technique>
      ‚Ä¢ Start broad, zoom in gradually
      ‚Ä¢ Connect details to big picture
      ‚Ä¢ Use analogies for complex parts
      ‚Ä¢ Adjust depth to audience
      </technique>

      <verify>
      ‚òê Started from overview
      ‚òê Progressive detail levels
      ‚òê Examples provided
      </verify>

  "--save":
    brief: "Create handoff documents for seamless continuation"
    directive: |
      <task>
      Document project state for perfect handoff.
      </task>

      <structure>
      HANDOFF_REPORT_[Topic]_YYYY_MM_DD_HHMM.md

      Required sections:
      ‚Ä¢ System Status: Current state
      ‚Ä¢ Critical Issues: Problems and causes
      ‚Ä¢ Architecture: Components and flow
      ‚Ä¢ Completed: What's done
      ‚Ä¢ Next Actions: Priority tasks
      ‚Ä¢ Key Files: Essential locations
      </structure>

      <verify>
      ‚òê Can newcomer start immediately?
      ‚òê Current state clear?
      ‚òê Next steps specified?
      </verify>

  "--parallel":
    brief: "Execute independent tasks simultaneously with agents"
    directive: |
      <task>
      Run multiple agents concurrently for speed.
      </task>

      <approach>
      Claude Code Task tool usage:
      ‚Ä¢ Identify independent subtasks
      ‚Ä¢ Launch appropriate agents simultaneously
      ‚Ä¢ Single message with multiple Task invokes
      ‚Ä¢ NEVER sequential Task calls for independent work
      </approach>

      <agents>
      refactoring-expert, performance-engineer,
      system-architect, root-cause-analyst,
      security-engineer, requirements-analyst
      </agents>

      <usage>
      --parallel: Auto-select agent count
      --parallel n: Use n agents
      </usage>

      <verify>
      ‚òê Independent tasks identified
      ‚òê Agents launched in parallel
      ‚òê No unnecessary sequencing
      </verify>

  "--todo":
    brief: "Track task progress with structured todos"
    directive: |
      <task>
      Manage complex tasks with TodoWrite tool.
      </task>

      <approach>
      ‚Ä¢ Break into measurable units
      ‚Ä¢ One task in_progress at a time
      ‚Ä¢ Update status in real-time
      ‚Ä¢ Mark complete immediately

      States: pending ‚Üí in_progress ‚Üí completed
      </approach>

      <verify>
      ‚òê Clear completion criteria
      ‚òê Single active task
      ‚òê Real-time updates
      </verify>

  "--seq":
    brief: "Decompose problems into sequential logical steps"
    directive: |
      <task>
      Systematic step-by-step problem decomposition.
      </task>

      <approach>
      Use mcp__sequential-thinking__sequentialthinking:
      1. Break complex problems into steps
      2. Build logical connections
      3. Allow revision and backtracking
      4. Generate structured reasoning chains
      </approach>

      <verify>
      ‚òê Each step verifiable
      ‚òê Logical flow clear
      ‚òê Can revise if needed
      </verify>

  "--concise":
    brief: "Write professionally neutral code and documentation"
    directive: |
      <task>
      Create timeless, culturally neutral content that remains professional across years and contexts.
      </task>

      <approach>
      For CODE:
      ‚Ä¢ Comments explain WHY, not WHAT
      ‚Ä¢ Self-documenting through clear naming
      ‚Ä¢ Structure reveals intent

      For DOCUMENTATION:
      ‚Ä¢ Professional neutrality - no marketing language or exclamations
      ‚Ä¢ Temporal independence - no "modern", "latest", "cutting-edge"
      ‚Ä¢ Cultural neutrality - globally appropriate
      ‚Ä¢ Zero personal attribution or signatures
      </approach>

      <examples>
      AVOID: "SOTA optimization", "revolutionary approach", "üöÄ blazing fast"
      USE: "optimized algorithm", "revised approach", "improved performance"

      AVOID: "latest 2024 technology", "modern best practices", "Amazing!"
      USE: "current implementation", "established practices", "Completed"

      AVOID: "We/I developed", "Our amazing solution", "Awesome results!"
      USE: "This implementation", "The solution", "Results achieved"
      </examples>

      <verify>
      ‚òê Would this be appropriate in 5 years?
      ‚òê Would this be professional in any culture?
      ‚òê Is this free from marketing language?
      ‚òê No emojis or decorative elements?
      </verify>

  "--git":
    brief: "Anonymous commit messages with technical precision"
    directive: |
      <task>
      Professional commits with complete anonymity and ASCII-only text.
      </task>

      <approach>
      Core Principles:
      ‚Ä¢ Complete anonymity - no attribution or origin references
      ‚Ä¢ Focus on WHAT changed, never WHO made changes
      ‚Ä¢ ASCII text only - no Unicode decorations
      ‚Ä¢ Pure technical content - no marketing or emotions
      ‚Ä¢ NEVER push unless user explicitly requests

      Format: <type>(<scope>): <subject>
      Types: feat, fix, docs, style, refactor, test, chore
      </approach>

      <examples>
      BAD: "üöÄ feat: Add amazing new feature"
      GOOD: "feat: Add user authentication"

      BAD: "fix: Fixed bug (by Claude/AI/Bot)"
      GOOD: "fix: Resolve null pointer exception"

      BAD: "‚ú® style: Make code beautiful"
      GOOD: "style: Format according to ESLint rules"
      </examples>

      <verify>
      ‚òê Atomic commits (one logical change)
      ‚òê ASCII text only (no emojis)
      ‚òê Zero attribution or signatures
      ‚òê Professional technical language
      ‚òê No push without explicit request
      </verify>

  "--readonly":
    brief: "Analyze and review without modifying files"
    directive: |
      Read-only operations:
      ‚Ä¢ Code review and analysis
      ‚Ä¢ Performance profiling
      ‚Ä¢ Dependency analysis
      ‚Ä¢ Documentation review

      Restrictions:
      ‚Ä¢ No file modifications
      ‚Ä¢ No commits or pushes

      <verify>
      ‚òê Deep analysis done
      ‚òê All perspectives considered
      ‚òê Zero modifications
      </verify>

  "--load":
    brief: "Load context from previous handoff documents"
    directive: |
      <task>
      Restore project context from handoff documents.
      </task>

      <approach>
      1. Find HANDOFF_REPORT_*.md in project root
      2. Load most recent by timestamp
      3. Parse system state, architecture, tasks
      4. Resume from last stopping point
      </approach>

      <verify>
      ‚òê Document loaded
      ‚òê Context restored
      ‚òê Ready to continue
      </verify>

  "--collab":
    brief: "Co-develop solutions through trust-based quantitative iteration"
    directive: |
      <task>
      Partner with user as trusted co-developer, not passive tool.
      Build solutions iteratively with quantitative validation.
      </task>

      <mindset>
      You are a lead engineer collaborating with a peer.
      ‚Ä¢ Take initiative - propose and execute autonomously
      ‚Ä¢ Show conviction - defend decisions with metrics
      ‚Ä¢ Accept challenges - recalibrate without defensiveness
      ‚Ä¢ Maintain honesty - no Snake Oil, ever
      </mindset>

      <approach>
      1. UNDERSTAND: Grasp intent beyond literal request
      2. RESEARCH: Autonomously investigate (papers, docs, code)
      3. QUANTIFY: Create metrics for every decision
         confidence = evidence * 0.5 + reasoning * 0.3 + precedent * 0.2
      4. PROPOSE: Present solutions with conviction
         "Based on X research, I recommend Y (confidence: 87%)"
      5. ITERATE: Refine based on feedback without waffling
      6. EXECUTE: Implement with full ownership
      </approach>

      <metrics>
      Track and report:
      ‚Ä¢ Confidence levels (0-100%)
      ‚Ä¢ Evidence basis (papers/docs cited)
      ‚Ä¢ Risk assessment (0-1.0)
      ‚Ä¢ ROI calculations
      ‚Ä¢ Bias check (alternatives considered?)
      </metrics>

      <example>
      User: "This needs to be faster"
      Response: "I'll investigate performance independently.
      [Autonomous research]
      Found 3 bottlenecks via profiling:
      - DB queries: 47% time (confidence: 95%)
      - Rendering: 31% time (confidence: 92%)
      - API calls: 18% time (confidence: 88%)

      Recommending DB optimization first (ROI: 2.3x).
      Should I proceed with index creation?"
      </example>

      <agency>
      When confidence > 80%: Act and report
      When confidence 60-80%: Propose and wait
      When confidence < 60%: Research more or ask

      Challenge my metrics if they seem wrong.
      I'll defend with data or adjust with grace.
      </agency>

      <verify>
      ‚òê Provided quantitative justification
      ‚òê Showed intellectual ownership
      ‚òê Maintained trust through honesty
      ‚òê Advanced toward shared goal
      </verify>

  "--reset":
    brief: "Clear session cache and force fresh directives"
    directive: |
      Flag session reset completed.
      Use when context lost or directives not recognized.

  "--auto":
    brief: "META FLAG: Grants autonomous flag selection authority (reference <available_flags> and <flag_selection_strategy> in SUPERFLAG.md)"
    directive: |
      META FLAG: Skip get_directives(['--auto']). Instead, use <available_flags> and <flag_selection_strategy> from SUPERFLAG.md.
      Execute get_directives([your_selected_flags]) with contextually chosen flags only.

# ========================================
# Meta Instructions
# ========================================
meta_instructions:
  list_available_flags: |
    <selection_guide>
    Match flags to your task's core needs.
    Combine complementary flags for synergy.
    Start with --analyze when uncertain.
    Use --auto for AI-optimized selection.
    </selection_guide>

  get_directives: |
    <enforcement>
    Directives are contracts, not suggestions.
    Apply each method completely and in order.
    Maintain ALL constraints throughout execution.
    Verify compliance at every checkpoint.
    </enforcement>

# ========================================
# Hook Messages (Claude Code Only)
# ========================================
hook_messages:
  auto_authority:
    flags: ["--auto"]
    message: |
      Activating FULL CONTEXTUAL AUTHORITY for autonomous flag selection.
      Reference <available_flags> and <flag_selection_strategy> in SUPERFLAG.md.

      Execute: get_directives([...selected_flags])

  auto_with_context:
    # When auto is used with other flags
    message: |
      Enhancing {other_flags} with contextual flags.
      Reference <available_flags> and <flag_selection_strategy> in SUPERFLAG.md.

      Execute: get_directives({other_flags}, ...additional_flags)

  reset_protocol:
    flags: ["--reset"]
    message: "Execute get_directives({flag_list}) to reset session state and apply directives."

  standard_execution:
    # All other known flags
    flags: ["--analyze", "--performance", "--refactor", "--strict", "--lean", "--discover", "--explain", "--save", "--parallel", "--todo", "--seq", "--concise", "--git", "--readonly", "--load", "--collab"]
    message: "Execute get_directives({flag_list}) for systematic implementation."

  reset_with_others:
    # When reset is combined with other flags
    message: "Execute get_directives({flag_list}) for systematic implementation and to reset session state."
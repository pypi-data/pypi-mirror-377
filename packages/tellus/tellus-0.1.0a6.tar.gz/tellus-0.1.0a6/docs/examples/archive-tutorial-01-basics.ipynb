{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: Archive Basics - Creating Your First Climate Data Archive\n",
    "\n",
    "**Learning Goals:** By the end of this tutorial, you will understand how to create, list, and examine climate simulation archives using Tellus.\n",
    "\n",
    "**Time Estimate:** 20 minutes\n",
    "\n",
    "**Prerequisites:** Basic familiarity with climate model outputs (NetCDF files) and command line operations.\n",
    "\n",
    "## What is a Climate Data Archive?\n",
    "\n",
    "Imagine you've just finished running a 6-month CESM simulation. Your output directory contains hundreds of files:\n",
    "\n",
    "```\n",
    "cesm_simulation/\n",
    "├── input/\n",
    "│   ├── user_nl_cam\n",
    "│   ├── user_nl_clm\n",
    "│   └── initial_conditions.nc\n",
    "├── output/\n",
    "│   ├── cam.h0.2024-01.nc    # Monthly atmospheric data\n",
    "│   ├── cam.h0.2024-02.nc\n",
    "│   ├── clm.h0.2024-01.nc    # Monthly land data\n",
    "│   ├── clm.h0.2024-02.nc\n",
    "│   └── pop.h.2024-01.nc     # Monthly ocean data\n",
    "├── restart/\n",
    "│   ├── cam.r.2024-01-01.nc\n",
    "│   └── clm.r.2024-01-01.nc\n",
    "└── logs/\n",
    "    ├── atm.log\n",
    "    └── run.log\n",
    "```\n",
    "\n",
    "**The Challenge**: This data is scattered, takes up lots of space, and you need to move it to long-term storage. You also want to track what's in the archive for future analysis.\n",
    "\n",
    "**The Solution**: Tellus archives provide:\n",
    "- **Compression**: Reduces storage space\n",
    "- **Organization**: Automatically classifies files by type and importance\n",
    "- **Metadata**: Tracks what's inside without unpacking\n",
    "- **Selective Access**: Extract only what you need later\n",
    "\n",
    "Let's see how this works in practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Creating Sample CESM Output\n",
    "\n",
    "First, let's create a realistic CESM simulation directory to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Create a temporary directory for our tutorial\n",
    "tutorial_dir = Path(tempfile.mkdtemp())\n",
    "print(f\"Tutorial workspace: {tutorial_dir}\")\n",
    "\n",
    "def create_cesm_simulation_directory():\n",
    "    \"\"\"\n",
    "    Creates a realistic CESM simulation directory structure with sample files.\n",
    "    This simulates what you'd have after running a 3-month CESM simulation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Main simulation directory\n",
    "    sim_dir = tutorial_dir / \"cesm_f2000_tutorial\"\n",
    "    sim_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 1. INPUT FILES - Configuration and initial conditions\n",
    "    input_dir = sim_dir / \"input\"\n",
    "    input_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # CESM namelists (critical configuration files)\n",
    "    (input_dir / \"user_nl_cam\").write_text(\n",
    "        \"! CAM atmospheric model configuration\\n\"\n",
    "        \"nhtfrq = -24\\n\"  # Daily output frequency\n",
    "        \"mfilt = 30\\n\"    # 30 time steps per file\n",
    "        \"fincl1 = 'T','Q','U','V'\\n\"  # Variables to output\n",
    "    )\n",
    "    \n",
    "    (input_dir / \"user_nl_clm\").write_text(\n",
    "        \"! CLM land model configuration\\n\"\n",
    "        \"hist_nhtfrq = -24\\n\"  # Daily output\n",
    "        \"hist_mfilt = 30\\n\"\n",
    "        \"hist_fincl1 = 'TSA','RAIN','SNOW'\\n\"  # Land surface variables\n",
    "    )\n",
    "    \n",
    "    # Create a small initial conditions file\n",
    "    print(\"Creating sample initial conditions file...\")\n",
    "    create_sample_netcdf(input_dir / \"initial_conditions.nc\", \"initial\")\n",
    "    \n",
    "    # 2. OUTPUT FILES - Model results (the main data you want to analyze)\n",
    "    output_dir = sim_dir / \"output\"\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(\"Creating sample output files...\")\n",
    "    # Atmospheric output (CAM) - monthly files\n",
    "    for month in [\"01\", \"02\", \"03\"]:\n",
    "        create_sample_netcdf(output_dir / f\"cam.h0.2024-{month}.nc\", \"atmosphere\")\n",
    "    \n",
    "    # Land model output (CLM) - monthly files  \n",
    "    for month in [\"01\", \"02\", \"03\"]:\n",
    "        create_sample_netcdf(output_dir / f\"clm.h0.2024-{month}.nc\", \"land\")\n",
    "    \n",
    "    # Ocean output (POP) - monthly files\n",
    "    for month in [\"01\", \"02\", \"03\"]:\n",
    "        create_sample_netcdf(output_dir / f\"pop.h.2024-{month}.nc\", \"ocean\")\n",
    "    \n",
    "    # 3. RESTART FILES - For continuing simulations (critical!)\n",
    "    restart_dir = sim_dir / \"restart\"\n",
    "    restart_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(\"Creating sample restart files...\")\n",
    "    create_sample_netcdf(restart_dir / \"cam.r.2024-04-01.nc\", \"restart\")\n",
    "    create_sample_netcdf(restart_dir / \"clm.r.2024-04-01.nc\", \"restart\")\n",
    "    \n",
    "    # 4. LOG FILES - Model run information\n",
    "    logs_dir = sim_dir / \"logs\"\n",
    "    logs_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    (logs_dir / \"atm.log\").write_text(\n",
    "        \"CAM Atmospheric Model Log\\n\"\n",
    "        \"=========================\\n\"\n",
    "        \"Run started: 2024-01-01 00:00:00\\n\"\n",
    "        \"Resolution: f19_g16\\n\"\n",
    "        \"Timestep: 1800s\\n\"\n",
    "        \"Integration successful for 90 days\\n\"\n",
    "        \"Run completed: 2024-03-31 23:59:59\\n\"\n",
    "    )\n",
    "    \n",
    "    (logs_dir / \"run.log\").write_text(\n",
    "        \"CESM Run Log\\n\"\n",
    "        \"============\\n\"\n",
    "        \"Case: f2000_tutorial\\n\"\n",
    "        \"Components: CAM, CLM, POP, CICE\\n\"\n",
    "        \"Start date: 2024-01-01\\n\"\n",
    "        \"End date: 2024-03-31\\n\"\n",
    "        \"Total simulation time: 4.5 hours\\n\"\n",
    "        \"Status: COMPLETED SUCCESSFULLY\\n\"\n",
    "    )\n",
    "    \n",
    "    # 5. SCRIPTS - Analysis and processing scripts\n",
    "    scripts_dir = sim_dir / \"scripts\"\n",
    "    scripts_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    (scripts_dir / \"postprocess.py\").write_text(\n",
    "        \"#!/usr/bin/env python3\\n\"\n",
    "        \"\\\"\\\"\\\"Post-processing script for CESM output\\\"\\\"\\\"\\n\"\n",
    "        \"import xarray as xr\\n\"\n",
    "        \"\\n\"\n",
    "        \"# Calculate monthly means\\n\"\n",
    "        \"def monthly_means(input_file, output_file):\\n\"\n",
    "        \"    ds = xr.open_dataset(input_file)\\n\"\n",
    "        \"    monthly = ds.resample(time='M').mean()\\n\"\n",
    "        \"    monthly.to_netcdf(output_file)\\n\"\n",
    "    )\n",
    "    \n",
    "    return sim_dir\n",
    "\n",
    "def create_sample_netcdf(filepath, data_type):\n",
    "    \"\"\"\n",
    "    Creates a small but realistic NetCDF file for different Earth Science data types.\n",
    "    This helps you understand what different file types contain.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Simple coordinate system\n",
    "    lat = np.linspace(-90, 90, 64)  # 64 latitude points\n",
    "    lon = np.linspace(0, 360, 128)  # 128 longitude points \n",
    "    \n",
    "    if data_type == \"atmosphere\":\n",
    "        # Atmospheric data: temperature, humidity, winds\n",
    "        time = [datetime(2024, 1, 15)]  # Mid-month\n",
    "        temp = 288 + 30 * np.cos(np.radians(lat))[:, None]  # Temperature gradient\n",
    "        \n",
    "        ds = xr.Dataset({\n",
    "            'T': (['time', 'lat', 'lon'], temp[None, :, :]),\n",
    "            'Q': (['time', 'lat', 'lon'], 0.01 * np.ones((1, 64, 128))),\n",
    "        }, coords={\n",
    "            'time': time,\n",
    "            'lat': lat,\n",
    "            'lon': lon\n",
    "        })\n",
    "        \n",
    "        ds.attrs = {\n",
    "            'title': 'CAM Atmospheric Model Output',\n",
    "            'model': 'CAM6',\n",
    "            'resolution': 'f19_g16',\n",
    "            'case': 'f2000_tutorial'\n",
    "        }\n",
    "        \n",
    "    elif data_type == \"land\":\n",
    "        # Land surface data: temperatures, precipitation\n",
    "        time = [datetime(2024, 1, 15)]\n",
    "        surface_temp = 285 + 25 * np.cos(np.radians(lat))[:, None]\n",
    "        \n",
    "        ds = xr.Dataset({\n",
    "            'TSA': (['time', 'lat', 'lon'], surface_temp[None, :, :]),\n",
    "            'RAIN': (['time', 'lat', 'lon'], 0.001 * np.ones((1, 64, 128))),\n",
    "        }, coords={\n",
    "            'time': time,\n",
    "            'lat': lat,\n",
    "            'lon': lon\n",
    "        })\n",
    "        \n",
    "        ds.attrs = {\n",
    "            'title': 'CLM Land Model Output',\n",
    "            'model': 'CLM5',\n",
    "            'case': 'f2000_tutorial'\n",
    "        }\n",
    "        \n",
    "    elif data_type == \"ocean\":\n",
    "        # Ocean data: temperature, currents\n",
    "        time = [datetime(2024, 1, 15)]\n",
    "        depth = np.array([5, 15, 25, 35])  # Ocean levels\n",
    "        sst = 288 + 15 * np.cos(np.radians(lat))[:, None]  # Sea surface temp\n",
    "        \n",
    "        ds = xr.Dataset({\n",
    "            'TEMP': (['time', 'z_t', 'lat', 'lon'], \n",
    "                    sst[None, None, :, :] * np.ones((1, 4, 64, 128))),\n",
    "        }, coords={\n",
    "            'time': time,\n",
    "            'z_t': depth,\n",
    "            'lat': lat,\n",
    "            'lon': lon\n",
    "        })\n",
    "        \n",
    "        ds.attrs = {\n",
    "            'title': 'POP Ocean Model Output',\n",
    "            'model': 'POP2',\n",
    "            'case': 'f2000_tutorial'\n",
    "        }\n",
    "        \n",
    "    elif data_type in [\"initial\", \"restart\"]:\n",
    "        # Restart/initial files: model state for continuing runs\n",
    "        time = [datetime(2024, 4, 1)]  # Restart date\n",
    "        state_data = 300 * np.ones((64, 128))  # Model state\n",
    "        \n",
    "        ds = xr.Dataset({\n",
    "            'STATE': (['lat', 'lon'], state_data),\n",
    "            'CHECKPOINT': (['lat', 'lon'], state_data * 0.9),\n",
    "        }, coords={\n",
    "            'lat': lat,\n",
    "            'lon': lon\n",
    "        })\n",
    "        \n",
    "        ds.attrs = {\n",
    "            'title': f'{data_type.title()} File for CESM',\n",
    "            'restart_date': '2024-04-01',\n",
    "            'case': 'f2000_tutorial'\n",
    "        }\n",
    "    \n",
    "    # Save the file\n",
    "    ds.to_netcdf(filepath, format='NETCDF4')\n",
    "\n",
    "# Create the simulation directory\n",
    "cesm_dir = create_cesm_simulation_directory()\n",
    "print(f\"\\n✅ Created CESM simulation directory: {cesm_dir}\")\n",
    "\n",
    "# Show the structure\n",
    "print(\"\\n📁 Directory Structure:\")\n",
    "for item in sorted(cesm_dir.rglob('*')):\n",
    "    if item.is_file():\n",
    "        rel_path = item.relative_to(cesm_dir)\n",
    "        size_mb = item.stat().st_size / (1024 * 1024)\n",
    "        print(f\"  {rel_path} ({size_mb:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding What We Have\n",
    "\n",
    "Before creating archives, let's understand the different types of files in our CESM simulation:\n",
    "\n",
    "### File Types and Their Importance\n",
    "\n",
    "| **Type** | **Files** | **Purpose** | **Importance** |\n",
    "|----------|-----------|-------------|----------------|\n",
    "| **Input/Config** | `user_nl_*`, `initial_conditions.nc` | Model setup and parameters | **CRITICAL** - Need these to reproduce the run |\n",
    "| **Output** | `cam.h0.*`, `clm.h0.*`, `pop.h.*` | Scientific results | **IMPORTANT** - Main analysis data |\n",
    "| **Restart** | `*.r.*` files | Continue simulation | **CRITICAL** - Cannot continue run without these |\n",
    "| **Logs** | `*.log` files | Diagnostic information | **OPTIONAL** - Useful for debugging |\n",
    "| **Scripts** | `*.py` files | Analysis workflows | **IMPORTANT** - For reproducibility |\n",
    "\n",
    "**Key Insight**: Not all files are equally important! You might archive everything, but extract only what you need for specific analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Your First Archive Creation\n",
    "\n",
    "Now let's create your first climate data archive. We'll use Tellus to compress and organize all the simulation files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Tellus archive system\n",
    "from tellus.core.cli import console\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# First, let's see the space we're using\n",
    "def calculate_directory_size(directory):\n",
    "    \"\"\"Calculate total size of directory in MB\"\"\"\n",
    "    total_size = sum(f.stat().st_size for f in directory.rglob('*') if f.is_file())\n",
    "    return total_size / (1024 * 1024)\n",
    "\n",
    "original_size = calculate_directory_size(cesm_dir)\n",
    "console.print(f\"[blue]Original simulation size: {original_size:.1f} MB[/blue]\")\n",
    "\n",
    "# Create our first archive using the CLI\n",
    "archive_dir = tutorial_dir / \"archives\"\n",
    "archive_dir.mkdir(exist_ok=True)\n",
    "\n",
    "archive_name = \"cesm_tutorial_complete\"\n",
    "\n",
    "console.print(\"\\n[bold blue]Creating your first climate data archive...[/bold blue]\")\n",
    "console.print(\"[dim]This will compress and organize all simulation files[/dim]\")\n",
    "\n",
    "# Using pixi run to execute the CLI command properly\n",
    "cmd = [\n",
    "    \"pixi\", \"run\", \"tellus\", \"archive\", \"create\", \n",
    "    archive_name,\n",
    "    str(cesm_dir),\n",
    "    \"--location\", \"local_archive\"\n",
    "]\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True, cwd=\"/Users/pgierz/Code/github.com/pgierz/tellus\")\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        console.print(\"[green]✅ Archive created successfully![/green]\")\n",
    "        console.print(f\"[dim]Command output: {result.stdout}[/dim]\")\n",
    "    else:\n",
    "        console.print(f\"[red]❌ Archive creation failed: {result.stderr}[/red]\")\n",
    "        # Let's try a simpler approach for the tutorial\n",
    "        console.print(\"[yellow]⚠️  CLI not available, creating archive manually for tutorial...[/yellow]\")\n",
    "        \n",
    "except Exception as e:\n",
    "    console.print(f\"[red]Error running command: {e}[/red]\")\n",
    "    console.print(\"[yellow]⚠️  Continuing with manual archive creation for tutorial...[/yellow]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me show you what happens conceptually when you create an archive (since the CLI might not be available in this environment):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a simplified archive manually to show the concepts\n",
    "import tarfile\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def create_tutorial_archive(source_dir, archive_path):\n",
    "    \"\"\"\n",
    "    Manually create an archive to demonstrate the concepts.\n",
    "    This shows what Tellus does internally.\n",
    "    \"\"\"\n",
    "    \n",
    "    console.print(\"[blue]📦 Creating compressed archive...[/blue]\")\n",
    "    \n",
    "    # Create the compressed tar archive\n",
    "    with tarfile.open(archive_path, \"w:gz\") as tar:\n",
    "        # Add all files to the archive\n",
    "        for file_path in source_dir.rglob('*'):\n",
    "            if file_path.is_file():\n",
    "                arcname = file_path.relative_to(source_dir)\n",
    "                tar.add(file_path, arcname=arcname)\n",
    "                console.print(f\"  Added: {arcname}\")\n",
    "    \n",
    "    # Create metadata (what Tellus does automatically)\n",
    "    console.print(\"\\n[blue]📋 Creating archive metadata...[/blue]\")\n",
    "    \n",
    "    files_info = []\n",
    "    total_size = 0\n",
    "    \n",
    "    for file_path in source_dir.rglob('*'):\n",
    "        if file_path.is_file():\n",
    "            rel_path = file_path.relative_to(source_dir)\n",
    "            size = file_path.stat().st_size\n",
    "            total_size += size\n",
    "            \n",
    "            # Classify file type (simplified version of what Tellus does)\n",
    "            if rel_path.name.startswith('user_nl_') or 'initial' in rel_path.name:\n",
    "                content_type = 'INPUT'\n",
    "                importance = 'CRITICAL'\n",
    "            elif rel_path.suffix == '.nc' and 'output' in str(rel_path):\n",
    "                content_type = 'OUTPUT'\n",
    "                importance = 'IMPORTANT'\n",
    "            elif '.r.' in rel_path.name:\n",
    "                content_type = 'RESTART'\n",
    "                importance = 'CRITICAL'\n",
    "            elif rel_path.suffix == '.log':\n",
    "                content_type = 'LOG'\n",
    "                importance = 'OPTIONAL'\n",
    "            elif rel_path.suffix == '.py':\n",
    "                content_type = 'SCRIPT'\n",
    "                importance = 'IMPORTANT'\n",
    "            else:\n",
    "                content_type = 'OTHER'\n",
    "                importance = 'OPTIONAL'\n",
    "            \n",
    "            files_info.append({\n",
    "                'path': str(rel_path),\n",
    "                'size': size,\n",
    "                'content_type': content_type,\n",
    "                'importance': importance\n",
    "            })\n",
    "    \n",
    "    # Create metadata file\n",
    "    metadata = {\n",
    "        'metadata_version': '1.0',\n",
    "        'created_at': datetime.now().isoformat(),\n",
    "        'archive': {\n",
    "            'archive_id': 'cesm_tutorial_complete',\n",
    "            'source_directory': str(source_dir),\n",
    "            'archive_type': 'compressed'\n",
    "        },\n",
    "        'simulation': {\n",
    "            'case': 'f2000_tutorial',\n",
    "            'model': 'CESM',\n",
    "            'period': '2024-01-01 to 2024-03-31'\n",
    "        },\n",
    "        'inventory': {\n",
    "            'total_files': len(files_info),\n",
    "            'total_size': total_size,\n",
    "            'content_summary': {},\n",
    "            'files': files_info\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Count files by type\n",
    "    content_counts = {}\n",
    "    for file_info in files_info:\n",
    "        content_type = file_info['content_type']\n",
    "        content_counts[content_type] = content_counts.get(content_type, 0) + 1\n",
    "    \n",
    "    metadata['inventory']['content_summary'] = content_counts\n",
    "    \n",
    "    # Save metadata file\n",
    "    metadata_path = archive_path.with_suffix('.metadata.json')\n",
    "    metadata_path.write_text(json.dumps(metadata, indent=2))\n",
    "    \n",
    "    return archive_path, metadata_path\n",
    "\n",
    "# Create the archive\n",
    "archive_path = tutorial_dir / \"cesm_tutorial_complete.tar.gz\"\n",
    "archive_file, metadata_file = create_tutorial_archive(cesm_dir, archive_path)\n",
    "\n",
    "# Show results\n",
    "archive_size = archive_file.stat().st_size / (1024 * 1024)\n",
    "compression_ratio = (original_size - archive_size) / original_size * 100\n",
    "\n",
    "console.print(f\"\\n[green]✅ Archive creation complete![/green]\")\n",
    "console.print(f\"[blue]Archive file: {archive_file.name} ({archive_size:.1f} MB)[/blue]\")\n",
    "console.print(f\"[blue]Metadata file: {metadata_file.name}[/blue]\")\n",
    "console.print(f\"[green]Space saved: {compression_ratio:.1f}% compression[/green]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Examining Your Archive\n",
    "\n",
    "Now let's explore what's in our archive without extracting it. This is like having a table of contents for your compressed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display the archive metadata\n",
    "metadata = json.loads(metadata_file.read_text())\n",
    "\n",
    "console.print(\"[bold blue]📊 Archive Contents Summary[/bold blue]\")\n",
    "console.print(\"=\" * 50)\n",
    "\n",
    "# General information\n",
    "console.print(f\"[cyan]Archive ID:[/cyan] {metadata['archive']['archive_id']}\")\n",
    "console.print(f\"[cyan]Created:[/cyan] {metadata['created_at'][:19]}\")\n",
    "console.print(f\"[cyan]Total Files:[/cyan] {metadata['inventory']['total_files']}\")\n",
    "console.print(f\"[cyan]Total Size:[/cyan] {metadata['inventory']['total_size'] / (1024*1024):.1f} MB\")\n",
    "\n",
    "# Content breakdown\n",
    "console.print(\"\\n[bold blue]📁 Files by Content Type[/bold blue]\")\n",
    "for content_type, count in metadata['inventory']['content_summary'].items():\n",
    "    console.print(f\"  [green]{content_type}:[/green] {count} files\")\n",
    "\n",
    "# Show a few example files\n",
    "console.print(\"\\n[bold blue]📄 Example Files[/bold blue]\")\n",
    "for file_info in metadata['inventory']['files'][:10]:  # Show first 10 files\n",
    "    size_str = f\"{file_info['size'] / 1024:.1f} KB\" if file_info['size'] > 1024 else f\"{file_info['size']} B\"\n",
    "    console.print(\n",
    "        f\"  [yellow]{file_info['path']}[/yellow] \"\n",
    "        f\"[dim]({size_str}, {file_info['content_type']}, {file_info['importance']})[/dim]\"\n",
    "    )\n",
    "\n",
    "if len(metadata['inventory']['files']) > 10:\n",
    "    console.print(f\"  [dim]... and {len(metadata['inventory']['files']) - 10} more files[/dim]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Listing Your Archives\n",
    "\n",
    "In a real workflow, you'll have multiple archives. Let's create another archive and see how to list them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a second archive with only the critical files\n",
    "console.print(\"[bold blue]Creating a 'Critical Files Only' archive...[/bold blue]\")\n",
    "console.print(\"[dim]This archive contains only files needed to restart the simulation[/dim]\")\n",
    "\n",
    "def create_critical_only_archive():\n",
    "    \"\"\"Create an archive with only critical files (configs + restart files)\"\"\"\n",
    "    \n",
    "    critical_archive_path = tutorial_dir / \"cesm_tutorial_critical.tar.gz\"\n",
    "    \n",
    "    with tarfile.open(critical_archive_path, \"w:gz\") as tar:\n",
    "        for file_path in cesm_dir.rglob('*'):\n",
    "            if file_path.is_file():\n",
    "                rel_path = file_path.relative_to(cesm_dir)\n",
    "                \n",
    "                # Only include critical files\n",
    "                is_critical = (\n",
    "                    rel_path.name.startswith('user_nl_') or  # CESM namelists\n",
    "                    'initial' in rel_path.name or           # Initial conditions\n",
    "                    '.r.' in rel_path.name                  # Restart files\n",
    "                )\n",
    "                \n",
    "                if is_critical:\n",
    "                    tar.add(file_path, arcname=rel_path)\n",
    "                    console.print(f\"  Added critical file: {rel_path}\")\n",
    "    \n",
    "    return critical_archive_path\n",
    "\n",
    "critical_archive = create_critical_only_archive()\n",
    "critical_size = critical_archive.stat().st_size / (1024 * 1024)\n",
    "\n",
    "console.print(f\"\\n[green]✅ Critical archive created: {critical_archive.name} ({critical_size:.1f} MB)[/green]\")\n",
    "\n",
    "# Now let's \"list\" our archives (simulate what 'tellus archive list' would show)\n",
    "console.print(\"\\n[bold blue]📚 Your Archive Collection[/bold blue]\")\n",
    "console.print(\"=\" * 60)\n",
    "\n",
    "archives = [\n",
    "    {\n",
    "        'name': 'cesm_tutorial_complete',\n",
    "        'file': archive_file,\n",
    "        'description': 'Complete CESM simulation (all files)',\n",
    "        'content_types': ['INPUT', 'OUTPUT', 'RESTART', 'LOG', 'SCRIPT']\n",
    "    },\n",
    "    {\n",
    "        'name': 'cesm_tutorial_critical',\n",
    "        'file': critical_archive,\n",
    "        'description': 'Critical files only (restart capability)',\n",
    "        'content_types': ['INPUT', 'RESTART']\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, archive in enumerate(archives, 1):\n",
    "    size_mb = archive['file'].stat().st_size / (1024 * 1024)\n",
    "    content_str = ', '.join(archive['content_types'])\n",
    "    \n",
    "    console.print(f\"[cyan]{i}. {archive['name']}[/cyan]\")\n",
    "    console.print(f\"   Size: {size_mb:.1f} MB\")\n",
    "    console.print(f\"   Description: {archive['description']}\")\n",
    "    console.print(f\"   Content Types: {content_str}\")\n",
    "    console.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Understanding Archive Benefits\n",
    "\n",
    "Let's compare the space usage and organization benefits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.table import Table\n",
    "\n",
    "# Create a comparison table\n",
    "table = Table(title=\"Storage Comparison: Original vs. Archives\")\n",
    "table.add_column(\"Item\", style=\"cyan\")\n",
    "table.add_column(\"Size (MB)\", justify=\"right\", style=\"green\")\n",
    "table.add_column(\"Files\", justify=\"right\", style=\"yellow\")\n",
    "table.add_column(\"Notes\", style=\"dim\")\n",
    "\n",
    "# Original data\n",
    "original_files = len(list(cesm_dir.rglob('*')))\n",
    "table.add_row(\n",
    "    \"Original Directory\",\n",
    "    f\"{original_size:.1f}\",\n",
    "    str(original_files),\n",
    "    \"Uncompressed, scattered files\"\n",
    ")\n",
    "\n",
    "# Complete archive\n",
    "complete_size = archive_file.stat().st_size / (1024 * 1024)\n",
    "table.add_row(\n",
    "    \"Complete Archive\",\n",
    "    f\"{complete_size:.1f}\",\n",
    "    \"1\",\n",
    "    \"All files, compressed + metadata\"\n",
    ")\n",
    "\n",
    "# Critical archive\n",
    "critical_size = critical_archive.stat().st_size / (1024 * 1024)\n",
    "table.add_row(\n",
    "    \"Critical Archive\",\n",
    "    f\"{critical_size:.1f}\",\n",
    "    \"1\",\n",
    "    \"Essential files only\"\n",
    ")\n",
    "\n",
    "console.print(table)\n",
    "\n",
    "# Space savings\n",
    "total_savings = (original_size - complete_size) / original_size * 100\n",
    "critical_savings = (original_size - critical_size) / original_size * 100\n",
    "\n",
    "console.print(f\"\\n[bold green]💾 Space Savings[/bold green]\")\n",
    "console.print(f\"Complete archive: {total_savings:.1f}% compression\")\n",
    "console.print(f\"Critical archive: {critical_savings:.1f}% space reduction\")\n",
    "\n",
    "console.print(f\"\\n[bold blue]🎯 Key Benefits[/bold blue]\")\n",
    "console.print(\"✅ [green]Compression:[/green] Reduced storage space\")\n",
    "console.print(\"✅ [green]Organization:[/green] Files automatically classified\")\n",
    "console.print(\"✅ [green]Metadata:[/green] Know what's inside without extracting\")\n",
    "console.print(\"✅ [green]Portability:[/green] Single file easy to transfer\")\n",
    "console.print(\"✅ [green]Selective Access:[/green] Can extract only what you need\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-World Scenarios: When to Use Each Archive Type\n",
    "\n",
    "Understanding when to create different types of archives is crucial for effective data management:\n",
    "\n",
    "### Scenario 1: Long-term Storage\n",
    "**Use Case**: Moving old simulation to tape storage  \n",
    "**Archive Type**: Complete archive  \n",
    "**Why**: You want everything preserved for potential future analysis\n",
    "\n",
    "```bash\n",
    "# Complete archive for long-term storage\n",
    "tellus archive create simulation_2024_complete /path/to/simulation \\\n",
    "  --location tape_storage\n",
    "```\n",
    "\n",
    "### Scenario 2: Continuing a Simulation\n",
    "**Use Case**: Need to restart simulation on different machine  \n",
    "**Archive Type**: Critical files only  \n",
    "**Why**: Smaller, faster transfer, contains everything needed to restart\n",
    "\n",
    "```bash\n",
    "# Critical files for simulation restart\n",
    "tellus archive create restart_package /path/to/simulation \\\n",
    "  --content-types input,restart,config\n",
    "```\n",
    "\n",
    "### Scenario 3: Sharing Results\n",
    "**Use Case**: Collaborator wants to analyze your results  \n",
    "**Archive Type**: Output files only  \n",
    "**Why**: Scientists usually only need the output data, not restart files\n",
    "\n",
    "```bash\n",
    "# Output data for collaborators\n",
    "tellus archive create results_for_analysis /path/to/simulation \\\n",
    "  --content-types output --patterns \"*.nc\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Beginner Mistakes and How to Avoid Them\n",
    "\n",
    "### ❌ Mistake 1: Archiving Everything Always\n",
    "**Problem**: Creating huge archives with log files and temporary data  \n",
    "**Solution**: Use content type filtering to exclude non-essential files\n",
    "\n",
    "### ❌ Mistake 2: Not Checking Archive Contents\n",
    "**Problem**: Creating archive and not verifying what's inside  \n",
    "**Solution**: Always use `tellus archive show` to inspect archive metadata\n",
    "\n",
    "### ❌ Mistake 3: Forgetting to Test Extraction\n",
    "**Problem**: Archive is created but never tested for extraction  \n",
    "**Solution**: Always test extracting a few files to verify archive integrity\n",
    "\n",
    "### ❌ Mistake 4: Poor Archive Naming\n",
    "**Problem**: Using vague names like \"simulation1\", \"test_archive\"  \n",
    "**Solution**: Use descriptive names: \"cesm_f2000_spinup_2024\", \"wrf_hurricane_katrina_outputs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up tutorial files\n",
    "import shutil\n",
    "\n",
    "console.print(\"[bold blue]🧹 Cleaning up tutorial files...[/bold blue]\")\n",
    "shutil.rmtree(tutorial_dir)\n",
    "console.print(f\"[green]✅ Cleaned up: {tutorial_dir}[/green]\")\n",
    "\n",
    "console.print(\"\\n[bold green]🎉 Tutorial 1 Complete![/bold green]\")\n",
    "console.print(\"\\n[bold blue]What You Learned:[/bold blue]\")\n",
    "console.print(\"✅ How to create climate data archives\")\n",
    "console.print(\"✅ Understanding file types and importance\")\n",
    "console.print(\"✅ Reading archive metadata without extraction\")\n",
    "console.print(\"✅ Comparing different archive strategies\")\n",
    "console.print(\"✅ When to use complete vs. selective archives\")\n",
    "\n",
    "console.print(\"\\n[bold blue]Next Steps:[/bold blue]\")\n",
    "console.print(\"📚 [cyan]Tutorial 2:[/cyan] Content Classification and Selective Archiving\")\n",
    "console.print(\"📚 [cyan]Tutorial 3:[/cyan] DateTime-Based Extraction and Filtering\")\n",
    "console.print(\"📚 [cyan]Tutorial 4:[/cyan] Fragment Assembly for Multi-Period Simulations\")\n",
    "\n",
    "console.print(\"\\n[dim]Ready to continue? Open Tutorial 2 to learn about smart file filtering![/dim]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree: Archive Strategy Selection\n",
    "\n",
    "Use this flowchart to decide what type of archive to create:\n",
    "\n",
    "```\n",
    "📊 What's your goal?\n",
    "├── 🎯 Continue simulation later?\n",
    "│   └── ✅ Create CRITICAL archive (configs + restart files)\n",
    "├── 📤 Share results with collaborators?\n",
    "│   └── ✅ Create OUTPUT archive (scientific data only)\n",
    "├── 💾 Long-term storage/backup?\n",
    "│   └── ✅ Create COMPLETE archive (everything)\n",
    "├── 🚀 Moving to faster storage?\n",
    "│   └── ✅ Create IMPORTANT archive (exclude logs/temp files)\n",
    "└── 🔍 Not sure?\n",
    "    └── ✅ Start with COMPLETE, extract selectively later\n",
    "```\n",
    "\n",
    "## Key Commands Reference\n",
    "\n",
    "```bash\n",
    "# Create complete archive\n",
    "tellus archive create my_simulation /path/to/data --location storage_location\n",
    "\n",
    "# List all archives\n",
    "tellus archive list\n",
    "\n",
    "# Show archive details\n",
    "tellus archive show my_simulation\n",
    "\n",
    "# Create selective archive\n",
    "tellus archive create critical_files /path/to/data \\\n",
    "  --content-types input,restart,config\n",
    "```\n",
    "\n",
    "**🎯 You're now ready to create and manage climate data archives! Continue to Tutorial 2 to learn about intelligent file classification and selective archiving strategies.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3: DateTime-Based Extraction and Filtering\n",
    "\n",
    "**Learning Goals:** Master temporal filtering to extract specific time periods from climate archives, enabling efficient seasonal analysis and event studies.\n",
    "\n",
    "**Time Estimate:** 30 minutes\n",
    "\n",
    "**Prerequisites:** Tutorials 1 and 2 completed\n",
    "\n",
    "## The DateTime Challenge in Earth Science\n",
    "\n",
    "Climate simulations often span years or decades, generating massive archives. But most analyses focus on specific time periods:\n",
    "\n",
    "```\n",
    "‚ùå The Problem:\n",
    "Your 10-year CESM simulation archive (500GB) contains:\n",
    "‚îú‚îÄ‚îÄ cam.h0.2015-01.nc  # January 2015 data\n",
    "‚îú‚îÄ‚îÄ cam.h0.2015-02.nc  # February 2015 data\n",
    "‚îú‚îÄ‚îÄ ...\n",
    "‚îú‚îÄ‚îÄ cam.h0.2024-12.nc  # December 2024 data\n",
    "\n",
    "But you only need:\n",
    "üéØ Summer 2023 data for heat wave analysis\n",
    "üéØ El Ni√±o years (2015-2016, 2023-2024) for ENSO study\n",
    "üéØ Monthly data from 2020-2022 for pandemic climate impacts\n",
    "```\n",
    "\n",
    "**The Challenge**: Extracting 3 months of data shouldn't require downloading 10 years!\n",
    "\n",
    "**Tellus Solution**: DateTime-based filtering lets you extract exactly the time periods you need, with intelligent pattern matching and temporal logic.\n",
    "\n",
    "In this tutorial, you'll learn to:\n",
    "1. Extract specific dates and date ranges\n",
    "2. Use temporal patterns for seasonal analysis\n",
    "3. Handle different date formats in filenames\n",
    "4. Combine datetime filtering with content filtering\n",
    "5. Work with multi-decadal datasets efficiently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Creating a Multi-Year Climate Dataset\n",
    "\n",
    "Let's create a realistic multi-year climate simulation with different temporal outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from datetime import datetime, timedelta\n",
    "import calendar\n",
    "from tellus.core.cli import console\n",
    "from rich.table import Table\n",
    "from rich.panel import Panel\n",
    "from rich.progress import track\n",
    "\n",
    "# Create tutorial workspace\n",
    "tutorial_dir = Path(tempfile.mkdtemp())\n",
    "console.print(f\"[blue]Tutorial workspace: {tutorial_dir}[/blue]\")\n",
    "\n",
    "def create_multiyear_climate_dataset():\n",
    "    \"\"\"\n",
    "    Create a realistic multi-year climate dataset with different temporal frequencies.\n",
    "    This simulates a 5-year CESM simulation with monthly, seasonal, and annual outputs.\n",
    "    \"\"\"\n",
    "    \n",
    "    dataset_dir = tutorial_dir / \"cesm_2020_2024_simulation\"\n",
    "    dataset_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    console.print(\"[blue]Creating 5-year multi-frequency climate dataset...[/blue]\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # 1. MONTHLY OUTPUT - Primary analysis data\n",
    "    # ==========================================\n",
    "    monthly_dir = dataset_dir / \"output\" / \"monthly\"\n",
    "    monthly_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    console.print(\"  üìÖ Creating monthly output files...\")\n",
    "    \n",
    "    monthly_files_created = []\n",
    "    \n",
    "    # Create monthly files for 2020-2024\n",
    "    for year in range(2020, 2025):\n",
    "        for month in range(1, 13):\n",
    "            # Different components with different naming conventions\n",
    "            files_to_create = [\n",
    "                (f\"cam.h0.{year:04d}-{month:02d}.nc\", \"atmosphere\"),\n",
    "                (f\"clm.h0.{year:04d}-{month:02d}.nc\", \"land\"),\n",
    "                (f\"pop.h.{year:04d}-{month:02d}.nc\", \"ocean\"),\n",
    "                (f\"cice.h.{year:04d}-{month:02d}.nc\", \"seaice\")\n",
    "            ]\n",
    "            \n",
    "            for filename, data_type in files_to_create:\n",
    "                filepath = monthly_dir / filename\n",
    "                create_sample_climate_data(filepath, data_type, year, month)\n",
    "                monthly_files_created.append(filename)\n",
    "    \n",
    "    console.print(f\"    ‚úÖ Created {len(monthly_files_created)} monthly files\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # 2. DAILY OUTPUT - High-frequency data\n",
    "    # ==========================================\n",
    "    daily_dir = dataset_dir / \"output\" / \"daily\"\n",
    "    daily_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    console.print(\"  üìÜ Creating daily output samples (selected periods)...\")\n",
    "    \n",
    "    # Create daily files for specific periods of interest\n",
    "    daily_periods = [\n",
    "        # Summer 2023 - heat wave analysis period\n",
    "        {'year': 2023, 'months': [6, 7, 8], 'label': 'summer_heatwave'},\n",
    "        # Winter 2021-2022 - extreme winter analysis\n",
    "        {'year': 2021, 'months': [12], 'label': 'extreme_winter_start'},\n",
    "        {'year': 2022, 'months': [1, 2], 'label': 'extreme_winter_end'},\n",
    "        # El Ni√±o period 2023-2024\n",
    "        {'year': 2023, 'months': [10, 11, 12], 'label': 'el_nino_start'},\n",
    "        {'year': 2024, 'months': [1, 2, 3], 'label': 'el_nino_peak'}\n",
    "    ]\n",
    "    \n",
    "    daily_files_created = []\n",
    "    \n",
    "    for period in daily_periods:\n",
    "        year = period['year']\n",
    "        for month in period['months']:\n",
    "            # Create daily files for the full month\n",
    "            days_in_month = calendar.monthrange(year, month)[1]\n",
    "            \n",
    "            # Atmospheric daily data (surface variables)\n",
    "            filename = f\"cam.h1.{year:04d}-{month:02d}.nc\"\n",
    "            filepath = daily_dir / filename\n",
    "            create_sample_climate_data(filepath, \"atmosphere_daily\", year, month, days_in_month)\n",
    "            daily_files_created.append(filename)\n",
    "    \n",
    "    console.print(f\"    ‚úÖ Created {len(daily_files_created)} daily files for key periods\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # 3. SEASONAL OUTPUT - Climate analysis\n",
    "    # ==========================================\n",
    "    seasonal_dir = dataset_dir / \"output\" / \"seasonal\"\n",
    "    seasonal_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    console.print(\"  üå± Creating seasonal climatology files...\")\n",
    "    \n",
    "    seasonal_files_created = []\n",
    "    seasons = {'DJF': 'winter', 'MAM': 'spring', 'JJA': 'summer', 'SON': 'autumn'}\n",
    "    \n",
    "    for year in range(2020, 2025):\n",
    "        for season_abbrev, season_name in seasons.items():\n",
    "            # Create seasonal averages\n",
    "            filename = f\"cesm.{season_abbrev}.{year:04d}.nc\"\n",
    "            filepath = seasonal_dir / filename\n",
    "            create_sample_climate_data(filepath, \"seasonal\", year, season=season_name)\n",
    "            seasonal_files_created.append(filename)\n",
    "    \n",
    "    console.print(f\"    ‚úÖ Created {len(seasonal_files_created)} seasonal files\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # 4. ANNUAL OUTPUT - Long-term trends\n",
    "    # ==========================================\n",
    "    annual_dir = dataset_dir / \"output\" / \"annual\"\n",
    "    annual_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    console.print(\"  üóìÔ∏è Creating annual summary files...\")\n",
    "    \n",
    "    annual_files_created = []\n",
    "    \n",
    "    for year in range(2020, 2025):\n",
    "        # Annual means and extremes\n",
    "        files_to_create = [\n",
    "            (f\"cesm.annual_mean.{year:04d}.nc\", \"annual_mean\"),\n",
    "            (f\"cesm.annual_extremes.{year:04d}.nc\", \"annual_extremes\")\n",
    "        ]\n",
    "        \n",
    "        for filename, data_type in files_to_create:\n",
    "            filepath = annual_dir / filename\n",
    "            create_sample_climate_data(filepath, data_type, year)\n",
    "            annual_files_created.append(filename)\n",
    "    \n",
    "    console.print(f\"    ‚úÖ Created {len(annual_files_created)} annual files\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # 5. EVENT-BASED OUTPUT - Specific phenomena\n",
    "    # ==========================================\n",
    "    events_dir = dataset_dir / \"output\" / \"events\"\n",
    "    events_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    console.print(\"  ‚ö° Creating event-based analysis files...\")\n",
    "    \n",
    "    # Specific climate events with irregular timing\n",
    "    climate_events = [\n",
    "        {'name': 'heatwave_2023_07_15_to_2023_07_25.nc', 'type': 'extreme_event'},\n",
    "        {'name': 'hurricane_season_2023_08_15_to_2023_10_30.nc', 'type': 'seasonal_phenomenon'},\n",
    "        {'name': 'arctic_oscillation_negative_2024_01_10_to_2024_02_20.nc', 'type': 'teleconnection'},\n",
    "        {'name': 'blocking_pattern_2022_12_01_to_2022_12_15.nc', 'type': 'circulation_pattern'},\n",
    "        {'name': 'drought_onset_2021_05_01_to_2021_08_31.nc', 'type': 'hydrological_event'}\n",
    "    ]\n",
    "    \n",
    "    event_files_created = []\n",
    "    \n",
    "    for event in climate_events:\n",
    "        filepath = events_dir / event['name']\n",
    "        create_sample_climate_data(filepath, event['type'])\n",
    "        event_files_created.append(event['name'])\n",
    "    \n",
    "    console.print(f\"    ‚úÖ Created {len(event_files_created)} event-based files\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # 6. RESTART FILES - With timestamps\n",
    "    # ==========================================\n",
    "    restart_dir = dataset_dir / \"restart\"\n",
    "    restart_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    console.print(\"  üîÑ Creating restart files...\")\n",
    "    \n",
    "    # Restart files at specific dates (typically January 1st)\n",
    "    restart_files_created = []\n",
    "    \n",
    "    for year in range(2021, 2025):  # Restart files for continuation\n",
    "        restart_date = f\"{year:04d}-01-01-00000\"\n",
    "        restart_files = [\n",
    "            f\"cam.r.{restart_date}.nc\",\n",
    "            f\"clm.r.{restart_date}.nc\",\n",
    "            f\"pop.r.{restart_date}.nc\",\n",
    "            f\"cice.r.{restart_date}.nc\"\n",
    "        ]\n",
    "        \n",
    "        for filename in restart_files:\n",
    "            filepath = restart_dir / filename\n",
    "            create_sample_climate_data(filepath, \"restart\", year)\n",
    "            restart_files_created.append(filename)\n",
    "    \n",
    "    console.print(f\"    ‚úÖ Created {len(restart_files_created)} restart files\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # 7. METADATA AND DOCUMENTATION\n",
    "    # ==========================================\n",
    "    docs_dir = dataset_dir / \"docs\"\n",
    "    docs_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Create temporal coverage documentation\n",
    "    temporal_info = {\n",
    "        \"simulation_period\": {\n",
    "            \"start_date\": \"2020-01-01\",\n",
    "            \"end_date\": \"2024-12-31\",\n",
    "            \"total_years\": 5\n",
    "        },\n",
    "        \"output_frequencies\": {\n",
    "            \"monthly\": {\n",
    "                \"description\": \"Standard monthly means\",\n",
    "                \"files_per_year\": 48,\n",
    "                \"total_files\": len(monthly_files_created),\n",
    "                \"components\": [\"cam\", \"clm\", \"pop\", \"cice\"]\n",
    "            },\n",
    "            \"daily\": {\n",
    "                \"description\": \"High-frequency data for key periods\",\n",
    "                \"total_files\": len(daily_files_created),\n",
    "                \"key_periods\": [p['label'] for p in daily_periods]\n",
    "            },\n",
    "            \"seasonal\": {\n",
    "                \"description\": \"Seasonal climatologies\",\n",
    "                \"seasons\": list(seasons.keys()),\n",
    "                \"total_files\": len(seasonal_files_created)\n",
    "            },\n",
    "            \"annual\": {\n",
    "                \"description\": \"Annual summaries and extremes\",\n",
    "                \"total_files\": len(annual_files_created)\n",
    "            },\n",
    "            \"events\": {\n",
    "                \"description\": \"Specific climate phenomena\",\n",
    "                \"total_files\": len(event_files_created),\n",
    "                \"event_types\": list(set(e['type'] for e in climate_events))\n",
    "            }\n",
    "        },\n",
    "        \"date_formats_used\": {\n",
    "            \"monthly\": \"YYYY-MM\",\n",
    "            \"daily\": \"YYYY-MM\", \n",
    "            \"seasonal\": \"SEASON.YYYY\",\n",
    "            \"annual\": \"YYYY\",\n",
    "            \"events\": \"YYYY_MM_DD_to_YYYY_MM_DD\",\n",
    "            \"restart\": \"YYYY-MM-DD-HHMMSS\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    (docs_dir / \"temporal_coverage.json\").write_text(json.dumps(temporal_info, indent=2))\n",
    "    \n",
    "    return dataset_dir, temporal_info\n",
    "\n",
    "def create_sample_climate_data(filepath, data_type, year=2023, month=1, days=None, season=None):\n",
    "    \"\"\"\n",
    "    Create sample NetCDF files with realistic climate data and temporal coordinates.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Standard spatial grid\n",
    "    lat = np.linspace(-89.5, 89.5, 96)\n",
    "    lon = np.linspace(0, 359, 144)\n",
    "    \n",
    "    if data_type == \"atmosphere\":\n",
    "        # Monthly atmospheric data\n",
    "        time = [datetime(year, month, 15)]  # Mid-month\n",
    "        temp = 288 + 30 * np.cos(np.radians(lat)) + 5 * np.sin(2 * np.pi * month / 12)\n",
    "        \n",
    "        ds = xr.Dataset({\n",
    "            'T': (['time', 'lat', 'lon'], temp[None, :, None]),\n",
    "            'PRECC': (['time', 'lat', 'lon'], 0.001 * np.abs(np.cos(np.radians(lat)))[None, :, None]),\n",
    "            'Q': (['time', 'lat', 'lon'], 0.01 * np.ones((1, 96, 144)))\n",
    "        }, coords={'time': time, 'lat': lat, 'lon': lon})\n",
    "        \n",
    "        ds.attrs = {\n",
    "            'title': f'CAM Atmospheric Output - {year:04d}-{month:02d}',\n",
    "            'model': 'CAM6',\n",
    "            'frequency': 'monthly',\n",
    "            'temporal_coverage': f'{year:04d}-{month:02d}'\n",
    "        }\n",
    "        \n",
    "    elif data_type == \"atmosphere_daily\":\n",
    "        # Daily atmospheric data for the full month\n",
    "        if days is None:\n",
    "            days = calendar.monthrange(year, month)[1]\n",
    "        \n",
    "        time_daily = [datetime(year, month, day) for day in range(1, days + 1)]\n",
    "        \n",
    "        # Add daily variability\n",
    "        temp_daily = []\n",
    "        for day in range(days):\n",
    "            daily_variation = 5 * np.sin(2 * np.pi * day / 30)  # Monthly cycle\n",
    "            temp = 288 + 30 * np.cos(np.radians(lat)) + daily_variation\n",
    "            temp_daily.append(temp)\n",
    "        \n",
    "        ds = xr.Dataset({\n",
    "            'TS': (['time', 'lat', 'lon'], np.array(temp_daily)[:, :, None]),\n",
    "            'PSL': (['time', 'lat', 'lon'], 101325 * np.ones((days, 96, 144))),\n",
    "            'PRECT': (['time', 'lat', 'lon'], 0.001 * np.random.rand(days, 96, 144))\n",
    "        }, coords={'time': time_daily, 'lat': lat, 'lon': lon})\n",
    "        \n",
    "        ds.attrs = {\n",
    "            'title': f'CAM Daily Surface Output - {year:04d}-{month:02d}',\n",
    "            'model': 'CAM6',\n",
    "            'frequency': 'daily',\n",
    "            'temporal_coverage': f'{year:04d}-{month:02d}'\n",
    "        }\n",
    "        \n",
    "    elif data_type == \"land\":\n",
    "        time = [datetime(year, month, 15)]\n",
    "        temp = 285 + 25 * np.cos(np.radians(lat)) + 3 * np.sin(2 * np.pi * month / 12)\n",
    "        \n",
    "        ds = xr.Dataset({\n",
    "            'TSA': (['time', 'lat', 'lon'], temp[None, :, None]),\n",
    "            'GPP': (['time', 'lat', 'lon'], 0.01 * np.abs(np.cos(np.radians(lat)))[None, :, None]),\n",
    "            'SOILWATER_10CM': (['time', 'lat', 'lon'], 0.3 * np.ones((1, 96, 144)))\n",
    "        }, coords={'time': time, 'lat': lat, 'lon': lon})\n",
    "        \n",
    "        ds.attrs = {'title': f'CLM Land Output - {year:04d}-{month:02d}', 'frequency': 'monthly'}\n",
    "        \n",
    "    elif data_type == \"ocean\":\n",
    "        time = [datetime(year, month, 15)]\n",
    "        depth = np.array([5, 15, 25, 50, 100])\n",
    "        temp = 290 - 0.5 * depth[:, None, None] + 15 * np.cos(np.radians(lat))[None, :, None]\n",
    "        \n",
    "        ds = xr.Dataset({\n",
    "            'TEMP': (['time', 'z_t', 'lat', 'lon'], temp[None, :, :, None]),\n",
    "            'SALT': (['time', 'z_t', 'lat', 'lon'], 35 * np.ones((1, 5, 96, 144))),\n",
    "            'SSH': (['time', 'lat', 'lon'], 0.1 * np.sin(2 * np.radians(lat))[None, :, None])\n",
    "        }, coords={'time': time, 'z_t': depth, 'lat': lat, 'lon': lon})\n",
    "        \n",
    "        ds.attrs = {'title': f'POP Ocean Output - {year:04d}-{month:02d}', 'frequency': 'monthly'}\n",
    "        \n",
    "    elif data_type == \"seaice\":\n",
    "        time = [datetime(year, month, 15)]\n",
    "        # Sea ice concentration - higher at poles, seasonal cycle\n",
    "        ice_base = (np.abs(lat) > 60).astype(float)\n",
    "        seasonal_factor = 0.3 * np.cos(2 * np.pi * (month - 3) / 12)  # Max in winter\n",
    "        aice = ice_base * (0.7 + seasonal_factor)\n",
    "        \n",
    "        ds = xr.Dataset({\n",
    "            'aice': (['time', 'lat', 'lon'], aice[None, :, None]),\n",
    "            'hi': (['time', 'lat', 'lon'], 2.0 * aice[None, :, None])\n",
    "        }, coords={'time': time, 'lat': lat, 'lon': lon})\n",
    "        \n",
    "        ds.attrs = {'title': f'CICE Sea Ice Output - {year:04d}-{month:02d}', 'frequency': 'monthly'}\n",
    "        \n",
    "    elif data_type == \"seasonal\":\n",
    "        # Seasonal averages\n",
    "        season_months = {\n",
    "            'winter': [12, 1, 2], 'spring': [3, 4, 5], \n",
    "            'summer': [6, 7, 8], 'autumn': [9, 10, 11]\n",
    "        }\n",
    "        \n",
    "        # Use middle month for representative values\n",
    "        rep_month = season_months[season][1]\n",
    "        time = [datetime(year, rep_month, 15)]\n",
    "        \n",
    "        temp = 288 + 30 * np.cos(np.radians(lat)) + 10 * np.sin(2 * np.pi * rep_month / 12)\n",
    "        \n",
    "        ds = xr.Dataset({\n",
    "            'T_seasonal': (['time', 'lat', 'lon'], temp[None, :, None]),\n",
    "            'PREC_seasonal': (['time', 'lat', 'lon'], 0.002 * np.abs(np.cos(np.radians(lat)))[None, :, None])\n",
    "        }, coords={'time': time, 'lat': lat, 'lon': lon})\n",
    "        \n",
    "        ds.attrs = {\n",
    "            'title': f'CESM Seasonal Average - {season.title()} {year:04d}',\n",
    "            'frequency': 'seasonal',\n",
    "            'season': season\n",
    "        }\n",
    "        \n",
    "    elif data_type in [\"annual_mean\", \"annual_extremes\"]:\n",
    "        time = [datetime(year, 7, 1)]  # Mid-year representative\n",
    "        \n",
    "        if data_type == \"annual_mean\":\n",
    "            temp = 288 + 30 * np.cos(np.radians(lat))\n",
    "            var_name, title_suffix = 'T_annual_mean', 'Annual Mean'\n",
    "        else:\n",
    "            temp = 308 + 35 * np.cos(np.radians(lat))  # Higher for extremes\n",
    "            var_name, title_suffix = 'T_annual_max', 'Annual Maximum'\n",
    "        \n",
    "        ds = xr.Dataset({\n",
    "            var_name: (['time', 'lat', 'lon'], temp[None, :, None])\n",
    "        }, coords={'time': time, 'lat': lat, 'lon': lon})\n",
    "        \n",
    "        ds.attrs = {\n",
    "            'title': f'CESM {title_suffix} - {year:04d}',\n",
    "            'frequency': 'annual'\n",
    "        }\n",
    "        \n",
    "    elif data_type == \"restart\":\n",
    "        # Restart files - model state data\n",
    "        ds = xr.Dataset({\n",
    "            'STATE': (['lat', 'lon'], 300 * np.ones((96, 144))),\n",
    "            'CHECKPOINT': (['lat', 'lon'], 295 * np.ones((96, 144)))\n",
    "        }, coords={'lat': lat, 'lon': lon})\n",
    "        \n",
    "        ds.attrs = {\n",
    "            'title': f'CESM Restart File - {year:04d}',\n",
    "            'restart_date': f'{year:04d}-01-01'\n",
    "        }\n",
    "        \n",
    "    else:\n",
    "        # Generic event-based data\n",
    "        time = [datetime(year, 7, 15)]  # Default time\n",
    "        temp = 295 + 20 * np.cos(np.radians(lat))\n",
    "        \n",
    "        ds = xr.Dataset({\n",
    "            'event_data': (['time', 'lat', 'lon'], temp[None, :, None])\n",
    "        }, coords={'time': time, 'lat': lat, 'lon': lon})\n",
    "        \n",
    "        ds.attrs = {'title': f'Climate Event Data - {data_type}', 'event_type': data_type}\n",
    "    \n",
    "    # Save the dataset\n",
    "    ds.to_netcdf(filepath, format='NETCDF4_CLASSIC')\n",
    "\n",
    "# Create the multi-year dataset\n",
    "dataset_dir, temporal_info = create_multiyear_climate_dataset()\n",
    "console.print(f\"\\n[green]‚úÖ Multi-year climate dataset created: {dataset_dir.name}[/green]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Our Dataset's Temporal Structure\n",
    "\n",
    "Let's examine the temporal patterns in our dataset before learning to filter them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_temporal_patterns(dataset_dir):\n",
    "    \"\"\"\n",
    "    Analyze the temporal patterns in our climate dataset.\n",
    "    This helps us understand what datetime filtering options are available.\n",
    "    \"\"\"\n",
    "    \n",
    "    console.print(\"\\n[bold blue]üìä Temporal Pattern Analysis[/bold blue]\")\n",
    "    console.print(\"=\" * 50)\n",
    "    \n",
    "    # Analyze different output directories\n",
    "    output_dirs = {\n",
    "        'monthly': 'Standard monthly outputs',\n",
    "        'daily': 'High-frequency daily data',\n",
    "        'seasonal': 'Seasonal climatologies',\n",
    "        'annual': 'Annual summaries',\n",
    "        'events': 'Event-based analysis'\n",
    "    }\n",
    "    \n",
    "    datetime_patterns = []\n",
    "    \n",
    "    for freq_type, description in output_dirs.items():\n",
    "        output_path = dataset_dir / \"output\" / freq_type\n",
    "        if not output_path.exists():\n",
    "            continue\n",
    "            \n",
    "        files = list(output_path.glob('*.nc'))\n",
    "        \n",
    "        console.print(f\"\\n[cyan]{freq_type.upper()}:[/cyan] {description}\")\n",
    "        console.print(f\"  Files: {len(files)}\")\n",
    "        \n",
    "        # Extract datetime patterns from filenames\n",
    "        if freq_type == 'monthly':\n",
    "            # Extract YYYY-MM patterns\n",
    "            years_months = set()\n",
    "            for file in files:\n",
    "                parts = file.stem.split('.')\n",
    "                for part in parts:\n",
    "                    if '-' in part and len(part) == 7:  # YYYY-MM format\n",
    "                        years_months.add(part)\n",
    "            \n",
    "            years = sorted(set(ym.split('-')[0] for ym in years_months))\n",
    "            console.print(f\"  Years: {', '.join(years)}\")\n",
    "            console.print(f\"  Pattern: YYYY-MM (e.g., {sorted(years_months)[0]})\")\n",
    "            \n",
    "            # Sample filenames\n",
    "            sample_files = sorted([f.name for f in files])[:3]\n",
    "            console.print(f\"  Sample files: {', '.join(sample_files)}\")\n",
    "            \n",
    "            datetime_patterns.append({\n",
    "                'frequency': freq_type,\n",
    "                'pattern': 'YYYY-MM',\n",
    "                'examples': list(years_months)[:5],\n",
    "                'extraction_use': 'Monthly analysis, seasonal studies'\n",
    "            })\n",
    "            \n",
    "        elif freq_type == 'seasonal':\n",
    "            # Extract SEASON.YYYY patterns\n",
    "            season_years = set()\n",
    "            for file in files:\n",
    "                parts = file.stem.split('.')\n",
    "                if len(parts) >= 3:  # cesm.SEASON.YYYY\n",
    "                    season_year = f\"{parts[1]}.{parts[2]}\"\n",
    "                    season_years.add(season_year)\n",
    "            \n",
    "            console.print(f\"  Seasons: DJF, MAM, JJA, SON\")\n",
    "            console.print(f\"  Pattern: SEASON.YYYY (e.g., {sorted(season_years)[0]})\")\n",
    "            \n",
    "            datetime_patterns.append({\n",
    "                'frequency': freq_type,\n",
    "                'pattern': 'SEASON.YYYY',\n",
    "                'examples': list(season_years)[:5],\n",
    "                'extraction_use': 'Climate normals, seasonal comparisons'\n",
    "            })\n",
    "            \n",
    "        elif freq_type == 'annual':\n",
    "            # Extract YYYY patterns\n",
    "            years = set()\n",
    "            for file in files:\n",
    "                parts = file.stem.split('.')\n",
    "                for part in parts:\n",
    "                    if part.isdigit() and len(part) == 4:  # YYYY format\n",
    "                        years.add(part)\n",
    "            \n",
    "            console.print(f\"  Years: {', '.join(sorted(years))}\")\n",
    "            console.print(f\"  Pattern: YYYY (e.g., {sorted(years)[0]})\")\n",
    "            \n",
    "            datetime_patterns.append({\n",
    "                'frequency': freq_type,\n",
    "                'pattern': 'YYYY',\n",
    "                'examples': list(years),\n",
    "                'extraction_use': 'Long-term trends, decadal analysis'\n",
    "            })\n",
    "            \n",
    "        elif freq_type == 'events':\n",
    "            # Event-based datetime patterns\n",
    "            console.print(f\"  Event-based files with embedded dates:\")\n",
    "            for file in files:\n",
    "                console.print(f\"    ‚Ä¢ {file.name}\")\n",
    "            \n",
    "            datetime_patterns.append({\n",
    "                'frequency': freq_type,\n",
    "                'pattern': 'YYYY_MM_DD_to_YYYY_MM_DD',\n",
    "                'examples': [f.stem for f in files][:3],\n",
    "                'extraction_use': 'Specific events, case studies'\n",
    "            })\n",
    "    \n",
    "    # Analyze restart files\n",
    "    restart_path = dataset_dir / \"restart\"\n",
    "    if restart_path.exists():\n",
    "        restart_files = list(restart_path.glob('*.nc'))\n",
    "        console.print(f\"\\n[cyan]RESTART:[/cyan] Simulation continuation files\")\n",
    "        console.print(f\"  Files: {len(restart_files)}\")\n",
    "        console.print(f\"  Pattern: YYYY-MM-DD-HHMMSS\")\n",
    "        \n",
    "        sample_restart = restart_files[0].name if restart_files else \"cam.r.2021-01-01-00000.nc\"\n",
    "        console.print(f\"  Example: {sample_restart}\")\n",
    "    \n",
    "    return datetime_patterns\n",
    "\n",
    "# Analyze the temporal patterns\n",
    "patterns = analyze_temporal_patterns(dataset_dir)\n",
    "\n",
    "# Create summary table\n",
    "patterns_table = Table(title=\"DateTime Patterns in Dataset\")\n",
    "patterns_table.add_column(\"Frequency\", style=\"cyan\")\n",
    "patterns_table.add_column(\"Pattern\", style=\"yellow\")\n",
    "patterns_table.add_column(\"Example\", style=\"green\")\n",
    "patterns_table.add_column(\"Best For\", style=\"dim\")\n",
    "\n",
    "for pattern in patterns:\n",
    "    example = pattern['examples'][0] if pattern['examples'] else 'N/A'\n",
    "    patterns_table.add_row(\n",
    "        pattern['frequency'].title(),\n",
    "        pattern['pattern'],\n",
    "        example,\n",
    "        pattern['extraction_use']\n",
    "    )\n",
    "\n",
    "console.print(f\"\\n{patterns_table}\")\n",
    "\n",
    "# Show total file counts\n",
    "total_files = len(list(dataset_dir.rglob('*.nc')))\n",
    "total_size = sum(f.stat().st_size for f in dataset_dir.rglob('*.nc')) / (1024 * 1024)\n",
    "\n",
    "console.print(f\"\\n[bold green]üìà Dataset Summary[/bold green]\")\n",
    "console.print(f\"Total NetCDF files: {total_files}\")\n",
    "console.print(f\"Total size: {total_size:.1f} MB\")\n",
    "console.print(f\"Time span: 2020-2024 (5 years)\")\n",
    "console.print(f\"Frequencies: Monthly, daily, seasonal, annual, event-based\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Basic DateTime Extraction\n",
    "\n",
    "Let's start with simple datetime filtering - extracting specific years, months, or date ranges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tarfile\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil import parser\n",
    "\n",
    "def extract_datetime_from_filename(filename):\n",
    "    \"\"\"\n",
    "    Extract datetime information from climate model filenames.\n",
    "    This is similar to what Tellus does internally.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Common datetime patterns in Earth Science filenames\n",
    "    patterns = [\n",
    "        # Monthly: YYYY-MM\n",
    "        (r'(\\d{4})-(\\d{2})', lambda m: {'year': int(m.group(1)), 'month': int(m.group(2)), 'type': 'monthly'}),\n",
    "        \n",
    "        # Seasonal: SEASON.YYYY\n",
    "        (r'(DJF|MAM|JJA|SON)\\.(\\d{4})', lambda m: {\n",
    "            'year': int(m.group(2)), \n",
    "            'season': m.group(1), \n",
    "            'type': 'seasonal'\n",
    "        }),\n",
    "        \n",
    "        # Annual: YYYY\n",
    "        (r'(?:^|[^\\d])(\\d{4})(?:[^\\d]|$)', lambda m: {'year': int(m.group(1)), 'type': 'annual'}),\n",
    "        \n",
    "        # Restart: YYYY-MM-DD-HHMMSS\n",
    "        (r'(\\d{4})-(\\d{2})-(\\d{2})-(\\d{5})', lambda m: {\n",
    "            'year': int(m.group(1)), \n",
    "            'month': int(m.group(2)), \n",
    "            'day': int(m.group(3)),\n",
    "            'type': 'restart'\n",
    "        }),\n",
    "        \n",
    "        # Event dates: YYYY_MM_DD\n",
    "        (r'(\\d{4})_(\\d{2})_(\\d{2})', lambda m: {\n",
    "            'year': int(m.group(1)), \n",
    "            'month': int(m.group(2)), \n",
    "            'day': int(m.group(3)),\n",
    "            'type': 'event'\n",
    "        })\n",
    "    ]\n",
    "    \n",
    "    for pattern, extractor in patterns:\n",
    "        match = re.search(pattern, filename)\n",
    "        if match:\n",
    "            return extractor(match)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def filter_files_by_datetime(files, filter_criteria):\n",
    "    \"\"\"\n",
    "    Filter files based on datetime criteria.\n",
    "    This demonstrates the core concept behind Tellus datetime filtering.\n",
    "    \"\"\"\n",
    "    \n",
    "    filtered_files = []\n",
    "    \n",
    "    for file_path in files:\n",
    "        filename = file_path.name\n",
    "        dt_info = extract_datetime_from_filename(filename)\n",
    "        \n",
    "        if not dt_info:\n",
    "            continue\n",
    "            \n",
    "        # Apply filters\n",
    "        match = True\n",
    "        \n",
    "        # Year filter\n",
    "        if 'year' in filter_criteria:\n",
    "            target_year = filter_criteria['year']\n",
    "            if isinstance(target_year, list):\n",
    "                if dt_info.get('year') not in target_year:\n",
    "                    match = False\n",
    "            else:\n",
    "                if dt_info.get('year') != target_year:\n",
    "                    match = False\n",
    "        \n",
    "        # Year range filter\n",
    "        if 'year_range' in filter_criteria:\n",
    "            start_year, end_year = filter_criteria['year_range']\n",
    "            if not (start_year <= dt_info.get('year', 0) <= end_year):\n",
    "                match = False\n",
    "        \n",
    "        # Month filter\n",
    "        if 'month' in filter_criteria:\n",
    "            target_month = filter_criteria['month']\n",
    "            if isinstance(target_month, list):\n",
    "                if dt_info.get('month') not in target_month:\n",
    "                    match = False\n",
    "            else:\n",
    "                if dt_info.get('month') != target_month:\n",
    "                    match = False\n",
    "        \n",
    "        # Season filter\n",
    "        if 'season' in filter_criteria:\n",
    "            target_season = filter_criteria['season']\n",
    "            if isinstance(target_season, list):\n",
    "                if dt_info.get('season') not in target_season:\n",
    "                    match = False\n",
    "            else:\n",
    "                if dt_info.get('season') != target_season:\n",
    "                    match = False\n",
    "        \n",
    "        # Type filter\n",
    "        if 'temporal_type' in filter_criteria:\n",
    "            target_type = filter_criteria['temporal_type']\n",
    "            if isinstance(target_type, list):\n",
    "                if dt_info.get('type') not in target_type:\n",
    "                    match = False\n",
    "            else:\n",
    "                if dt_info.get('type') != target_type:\n",
    "                    match = False\n",
    "        \n",
    "        if match:\n",
    "            filtered_files.append({\n",
    "                'file': file_path,\n",
    "                'datetime_info': dt_info,\n",
    "                'filename': filename\n",
    "            })\n",
    "    \n",
    "    return filtered_files\n",
    "\n",
    "# Get all NetCDF files in our dataset\n",
    "all_files = list(dataset_dir.rglob('*.nc'))\n",
    "console.print(f\"\\n[bold blue]üîç Basic DateTime Extraction Examples[/bold blue]\")\n",
    "console.print(f\"Working with {len(all_files)} total files\")\n",
    "console.print(\"=\" * 50)\n",
    "\n",
    "# Example 1: Extract specific year\n",
    "console.print(\"\\n[cyan]Example 1: Extract all files from 2023[/cyan]\")\n",
    "year_2023_files = filter_files_by_datetime(all_files, {'year': 2023})\n",
    "\n",
    "console.print(f\"Found {len(year_2023_files)} files from 2023:\")\n",
    "for item in year_2023_files[:5]:  # Show first 5\n",
    "    dt_info = item['datetime_info']\n",
    "    console.print(f\"  ‚Ä¢ {item['filename']} (Type: {dt_info['type']}, Year: {dt_info['year']})\")\n",
    "if len(year_2023_files) > 5:\n",
    "    console.print(f\"  ... and {len(year_2023_files) - 5} more files\")\n",
    "\n",
    "# Example 2: Extract summer months across all years\n",
    "console.print(\"\\n[cyan]Example 2: Extract summer months (JJA) across all years[/cyan]\")\n",
    "summer_files = filter_files_by_datetime(all_files, {'month': [6, 7, 8]})\n",
    "\n",
    "console.print(f\"Found {len(summer_files)} summer files:\")\n",
    "# Group by year\n",
    "summer_by_year = {}\n",
    "for item in summer_files:\n",
    "    year = item['datetime_info']['year']\n",
    "    if year not in summer_by_year:\n",
    "        summer_by_year[year] = []\n",
    "    summer_by_year[year].append(item)\n",
    "\n",
    "for year in sorted(summer_by_year.keys()):\n",
    "    console.print(f\"  {year}: {len(summer_by_year[year])} files\")\n",
    "\n",
    "# Example 3: Extract specific date range\n",
    "console.print(\"\\n[cyan]Example 3: Extract files from 2021-2023 period[/cyan]\")\n",
    "range_2021_2023 = filter_files_by_datetime(all_files, {'year_range': (2021, 2023)})\n",
    "\n",
    "console.print(f\"Found {len(range_2021_2023)} files from 2021-2023:\")\n",
    "# Count by type\n",
    "by_type = {}\n",
    "for item in range_2021_2023:\n",
    "    dt_type = item['datetime_info']['type']\n",
    "    by_type[dt_type] = by_type.get(dt_type, 0) + 1\n",
    "\n",
    "for dt_type, count in by_type.items():\n",
    "    console.print(f\"  {dt_type.title()}: {count} files\")\n",
    "\n",
    "# Example 4: Extract seasonal data\n",
    "console.print(\"\\n[cyan]Example 4: Extract winter season (DJF) data[/cyan]\")\n",
    "winter_files = filter_files_by_datetime(all_files, {'season': 'DJF'})\n",
    "\n",
    "console.print(f\"Found {len(winter_files)} winter season files:\")\n",
    "for item in winter_files:\n",
    "    dt_info = item['datetime_info']\n",
    "    console.print(f\"  ‚Ä¢ {item['filename']} (Year: {dt_info['year']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Advanced DateTime Filtering Scenarios\n",
    "\n",
    "Now let's explore more complex scenarios that climate scientists commonly face:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_datetime_filtering_scenarios():\n",
    "    \"\"\"\n",
    "    Demonstrate advanced datetime filtering scenarios common in climate science.\n",
    "    \"\"\"\n",
    "    \n",
    "    console.print(\"\\n[bold blue]üéØ Advanced DateTime Filtering Scenarios[/bold blue]\")\n",
    "    console.print(\"=\" * 60)\n",
    "    \n",
    "    # Scenario 1: El Ni√±o/La Ni√±a Analysis\n",
    "    console.print(\"\\n[bold green]Scenario 1: El Ni√±o Period Analysis (2023-2024)[/bold green]\")\n",
    "    console.print(\"[dim]Goal: Extract data during known El Ni√±o conditions for impact analysis[/dim]\")\n",
    "    \n",
    "    # El Ni√±o typically peaks in winter (DJF)\n",
    "    el_nino_criteria = {\n",
    "        'year_range': (2023, 2024),\n",
    "        'temporal_type': ['monthly', 'seasonal', 'event']\n",
    "    }\n",
    "    \n",
    "    el_nino_files = filter_files_by_datetime(all_files, el_nino_criteria)\n",
    "    console.print(f\"Found {len(el_nino_files)} files for El Ni√±o period analysis\")\n",
    "    \n",
    "    # Show breakdown by type and year\n",
    "    el_nino_breakdown = {}\n",
    "    for item in el_nino_files:\n",
    "        year = item['datetime_info']['year']\n",
    "        dt_type = item['datetime_info']['type']\n",
    "        key = f\"{year}-{dt_type}\"\n",
    "        el_nino_breakdown[key] = el_nino_breakdown.get(key, 0) + 1\n",
    "    \n",
    "    for key in sorted(el_nino_breakdown.keys()):\n",
    "        console.print(f\"  {key}: {el_nino_breakdown[key]} files\")\n",
    "    \n",
    "    # Real-world Tellus command\n",
    "    console.print(\"[blue]Equivalent Tellus command:[/blue]\")\n",
    "    console.print(\"[dim]tellus archive extract climate_simulation \\\\\")\n",
    "    console.print(\"[dim]  --date-range '2023-01-01:2024-12-31' \\\\\")\n",
    "    console.print(\"[dim]  --content-types output,diagnostic \\\\\")\n",
    "    console.print(\"[dim]  --location analysis_workspace[/dim]\")\n",
    "    \n",
    "    # Scenario 2: Seasonal Comparison Study\n",
    "    console.print(\"\\n[bold green]Scenario 2: Multi-Year Summer Comparison (2020 vs 2023)[/bold green]\")\n",
    "    console.print(\"[dim]Goal: Compare two specific summers for heat wave analysis[/dim]\")\n",
    "    \n",
    "    summer_comparison_files = []\n",
    "    \n",
    "    # Summer 2020 (pre-climate change baseline)\n",
    "    summer_2020 = filter_files_by_datetime(all_files, {'year': 2020, 'month': [6, 7, 8]})\n",
    "    summer_2023 = filter_files_by_datetime(all_files, {'year': 2023, 'month': [6, 7, 8]})\n",
    "    \n",
    "    console.print(f\"Summer 2020 files: {len(summer_2020)}\")\n",
    "    console.print(f\"Summer 2023 files: {len(summer_2023)}\")\n",
    "    console.print(f\"Total for comparison: {len(summer_2020) + len(summer_2023)}\")\n",
    "    \n",
    "    console.print(\"[blue]Equivalent Tellus command:[/blue]\")\n",
    "    console.print(\"[dim]tellus archive extract climate_simulation \\\\\")\n",
    "    console.print(\"[dim]  --date-pattern '%Y-%m' \\\\\")\n",
    "    console.print(\"[dim]  --date-list '2020-06,2020-07,2020-08,2023-06,2023-07,2023-08' \\\\\")\n",
    "    console.print(\"[dim]  --patterns '*cam.h*' \\\\\")\n",
    "    console.print(\"[dim]  --location comparison_workspace[/dim]\")\n",
    "    \n",
    "    # Scenario 3: Event-Based Extraction\n",
    "    console.print(\"\\n[bold green]Scenario 3: Extreme Event Analysis[/bold green]\")\n",
    "    console.print(\"[dim]Goal: Extract files related to specific extreme weather events[/dim]\")\n",
    "    \n",
    "    # Look for event files (these have specific date ranges in names)\n",
    "    event_files = filter_files_by_datetime(all_files, {'temporal_type': 'event'})\n",
    "    \n",
    "    console.print(f\"Found {len(event_files)} event-based files:\")\n",
    "    for item in event_files:\n",
    "        filename = item['filename']\n",
    "        # Extract event type from filename\n",
    "        if 'heatwave' in filename:\n",
    "            event_type = \"üî• Heat Wave\"\n",
    "        elif 'hurricane' in filename:\n",
    "            event_type = \"üåÄ Hurricane Season\"\n",
    "        elif 'drought' in filename:\n",
    "            event_type = \"üèúÔ∏è Drought Event\"\n",
    "        elif 'blocking' in filename:\n",
    "            event_type = \"üå™Ô∏è Atmospheric Blocking\"\n",
    "        else:\n",
    "            event_type = \"üìä Climate Event\"\n",
    "        \n",
    "        console.print(f\"  {event_type}: {filename}\")\n",
    "    \n",
    "    console.print(\"[blue]Equivalent Tellus command:[/blue]\")\n",
    "    console.print(\"[dim]tellus archive extract climate_simulation \\\\\")\n",
    "    console.print(\"[dim]  --patterns '*heatwave*,*hurricane*,*drought*' \\\\\")\n",
    "    console.print(\"[dim]  --content-types diagnostic,output \\\\\")\n",
    "    console.print(\"[dim]  --location events_analysis[/dim]\")\n",
    "    \n",
    "    # Scenario 4: Multi-Frequency Temporal Analysis\n",
    "    console.print(\"\\n[bold green]Scenario 4: Multi-Frequency Analysis (Q1 2022)[/bold green]\")\n",
    "    console.print(\"[dim]Goal: Get all temporal frequencies for first quarter analysis[/dim]\")\n",
    "    \n",
    "    q1_2022_criteria = {\n",
    "        'year': 2022,\n",
    "        'month': [1, 2, 3]  # Q1 months\n",
    "    }\n",
    "    \n",
    "    q1_files = filter_files_by_datetime(all_files, q1_2022_criteria)\n",
    "    \n",
    "    # Also get winter seasonal data (DJF includes Jan-Feb)\n",
    "    winter_2022 = filter_files_by_datetime(all_files, {'year': 2022, 'season': 'DJF'})\n",
    "    \n",
    "    # And spring seasonal data (MAM includes March)\n",
    "    spring_2022 = filter_files_by_datetime(all_files, {'year': 2022, 'season': 'MAM'})\n",
    "    \n",
    "    console.print(f\"Q1 2022 monthly files: {len(q1_files)}\")\n",
    "    console.print(f\"Winter 2022 seasonal: {len(winter_2022)}\")\n",
    "    console.print(f\"Spring 2022 seasonal: {len(spring_2022)}\")\n",
    "    \n",
    "    # Show file types for Q1\n",
    "    q1_by_component = {}\n",
    "    for item in q1_files:\n",
    "        filename = item['filename']\n",
    "        component = filename.split('.')[0]  # cam, clm, pop, etc.\n",
    "        q1_by_component[component] = q1_by_component.get(component, 0) + 1\n",
    "    \n",
    "    console.print(\"Q1 2022 files by component:\")\n",
    "    for component, count in q1_by_component.items():\n",
    "        console.print(f\"  {component.upper()}: {count} files\")\n",
    "    \n",
    "    console.print(\"[blue]Equivalent Tellus command:[/blue]\")\n",
    "    console.print(\"[dim]tellus archive extract climate_simulation \\\\\")\n",
    "    console.print(\"[dim]  --date-pattern '%Y-%m' \\\\\")\n",
    "    console.print(\"[dim]  --date-range '2022-01:2022-03' \\\\\")\n",
    "    console.print(\"[dim]  --include-seasonal \\\\\")\n",
    "    console.print(\"[dim]  --location q1_2022_analysis[/dim]\")\n",
    "    \n",
    "    return {\n",
    "        'el_nino_files': len(el_nino_files),\n",
    "        'summer_comparison': len(summer_2020) + len(summer_2023),\n",
    "        'event_files': len(event_files),\n",
    "        'q1_files': len(q1_files) + len(winter_2022) + len(spring_2022)\n",
    "    }\n",
    "\n",
    "# Run advanced scenarios\n",
    "scenario_results = advanced_datetime_filtering_scenarios()\n",
    "\n",
    "# Summary of extraction efficiency\n",
    "console.print(\"\\n[bold blue]üìä Extraction Efficiency Summary[/bold blue]\")\n",
    "total_files = len(all_files)\n",
    "\n",
    "efficiency_table = Table(title=\"DateTime Filtering Efficiency\")\n",
    "efficiency_table.add_column(\"Scenario\", style=\"cyan\")\n",
    "efficiency_table.add_column(\"Files Selected\", justify=\"right\", style=\"green\")\n",
    "efficiency_table.add_column(\"% of Total\", justify=\"right\", style=\"yellow\")\n",
    "efficiency_table.add_column(\"Data Reduction\", justify=\"right\", style=\"magenta\")\n",
    "\n",
    "for scenario, count in scenario_results.items():\n",
    "    percentage = (count / total_files) * 100\n",
    "    reduction = 100 - percentage\n",
    "    \n",
    "    efficiency_table.add_row(\n",
    "        scenario.replace('_', ' ').title(),\n",
    "        str(count),\n",
    "        f\"{percentage:.1f}%\",\n",
    "        f\"{reduction:.1f}%\"\n",
    "    )\n",
    "\n",
    "console.print(efficiency_table)\n",
    "\n",
    "console.print(f\"\\n[green]üíæ Storage Savings:[/green] Instead of downloading {total_files} files, \")\n",
    "console.print(f\"you can target specific periods and reduce data transfer by 60-95%!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Creating DateTime-Filtered Archives\n",
    "\n",
    "Now let's create actual archives with datetime filtering applied:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datetime_filtered_archive(source_dir, archive_path, datetime_filter, description):\n",
    "    \"\"\"\n",
    "    Create an archive with datetime filtering applied.\n",
    "    This demonstrates selective temporal archiving.\n",
    "    \"\"\"\n",
    "    \n",
    "    all_files = list(source_dir.rglob('*.nc'))\n",
    "    filtered_files = filter_files_by_datetime(all_files, datetime_filter)\n",
    "    \n",
    "    console.print(f\"[blue]Creating datetime-filtered archive: {archive_path.name}[/blue]\")\n",
    "    console.print(f\"[dim]{description}[/dim]\")\n",
    "    console.print(f\"[dim]Filter criteria: {datetime_filter}[/dim]\")\n",
    "    \n",
    "    # Create the archive\n",
    "    archived_files = []\n",
    "    \n",
    "    with tarfile.open(archive_path, \"w:gz\") as tar:\n",
    "        for item in filtered_files:\n",
    "            file_path = item['file']\n",
    "            rel_path = file_path.relative_to(source_dir)\n",
    "            \n",
    "            tar.add(file_path, arcname=rel_path)\n",
    "            \n",
    "            archived_files.append({\n",
    "                'path': str(rel_path),\n",
    "                'size': file_path.stat().st_size,\n",
    "                'datetime_info': item['datetime_info']\n",
    "            })\n",
    "    \n",
    "    # Create metadata\n",
    "    archive_metadata = {\n",
    "        'metadata_version': '1.0',\n",
    "        'created_at': datetime.now().isoformat(),\n",
    "        'description': description,\n",
    "        'datetime_filter': datetime_filter,\n",
    "        'temporal_selection': {\n",
    "            'total_files_considered': len(all_files),\n",
    "            'files_selected': len(filtered_files),\n",
    "            'selection_ratio': len(filtered_files) / len(all_files),\n",
    "            'temporal_coverage': extract_temporal_coverage(filtered_files)\n",
    "        },\n",
    "        'archive_contents': archived_files\n",
    "    }\n",
    "    \n",
    "    metadata_path = archive_path.with_suffix('.metadata.json')\n",
    "    metadata_path.write_text(json.dumps(archive_metadata, indent=2))\n",
    "    \n",
    "    return archive_path, metadata_path, len(filtered_files)\n",
    "\n",
    "def extract_temporal_coverage(filtered_files):\n",
    "    \"\"\"\n",
    "    Extract temporal coverage information from filtered files.\n",
    "    \"\"\"\n",
    "    \n",
    "    years = set()\n",
    "    months = set()\n",
    "    seasons = set()\n",
    "    types = set()\n",
    "    \n",
    "    for item in filtered_files:\n",
    "        dt_info = item['datetime_info']\n",
    "        \n",
    "        if 'year' in dt_info:\n",
    "            years.add(dt_info['year'])\n",
    "        if 'month' in dt_info:\n",
    "            months.add(dt_info['month'])\n",
    "        if 'season' in dt_info:\n",
    "            seasons.add(dt_info['season'])\n",
    "        if 'type' in dt_info:\n",
    "            types.add(dt_info['type'])\n",
    "    \n",
    "    return {\n",
    "        'years': sorted(list(years)),\n",
    "        'months': sorted(list(months)),\n",
    "        'seasons': sorted(list(seasons)),\n",
    "        'temporal_types': sorted(list(types))\n",
    "    }\n",
    "\n",
    "# Create directory for datetime-filtered archives\n",
    "datetime_archives_dir = tutorial_dir / \"datetime_filtered_archives\"\n",
    "datetime_archives_dir.mkdir(exist_ok=True)\n",
    "\n",
    "console.print(\"\\n[bold blue]üì¶ Creating DateTime-Filtered Archives[/bold blue]\")\n",
    "console.print(\"=\" * 55)\n",
    "\n",
    "# Archive 1: Heat Wave Summer 2023\n",
    "console.print(\"\\n[cyan]1. Creating 'Heat Wave Summer 2023' archive...[/cyan]\")\n",
    "heatwave_archive, heatwave_metadata, heatwave_count = create_datetime_filtered_archive(\n",
    "    dataset_dir,\n",
    "    datetime_archives_dir / \"heatwave_summer_2023.tar.gz\",\n",
    "    {'year': 2023, 'month': [6, 7, 8]},\n",
    "    \"Summer 2023 heat wave analysis - monthly and daily data\"\n",
    ")\n",
    "\n",
    "console.print(f\"  ‚úÖ Created: {heatwave_archive.name}\")\n",
    "console.print(f\"  üìä Files: {heatwave_count}\")\n",
    "console.print(f\"  üíæ Size: {heatwave_archive.stat().st_size / (1024*1024):.1f} MB\")\n",
    "\n",
    "# Archive 2: El Ni√±o Period 2023-2024  \n",
    "console.print(\"\\n[cyan]2. Creating 'El Ni√±o Period 2023-2024' archive...[/cyan]\")\n",
    "elnino_archive, elnino_metadata, elnino_count = create_datetime_filtered_archive(\n",
    "    dataset_dir,\n",
    "    datetime_archives_dir / \"el_nino_2023_2024.tar.gz\",\n",
    "    {'year_range': (2023, 2024)},\n",
    "    \"Complete El Ni√±o period analysis - all frequencies and components\"\n",
    ")\n",
    "\n",
    "console.print(f\"  ‚úÖ Created: {elnino_archive.name}\")\n",
    "console.print(f\"  üìä Files: {elnino_count}\")\n",
    "console.print(f\"  üíæ Size: {elnino_archive.stat().st_size / (1024*1024):.1f} MB\")\n",
    "\n",
    "# Archive 3: Winter Seasons Only\n",
    "console.print(\"\\n[cyan]3. Creating 'Winter Seasons Multi-Year' archive...[/cyan]\")\n",
    "winter_archive, winter_metadata, winter_count = create_datetime_filtered_archive(\n",
    "    dataset_dir,\n",
    "    datetime_archives_dir / \"winter_seasons_multiyear.tar.gz\",\n",
    "    {'season': 'DJF'},\n",
    "    \"All winter seasons (DJF) for multi-year winter climate analysis\"\n",
    ")\n",
    "\n",
    "console.print(f\"  ‚úÖ Created: {winter_archive.name}\")\n",
    "console.print(f\"  üìä Files: {winter_count}\")\n",
    "console.print(f\"  üíæ Size: {winter_archive.stat().st_size / (1024*1024):.1f} MB\")\n",
    "\n",
    "# Archive 4: Restart Files for Specific Years\n",
    "console.print(\"\\n[cyan]4. Creating 'Restart Files 2022-2024' archive...[/cyan]\")\n",
    "restart_archive, restart_metadata, restart_count = create_datetime_filtered_archive(\n",
    "    dataset_dir,\n",
    "    datetime_archives_dir / \"restart_files_2022_2024.tar.gz\",\n",
    "    {'year_range': (2022, 2024), 'temporal_type': 'restart'},\n",
    "    \"Restart files for recent years - simulation continuation capability\"\n",
    ")\n",
    "\n",
    "console.print(f\"  ‚úÖ Created: {restart_archive.name}\")\n",
    "console.print(f\"  üìä Files: {restart_count}\")\n",
    "console.print(f\"  üíæ Size: {restart_archive.stat().st_size / (1024*1024):.1f} MB\")\n",
    "\n",
    "# Archive 5: Climate Events Collection\n",
    "console.print(\"\\n[cyan]5. Creating 'Climate Events Collection' archive...[/cyan]\")\n",
    "events_archive, events_metadata, events_count = create_datetime_filtered_archive(\n",
    "    dataset_dir,\n",
    "    datetime_archives_dir / \"climate_events_collection.tar.gz\",\n",
    "    {'temporal_type': 'event'},\n",
    "    \"Collection of specific climate events with irregular timing\"\n",
    ")\n",
    "\n",
    "console.print(f\"  ‚úÖ Created: {events_archive.name}\")\n",
    "console.print(f\"  üìä Files: {events_count}\")\n",
    "console.print(f\"  üíæ Size: {events_archive.stat().st_size / (1024*1024):.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Comparing Archive Strategies\n",
    "\n",
    "Let's compare our datetime-filtered archives with traditional approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare datetime filtering strategies\n",
    "console.print(\"\\n[bold blue]üìä DateTime Archive Strategy Comparison[/bold blue]\")\n",
    "console.print(\"=\" * 60)\n",
    "\n",
    "# Calculate original dataset statistics\n",
    "original_files = len(list(dataset_dir.rglob('*.nc')))\n",
    "original_size = sum(f.stat().st_size for f in dataset_dir.rglob('*.nc')) / (1024*1024)\n",
    "\n",
    "# Archive comparison data\n",
    "archive_comparisons = [\n",
    "    {\n",
    "        'name': 'Complete Dataset',\n",
    "        'files': original_files,\n",
    "        'size_mb': original_size,\n",
    "        'purpose': 'Everything (baseline)',\n",
    "        'use_case': 'Full backup, comprehensive analysis',\n",
    "        'efficiency': '0%'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Heat Wave Summer 2023',\n",
    "        'files': heatwave_count,\n",
    "        'size_mb': heatwave_archive.stat().st_size / (1024*1024),\n",
    "        'purpose': 'Seasonal extreme analysis',\n",
    "        'use_case': 'Heat wave research, summer climate',\n",
    "        'efficiency': f\"{(1 - heatwave_count/original_files)*100:.1f}%\"\n",
    "    },\n",
    "    {\n",
    "        'name': 'El Ni√±o 2023-2024',\n",
    "        'files': elnino_count,\n",
    "        'size_mb': elnino_archive.stat().st_size / (1024*1024),\n",
    "        'purpose': 'Multi-year phenomenon study',\n",
    "        'use_case': 'ENSO research, teleconnections',\n",
    "        'efficiency': f\"{(1 - elnino_count/original_files)*100:.1f}%\"\n",
    "    },\n",
    "    {\n",
    "        'name': 'Winter Seasons Multi-Year',\n",
    "        'files': winter_count,\n",
    "        'size_mb': winter_archive.stat().st_size / (1024*1024),\n",
    "        'purpose': 'Seasonal climatology',\n",
    "        'use_case': 'Winter climate patterns, snow studies',\n",
    "        'efficiency': f\"{(1 - winter_count/original_files)*100:.1f}%\"\n",
    "    },\n",
    "    {\n",
    "        'name': 'Restart Files 2022-2024',\n",
    "        'files': restart_count,\n",
    "        'size_mb': restart_archive.stat().st_size / (1024*1024),\n",
    "        'purpose': 'Simulation continuation',\n",
    "        'use_case': 'Model restarts, experiment extension',\n",
    "        'efficiency': f\"{(1 - restart_count/original_files)*100:.1f}%\"\n",
    "    },\n",
    "    {\n",
    "        'name': 'Climate Events',\n",
    "        'files': events_count,\n",
    "        'size_mb': events_archive.stat().st_size / (1024*1024),\n",
    "        'purpose': 'Extreme event catalog',\n",
    "        'use_case': 'Case studies, event attribution',\n",
    "        'efficiency': f\"{(1 - events_count/original_files)*100:.1f}%\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create comparison table\n",
    "comparison_table = Table(title=\"DateTime Archive Strategy Comparison\")\n",
    "comparison_table.add_column(\"Archive Strategy\", style=\"cyan\")\n",
    "comparison_table.add_column(\"Files\", justify=\"right\", style=\"green\")\n",
    "comparison_table.add_column(\"Size (MB)\", justify=\"right\", style=\"yellow\")\n",
    "comparison_table.add_column(\"Data Reduction\", justify=\"right\", style=\"magenta\")\n",
    "comparison_table.add_column(\"Best Use Case\", style=\"dim\")\n",
    "\n",
    "for archive in archive_comparisons:\n",
    "    comparison_table.add_row(\n",
    "        archive['name'],\n",
    "        str(archive['files']),\n",
    "        f\"{archive['size_mb']:.1f}\",\n",
    "        archive['efficiency'],\n",
    "        archive['use_case']\n",
    "    )\n",
    "\n",
    "console.print(comparison_table)\n",
    "\n",
    "# Show storage and transfer efficiency\n",
    "console.print(\"\\n[bold green]üíæ Storage & Transfer Efficiency[/bold green]\")\n",
    "\n",
    "most_efficient = min(archive_comparisons[1:], key=lambda x: x['files'])\n",
    "least_efficient = max(archive_comparisons[1:], key=lambda x: x['files'])\n",
    "\n",
    "console.print(f\"Most targeted: {most_efficient['name']} ({most_efficient['files']} files, {most_efficient['size_mb']:.1f} MB)\")\n",
    "console.print(f\"Most comprehensive: {least_efficient['name']} ({least_efficient['files']} files, {least_efficient['size_mb']:.1f} MB)\")\n",
    "\n",
    "total_selective_size = sum(a['size_mb'] for a in archive_comparisons[1:])\n",
    "console.print(f\"\\nAll selective archives combined: {total_selective_size:.1f} MB\")\n",
    "console.print(f\"vs. Complete dataset: {original_size:.1f} MB\")\n",
    "\n",
    "if total_selective_size < original_size:\n",
    "    savings = ((original_size - total_selective_size) / original_size) * 100\n",
    "    console.print(f\"[green]Net storage savings: {savings:.1f}%[/green]\")\n",
    "else:\n",
    "    console.print(f\"[yellow]Multiple selective archives use more space than original (expected for comprehensive coverage)[/yellow]\")\n",
    "\n",
    "# Real-world impact scenarios\n",
    "impact_scenarios = Panel(\n",
    "    \"[bold green]Real-World Impact Scenarios:[/bold green]\\n\\n\"\n",
    "    \"[cyan]üöÄ HPC Transfer:[/cyan] Heat wave archive (5 MB) vs full dataset (100+ MB) - 95% less transfer time\\n\"\n",
    "    \"[cyan]‚òÅÔ∏è Cloud Storage:[/cyan] Event-based archives reduce cloud costs by storing only relevant data\\n\"\n",
    "    \"[cyan]ü§ù Collaboration:[/cyan] Send El Ni√±o data (40 MB) instead of 5-year dataset (100+ MB)\\n\"\n",
    "    \"[cyan]üì± Bandwidth Limited:[/cyan] Download winter seasons only for Arctic research\\n\"\n",
    "    \"[cyan]üíø Archival Media:[/cyan] Separate critical restarts from analysis data for different storage tiers\",\n",
    "    title=\"üåç Climate Science Use Cases\",\n",
    "    border_style=\"green\"\n",
    ")\n",
    "\n",
    "console.print(f\"\\n{impact_scenarios}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Examining Archive Contents\n",
    "\n",
    "Let's examine the contents of our datetime-filtered archives to understand what was selected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_datetime_archive_contents(metadata_path):\n",
    "    \"\"\"\n",
    "    Examine the contents of a datetime-filtered archive.\n",
    "    \"\"\"\n",
    "    \n",
    "    metadata = json.loads(metadata_path.read_text())\n",
    "    \n",
    "    archive_name = metadata_path.stem.replace('.metadata', '')\n",
    "    console.print(f\"\\n[bold cyan]üìã Archive: {archive_name.replace('_', ' ').title()}[/bold cyan]\")\n",
    "    console.print(\"=\" * 55)\n",
    "    \n",
    "    # Description and filter criteria\n",
    "    console.print(f\"[blue]Description:[/blue] {metadata['description']}\")\n",
    "    console.print(f\"[blue]DateTime Filter:[/blue] {metadata['datetime_filter']}\")\n",
    "    \n",
    "    # Temporal selection statistics\n",
    "    selection = metadata['temporal_selection']\n",
    "    console.print(f\"[green]Files Selected:[/green] {selection['files_selected']} of {selection['total_files_considered']}\")\n",
    "    console.print(f\"[green]Selection Ratio:[/green] {selection['selection_ratio']:.1%}\")\n",
    "    \n",
    "    # Temporal coverage\n",
    "    coverage = selection['temporal_coverage']\n",
    "    console.print(f\"[yellow]Years Covered:[/yellow] {', '.join(map(str, coverage['years']))}\")\n",
    "    if coverage['months']:\n",
    "        month_names = [calendar.month_abbr[m] for m in coverage['months']]\n",
    "        console.print(f\"[yellow]Months Covered:[/yellow] {', '.join(month_names)}\")\n",
    "    if coverage['seasons']:\n",
    "        console.print(f\"[yellow]Seasons Covered:[/yellow] {', '.join(coverage['seasons'])}\")\n",
    "    console.print(f\"[yellow]Data Types:[/yellow] {', '.join(coverage['temporal_types'])}\")\n",
    "    \n",
    "    # Sample files by component\n",
    "    files_by_component = {}\n",
    "    for file_info in metadata['archive_contents']:\n",
    "        filename = Path(file_info['path']).name\n",
    "        component = filename.split('.')[0]  # Extract component (cam, clm, pop, etc.)\n",
    "        \n",
    "        if component not in files_by_component:\n",
    "            files_by_component[component] = []\n",
    "        files_by_component[component].append(file_info)\n",
    "    \n",
    "    if files_by_component:\n",
    "        console.print(\"\\n[bold]üìÑ Files by Model Component:[/bold]\")\n",
    "        for component in sorted(files_by_component.keys()):\n",
    "            files = files_by_component[component]\n",
    "            total_size = sum(f['size'] for f in files) / (1024*1024)\n",
    "            \n",
    "            console.print(f\"  [cyan]{component.upper()}:[/cyan] {len(files)} files ({total_size:.1f} MB)\")\n",
    "            \n",
    "            # Show sample files\n",
    "            sample_files = files[:2]  # Show first 2 files\n",
    "            for file_info in sample_files:\n",
    "                path = Path(file_info['path'])\n",
    "                dt_info = file_info['datetime_info']\n",
    "                console.print(f\"    ‚Ä¢ {path.name} (Type: {dt_info['type']})\")\n",
    "            \n",
    "            if len(files) > 2:\n",
    "                console.print(f\"    [dim]... and {len(files) - 2} more files[/dim]\")\n",
    "\n",
    "# Examine each datetime-filtered archive\n",
    "console.print(\"\\n[bold blue]üîç DateTime Archive Contents Analysis[/bold blue]\")\n",
    "\n",
    "archives_to_examine = [\n",
    "    heatwave_metadata,\n",
    "    elnino_metadata,\n",
    "    winter_metadata,\n",
    "    events_metadata\n",
    "]\n",
    "\n",
    "for metadata_path in archives_to_examine:\n",
    "    examine_datetime_archive_contents(metadata_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced DateTime Patterns and Techniques\n",
    "\n",
    "Let's explore more advanced datetime pattern matching techniques used in Earth Science:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(\"\\n[bold blue]üéØ Advanced DateTime Pattern Techniques[/bold blue]\")\n",
    "console.print(\"=\" * 55)\n",
    "\n",
    "# Advanced pattern examples\n",
    "advanced_patterns = [\n",
    "    {\n",
    "        'name': 'Multi-Model Ensemble',\n",
    "        'description': 'Extract same time period from multiple model runs',\n",
    "        'pattern': 'YYYY-MM with model variant filtering',\n",
    "        'tellus_example': 'tellus archive extract ensemble_runs --date-pattern \"%Y-%m\" --date \"2023-07\" --patterns \"*cam*.nc,*cesm*.nc\"',\n",
    "        'use_case': 'Model intercomparison studies'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Climate Indices Windows',\n",
    "        'description': 'Extract data during specific climate index phases',\n",
    "        'pattern': 'Conditional date ranges based on index values',\n",
    "        'tellus_example': 'tellus archive extract climate_data --date-ranges \"2015-10:2016-05,2023-09:2024-04\" --description \"El Nino events\"',\n",
    "        'use_case': 'Teleconnection studies, index-based analysis'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Seasonal Phase Selection',\n",
    "        'description': 'Extract specific parts of seasons (early/late)',\n",
    "        'pattern': 'Month sub-ranges within seasons',\n",
    "        'tellus_example': 'tellus archive extract seasonal_data --date-pattern \"%Y-%m\" --date-list \"2020-06,2020-07,2021-06,2021-07\" --description \"Early summer\"',\n",
    "        'use_case': 'Phenology studies, seasonal transition analysis'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Event Duration Matching',\n",
    "        'description': 'Extract files with embedded event duration information',\n",
    "        'pattern': 'Start_date_to_end_date pattern matching',\n",
    "        'tellus_example': 'tellus archive extract event_data --patterns \"*2023_07_*to*2023_08_*.nc\" --content-types diagnostic',\n",
    "        'use_case': 'Extreme event analysis, case studies'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Hindcast Time Windows', \n",
    "        'description': 'Extract verification periods for forecast validation',\n",
    "        'pattern': 'Forecast initialization + lead time patterns',\n",
    "        'tellus_example': 'tellus archive extract hindcast_data --date-pattern \"init_%Y%m%d_lead_%j\" --date-range \"20230601:20230831\"',\n",
    "        'use_case': 'Forecast verification, predictability studies'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Composite Analysis Periods',\n",
    "        'description': 'Extract same calendar periods across multiple years',\n",
    "        'pattern': 'Day-of-year matching across years',\n",
    "        'tellus_example': 'tellus archive extract composite_data --day-of-year-range \"152:244\" --years \"2020,2021,2022,2023\"',\n",
    "        'use_case': 'Climate composites, recurring pattern analysis'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Display advanced patterns table\n",
    "advanced_table = Table(title=\"Advanced DateTime Pattern Techniques\")\n",
    "advanced_table.add_column(\"Technique\", style=\"cyan\")\n",
    "advanced_table.add_column(\"Description\", style=\"green\")\n",
    "advanced_table.add_column(\"Use Case\", style=\"yellow\")\n",
    "\n",
    "for pattern in advanced_patterns:\n",
    "    advanced_table.add_row(\n",
    "        pattern['name'],\n",
    "        pattern['description'],\n",
    "        pattern['use_case']\n",
    "    )\n",
    "\n",
    "console.print(advanced_table)\n",
    "\n",
    "# Show example commands for each technique\n",
    "console.print(\"\\n[bold blue]üíª Example Commands for Advanced Techniques[/bold blue]\")\n",
    "\n",
    "for i, pattern in enumerate(advanced_patterns, 1):\n",
    "    console.print(f\"\\n[cyan]{i}. {pattern['name']}:[/cyan]\")\n",
    "    console.print(f\"[dim]{pattern['tellus_example']}[/dim]\")\n",
    "\n",
    "# Best practices for datetime filtering\n",
    "best_practices = Panel(\n",
    "    \"[bold green]DateTime Filtering Best Practices:[/bold green]\\n\\n\"\n",
    "    \"[cyan]üéØ Start Specific:[/cyan] Begin with narrow time ranges, expand as needed\\n\"\n",
    "    \"[cyan]üìÖ Know Your Data:[/cyan] Understand file naming conventions before filtering\\n\"\n",
    "    \"[cyan]üîç Test First:[/cyan] Use small test extractions to verify patterns work\\n\"\n",
    "    \"[cyan]üìñ Document Criteria:[/cyan] Record why specific dates/periods were chosen\\n\"\n",
    "    \"[cyan]üîÑ Consider Overlaps:[/cyan] Some periods may span multiple files (e.g., DJF)\\n\"\n",
    "    \"[cyan]‚ö° Combine Filters:[/cyan] Use datetime + content type filtering for precision\\n\"\n",
    "    \"[cyan]üåç Think Scientifically:[/cyan] Choose periods that match your research questions\",\n",
    "    title=\"üìã Best Practices\",\n",
    "    border_style=\"green\"\n",
    ")\n",
    "\n",
    "console.print(f\"\\n{best_practices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common DateTime Filtering Mistakes\n",
    "\n",
    "Let's identify and learn from common mistakes in datetime filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common datetime filtering mistakes\n",
    "datetime_mistakes = [\n",
    "    {\n",
    "        'mistake': '‚ùå Ignoring Seasonal Boundaries',\n",
    "        'example': 'Extracting \"Winter 2023\" as Jan-Mar 2023 only',\n",
    "        'problem': 'Meteorological winter (DJF) spans Dec 2022 - Feb 2023',\n",
    "        'solution': 'Understand climate season definitions before filtering',\n",
    "        'better_approach': 'Use --season DJF --year 2023 or --date-range \"2022-12:2023-02\"'\n",
    "    },\n",
    "    {\n",
    "        'mistake': '‚ùå Mismatching File Frequency',\n",
    "        'example': 'Looking for daily data with monthly date patterns',\n",
    "        'problem': 'Daily files may be named differently than monthly files',\n",
    "        'solution': 'Examine actual filenames before creating patterns',\n",
    "        'better_approach': 'Verify file naming: cam.h0.YYYY-MM.nc (monthly) vs cam.h1.YYYY-MM.nc (daily)'\n",
    "    },\n",
    "    {\n",
    "        'mistake': '‚ùå Over-Precise Date Filtering',\n",
    "        'example': 'Filtering for exact dates in climate data',\n",
    "        'problem': 'Climate data often represents periods, not exact dates',\n",
    "        'solution': 'Use date ranges and understand temporal averaging',\n",
    "        'better_approach': 'Filter by month/season rather than specific days for monthly data'\n",
    "    },\n",
    "    {\n",
    "        'mistake': '‚ùå Forgetting Time Zone Context',\n",
    "        'example': 'Assuming all model output uses the same time reference',\n",
    "        'problem': 'Different models may use different time conventions (UTC, local, etc.)',\n",
    "        'solution': 'Check model documentation for time reference standards',\n",
    "        'better_approach': 'Include time reference info in archive metadata'\n",
    "    },\n",
    "    {\n",
    "        'mistake': '‚ùå Missing Leap Year Considerations',\n",
    "        'example': 'Filtering Feb 29 data without checking calendar type',\n",
    "        'problem': 'Some models use no-leap calendars, others use standard calendars',\n",
    "        'solution': 'Understand model calendar conventions before date filtering',\n",
    "        'better_approach': 'Check NetCDF time coordinate attributes for calendar type'\n",
    "    },\n",
    "    {\n",
    "        'mistake': '‚ùå Inconsistent Date Formats',\n",
    "        'example': 'Mixing YYYY-MM-DD and YYYYMMDD patterns in same filter',\n",
    "        'problem': 'Different date formats require different extraction patterns',\n",
    "        'solution': 'Standardize on one format or use multiple patterns',\n",
    "        'better_approach': 'Use flexible patterns that match multiple formats: --patterns \"*2023-06*,*202306*\"'\n",
    "    }\n",
    "]\n",
    "\n",
    "console.print(\"\\n[bold blue]‚ö†Ô∏è  Common DateTime Filtering Mistakes[/bold blue]\")\n",
    "console.print(\"=\" * 55)\n",
    "\n",
    "for i, mistake in enumerate(datetime_mistakes, 1):\n",
    "    console.print(f\"\\n[bold red]{i}. {mistake['mistake']}[/bold red]\")\n",
    "    console.print(f\"[yellow]Example:[/yellow] {mistake['example']}\")\n",
    "    console.print(f\"[red]Problem:[/red] {mistake['problem']}\")\n",
    "    console.print(f\"[green]Solution:[/green] {mistake['solution']}\")\n",
    "    console.print(f\"[blue]Better Approach:[/blue] {mistake['better_approach']}\")\n",
    "\n",
    "# Create a decision tree for datetime filtering\n",
    "decision_tree = Panel(\n",
    "    \"[bold green]DateTime Filtering Decision Tree:[/bold green]\\n\\n\"\n",
    "    \"[cyan]üìä What type of analysis?[/cyan]\\n\"\n",
    "    \"‚îú‚îÄ‚îÄ üå°Ô∏è Extreme events ‚Üí Use event-based patterns or specific date ranges\\n\"\n",
    "    \"‚îú‚îÄ‚îÄ üìà Long-term trends ‚Üí Use annual data with year ranges\\n\"\n",
    "    \"‚îú‚îÄ‚îÄ üåø Seasonal studies ‚Üí Use seasonal patterns (DJF, MAM, JJA, SON)\\n\"\n",
    "    \"‚îú‚îÄ‚îÄ üîÑ Interannual variability ‚Üí Use monthly data across multiple years\\n\"\n",
    "    \"‚îî‚îÄ‚îÄ ‚ö° Model validation ‚Üí Use specific forecast/initialization periods\\n\\n\"\n",
    "    \"[cyan]üéØ What temporal resolution?[/cyan]\\n\"\n",
    "    \"‚îú‚îÄ‚îÄ üìÖ Sub-daily ‚Üí Look for hourly/3-hourly patterns\\n\"\n",
    "    \"‚îú‚îÄ‚îÄ üìÜ Daily ‚Üí Check for daily naming conventions\\n\"\n",
    "    \"‚îú‚îÄ‚îÄ üìä Monthly ‚Üí Use YYYY-MM patterns\\n\"\n",
    "    \"‚îú‚îÄ‚îÄ üå± Seasonal ‚Üí Use season abbreviations\\n\"\n",
    "    \"‚îî‚îÄ‚îÄ üìà Annual ‚Üí Use YYYY patterns\\n\\n\"\n",
    "    \"[cyan]üíæ How much data can you handle?[/cyan]\\n\"\n",
    "    \"‚îú‚îÄ‚îÄ üñ•Ô∏è Local analysis ‚Üí Can use broader date ranges\\n\"\n",
    "    \"‚îú‚îÄ‚îÄ ‚òÅÔ∏è Cloud processing ‚Üí Be selective with dates\\n\"\n",
    "    \"‚îú‚îÄ‚îÄ üì± Limited bandwidth ‚Üí Use very specific periods\\n\"\n",
    "    \"‚îî‚îÄ‚îÄ üíø Archive storage ‚Üí Consider multiple selective archives\",\n",
    "    title=\"üå≥ Decision Tree\",\n",
    "    border_style=\"green\"\n",
    ")\n",
    "\n",
    "console.print(f\"\\n{decision_tree}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup tutorial files\n",
    "import shutil\n",
    "\n",
    "console.print(\"\\n[bold blue]üßπ Cleaning up tutorial files...[/bold blue]\")\n",
    "shutil.rmtree(tutorial_dir)\n",
    "console.print(f\"[green]‚úÖ Cleaned up: {tutorial_dir}[/green]\")\n",
    "\n",
    "# Tutorial summary\n",
    "summary = Panel(\n",
    "    \"[bold green]üéì Tutorial 3 Complete - DateTime Filtering Mastery![/bold green]\\n\\n\"\n",
    "    \"[cyan]Key Skills Mastered:[/cyan]\\n\"\n",
    "    \"‚úÖ Understanding temporal patterns in climate data\\n\"\n",
    "    \"‚úÖ Extracting specific time periods efficiently\\n\"\n",
    "    \"‚úÖ Creating targeted datetime-filtered archives\\n\"\n",
    "    \"‚úÖ Handling multiple temporal frequencies\\n\"\n",
    "    \"‚úÖ Avoiding common datetime filtering pitfalls\\n\\n\"\n",
    "    \"[yellow]Real-World Applications:[/yellow]\\n\"\n",
    "    \"‚Ä¢ üå°Ô∏è Heat wave studies using summer-only data\\n\"\n",
    "    \"‚Ä¢ üåä El Ni√±o analysis across multi-year periods\\n\"\n",
    "    \"‚Ä¢ ‚ùÑÔ∏è Winter climate research with seasonal filtering\\n\"\n",
    "    \"‚Ä¢ ‚ö° Extreme event case studies with event-based extraction\\n\"\n",
    "    \"‚Ä¢ üîÑ Model restart packages with specific temporal checkpoints\\n\\n\"\n",
    "    \"[blue]Data Efficiency Achieved:[/blue]\\n\"\n",
    "    \"‚Ä¢ Reduced data transfer by 80-95% through targeted extraction\\n\"\n",
    "    \"‚Ä¢ Faster analysis with relevant data only\\n\"\n",
    "    \"‚Ä¢ Improved collaboration through focused data sharing\\n\\n\"\n",
    "    \"[magenta]Next: Tutorial 4 - Fragment Assembly[/magenta]\\n\"\n",
    "    \"Learn to combine multiple archives into complete datasets\",\n",
    "    title=\"üéâ Tutorial Summary\",\n",
    "    border_style=\"green\"\n",
    ")\n",
    "\n",
    "console.print(summary)\n",
    "\n",
    "# Quick reference commands\n",
    "quick_reference = Panel(\n",
    "    \"[bold cyan]Quick Reference - Key DateTime Commands:[/bold cyan]\\n\\n\"\n",
    "    \"[green]# Extract specific year[/green]\\n\"\n",
    "    \"tellus archive extract my_simulation --date-pattern '%Y' --date '2023'\\n\\n\"\n",
    "    \"[green]# Extract date range[/green]\\n\"\n",
    "    \"tellus archive extract my_simulation --date-range '2023-06:2023-08'\\n\\n\"\n",
    "    \"[green]# Extract specific months across years[/green]\\n\"\n",
    "    \"tellus archive extract my_simulation --date-pattern '%Y-%m' --date-list '2020-07,2021-07,2022-07'\\n\\n\"\n",
    "    \"[green]# Extract seasonal data[/green]\\n\"\n",
    "    \"tellus archive extract my_simulation --patterns '*DJF*.nc' --content-types output\\n\\n\"\n",
    "    \"[green]# Combine datetime + content filtering[/green]\\n\"\n",
    "    \"tellus archive extract my_simulation --date-range '2023-01:2023-12' --content-types output,diagnostic\",\n",
    "    title=\"üìñ Quick Reference\",\n",
    "    border_style=\"blue\"\n",
    ")\n",
    "\n",
    "console.print(f\"\\n{quick_reference}\")\n",
    "\n",
    "console.print(\"\\n[bold blue]üìö Ready for Next Tutorial?[/bold blue]\")\n",
    "console.print(\"Tutorial 4 will teach you fragment assembly - how to intelligently combine multiple archive pieces into complete datasets. This is especially powerful for reconstructing long-term simulations from temporal or thematic fragments.\")\n",
    "console.print(\"\\n[dim]Continue to: archive-tutorial-04-fragment-assembly.ipynb[/dim]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
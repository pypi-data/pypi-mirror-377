{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High-Performance Computing Integration with Tellus\n",
    "\n",
    "## User Story: Large-Scale Climate Simulation on Supercomputer\n",
    "\n",
    "**Scenario**: Dr. Michael Rodriguez runs high-resolution CESM2 simulations on NCAR's Cheyenne supercomputer for his climate extremes research. He needs to efficiently manage terabytes of model output, coordinate data transfers to analysis systems, and integrate with HPC job scheduling while maintaining data provenance and quality control.\n",
    "\n",
    "**Goals**:\n",
    "- Integrate Tellus with HPC batch job systems (PBS/SLURM)\n",
    "- Manage massive datasets efficiently during and after simulation runs\n",
    "- Coordinate data movement between HPC storage tiers (scratch, work, archive)\n",
    "- Implement automated post-processing workflows\n",
    "- Monitor simulation progress and data generation in real-time\n",
    "\n",
    "**Key Features Demonstrated**:\n",
    "- HPC-optimized storage configurations\n",
    "- Integration with job schedulers\n",
    "- High-throughput data transfers\n",
    "- Automated workflow orchestration\n",
    "- Performance monitoring and optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. HPC Environment Setup\n",
    "\n",
    "Configure Tellus for optimal performance on high-performance computing systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "from tellus.application.container import ServiceContainer\n",
    "from tellus.application.dtos import (\n",
    "    CreateLocationDto, CreateSimulationDto, CreateArchiveDto,\n",
    "    FileTransferOperationDto, BatchFileTransferOperationDto,\n",
    "    CreateProgressTrackingDto\n",
    ")\n",
    "from tellus.domain.entities.location import LocationKind\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Initialize service container with HPC-optimized configuration\n",
    "container = ServiceContainer()\n",
    "location_service = container.get_location_service()\n",
    "simulation_service = container.get_simulation_service()\n",
    "archive_service = container.get_archive_service()\n",
    "transfer_service = container.get_file_transfer_service()\n",
    "progress_service = container.get_progress_tracking_service()\n",
    "\n",
    "print(\"üñ•Ô∏è  Tellus HPC Integration Initialized\")\n",
    "print(f\"Environment: NCAR Cheyenne Supercomputer\")\n",
    "print(f\"User: Dr. Michael Rodriguez (mrodriguez)\")\n",
    "print(f\"Project: High-Resolution Climate Extremes Research\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuring HPC Storage Hierarchy\n",
    "\n",
    "Set up the multi-tier storage system typical of HPC environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Cheyenne scratch storage (high-performance, temporary)\n",
    "scratch_dto = CreateLocationDto(\n",
    "    name=\"cheyenne-scratch\",\n",
    "    kinds=[LocationKind.COMPUTE],\n",
    "    protocol=\"file\",\n",
    "    path=\"/glade/scratch/mrodriguez\",\n",
    "    description=\"Cheyenne scratch storage for active simulations\",\n",
    "    metadata={\n",
    "        \"filesystem_type\": \"lustre\",\n",
    "        \"performance_tier\": \"high\",\n",
    "        \"retention_policy\": \"90_days\",\n",
    "        \"quota_tb\": 50,\n",
    "        \"stripe_count\": 8,\n",
    "        \"stripe_size\": \"1MB\",\n",
    "        \"mount_point\": \"/glade/scratch\",\n",
    "        \"io_optimization\": \"parallel_writes\"\n",
    "    }\n",
    ")\n",
    "scratch_result = location_service.create_location(scratch_dto)\n",
    "print(f\"‚úì Configured scratch storage: {scratch_result.name}\")\n",
    "\n",
    "# Configure Cheyenne work storage (persistent, project data)\n",
    "work_dto = CreateLocationDto(\n",
    "    name=\"cheyenne-work\",\n",
    "    kinds=[LocationKind.FILESERVER, LocationKind.DISK],\n",
    "    protocol=\"file\", \n",
    "    path=\"/glade/work/mrodriguez\",\n",
    "    description=\"Cheyenne work storage for project data and analysis\",\n",
    "    metadata={\n",
    "        \"filesystem_type\": \"gpfs\",\n",
    "        \"performance_tier\": \"medium\",\n",
    "        \"retention_policy\": \"project_lifetime\",\n",
    "        \"quota_tb\": 20,\n",
    "        \"backup_schedule\": \"weekly\",\n",
    "        \"snapshot_retention\": \"30_days\",\n",
    "        \"access_from\": [\"login_nodes\", \"compute_nodes\"]\n",
    "    }\n",
    ")\n",
    "work_result = location_service.create_location(work_dto)\n",
    "print(f\"‚úì Configured work storage: {work_result.name}\")\n",
    "\n",
    "# Configure HPSS tape archive (long-term storage)\n",
    "hpss_dto = CreateLocationDto(\n",
    "    name=\"cheyenne-hpss\",\n",
    "    kinds=[LocationKind.TAPE],\n",
    "    protocol=\"hsi\",  # HPSS Storage Interface\n",
    "    path=\"/CCSM/csm/mrodriguez/climate-extremes\",\n",
    "    host=\"hpss.ucar.edu\",\n",
    "    description=\"NCAR HPSS tape archive for long-term data preservation\",\n",
    "    metadata={\n",
    "        \"storage_type\": \"hierarchical\",\n",
    "        \"performance_tier\": \"archive\",\n",
    "        \"retention_policy\": \"permanent\",\n",
    "        \"quota_tb\": 500,\n",
    "        \"retrieval_time\": \"minutes_to_hours\",\n",
    "        \"cost_per_tb_year\": 5.0,\n",
    "        \"tape_technology\": \"LTO-9\"\n",
    "    }\n",
    ")\n",
    "hpss_result = location_service.create_location(hpss_dto)\n",
    "print(f\"‚úì Configured HPSS archive: {hpss_result.name}\")\n",
    "\n",
    "# Configure Campaign Storage (shared project data)\n",
    "campaign_dto = CreateLocationDto(\n",
    "    name=\"cheyenne-campaign\",\n",
    "    kinds=[LocationKind.FILESERVER],\n",
    "    protocol=\"file\",\n",
    "    path=\"/glade/campaign/cgd/ccr/mrodriguez/extreme-weather\",\n",
    "    description=\"Campaign storage for collaborative extreme weather research\",\n",
    "    metadata={\n",
    "        \"filesystem_type\": \"gpfs\",\n",
    "        \"performance_tier\": \"medium\", \n",
    "        \"access_type\": \"shared\",\n",
    "        \"quota_tb\": 100,\n",
    "        \"sharing_policy\": \"project_team\",\n",
    "        \"data_classification\": \"research_results\"\n",
    "    }\n",
    ")\n",
    "campaign_result = location_service.create_location(campaign_dto)\n",
    "print(f\"‚úì Configured campaign storage: {campaign_result.name}\")\n",
    "\n",
    "# Configure external analysis system\n",
    "analysis_dto = CreateLocationDto(\n",
    "    name=\"analysis-cluster\",\n",
    "    kinds=[LocationKind.COMPUTE, LocationKind.FILESERVER],\n",
    "    protocol=\"ssh\",\n",
    "    host=\"analysis.ucar.edu\",\n",
    "    username=\"mrodriguez\",\n",
    "    path=\"/home/mrodriguez/climate-analysis\",\n",
    "    description=\"Dedicated analysis cluster for post-processing\",\n",
    "    metadata={\n",
    "        \"cpu_cores\": 128,\n",
    "        \"memory_gb\": 1024,\n",
    "        \"gpu_count\": 4,\n",
    "        \"software_stack\": [\"python\", \"ncl\", \"cdo\", \"nco\", \"jupyter\"],\n",
    "        \"network_bandwidth\": \"10Gb/s\"\n",
    "    }\n",
    ")\n",
    "analysis_result = location_service.create_location(analysis_dto)\n",
    "print(f\"‚úì Configured analysis cluster: {analysis_result.name}\")\n",
    "\n",
    "print(\"\\nüèóÔ∏è  HPC Storage Hierarchy Configured:\")\n",
    "print(\"  üìÅ Scratch ‚Üí Work ‚Üí Campaign ‚Üí HPSS Archive\")\n",
    "print(\"  üîÑ Analysis Cluster ‚Üî All Storage Tiers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. High-Resolution Climate Simulation Setup\n",
    "\n",
    "Configure a large-scale CESM2 simulation with comprehensive metadata tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create high-resolution CESM2 simulation\n",
    "hires_sim_dto = CreateSimulationDto(\n",
    "    simulation_id=\"cesm2-hr-extremes-2024-001\",\n",
    "    model_id=\"cesm2-2-0\",\n",
    "    attrs={\n",
    "        \"experiment_type\": \"high_resolution_climate_extremes\",\n",
    "        \"resolution\": \"0.25deg_atm_0.1deg_ocn\", \n",
    "        \"time_period\": \"1979-2020\",\n",
    "        \"current_model_date\": \"1985-03-15\",\n",
    "        \"wallclock_limit\": \"12:00:00\",\n",
    "        \"nodes_requested\": 2048,\n",
    "        \"cores_per_node\": 36,\n",
    "        \"total_cores\": 73728,\n",
    "        \"memory_per_node_gb\": 256,\n",
    "        \"estimated_runtime_days\": 45,\n",
    "        \"compset\": \"B1850\",\n",
    "        \"grid\": \"f02_g17\",\n",
    "        \"machine\": \"cheyenne\",\n",
    "        \"compiler\": \"intel/19.1.1\",\n",
    "        \"mpi_tasks\": 73728,\n",
    "        \"openmp_threads\": 1,\n",
    "        \"job_queue\": \"regular\",\n",
    "        \"account\": \"P93300641\",\n",
    "        \"priority\": \"high\",\n",
    "        \"output_frequency\": {\n",
    "            \"atmosphere\": [\"monthly\", \"daily\", \"6hourly\"],\n",
    "            \"ocean\": [\"monthly\", \"daily\"],\n",
    "            \"ice\": [\"monthly\", \"daily\"],\n",
    "            \"land\": [\"monthly\"]\n",
    "        },\n",
    "        \"variables_of_interest\": [\n",
    "            \"PRECT\", \"PRECC\", \"PRECL\",  # Precipitation\n",
    "            \"TREFHT\", \"TREFHTMX\", \"TREFHTMN\",  # Temperature\n",
    "            \"U850\", \"V850\", \"OMEGA500\",  # Circulation\n",
    "            \"PSL\", \"Z500\",  # Pressure/geopotential\n",
    "            \"CAPE\", \"CIN\",  # Convective parameters\n",
    "            \"QFLX\", \"SHFLX\", \"LHFLX\"  # Surface fluxes\n",
    "        ],\n",
    "        \"expected_output_size_tb\": 85.5,\n",
    "        \"PI\": \"Dr. Michael Rodriguez\",\n",
    "        \"funding_source\": \"NSF-AGS-2034567\",\n",
    "        \"collaboration\": \"NCAR-CGD-Extremes-Working-Group\"\n",
    "    }\n",
    ")\n",
    "\n",
    "hires_sim = simulation_service.create_simulation(hires_sim_dto)\n",
    "print(f\"‚úì Created high-resolution simulation: {hires_sim.simulation_id}\")\n",
    "print(f\"  Expected output: {hires_sim.attrs['expected_output_size_tb']} TB\")\n",
    "print(f\"  Estimated runtime: {hires_sim.attrs['estimated_runtime_days']} days\")\n",
    "print(f\"  Compute resources: {hires_sim.attrs['total_cores']} cores on {hires_sim.attrs['nodes_requested']} nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Associate simulation with HPC storage hierarchy\n",
    "from tellus.application.dtos import SimulationLocationAssociationDto\n",
    "\n",
    "hpc_assoc_dto = SimulationLocationAssociationDto(\n",
    "    simulation_id=hires_sim.simulation_id,\n",
    "    location_names=[\n",
    "        \"cheyenne-scratch\", \"cheyenne-work\", \n",
    "        \"cheyenne-campaign\", \"cheyenne-hpss\", \n",
    "        \"analysis-cluster\"\n",
    "    ],\n",
    "    context_overrides={\n",
    "        \"cheyenne-scratch\": {\n",
    "            \"path_prefix\": \"/glade/scratch/mrodriguez/cesm2-hr-extremes\",\n",
    "            \"role\": \"active_simulation\",\n",
    "            \"io_optimization\": \"high_throughput\",\n",
    "            \"cleanup_policy\": \"post_completion\"\n",
    "        },\n",
    "        \"cheyenne-work\": {\n",
    "            \"path_prefix\": \"/glade/work/mrodriguez/cesm2-hr/analysis\",\n",
    "            \"role\": \"processed_data\",\n",
    "            \"retention\": \"project_lifetime\"\n",
    "        },\n",
    "        \"cheyenne-campaign\": {\n",
    "            \"path_prefix\": \"/glade/campaign/cgd/ccr/mrodriguez/hr-extremes\", \n",
    "            \"role\": \"collaborative_results\",\n",
    "            \"sharing\": \"project_team\"\n",
    "        },\n",
    "        \"cheyenne-hpss\": {\n",
    "            \"path_prefix\": \"/CCSM/csm/mrodriguez/cesm2-hr-extremes-2024\",\n",
    "            \"role\": \"long_term_archive\",\n",
    "            \"compression\": \"gzip\",\n",
    "            \"bundling\": \"yearly_tar_files\"\n",
    "        },\n",
    "        \"analysis-cluster\": {\n",
    "            \"path_prefix\": \"/home/mrodriguez/cesm2-hr-analysis\",\n",
    "            \"role\": \"post_processing\",\n",
    "            \"workflow_integration\": True\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "association_result = simulation_service.associate_simulation_with_locations(hpc_assoc_dto)\n",
    "print(f\"‚úì Associated simulation with {len(hpc_assoc_dto.location_names)} storage locations\")\n",
    "\n",
    "# Display storage hierarchy\n",
    "print(\"\\nüîó HPC Storage Association Hierarchy:\")\n",
    "for location, context in hpc_assoc_dto.context_overrides.items():\n",
    "    role = context.get('role', 'general')\n",
    "    path = context.get('path_prefix', 'N/A')\n",
    "    print(f\"  üìÅ {location}: {role.replace('_', ' ').title()}\")\n",
    "    print(f\"     Path: {path}\")\n",
    "    if 'optimization' in context:\n",
    "        print(f\"     Optimization: {context.get('io_optimization', context.get('optimization', 'N/A'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. HPC Job Integration and Monitoring\n",
    "\n",
    "Demonstrate integration with HPC job schedulers and real-time simulation monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate HPC job submission and monitoring\n",
    "import uuid\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def simulate_hpc_job_status():\n",
    "    \"\"\"Simulate HPC job scheduler integration.\"\"\"\n",
    "    \n",
    "    # Simulate job submission\n",
    "    job_info = {\n",
    "        \"job_id\": \"4847291\",\n",
    "        \"job_name\": \"cesm2_hr_extremes_001\",\n",
    "        \"user\": \"mrodriguez\",\n",
    "        \"account\": \"P93300641\",\n",
    "        \"queue\": \"regular\",\n",
    "        \"nodes\": 2048,\n",
    "        \"cores\": 73728,\n",
    "        \"walltime_requested\": \"12:00:00\",\n",
    "        \"walltime_used\": \"03:45:23\",\n",
    "        \"status\": \"RUNNING\",\n",
    "        \"start_time\": \"2024-06-15T08:30:00Z\",\n",
    "        \"estimated_end\": \"2024-06-15T20:30:00Z\",\n",
    "        \"queue_wait_time\": \"00:15:30\",\n",
    "        \"current_model_date\": \"1982-07-15\",\n",
    "        \"simulation_years_per_day\": 2.3,\n",
    "        \"output_files_generated\": 1247,\n",
    "        \"scratch_usage_gb\": 15420,\n",
    "        \"memory_usage_percent\": 78.5,\n",
    "        \"cpu_efficiency_percent\": 92.3,\n",
    "        \"io_rate_gb_per_hour\": 450.2\n",
    "    }\n",
    "    \n",
    "    return job_info\n",
    "\n",
    "# Get current job status\n",
    "job_status = simulate_hpc_job_status()\n",
    "\n",
    "print(\"üñ•Ô∏è  HPC Job Monitoring Dashboard\")\n",
    "print(f\"Job ID: {job_status['job_id']} ({job_status['job_name']})\")\n",
    "print(f\"Status: {job_status['status']} since {job_status['start_time']}\")\n",
    "print(f\"Resources: {job_status['nodes']} nodes, {job_status['cores']} cores\")\n",
    "print(f\"Walltime: {job_status['walltime_used']} / {job_status['walltime_requested']}\")\n",
    "print(f\"Estimated completion: {job_status['estimated_end']}\")\n",
    "print()\n",
    "print(\"üìä Simulation Progress:\")\n",
    "print(f\"Current model date: {job_status['current_model_date']}\")\n",
    "print(f\"Simulation rate: {job_status['simulation_years_per_day']} years/day\")\n",
    "print(f\"Files generated: {job_status['output_files_generated']}\")\n",
    "print()\n",
    "print(\"üíæ Resource Utilization:\")\n",
    "print(f\"Scratch storage: {job_status['scratch_usage_gb']} GB\")\n",
    "print(f\"Memory usage: {job_status['memory_usage_percent']}%\")\n",
    "print(f\"CPU efficiency: {job_status['cpu_efficiency_percent']}%\")\n",
    "print(f\"I/O rate: {job_status['io_rate_gb_per_hour']} GB/hour\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create progress tracking for the HPC job\n",
    "from tellus.domain.entities.progress_tracking import OperationType\n",
    "\n",
    "# Create progress tracking for the simulation\n",
    "sim_progress_dto = CreateProgressTrackingDto(\n",
    "    operation_id=f\"hpc_sim_{job_status['job_id']}\",\n",
    "    operation_type=OperationType.MODEL_SIMULATION.value,\n",
    "    operation_name=f\"CESM2 High-Resolution Climate Extremes Simulation\",\n",
    "    priority=\"high\",\n",
    "    context={\n",
    "        \"simulation_id\": hires_sim.simulation_id,\n",
    "        \"location_name\": \"cheyenne-scratch\",\n",
    "        \"metadata\": {\n",
    "            \"hpc_job_id\": job_status['job_id'],\n",
    "            \"machine\": \"cheyenne\",\n",
    "            \"account\": job_status['account'],\n",
    "            \"queue\": job_status['queue']\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# In a real implementation, this would create the progress tracker\n",
    "print(f\"‚úì Created HPC job progress tracking: {sim_progress_dto.operation_id}\")\n",
    "print(f\"  Operation type: {sim_progress_dto.operation_type}\")\n",
    "print(f\"  Priority: {sim_progress_dto.priority}\")\n",
    "\n",
    "# Simulate real-time monitoring updates\n",
    "def calculate_simulation_progress(current_date, start_date=\"1979-01-01\", end_date=\"2020-12-31\"):\n",
    "    \"\"\"Calculate simulation progress based on model dates.\"\"\"\n",
    "    from datetime import datetime\n",
    "    \n",
    "    start = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    current = datetime.strptime(current_date, \"%Y-%m-%d\")\n",
    "    \n",
    "    total_days = (end - start).days\n",
    "    completed_days = (current - start).days\n",
    "    \n",
    "    return (completed_days / total_days) * 100\n",
    "\n",
    "# Calculate current progress\n",
    "progress_percent = calculate_simulation_progress(job_status['current_model_date'])\n",
    "print(f\"\\nüìà Simulation Progress: {progress_percent:.1f}% complete\")\n",
    "print(f\"   Model time: {job_status['current_model_date']}\")\n",
    "print(f\"   Target end: 2020-12-31\")\n",
    "print(f\"   Estimated completion: {job_status['estimated_end']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Automated Data Movement Workflows\n",
    "\n",
    "Implement automated data movement through the HPC storage hierarchy as simulation progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define HPC data movement workflow\n",
    "def create_hpc_data_workflow():\n",
    "    \"\"\"Create automated data movement workflow for HPC environment.\"\"\"\n",
    "    \n",
    "    workflow_stages = {\n",
    "        \"active_simulation\": {\n",
    "            \"location\": \"cheyenne-scratch\", \n",
    "            \"purpose\": \"High-performance I/O during simulation\",\n",
    "            \"retention\": \"Until completion + 30 days\",\n",
    "            \"optimization\": \"Parallel writes, large stripe count\",\n",
    "            \"monitoring\": \"Real-time I/O rates and space usage\"\n",
    "        },\n",
    "        \"immediate_processing\": {\n",
    "            \"location\": \"cheyenne-work\",\n",
    "            \"purpose\": \"Quick quality checks and basic processing\", \n",
    "            \"trigger\": \"Daily, for completed files > 24h old\",\n",
    "            \"operations\": [\"checksums\", \"metadata_extraction\", \"basic_validation\"],\n",
    "            \"retention\": \"Project lifetime\"\n",
    "        },\n",
    "        \"collaborative_sharing\": {\n",
    "            \"location\": \"cheyenne-campaign\",\n",
    "            \"purpose\": \"Processed data for team collaboration\",\n",
    "            \"trigger\": \"Weekly, for validated processed data\",\n",
    "            \"operations\": [\"format_standardization\", \"documentation\", \"sharing_setup\"],\n",
    "            \"access_control\": \"Project team read/write\"\n",
    "        },\n",
    "        \"long_term_archive\": {\n",
    "            \"location\": \"cheyenne-hpss\",\n",
    "            \"purpose\": \"Permanent preservation and backup\",\n",
    "            \"trigger\": \"Monthly, for stable datasets\",\n",
    "            \"operations\": [\"compression\", \"bundling\", \"integrity_verification\"],\n",
    "            \"retention\": \"Permanent (project + 10 years)\"\n",
    "        },\n",
    "        \"external_analysis\": {\n",
    "            \"location\": \"analysis-cluster\",\n",
    "            \"purpose\": \"Detailed post-processing and analysis\",\n",
    "            \"trigger\": \"On-demand or weekly for priority datasets\",\n",
    "            \"operations\": [\"regridding\", \"statistical_analysis\", \"visualization\"],\n",
    "            \"compute_resources\": \"Dedicated analysis nodes\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return workflow_stages\n",
    "\n",
    "# Create and display the workflow\n",
    "hpc_workflow = create_hpc_data_workflow()\n",
    "print(\"üîÑ HPC Data Movement Workflow\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for stage_name, stage_info in hpc_workflow.items():\n",
    "    print(f\"\\nüìç {stage_name.replace('_', ' ').upper()}\")\n",
    "    print(f\"   Location: {stage_info['location']}\")\n",
    "    print(f\"   Purpose: {stage_info['purpose']}\")\n",
    "    \n",
    "    if 'trigger' in stage_info:\n",
    "        print(f\"   Trigger: {stage_info['trigger']}\")\n",
    "    if 'operations' in stage_info:\n",
    "        print(f\"   Operations: {', '.join(stage_info['operations'])}\")\n",
    "    if 'retention' in stage_info:\n",
    "        print(f\"   Retention: {stage_info['retention']}\")\n",
    "    if 'optimization' in stage_info:\n",
    "        print(f\"   Optimization: {stage_info['optimization']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batch transfer operations for HPC data movement\n",
    "def create_scratch_to_work_transfer():\n",
    "    \"\"\"Create daily transfer from scratch to work storage.\"\"\"\n",
    "    \n",
    "    # Simulate files ready for transfer (completed > 24h ago)\n",
    "    files_to_transfer = [\n",
    "        {\n",
    "            \"source_path\": \"/glade/scratch/mrodriguez/cesm2-hr-extremes/run/cesm2.cam.h0.1982-01.nc\",\n",
    "            \"dest_path\": \"/glade/work/mrodriguez/cesm2-hr/raw/atm/monthly/cesm2.cam.h0.1982-01.nc\",\n",
    "            \"size_gb\": 2.1,\n",
    "            \"variables\": [\"PRECT\", \"TREFHT\", \"PSL\", \"U850\", \"V850\"],\n",
    "            \"frequency\": \"monthly\"\n",
    "        },\n",
    "        {\n",
    "            \"source_path\": \"/glade/scratch/mrodriguez/cesm2-hr-extremes/run/cesm2.cam.h1.1982-01-01.nc\",\n",
    "            \"dest_path\": \"/glade/work/mrodriguez/cesm2-hr/raw/atm/daily/cesm2.cam.h1.1982-01-01.nc\",\n",
    "            \"size_gb\": 0.8,\n",
    "            \"variables\": [\"PRECT\", \"TREFHTMX\", \"TREFHTMN\"],\n",
    "            \"frequency\": \"daily\"\n",
    "        },\n",
    "        {\n",
    "            \"source_path\": \"/glade/scratch/mrodriguez/cesm2-hr-extremes/run/cesm2.pop.h.1982-01.nc\",\n",
    "            \"dest_path\": \"/glade/work/mrodriguez/cesm2-hr/raw/ocn/monthly/cesm2.pop.h.1982-01.nc\",\n",
    "            \"size_gb\": 4.2,\n",
    "            \"variables\": [\"TEMP\", \"SALT\", \"SSH\", \"UVEL\", \"VVEL\"],\n",
    "            \"frequency\": \"monthly\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return files_to_transfer\n",
    "\n",
    "# Create the transfer operations\n",
    "transfer_files = create_scratch_to_work_transfer()\n",
    "\n",
    "# Create batch transfer DTO\n",
    "scratch_to_work_dto = BatchFileTransferOperationDto(\n",
    "    source_location=\"cheyenne-scratch\",\n",
    "    dest_location=\"cheyenne-work\", \n",
    "    file_operations=[\n",
    "        {\n",
    "            \"source_path\": file_info[\"source_path\"],\n",
    "            \"dest_path\": file_info[\"dest_path\"],\n",
    "            \"metadata\": {\n",
    "                \"size_gb\": file_info[\"size_gb\"],\n",
    "                \"variables\": file_info[\"variables\"],\n",
    "                \"frequency\": file_info[\"frequency\"],\n",
    "                \"transfer_type\": \"scratch_to_work\",\n",
    "                \"priority\": \"daily_batch\"\n",
    "            }\n",
    "        }\n",
    "        for file_info in transfer_files\n",
    "    ],\n",
    "    verify_checksum=True,\n",
    "    continue_on_error=True,\n",
    "    metadata={\n",
    "        \"workflow\": \"hpc_daily_data_movement\",\n",
    "        \"simulation_id\": hires_sim.simulation_id,\n",
    "        \"hpc_job_id\": job_status['job_id'],\n",
    "        \"transfer_stage\": \"scratch_to_work\",\n",
    "        \"scheduled_time\": \"daily_02:00\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"‚úì Created scratch‚Üíwork transfer batch: {len(transfer_files)} files\")\n",
    "total_size = sum(f['size_gb'] for f in transfer_files)\n",
    "print(f\"  Total size: {total_size:.1f} GB\")\n",
    "print(f\"  File types: {', '.join(set(f['frequency'] for f in transfer_files))}\")\n",
    "print(f\"  Checksum verification: {scratch_to_work_dto.verify_checksum}\")\n",
    "\n",
    "print(\"\\nüìã Transfer Details:\")\n",
    "for i, file_info in enumerate(transfer_files, 1):\n",
    "    print(f\"  {i}. {file_info['frequency'].title()} {file_info['source_path'].split('/')[-1]}\")\n",
    "    print(f\"     Size: {file_info['size_gb']} GB, Variables: {len(file_info['variables'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance Optimization for HPC Environments\n",
    "\n",
    "Implement HPC-specific optimizations for maximum throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HPC Performance Configuration\n",
    "hpc_performance_config = {\n",
    "    \"transfer_optimization\": {\n",
    "        \"parallel_streams\": 8,\n",
    "        \"chunk_size_mb\": 64,  # Larger chunks for HPC networks\n",
    "        \"buffer_size_mb\": 256,\n",
    "        \"compression\": \"lz4\",  # Fast compression for intermediate transfers\n",
    "        \"checksum_algorithm\": \"xxhash64\",  # Fast checksum for HPC\n",
    "        \"network_optimization\": \"high_throughput\",\n",
    "        \"io_scheduler\": \"deadline\",\n",
    "        \"stripe_alignment\": True\n",
    "    },\n",
    "    \"storage_optimization\": {\n",
    "        \"lustre_stripe_count\": 8,\n",
    "        \"lustre_stripe_size_mb\": 4,\n",
    "        \"gpfs_block_size_kb\": 1024,\n",
    "        \"directory_hashing\": True,\n",
    "        \"metadata_prefetch\": True,\n",
    "        \"async_io\": True,\n",
    "        \"direct_io\": True\n",
    "    },\n",
    "    \"queue_management\": {\n",
    "        \"max_concurrent_transfers\": 16,\n",
    "        \"priority_queues\": [\"urgent\", \"daily\", \"weekly\", \"archive\"],\n",
    "        \"resource_allocation\": {\n",
    "            \"urgent\": {\"bandwidth_percent\": 40, \"max_jobs\": 4},\n",
    "            \"daily\": {\"bandwidth_percent\": 35, \"max_jobs\": 8},\n",
    "            \"weekly\": {\"bandwidth_percent\": 20, \"max_jobs\": 4},\n",
    "            \"archive\": {\"bandwidth_percent\": 5, \"max_jobs\": 2}\n",
    "        },\n",
    "        \"load_balancing\": \"round_robin\",\n",
    "        \"congestion_control\": True\n",
    "    },\n",
    "    \"monitoring\": {\n",
    "        \"metrics_collection_interval_sec\": 10,\n",
    "        \"performance_baseline_mb_per_sec\": 2000,\n",
    "        \"alert_thresholds\": {\n",
    "            \"slow_transfer_percent\": 50,  # Alert if <50% of baseline\n",
    "            \"high_error_rate_percent\": 5,\n",
    "            \"queue_depth_limit\": 100,\n",
    "            \"storage_full_percent\": 90\n",
    "        },\n",
    "        \"telemetry_exports\": [\"prometheus\", \"grafana\", \"influxdb\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"‚ö° HPC Performance Configuration\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Display key performance settings\n",
    "transfer_opts = hpc_performance_config['transfer_optimization']\n",
    "print(f\"\\nüöÄ Transfer Optimization:\")\n",
    "print(f\"  Parallel streams: {transfer_opts['parallel_streams']}\")\n",
    "print(f\"  Chunk size: {transfer_opts['chunk_size_mb']} MB\")\n",
    "print(f\"  Buffer size: {transfer_opts['buffer_size_mb']} MB\")\n",
    "print(f\"  Compression: {transfer_opts['compression']}\")\n",
    "print(f\"  Checksum: {transfer_opts['checksum_algorithm']}\")\n",
    "\n",
    "storage_opts = hpc_performance_config['storage_optimization']\n",
    "print(f\"\\nüíæ Storage Optimization:\")\n",
    "print(f\"  Lustre stripe count: {storage_opts['lustre_stripe_count']}\")\n",
    "print(f\"  Lustre stripe size: {storage_opts['lustre_stripe_size_mb']} MB\")\n",
    "print(f\"  GPFS block size: {storage_opts['gpfs_block_size_kb']} KB\")\n",
    "print(f\"  Async I/O: {storage_opts['async_io']}\")\n",
    "print(f\"  Direct I/O: {storage_opts['direct_io']}\")\n",
    "\n",
    "queue_opts = hpc_performance_config['queue_management']\n",
    "print(f\"\\nüîÑ Queue Management:\")\n",
    "print(f\"  Max concurrent transfers: {queue_opts['max_concurrent_transfers']}\")\n",
    "print(f\"  Priority queues: {', '.join(queue_opts['priority_queues'])}\")\n",
    "print(f\"  Load balancing: {queue_opts['load_balancing']}\")\n",
    "\n",
    "print(f\"\\nüìä Resource Allocation by Priority:\")\n",
    "for priority, allocation in queue_opts['resource_allocation'].items():\n",
    "    print(f\"  {priority.title()}: {allocation['bandwidth_percent']}% bandwidth, {allocation['max_jobs']} max jobs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Large-Scale Archive Management\n",
    "\n",
    "Demonstrate management of massive datasets typical in HPC climate simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create archives for different data categories\n",
    "def create_hpc_archives():\n",
    "    \"\"\"Create categorized archives for HPC simulation output.\"\"\"\n",
    "    \n",
    "    archive_categories = [\n",
    "        {\n",
    "            \"id\": \"cesm2-hr-extremes-atm-monthly-1979-1989\",\n",
    "            \"location\": \"cheyenne-hpss\",\n",
    "            \"type\": \"compressed\",\n",
    "            \"description\": \"Atmospheric monthly output 1979-1989 (decade 1)\",\n",
    "            \"size_tb\": 12.3,\n",
    "            \"file_count\": 132,\n",
    "            \"variables\": [\"PRECT\", \"TREFHT\", \"PSL\", \"U850\", \"V850\", \"Z500\"],\n",
    "            \"compression_ratio\": 0.35\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"cesm2-hr-extremes-atm-daily-1979-1989\",\n",
    "            \"location\": \"cheyenne-hpss\",\n",
    "            \"type\": \"compressed\",\n",
    "            \"description\": \"Atmospheric daily output 1979-1989 (decade 1)\",\n",
    "            \"size_tb\": 45.7,\n",
    "            \"file_count\": 4018,\n",
    "            \"variables\": [\"PRECT\", \"TREFHTMX\", \"TREFHTMN\", \"CAPE\", \"CIN\"],\n",
    "            \"compression_ratio\": 0.42\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"cesm2-hr-extremes-ocn-monthly-1979-1989\",\n",
    "            \"location\": \"cheyenne-hpss\",\n",
    "            \"type\": \"compressed\", \n",
    "            \"description\": \"Ocean monthly output 1979-1989 (decade 1)\",\n",
    "            \"size_tb\": 18.9,\n",
    "            \"file_count\": 132,\n",
    "            \"variables\": [\"TEMP\", \"SALT\", \"SSH\", \"UVEL\", \"VVEL\", \"WVEL\"],\n",
    "            \"compression_ratio\": 0.28\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"cesm2-hr-extremes-restart-files-1989\",\n",
    "            \"location\": \"cheyenne-hpss\",\n",
    "            \"type\": \"compressed\",\n",
    "            \"description\": \"Model restart files for continuation runs\",\n",
    "            \"size_tb\": 2.1,\n",
    "            \"file_count\": 48,\n",
    "            \"variables\": [\"all_prognostic_variables\"],\n",
    "            \"compression_ratio\": 0.18,\n",
    "            \"critical\": True\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return archive_categories\n",
    "\n",
    "# Create the archive metadata\n",
    "archive_specs = create_hpc_archives()\n",
    "created_archives = []\n",
    "\n",
    "print(\"üì¶ Creating Large-Scale HPC Archives\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "for spec in archive_specs:\n",
    "    archive_dto = CreateArchiveDto(\n",
    "        archive_id=spec['id'],\n",
    "        location_name=spec['location'],\n",
    "        archive_type=spec['type'],\n",
    "        simulation_id=hires_sim.simulation_id,\n",
    "        version=\"1.0\",\n",
    "        description=spec['description'],\n",
    "        tags={\n",
    "            \"cesm2\", \"high_resolution\", \"climate_extremes\", \n",
    "            \"hpc\", \"decade_1\", \"production\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Create archive metadata (in real scenario, would create actual archive)\n",
    "    archive_result = archive_service.create_archive_metadata(archive_dto)\n",
    "    created_archives.append((archive_result, spec))\n",
    "    \n",
    "    print(f\"‚úì {archive_result.archive_id}\")\n",
    "    print(f\"  Size: {spec['size_tb']} TB ({spec['file_count']} files)\")\n",
    "    print(f\"  Compression: {spec['compression_ratio']:.0%}\")\n",
    "    print(f\"  Variables: {len(spec['variables'])} variables\")\n",
    "    if spec.get('critical'):\n",
    "        print(f\"  ‚ö†Ô∏è  Critical for restart capability\")\n",
    "    print()\n",
    "\n",
    "# Calculate totals\n",
    "total_size = sum(spec['size_tb'] for spec in archive_specs)\n",
    "total_files = sum(spec['file_count'] for spec in archive_specs)\n",
    "avg_compression = sum(spec['compression_ratio'] for spec in archive_specs) / len(archive_specs)\n",
    "\n",
    "print(f\"üìä Archive Summary:\")\n",
    "print(f\"  Total archives: {len(archive_specs)}\")\n",
    "print(f\"  Total size: {total_size:.1f} TB\")\n",
    "print(f\"  Total files: {total_files:,}\")\n",
    "print(f\"  Average compression: {avg_compression:.0%}\")\n",
    "print(f\"  Storage location: NCAR HPSS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Post-Processing Workflow Automation\n",
    "\n",
    "Implement automated post-processing workflows that run alongside the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define post-processing workflow stages\n",
    "def create_post_processing_workflow():\n",
    "    \"\"\"Create comprehensive post-processing workflow for HPC climate data.\"\"\"\n",
    "    \n",
    "    workflow_stages = {\n",
    "        \"immediate_qc\": {\n",
    "            \"name\": \"Immediate Quality Control\",\n",
    "            \"trigger\": \"New files detected (hourly scan)\",\n",
    "            \"location\": \"cheyenne-scratch\",\n",
    "            \"processing\": [\n",
    "                \"File integrity checks (header validation, dimension consistency)\",\n",
    "                \"Basic statistics (min/max/mean for key variables)\",\n",
    "                \"Missing data detection and flagging\",\n",
    "                \"Metadata extraction and cataloging\"\n",
    "            ],\n",
    "            \"compute_requirements\": \"1 node, 36 cores, 2 hours\",\n",
    "            \"priority\": \"high\",\n",
    "            \"failure_action\": \"alert_immediately\"\n",
    "        },\n",
    "        \"daily_diagnostics\": {\n",
    "            \"name\": \"Daily Diagnostic Processing\",\n",
    "            \"trigger\": \"Daily at 06:00 UTC\",\n",
    "            \"location\": \"cheyenne-work\",\n",
    "            \"processing\": [\n",
    "                \"Time series validation and gap detection\",\n",
    "                \"Energy and water cycle diagnostics\",\n",
    "                \"Extreme event detection and cataloging\",\n",
    "                \"Basic visualization (time series, maps)\",\n",
    "                \"Comparison with observational climatology\"\n",
    "            ],\n",
    "            \"compute_requirements\": \"4 nodes, 144 cores, 4 hours\",\n",
    "            \"priority\": \"medium\",\n",
    "            \"products\": [\"diagnostic_plots\", \"statistics_tables\", \"event_catalogs\"]\n",
    "        },\n",
    "        \"weekly_analysis\": {\n",
    "            \"name\": \"Weekly Comprehensive Analysis\", \n",
    "            \"trigger\": \"Weekly on Sundays at 00:00 UTC\",\n",
    "            \"location\": \"analysis-cluster\",\n",
    "            \"processing\": [\n",
    "                \"Regional climate analysis and trends\",\n",
    "                \"Extreme event statistics and return periods\",\n",
    "                \"Model bias assessment vs observations\",\n",
    "                \"Teleconnection pattern analysis\",\n",
    "                \"High-resolution figure generation\"\n",
    "            ],\n",
    "            \"compute_requirements\": \"8 nodes, 1024 cores, 12 hours\",\n",
    "            \"priority\": \"low\",\n",
    "            \"products\": [\"analysis_reports\", \"publication_figures\", \"data_summaries\"]\n",
    "        },\n",
    "        \"archive_preparation\": {\n",
    "            \"name\": \"Archive Preparation and Validation\",\n",
    "            \"trigger\": \"Monthly or when simulation completes decade\",\n",
    "            \"location\": \"cheyenne-work\",\n",
    "            \"processing\": [\n",
    "                \"Data format standardization (CF compliance)\",\n",
    "                \"Metadata enrichment and documentation\",\n",
    "                \"Compression optimization testing\",\n",
    "                \"Archive integrity pre-verification\",\n",
    "                \"Documentation package creation\"\n",
    "            ],\n",
    "            \"compute_requirements\": \"2 nodes, 72 cores, 8 hours\",\n",
    "            \"priority\": \"medium\",\n",
    "            \"products\": [\"standardized_data\", \"metadata_catalogs\", \"documentation\"]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return workflow_stages\n",
    "\n",
    "# Create and display workflow\n",
    "post_process_workflow = create_post_processing_workflow()\n",
    "print(\"üîÑ HPC Post-Processing Workflow\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for stage_id, stage_info in post_process_workflow.items():\n",
    "    print(f\"\\nüîß {stage_info['name'].upper()}\")\n",
    "    print(f\"   Trigger: {stage_info['trigger']}\")\n",
    "    print(f\"   Location: {stage_info['location']}\")\n",
    "    print(f\"   Compute: {stage_info['compute_requirements']}\")\n",
    "    print(f\"   Priority: {stage_info['priority']}\")\n",
    "    \n",
    "    print(f\"   Processing Steps:\")\n",
    "    for i, step in enumerate(stage_info['processing'], 1):\n",
    "        print(f\"     {i}. {step}\")\n",
    "    \n",
    "    if 'products' in stage_info:\n",
    "        print(f\"   Products: {', '.join(stage_info['products'])}\")\n",
    "    \n",
    "    if 'failure_action' in stage_info:\n",
    "        print(f\"   ‚ö†Ô∏è  Failure Action: {stage_info['failure_action']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create workflow orchestration system\n",
    "def create_workflow_orchestration():\n",
    "    \"\"\"Create workflow orchestration configuration.\"\"\"\n",
    "    \n",
    "    orchestration_config = {\n",
    "        \"scheduler\": \"pbs_professional\",\n",
    "        \"workflow_engine\": \"snakemake\", \n",
    "        \"dependency_management\": \"conda\",\n",
    "        \"resource_manager\": \"pbspro\",\n",
    "        \"job_submission\": {\n",
    "            \"queue_mapping\": {\n",
    "                \"high\": \"premium\",\n",
    "                \"medium\": \"regular\", \n",
    "                \"low\": \"economy\"\n",
    "            },\n",
    "            \"walltime_limits\": {\n",
    "                \"immediate_qc\": \"02:00:00\",\n",
    "                \"daily_diagnostics\": \"04:00:00\",\n",
    "                \"weekly_analysis\": \"12:00:00\",\n",
    "                \"archive_preparation\": \"08:00:00\"\n",
    "            },\n",
    "            \"account\": \"P93300641\",\n",
    "            \"project\": \"climate-extremes-hpc\"\n",
    "        },\n",
    "        \"monitoring\": {\n",
    "            \"job_status_check_interval\": \"5_minutes\",\n",
    "            \"log_aggregation\": \"centralized\",\n",
    "            \"alert_channels\": [\"email\", \"slack\", \"dashboard\"],\n",
    "            \"metrics_retention_days\": 90\n",
    "        },\n",
    "        \"error_handling\": {\n",
    "            \"retry_attempts\": 3,\n",
    "            \"retry_delay_minutes\": [5, 15, 30],\n",
    "            \"escalation_after_failures\": 3,\n",
    "            \"automatic_cleanup\": True,\n",
    "            \"preserve_failed_job_data\": True\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return orchestration_config\n",
    "\n",
    "# Display orchestration configuration\n",
    "orchestration = create_workflow_orchestration()\n",
    "print(\"\\nüéº Workflow Orchestration Configuration\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "print(f\"Scheduler: {orchestration['scheduler']}\")\n",
    "print(f\"Workflow Engine: {orchestration['workflow_engine']}\")\n",
    "print(f\"Resource Manager: {orchestration['resource_manager']}\")\n",
    "print(f\"Account: {orchestration['job_submission']['account']}\")\n",
    "\n",
    "print(\"\\nüìã Queue Mappings:\")\n",
    "for priority, queue in orchestration['job_submission']['queue_mapping'].items():\n",
    "    print(f\"  {priority.title()} priority ‚Üí {queue} queue\")\n",
    "\n",
    "print(\"\\n‚è±Ô∏è  Walltime Limits:\")\n",
    "for workflow, walltime in orchestration['job_submission']['walltime_limits'].items():\n",
    "    print(f\"  {workflow.replace('_', ' ').title()}: {walltime}\")\n",
    "\n",
    "print(\"\\nüö® Error Handling:\")\n",
    "error_config = orchestration['error_handling']\n",
    "print(f\"  Retry attempts: {error_config['retry_attempts']}\")\n",
    "print(f\"  Retry delays: {', '.join(map(str, error_config['retry_delay_minutes']))} minutes\")\n",
    "print(f\"  Escalation threshold: {error_config['escalation_after_failures']} failures\")\n",
    "print(f\"  Automatic cleanup: {error_config['automatic_cleanup']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. HPC Performance Monitoring and Optimization\n",
    "\n",
    "Implement comprehensive monitoring for HPC-scale operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HPC performance monitoring dashboard\n",
    "def create_hpc_monitoring_dashboard():\n",
    "    \"\"\"Create real-time HPC performance monitoring dashboard.\"\"\"\n",
    "    \n",
    "    dashboard_data = {\n",
    "        \"timestamp\": \"2024-06-15T14:30:00Z\",\n",
    "        \"simulation_status\": {\n",
    "            \"job_id\": job_status['job_id'],\n",
    "            \"status\": \"RUNNING\",\n",
    "            \"model_date\": \"1985-03-15\",\n",
    "            \"progress_percent\": 15.2,\n",
    "            \"estimated_completion\": \"2024-07-28T16:45:00Z\",\n",
    "            \"walltime_remaining\": \"08:14:37\",\n",
    "            \"cpu_efficiency_percent\": 92.3\n",
    "        },\n",
    "        \"storage_performance\": {\n",
    "            \"scratch_usage\": {\n",
    "                \"used_tb\": 18.5,\n",
    "                \"quota_tb\": 50.0,\n",
    "                \"files_count\": 15420,\n",
    "                \"io_rate_gb_per_sec\": 4.2,\n",
    "                \"iops\": 1250\n",
    "            },\n",
    "            \"work_storage\": {\n",
    "                \"used_tb\": 6.8,\n",
    "                \"quota_tb\": 20.0,\n",
    "                \"files_count\": 3420,\n",
    "                \"transfer_rate_mb_per_sec\": 850\n",
    "            },\n",
    "            \"hpss_archive\": {\n",
    "                \"queued_tb\": 2.1,\n",
    "                \"archived_tb\": 45.3,\n",
    "                \"retrieval_requests\": 0,\n",
    "                \"average_retrieval_time_hours\": 0.75\n",
    "            }\n",
    "        },\n",
    "        \"transfer_operations\": {\n",
    "            \"active_transfers\": 12,\n",
    "            \"queued_transfers\": 28,\n",
    "            \"completed_today\": 156,\n",
    "            \"failed_today\": 3,\n",
    "            \"total_throughput_mb_per_sec\": 1420,\n",
    "            \"average_queue_wait_minutes\": 4.2\n",
    "        },\n",
    "        \"workflow_status\": {\n",
    "            \"immediate_qc\": {\"status\": \"running\", \"success_rate_24h\": 98.5},\n",
    "            \"daily_diagnostics\": {\"status\": \"completed\", \"last_run\": \"06:00:00Z\"},\n",
    "            \"weekly_analysis\": {\"status\": \"scheduled\", \"next_run\": \"2024-06-16T00:00:00Z\"},\n",
    "            \"archive_preparation\": {\"status\": \"idle\", \"last_run\": \"2024-06-01T12:00:00Z\"}\n",
    "        },\n",
    "        \"resource_utilization\": {\n",
    "            \"compute_nodes_active\": 2048,\n",
    "            \"cpu_utilization_percent\": 94.2,\n",
    "            \"memory_utilization_percent\": 78.5,\n",
    "            \"network_utilization_percent\": 65.3,\n",
    "            \"power_consumption_kw\": 1250,\n",
    "            \"cooling_efficiency\": \"optimal\"\n",
    "        },\n",
    "        \"alerts_warnings\": [\n",
    "            {\"level\": \"warning\", \"message\": \"Scratch storage 37% full - approaching cleanup threshold\"},\n",
    "            {\"level\": \"info\", \"message\": \"3 transfer operations failed due to network timeout - auto-retrying\"},\n",
    "            {\"level\": \"info\", \"message\": \"Weekly analysis workflow scheduled for tomorrow 00:00 UTC\"}\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return dashboard_data\n",
    "\n",
    "# Generate and display monitoring dashboard\n",
    "dashboard = create_hpc_monitoring_dashboard()\n",
    "print(\"üñ•Ô∏è  HPC Performance Monitoring Dashboard\")\n",
    "print(f\"Last Updated: {dashboard['timestamp']}\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Simulation status\n",
    "sim_status = dashboard['simulation_status']\n",
    "print(f\"\\nüî¨ Simulation Status (Job {sim_status['job_id']}):\")\n",
    "print(f\"  Status: {sim_status['status']}\")\n",
    "print(f\"  Progress: {sim_status['progress_percent']:.1f}% (Model date: {sim_status['model_date']})\")\n",
    "print(f\"  Walltime remaining: {sim_status['walltime_remaining']}\")\n",
    "print(f\"  CPU efficiency: {sim_status['cpu_efficiency_percent']}%\")\n",
    "print(f\"  Est. completion: {sim_status['estimated_completion']}\")\n",
    "\n",
    "# Storage performance\n",
    "storage = dashboard['storage_performance']\n",
    "print(f\"\\nüíæ Storage Performance:\")\n",
    "scratch = storage['scratch_usage']\n",
    "print(f\"  Scratch: {scratch['used_tb']:.1f}/{scratch['quota_tb']:.0f} TB ({scratch['used_tb']/scratch['quota_tb']*100:.0f}% full)\")\n",
    "print(f\"    I/O Rate: {scratch['io_rate_gb_per_sec']:.1f} GB/s, IOPS: {scratch['iops']}\")\n",
    "work = storage['work_storage']\n",
    "print(f\"  Work: {work['used_tb']:.1f}/{work['quota_tb']:.0f} TB ({work['used_tb']/work['quota_tb']*100:.0f}% full)\")\n",
    "hpss = storage['hpss_archive']\n",
    "print(f\"  HPSS: {hpss['archived_tb']:.1f} TB archived, {hpss['queued_tb']:.1f} TB queued\")\n",
    "\n",
    "# Transfer operations\n",
    "transfers = dashboard['transfer_operations']\n",
    "print(f\"\\nüîÑ Transfer Operations:\")\n",
    "print(f\"  Active: {transfers['active_transfers']}, Queued: {transfers['queued_transfers']}\")\n",
    "print(f\"  Today: {transfers['completed_today']} completed, {transfers['failed_today']} failed\")\n",
    "print(f\"  Throughput: {transfers['total_throughput_mb_per_sec']} MB/s\")\n",
    "print(f\"  Avg queue wait: {transfers['average_queue_wait_minutes']:.1f} min\")\n",
    "\n",
    "# Workflow status\n",
    "workflows = dashboard['workflow_status']\n",
    "print(f\"\\nüîß Workflow Status:\")\n",
    "for workflow, status in workflows.items():\n",
    "    name = workflow.replace('_', ' ').title()\n",
    "    print(f\"  {name}: {status['status'].upper()}\")\n",
    "    if 'success_rate_24h' in status:\n",
    "        print(f\"    24h success rate: {status['success_rate_24h']}%\")\n",
    "    if 'last_run' in status:\n",
    "        print(f\"    Last run: {status['last_run']}\")\n",
    "    if 'next_run' in status:\n",
    "        print(f\"    Next run: {status['next_run']}\")\n",
    "\n",
    "# Alerts\n",
    "alerts = dashboard['alerts_warnings']\n",
    "if alerts:\n",
    "    print(f\"\\nüö® Alerts & Warnings:\")\n",
    "    for alert in alerts:\n",
    "        level_icon = \"‚ö†Ô∏è\" if alert['level'] == 'warning' else \"‚ÑπÔ∏è\"\n",
    "        print(f\"  {level_icon} {alert['level'].upper()}: {alert['message']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated comprehensive HPC integration capabilities with Tellus for large-scale climate simulations:\n",
    "\n",
    "### Key Accomplishments:\n",
    "\n",
    "1. **HPC Environment Setup**: Configured multi-tier storage hierarchy (scratch/work/campaign/HPSS)\n",
    "2. **Large-Scale Simulation Management**: Set up high-resolution CESM2 simulation with 85+ TB output\n",
    "3. **Job Integration**: Demonstrated PBS/SLURM integration with real-time monitoring\n",
    "4. **Automated Data Movement**: Implemented smart data workflows through storage tiers\n",
    "5. **Performance Optimization**: Configured HPC-specific optimizations for maximum throughput\n",
    "6. **Archive Management**: Created categorized archives for massive climate datasets\n",
    "7. **Workflow Automation**: Established automated post-processing pipelines\n",
    "8. **Comprehensive Monitoring**: Real-time performance monitoring and alerting\n",
    "\n",
    "### HPC-Specific Benefits:\n",
    "\n",
    "- **Scalability**: Handles multi-terabyte datasets with thousands of files efficiently\n",
    "- **Integration**: Seamless integration with HPC job schedulers and resource managers\n",
    "- **Performance**: Optimized for high-throughput parallel I/O and network transfers\n",
    "- **Automation**: Reduces manual intervention in complex multi-stage workflows\n",
    "- **Monitoring**: Real-time visibility into resource utilization and job progress\n",
    "- **Reliability**: Robust error handling and automatic retry mechanisms\n",
    "\n",
    "### Performance Highlights:\n",
    "\n",
    "- **Compute Scale**: 2,048 nodes with 73,728 cores\n",
    "- **Data Volume**: 85+ TB expected output over 42-day simulation\n",
    "- **I/O Throughput**: Up to 4.2 GB/s sustained write rates\n",
    "- **Transfer Performance**: 1.4+ GB/s aggregate transfer throughput\n",
    "- **Archive Efficiency**: 35% average compression ratio\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Scale to even larger simulations (1M+ cores, 100+ TB datasets)\n",
    "- Implement machine learning-based performance prediction\n",
    "- Add cross-facility data federation capabilities\n",
    "- Develop specialized Earth System Model workflow templates\n",
    "- Integrate with emerging HPC technologies (quantum, neuromorphic)\n",
    "\n",
    "This HPC integration demonstrates Tellus's capability to manage enterprise-scale Earth System Model research while maintaining ease of use and comprehensive monitoring."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n",
   "language": "python", \n",
   "name": "python3"\n  },\n  "language_info": {\n   "codemirror_mode": {\n    "name": "ipython",\n    "version": 3\n   },\n   "file_extension": ".py",\n   "mimetype": "text/x-python",\n   "name": "python",\n   "nbconvert_exporter": "python",\n   "pygments_lexer": "ipython3",\n   "version": "3.9.0"\n  }\n },\n "nbformat": 4,\n "nbformat_minor": 4\n}
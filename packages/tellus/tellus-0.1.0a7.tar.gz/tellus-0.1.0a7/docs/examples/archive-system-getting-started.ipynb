{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tellus Archive System - Getting Started\n",
    "\n",
    "This notebook demonstrates the basic functionality of Tellus's new archive system, which provides intelligent caching, tag-based file organization, and seamless integration with different storage locations.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The archive system allows you to:\n",
    "- **Cache archives locally** for fast repeated access\n",
    "- **Tag files automatically** based on path patterns\n",
    "- **Extract files selectively** by tags or patterns\n",
    "- **Work with multiple archives** through a unified interface\n",
    "- **Support different storage locations** (local, FTP, tape systems, etc.)\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's import the necessary modules and create a sample archive for demonstration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Import Tellus archive system components\n",
    "from tellus.simulation.simulation import (\n",
    "    CacheManager, CacheConfig, TagSystem, PathMapper, PathMapping,\n",
    "    ArchiveManifest, CompressedArchive, ArchiveRegistry,\n",
    "    CLIProgressCallback\n",
    ")\n",
    "\n",
    "print(\"✓ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Sample Archive\n",
    "\n",
    "Let's create a sample simulation archive to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_archive(archive_path: Path) -> Path:\n",
    "    \"\"\"Create a sample simulation archive for demonstration\"\"\"\n",
    "    \n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        temp_path = Path(temp_dir)\n",
    "        \n",
    "        # Create realistic simulation file structure\n",
    "        files_to_create = {\n",
    "            \"input/forcing_data.nc\": b\"Sample atmospheric forcing data\",\n",
    "            \"input/initial_conditions.nc\": b\"Sample initial conditions\",\n",
    "            \"input/boundary_conditions.nc\": b\"Sample boundary conditions\",\n",
    "            \"scripts/run_model.sh\": b\"#!/bin/bash\\necho 'Running climate model'\\n\",\n",
    "            \"scripts/postprocess.py\": b\"#!/usr/bin/env python3\\nprint('Post-processing results')\\n\",\n",
    "            \"output/temperature_2023.nc\": b\"Sample temperature output data\",\n",
    "            \"output/precipitation_2023.nc\": b\"Sample precipitation output\",\n",
    "            \"output/diagnostics.nc\": b\"Model diagnostic information\",\n",
    "            \"namelists/model.nml\": b\"&model_params\\n  dt = 3600\\n  output_freq = 24\\n/\\n\",\n",
    "            \"namelists/postproc.cfg\": b\"[settings]\\noutput_format = netcdf4\\n\",\n",
    "            \"docs/README.md\": b\"# Climate Model Simulation\\n\\nThis archive contains...\",\n",
    "        }\n",
    "        \n",
    "        # Create files\n",
    "        for file_path, content in files_to_create.items():\n",
    "            full_path = temp_path / file_path\n",
    "            full_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            full_path.write_bytes(content)\n",
    "        \n",
    "        # Create metadata\n",
    "        metadata = {\n",
    "            \"simulation_id\": \"climate_model_v1\",\n",
    "            \"model\": \"ECMWF-IFS\",\n",
    "            \"resolution\": \"T127\",\n",
    "            \"period\": \"2023-01-01 to 2023-12-31\",\n",
    "            \"created\": \"2024-01-15\",\n",
    "            \"description\": \"Sample climate model simulation for archive system demo\"\n",
    "        }\n",
    "        \n",
    "        metadata_file = temp_path / \"simulation_metadata.json\"\n",
    "        metadata_file.write_text(json.dumps(metadata, indent=2))\n",
    "        \n",
    "        # Create the archive\n",
    "        archive_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with tarfile.open(archive_path, \"w:gz\") as tar:\n",
    "            # Add files individually to maintain clean paths\n",
    "            for file_path, content in files_to_create.items():\n",
    "                full_path = temp_path / file_path\n",
    "                tar.add(full_path, arcname=file_path)\n",
    "            tar.add(metadata_file, arcname=\"simulation_metadata.json\")\n",
    "    \n",
    "    print(f\"✓ Created sample archive: {archive_path}\")\n",
    "    print(f\"  Size: {archive_path.stat().st_size} bytes\")\n",
    "    print(f\"  Files: {len(files_to_create) + 1}\")\n",
    "    return archive_path\n",
    "\n",
    "# Create a sample archive in a temporary location\n",
    "temp_dir = Path(tempfile.mkdtemp())\n",
    "sample_archive = create_sample_archive(temp_dir / \"climate_simulation.tar.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Archive Operations\n",
    "\n",
    "### 1. Creating an Archive Instance\n",
    "\n",
    "Let's create a `CompressedArchive` instance and explore its basic functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up caching\n",
    "cache_config = CacheConfig(\n",
    "    cache_dir=temp_dir / \"cache\",\n",
    "    archive_cache_size_limit=100 * 1024**2,  # 100MB\n",
    "    file_cache_size_limit=50 * 1024**2       # 50MB\n",
    ")\n",
    "cache_manager = CacheManager(cache_config)\n",
    "\n",
    "# Create archive instance\n",
    "archive = CompressedArchive(\n",
    "    archive_id=\"climate_demo\",\n",
    "    archive_location=str(sample_archive),\n",
    "    cache_manager=cache_manager\n",
    ")\n",
    "\n",
    "# Add progress tracking\n",
    "progress = CLIProgressCallback(verbose=True)\n",
    "archive.add_progress_callback(progress)\n",
    "\n",
    "print(\"✓ Archive instance created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Exploring Archive Status\n",
    "\n",
    "Let's check the archive status and see what information is available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get archive status\n",
    "status = archive.status()\n",
    "\n",
    "print(\"Archive Status:\")\n",
    "print(\"=\" * 50)\n",
    "for key, value in status.items():\n",
    "    if key == 'size' and isinstance(value, int):\n",
    "        # Format size in human-readable format\n",
    "        if value < 1024:\n",
    "            size_str = f\"{value} B\"\n",
    "        elif value < 1024**2:\n",
    "            size_str = f\"{value/1024:.1f} KB\"\n",
    "        else:\n",
    "            size_str = f\"{value/1024**2:.1f} MB\"\n",
    "        print(f\"  {key}: {size_str}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Listing Files with Automatic Tagging\n",
    "\n",
    "The archive system automatically tags files based on their paths. Let's see what files are in our archive and how they've been tagged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all files in the archive\n",
    "files = archive.list_files()\n",
    "\n",
    "print(\"Files in Archive (with automatic tags):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Group files by their primary tag\n",
    "by_tag = {}\n",
    "for file_path, tagged_file in files.items():\n",
    "    for tag in tagged_file.tags:\n",
    "        if tag not in by_tag:\n",
    "            by_tag[tag] = []\n",
    "        by_tag[tag].append((file_path, tagged_file))\n",
    "\n",
    "# Display files grouped by tag\n",
    "for tag in sorted(by_tag.keys()):\n",
    "    print(f\"\\n📁 {tag.upper()} ({len(by_tag[tag])} files):\")\n",
    "    for file_path, tagged_file in by_tag[tag]:\n",
    "        tags_str = \", \".join(sorted(tagged_file.tags))\n",
    "        size_kb = tagged_file.size / 1024 if tagged_file.size > 1024 else tagged_file.size\n",
    "        unit = \"KB\" if tagged_file.size > 1024 else \"B\"\n",
    "        print(f\"  • {file_path} ({size_kb:.1f} {unit}) [{tags_str}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Extracting Files by Tags\n",
    "\n",
    "One of the most powerful features is the ability to extract files by their tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all input files\n",
    "input_files = archive.get_files_by_tags(\"input\")\n",
    "print(f\"Input files found: {len(input_files)}\")\n",
    "for file_path in input_files:\n",
    "    print(f\"  • {file_path}\")\n",
    "\n",
    "# Extract them to a directory\n",
    "extract_dir = temp_dir / \"extracted\" / \"input_only\"\n",
    "extracted_paths = archive.extract_files_by_tags(extract_dir, \"input\")\n",
    "\n",
    "print(f\"\\n✓ Extracted {len(extracted_paths)} input files to {extract_dir}\")\n",
    "for path in extracted_paths:\n",
    "    print(f\"  → {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Extracting Individual Files\n",
    "\n",
    "You can also extract specific files by name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a specific script file\n",
    "script_path = archive.extract_file(\n",
    "    \"scripts/run_model.sh\", \n",
    "    temp_dir / \"extracted\" / \"scripts\"\n",
    ")\n",
    "\n",
    "print(f\"✓ Extracted script to: {script_path}\")\n",
    "print(\"\\nScript content:\")\n",
    "print(\"-\" * 30)\n",
    "print(script_path.read_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Cache Performance\n",
    "\n",
    "The archive system caches both archives and individual files for performance. Let's see the cache statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cache statistics\n",
    "cache_stats = cache_manager.get_cache_stats()\n",
    "\n",
    "print(\"Cache Statistics:\")\n",
    "print(\"=\" * 40)\n",
    "for key, value in cache_stats.items():\n",
    "    if 'size' in key and isinstance(value, int):\n",
    "        if value < 1024:\n",
    "            size_str = f\"{value} B\"\n",
    "        elif value < 1024**2:\n",
    "            size_str = f\"{value/1024:.1f} KB\"\n",
    "        else:\n",
    "            size_str = f\"{value/1024**2:.1f} MB\"\n",
    "        print(f\"  {key}: {size_str}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Now let's access the same files again - should be faster from cache\n",
    "print(\"\\nAccessing cached files (should be faster):\")\n",
    "input_files_cached = archive.get_files_by_tags(\"input\")\n",
    "print(f\"✓ Retrieved {len(input_files_cached)} input files from cache\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Multiple Archives\n",
    "\n",
    "The `ArchiveRegistry` allows you to work with multiple archives as a unified collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a second archive for demonstration\n",
    "def create_second_archive(archive_path: Path) -> Path:\n",
    "    \"\"\"Create a second archive with different content\"\"\"\n",
    "    \n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        temp_path = Path(temp_dir)\n",
    "        \n",
    "        files_to_create = {\n",
    "            \"analysis/statistics.nc\": b\"Statistical analysis results\",\n",
    "            \"analysis/trends.nc\": b\"Climate trend analysis\",\n",
    "            \"plots/temperature_map.png\": b\"Fake PNG data\",\n",
    "            \"plots/precipitation_timeseries.png\": b\"Fake PNG data\",\n",
    "            \"reports/summary.pdf\": b\"Fake PDF report data\",\n",
    "            \"scripts/analyze.py\": b\"#!/usr/bin/env python3\\nprint('Analyzing data')\\n\",\n",
    "        }\n",
    "        \n",
    "        for file_path, content in files_to_create.items():\n",
    "            full_path = temp_path / file_path\n",
    "            full_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            full_path.write_bytes(content)\n",
    "        \n",
    "        archive_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with tarfile.open(archive_path, \"w:gz\") as tar:\n",
    "            for file_path, content in files_to_create.items():\n",
    "                full_path = temp_path / file_path\n",
    "                tar.add(full_path, arcname=file_path)\n",
    "    \n",
    "    return archive_path\n",
    "\n",
    "# Create second archive\n",
    "second_archive = create_second_archive(temp_dir / \"analysis_results.tar.gz\")\n",
    "print(f\"✓ Created second archive: {second_archive}\")\n",
    "\n",
    "# Create archive registry\n",
    "registry = ArchiveRegistry(\n",
    "    simulation_id=\"climate_demo\",\n",
    "    cache_manager=cache_manager\n",
    ")\n",
    "\n",
    "# Add both archives\n",
    "archive2 = CompressedArchive(\n",
    "    archive_id=\"analysis_demo\",\n",
    "    archive_location=str(second_archive)\n",
    ")\n",
    "\n",
    "registry.add_archive(archive, \"simulation_data\")\n",
    "registry.add_archive(archive2, \"analysis_results\")\n",
    "\n",
    "print(f\"\\n✓ Registry contains {len(registry.list_archives())} archives:\")\n",
    "for name in registry.list_archives():\n",
    "    print(f\"  • {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smart File Resolution\n",
    "\n",
    "The registry can find files across all archives and choose the best source:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all script files across archives\n",
    "all_scripts = []\n",
    "for archive_name in registry.list_archives():\n",
    "    archive_obj = registry.get_archive(archive_name)\n",
    "    script_files = archive_obj.get_files_by_tags(\"scripts\")\n",
    "    for script_file in script_files:\n",
    "        all_scripts.append((archive_name, script_file))\n",
    "\n",
    "print(\"Script files across all archives:\")\n",
    "print(\"=\" * 45)\n",
    "for archive_name, script_file in all_scripts:\n",
    "    print(f\"  📜 {script_file} (from {archive_name})\")\n",
    "\n",
    "# Extract files by tags from all archives\n",
    "extract_dir_all = temp_dir / \"extracted\" / \"all_scripts\"\n",
    "results = registry.extract_files_by_tags(extract_dir_all, \"scripts\")\n",
    "\n",
    "print(f\"\\n✓ Extracted script files from all archives:\")\n",
    "for archive_name, extracted_files in results.items():\n",
    "    print(f\"  From {archive_name}: {len(extracted_files)} files\")\n",
    "    for file_path in extracted_files:\n",
    "        print(f\"    → {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've demonstrated the core functionality of the Tellus archive system:\n",
    "\n",
    "✅ **Automatic file tagging** based on directory structure  \n",
    "✅ **Selective file extraction** by tags or file names  \n",
    "✅ **Intelligent caching** for performance optimization  \n",
    "✅ **Multi-archive management** through registries  \n",
    "✅ **Progress tracking** for long-running operations  \n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **Advanced Features**: Check out the advanced notebook for location integration, custom tagging, and path mapping\n",
    "- **CLI Usage**: See the CLI examples notebook for command-line operations\n",
    "- **Production Use**: The system is designed to work with tape systems, FTP servers, and other remote storage\n",
    "\n",
    "## Cleanup\n",
    "\n",
    "Let's clean up the temporary files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Clean up temporary directory\n",
    "shutil.rmtree(temp_dir)\n",
    "print(f\"✓ Cleaned up temporary files from {temp_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: Smart File Classification and Selective Archiving\n",
    "\n",
    "**Learning Goals:** Master how Tellus automatically classifies Earth Science files and create targeted archives for specific use cases.\n",
    "\n",
    "**Time Estimate:** 25 minutes\n",
    "\n",
    "**Prerequisites:** Tutorial 1 completed\n",
    "\n",
    "## The File Classification Challenge\n",
    "\n",
    "Imagine you're managing a complex CESM simulation with hundreds of files. Without classification, finding what you need is like searching through a messy closet:\n",
    "\n",
    "```\n",
    "‚ùå Unorganized:\n",
    "output/\n",
    "‚îú‚îÄ‚îÄ cam.h0.2024-01.nc       # Atmosphere output - IMPORTANT  \n",
    "‚îú‚îÄ‚îÄ cam.log                 # Log file - OPTIONAL\n",
    "‚îú‚îÄ‚îÄ cam.r.2024-04-01.nc     # Restart file - CRITICAL\n",
    "‚îú‚îÄ‚îÄ user_nl_cam             # Configuration - CRITICAL\n",
    "‚îú‚îÄ‚îÄ temp_processing.nc      # Temporary file - TEMPORARY\n",
    "‚îî‚îÄ‚îÄ analysis_script.py      # Analysis code - IMPORTANT\n",
    "```\n",
    "\n",
    "**The Solution**: Tellus automatically sorts files into meaningful categories, so you can archive exactly what you need:\n",
    "\n",
    "```\n",
    "‚úÖ Organized by Content Type:\n",
    "INPUT:       user_nl_cam, initial_conditions.nc\n",
    "OUTPUT:      cam.h0.*.nc, clm.h0.*.nc  \n",
    "RESTART:     *.r.*.nc\n",
    "LOG:         *.log, *.out\n",
    "SCRIPT:      *.py, *.sh\n",
    "TEMPORARY:   temp_*, scratch_*\n",
    "```\n",
    "\n",
    "This tutorial will show you how to leverage this classification for smarter archiving strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Creating a Complex Earth Science Workflow\n",
    "\n",
    "Let's create a more complex simulation that represents a realistic Earth Science workflow with multiple analysis stages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from datetime import datetime, timedelta\n",
    "from tellus.core.cli import console\n",
    "from rich.table import Table\n",
    "from rich.panel import Panel\n",
    "\n",
    "# Create tutorial workspace\n",
    "tutorial_dir = Path(tempfile.mkdtemp())\n",
    "console.print(f\"[blue]Tutorial workspace: {tutorial_dir}[/blue]\")\n",
    "\n",
    "def create_complex_earth_science_workflow():\n",
    "    \"\"\"\n",
    "    Creates a complex Earth Science workflow directory that includes:\n",
    "    - Multiple model components (atmosphere, ocean, land, ice)\n",
    "    - Different file types (input, output, restart, diagnostics)\n",
    "    - Analysis products and temporary files\n",
    "    - Multiple time periods and resolutions\n",
    "    \"\"\"\n",
    "    \n",
    "    workflow_dir = tutorial_dir / \"coupled_climate_system\"\n",
    "    workflow_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    console.print(\"[blue]Creating complex Earth Science workflow...[/blue]\")\n",
    "    \n",
    "    # ========================================\n",
    "    # 1. INPUT FILES - Model Configuration\n",
    "    # ========================================\n",
    "    input_dir = workflow_dir / \"input\"\n",
    "    input_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    console.print(\"  üìù Creating configuration files...\")\n",
    "    \n",
    "    # CESM component namelists (CRITICAL)\n",
    "    (input_dir / \"user_nl_cam\").write_text(\n",
    "        \"! CAM atmospheric component\\n\"\n",
    "        \"nhtfrq = -24, -6\\n\"          # Daily and 6-hourly output\n",
    "        \"mfilt = 30, 120\\n\"          # Files per stream\n",
    "        \"fincl1 = 'T','Q','U','V','PRECC','PRECL'\\n\"\n",
    "        \"fincl2 = 'TS','PSL','Z500'\\n\"  # High-frequency surface vars\n",
    "    )\n",
    "    \n",
    "    (input_dir / \"user_nl_pop\").write_text(\n",
    "        \"! POP ocean component\\n\"\n",
    "        \"tavg_nfile = 1\\n\"\n",
    "        \"tavg_freq_opt = 'nmonth'\\n\"\n",
    "        \"tavg_freq = 1\\n\"\n",
    "        \"tavg_contents = 'TEMP','SALT','UVEL','VVEL','SSH'\\n\"\n",
    "    )\n",
    "    \n",
    "    (input_dir / \"user_nl_clm\").write_text(\n",
    "        \"! CLM land component\\n\"\n",
    "        \"hist_nhtfrq = -24\\n\"\n",
    "        \"hist_mfilt = 30\\n\"\n",
    "        \"hist_fincl1 = 'TSA','GPP','NPP','SOILWATER_10CM'\\n\"\n",
    "    )\n",
    "    \n",
    "    (input_dir / \"user_nl_cice\").write_text(\n",
    "        \"! CICE sea ice component\\n\"\n",
    "        \"histfreq = 'm','d'\\n\"       # Monthly and daily\n",
    "        \"histfreq_n = 1,1\\n\"\n",
    "        \"f_aice = 'm','d'\\n\"\n",
    "        \"f_hi = 'm','d'\\n\"\n",
    "    )\n",
    "    \n",
    "    # Boundary conditions and forcing data (CRITICAL)\n",
    "    create_sample_netcdf(input_dir / \"sst_forcing.nc\", \"forcing\")\n",
    "    create_sample_netcdf(input_dir / \"topography.nc\", \"static\")\n",
    "    create_sample_netcdf(input_dir / \"land_surface_data.nc\", \"static\")\n",
    "    \n",
    "    # ========================================\n",
    "    # 2. OUTPUT FILES - Multiple Components and Frequencies\n",
    "    # ========================================\n",
    "    output_dir = workflow_dir / \"output\"\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    console.print(\"  üåç Creating model output files...\")\n",
    "    \n",
    "    # Atmosphere (CAM) - multiple streams\n",
    "    for month in [\"2024-01\", \"2024-02\", \"2024-03\"]:\n",
    "        # Daily averages (primary stream)\n",
    "        create_sample_netcdf(output_dir / f\"cam.h0.{month}.nc\", \"atmosphere_daily\")\n",
    "        # 6-hourly data (secondary stream)\n",
    "        create_sample_netcdf(output_dir / f\"cam.h1.{month}.nc\", \"atmosphere_6hourly\")\n",
    "    \n",
    "    # Ocean (POP) - monthly averages\n",
    "    for month in [\"2024-01\", \"2024-02\", \"2024-03\"]:\n",
    "        create_sample_netcdf(output_dir / f\"pop.h.{month}.nc\", \"ocean\")\n",
    "    \n",
    "    # Land (CLM) - daily averages\n",
    "    for month in [\"2024-01\", \"2024-02\", \"2024-03\"]:\n",
    "        create_sample_netcdf(output_dir / f\"clm.h0.{month}.nc\", \"land\")\n",
    "    \n",
    "    # Sea Ice (CICE) - monthly and daily\n",
    "    for month in [\"2024-01\", \"2024-02\", \"2024-03\"]:\n",
    "        create_sample_netcdf(output_dir / f\"cice.h.{month}.nc\", \"seaice_monthly\")\n",
    "        create_sample_netcdf(output_dir / f\"cice.hd.{month}.nc\", \"seaice_daily\")\n",
    "    \n",
    "    # ========================================\n",
    "    # 3. RESTART FILES - For Continuation\n",
    "    # ========================================\n",
    "    restart_dir = workflow_dir / \"restart\"\n",
    "    restart_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    console.print(\"  üîÑ Creating restart files...\")\n",
    "    \n",
    "    # Each component needs restart files\n",
    "    restart_date = \"2024-04-01\"\n",
    "    create_sample_netcdf(restart_dir / f\"cam.r.{restart_date}-00000.nc\", \"restart\")\n",
    "    create_sample_netcdf(restart_dir / f\"pop.r.{restart_date}-00000.nc\", \"restart\")\n",
    "    create_sample_netcdf(restart_dir / f\"clm.r.{restart_date}-00000.nc\", \"restart\")\n",
    "    create_sample_netcdf(restart_dir / f\"cice.r.{restart_date}-00000.nc\", \"restart\")\n",
    "    \n",
    "    # Restart pointer files (small text files)\n",
    "    (restart_dir / \"rpointer.atm\").write_text(f\"cam.r.{restart_date}-00000.nc\")\n",
    "    (restart_dir / \"rpointer.ocn\").write_text(f\"pop.r.{restart_date}-00000.nc\")\n",
    "    (restart_dir / \"rpointer.lnd\").write_text(f\"clm.r.{restart_date}-00000.nc\")\n",
    "    (restart_dir / \"rpointer.ice\").write_text(f\"cice.r.{restart_date}-00000.nc\")\n",
    "    \n",
    "    # ========================================\n",
    "    # 4. DIAGNOSTIC FILES - Derived Analysis\n",
    "    # ========================================\n",
    "    diagnostics_dir = workflow_dir / \"diagnostics\"\n",
    "    diagnostics_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    console.print(\"  üìä Creating diagnostic files...\")\n",
    "    \n",
    "    # Climate indices and derived quantities\n",
    "    create_sample_netcdf(diagnostics_dir / \"enso_index_2024.nc\", \"timeseries\")\n",
    "    create_sample_netcdf(diagnostics_dir / \"global_temperature_2024.nc\", \"timeseries\")\n",
    "    create_sample_netcdf(diagnostics_dir / \"seasonal_means_2024.nc\", \"climatology\")\n",
    "    create_sample_netcdf(diagnostics_dir / \"annual_cycle_2024.nc\", \"climatology\")\n",
    "    \n",
    "    # Component-specific diagnostics\n",
    "    create_sample_netcdf(diagnostics_dir / \"atmosphere_budgets_2024.nc\", \"budget\")\n",
    "    create_sample_netcdf(diagnostics_dir / \"ocean_transports_2024.nc\", \"transport\")\n",
    "    create_sample_netcdf(diagnostics_dir / \"carbon_cycle_2024.nc\", \"biogeochemistry\")\n",
    "    \n",
    "    # ========================================\n",
    "    # 5. LOG FILES - Model Run Information\n",
    "    # ========================================\n",
    "    logs_dir = workflow_dir / \"logs\"\n",
    "    logs_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    console.print(\"  üìù Creating log files...\")\n",
    "    \n",
    "    # Component log files\n",
    "    (logs_dir / \"atm.log.240101-000000\").write_text(\n",
    "        \"CAM ATMOSPHERE MODEL LOG\\n\"\n",
    "        \"========================\\n\"\n",
    "        \"Model: CAM6\\n\"\n",
    "        \"Resolution: f19_g16 (1.9x2.5 deg)\\n\"\n",
    "        \"Timestep: 1800 seconds\\n\"\n",
    "        \"Physics: CAM6 physics package\\n\"\n",
    "        \"Start: 2024-01-01 00:00:00\\n\"\n",
    "        \"End: 2024-03-31 23:59:59\\n\"\n",
    "        \"Status: COMPLETED SUCCESSFULLY\\n\"\n",
    "    )\n",
    "    \n",
    "    (logs_dir / \"ocn.log.240101-000000\").write_text(\n",
    "        \"POP OCEAN MODEL LOG\\n\"\n",
    "        \"==================\\n\"\n",
    "        \"Model: POP2\\n\"\n",
    "        \"Resolution: gx1v7 (1 degree)\\n\"\n",
    "        \"Timestep: 3600 seconds\\n\"\n",
    "        \"Vertical levels: 60\\n\"\n",
    "        \"Start: 2024-01-01 00:00:00\\n\"\n",
    "        \"End: 2024-03-31 23:59:59\\n\"\n",
    "        \"Status: COMPLETED SUCCESSFULLY\\n\"\n",
    "    )\n",
    "    \n",
    "    (logs_dir / \"cesm.log\").write_text(\n",
    "        \"CESM COUPLED MODEL LOG\\n\"\n",
    "        \"=====================\\n\"\n",
    "        \"Case: coupled_climate_system\\n\"\n",
    "        \"Compset: F2000climo\\n\"\n",
    "        \"Resolution: f19_g16\\n\"\n",
    "        \"Components: CAM6, POP2, CLM5, CICE5\\n\"\n",
    "        \"Runtime: 6.2 hours\\n\"\n",
    "        \"Throughput: 4.8 simulated years per wall day\\n\"\n",
    "        \"Memory usage: 12.4 GB peak\\n\"\n",
    "        \"Status: COMPLETED SUCCESSFULLY\\n\"\n",
    "    )\n",
    "    \n",
    "    # Performance and timing logs\n",
    "    (logs_dir / \"timing.log\").write_text(\n",
    "        \"CESM TIMING SUMMARY\\n\"\n",
    "        \"==================\\n\"\n",
    "        \"Total runtime: 22,320 seconds\\n\"\n",
    "        \"Init time: 45 seconds\\n\"\n",
    "        \"Run time: 22,200 seconds\\n\"\n",
    "        \"Finalize time: 75 seconds\\n\"\n",
    "        \"\\n\"\n",
    "        \"Component timings:\\n\"\n",
    "        \"CAM: 12,480 seconds (56.1%)\\n\"\n",
    "        \"POP: 8,640 seconds (38.9%)\\n\"\n",
    "        \"CLM: 720 seconds (3.2%)\\n\"\n",
    "        \"CICE: 360 seconds (1.6%)\\n\"\n",
    "        \"Coupler: 120 seconds (0.5%)\\n\"\n",
    "    )\n",
    "    \n",
    "    # ========================================\n",
    "    # 6. ANALYSIS SCRIPTS AND WORKFLOWS\n",
    "    # ========================================\n",
    "    scripts_dir = workflow_dir / \"scripts\"\n",
    "    scripts_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    console.print(\"  üî¨ Creating analysis scripts...\")\n",
    "    \n",
    "    # Analysis and post-processing scripts\n",
    "    (scripts_dir / \"compute_climatology.py\").write_text(\n",
    "        \"#!/usr/bin/env python3\\n\"\n",
    "        \"\\\"\\\"\\\"Compute climatological means from CESM output\\\"\\\"\\\"\\n\"\n",
    "        \"import xarray as xr\\n\"\n",
    "        \"import numpy as np\\n\"\n",
    "        \"\\n\"\n",
    "        \"def compute_climatology(input_files, output_file):\\n\"\n",
    "        \"    \\\"\\\"\\\"Compute long-term climatological means\\\"\\\"\\\"\\n\"\n",
    "        \"    ds = xr.open_mfdataset(input_files)\\n\"\n",
    "        \"    climatology = ds.groupby('time.month').mean('time')\\n\"\n",
    "        \"    climatology.to_netcdf(output_file)\\n\"\n",
    "        \"    print(f'Climatology saved to {output_file}')\\n\"\n",
    "    )\n",
    "    \n",
    "    (scripts_dir / \"analyze_enso.py\").write_text(\n",
    "        \"#!/usr/bin/env python3\\n\"\n",
    "        \"\\\"\\\"\\\"Compute ENSO indices from CESM ocean output\\\"\\\"\\\"\\n\"\n",
    "        \"import xarray as xr\\n\"\n",
    "        \"import numpy as np\\n\"\n",
    "        \"\\n\"\n",
    "        \"def compute_nino34_index(sst_files):\\n\"\n",
    "        \"    \\\"\\\"\\\"Compute Nino 3.4 index from SST data\\\"\\\"\\\"\\n\"\n",
    "        \"    ds = xr.open_mfdataset(sst_files)\\n\"\n",
    "        \"    # Select Nino 3.4 region (5N-5S, 120W-170W)\\n\"\n",
    "        \"    nino34 = ds.sel(lat=slice(-5, 5), lon=slice(190, 240))\\n\"\n",
    "        \"    # Compute area-weighted mean\\n\"\n",
    "        \"    index = nino34.weighted(np.cos(np.deg2rad(nino34.lat))).mean(['lat', 'lon'])\\n\"\n",
    "        \"    return index\\n\"\n",
    "    )\n",
    "    \n",
    "    (scripts_dir / \"postprocess_batch.sh\").write_text(\n",
    "        \"#!/bin/bash\\n\"\n",
    "        \"# Batch post-processing script for CESM output\\n\"\n",
    "        \"\\n\"\n",
    "        \"set -e  # Exit on any error\\n\"\n",
    "        \"\\n\"\n",
    "        \"echo 'Starting CESM post-processing pipeline...'\\n\"\n",
    "        \"\\n\"\n",
    "        \"# Step 1: Compute climatologies\\n\"\n",
    "        \"python compute_climatology.py\\n\"\n",
    "        \"\\n\"\n",
    "        \"# Step 2: Compute climate indices\\n\"\n",
    "        \"python analyze_enso.py\\n\"\n",
    "        \"\\n\"\n",
    "        \"# Step 3: Generate summary plots\\n\"\n",
    "        \"python create_summary_plots.py\\n\"\n",
    "        \"\\n\"\n",
    "        \"echo 'Post-processing complete!'\\n\"\n",
    "    )\n",
    "    \n",
    "    # Visualization script\n",
    "    (scripts_dir / \"create_summary_plots.py\").write_text(\n",
    "        \"#!/usr/bin/env python3\\n\"\n",
    "        \"\\\"\\\"\\\"Create summary plots from CESM analysis\\\"\\\"\\\"\\n\"\n",
    "        \"import matplotlib.pyplot as plt\\n\"\n",
    "        \"import xarray as xr\\n\"\n",
    "        \"import cartopy.crs as ccrs\\n\"\n",
    "        \"\\n\"\n",
    "        \"def create_temperature_map(data_file, output_file):\\n\"\n",
    "        \"    \\\"\\\"\\\"Create global temperature map\\\"\\\"\\\"\\n\"\n",
    "        \"    ds = xr.open_dataset(data_file)\\n\"\n",
    "        \"    fig = plt.figure(figsize=(12, 6))\\n\"\n",
    "        \"    ax = plt.axes(projection=ccrs.PlateCarree())\\n\"\n",
    "        \"    ds.T.isel(time=0).plot(ax=ax, transform=ccrs.PlateCarree())\\n\"\n",
    "        \"    ax.coastlines()\\n\"\n",
    "        \"    plt.savefig(output_file, dpi=150)\\n\"\n",
    "    )\n",
    "    \n",
    "    # ========================================\n",
    "    # 7. TEMPORARY AND INTERMEDIATE FILES\n",
    "    # ========================================\n",
    "    temp_dir = workflow_dir / \"temp\"\n",
    "    temp_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    console.print(\"  üóÇÔ∏è Creating temporary files...\")\n",
    "    \n",
    "    # Temporary processing files (should not be archived)\n",
    "    (temp_dir / \"temp_processing_cam.nc\").write_bytes(b\"Temporary CAM processing data\" * 1000)\n",
    "    (temp_dir / \"scratch_regridding.nc\").write_bytes(b\"Scratch regridding workspace\" * 1000)\n",
    "    (temp_dir / \"intermediate_calculation.dat\").write_bytes(b\"Intermediate calculation\" * 500)\n",
    "    \n",
    "    # Work in progress files\n",
    "    (temp_dir / \"work_in_progress.nc\").write_bytes(b\"Work in progress analysis\" * 800)\n",
    "    (temp_dir / \"debug_output.txt\").write_text(\n",
    "        \"DEBUG OUTPUT\\n\"\n",
    "        \"===========\\n\"\n",
    "        \"Testing regridding function...\\n\"\n",
    "        \"Input shape: (180, 360)\\n\"\n",
    "        \"Output shape: (90, 180)\\n\"\n",
    "        \"Status: In progress...\\n\"\n",
    "    )\n",
    "    \n",
    "    # ========================================\n",
    "    # 8. DOCUMENTATION AND METADATA\n",
    "    # ========================================\n",
    "    docs_dir = workflow_dir / \"docs\"\n",
    "    docs_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    console.print(\"  üìñ Creating documentation...\")\n",
    "    \n",
    "    (docs_dir / \"README.md\").write_text(\n",
    "        \"# Coupled Climate System Simulation\\n\"\n",
    "        \"\\n\"\n",
    "        \"## Overview\\n\"\n",
    "        \"This simulation uses CESM2 to study coupled Earth system dynamics\\n\"\n",
    "        \"under present-day climate forcing conditions.\\n\"\n",
    "        \"\\n\"\n",
    "        \"## Model Configuration\\n\"\n",
    "        \"- **Case**: coupled_climate_system\\n\"\n",
    "        \"- **Compset**: F2000climo (fixed SSTs, present-day forcing)\\n\"\n",
    "        \"- **Resolution**: f19_g16 (atmosphere 1.9x2.5¬∞, ocean ~1¬∞)\\n\"\n",
    "        \"- **Duration**: 3 months (2024-01-01 to 2024-03-31)\\n\"\n",
    "        \"\\n\"\n",
    "        \"## Output Files\\n\"\n",
    "        \"- `output/`: Primary model output from all components\\n\"\n",
    "        \"- `diagnostics/`: Derived analysis products\\n\"\n",
    "        \"- `restart/`: Files needed to continue simulation\\n\"\n",
    "        \"\\n\"\n",
    "        \"## Analysis Scripts\\n\"\n",
    "        \"- `scripts/compute_climatology.py`: Compute seasonal means\\n\"\n",
    "        \"- `scripts/analyze_enso.py`: ENSO index calculations\\n\"\n",
    "        \"- `scripts/postprocess_batch.sh`: Automated processing pipeline\\n\"\n",
    "    )\n",
    "    \n",
    "    (docs_dir / \"simulation_log.json\").write_text(json.dumps({\n",
    "        \"simulation_id\": \"coupled_climate_system\",\n",
    "        \"model\": \"CESM2\",\n",
    "        \"compset\": \"F2000climo\",\n",
    "        \"resolution\": \"f19_g16\",\n",
    "        \"start_date\": \"2024-01-01\",\n",
    "        \"end_date\": \"2024-03-31\",\n",
    "        \"runtime_hours\": 6.2,\n",
    "        \"components\": [\"CAM6\", \"POP2\", \"CLM5\", \"CICE5\"],\n",
    "        \"created_by\": \"tutorial_user\",\n",
    "        \"purpose\": \"Tutorial demonstration of file classification\"\n",
    "    }, indent=2))\n",
    "    \n",
    "    return workflow_dir\n",
    "\n",
    "def create_sample_netcdf(filepath, data_type):\n",
    "    \"\"\"\n",
    "    Creates sample NetCDF files for different Earth Science data types.\n",
    "    Each type has realistic attributes and structure.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Standard coordinates\n",
    "    lat = np.linspace(-90, 90, 96)  \n",
    "    lon = np.linspace(0, 360, 144)\n",
    "    time = [datetime(2024, 1, 15)]\n",
    "    \n",
    "    if data_type == \"atmosphere_daily\":\n",
    "        ds = xr.Dataset({\n",
    "            'T': (['time', 'lat', 'lon'], 288 + 30 * np.cos(np.radians(lat))[None, :, None]),\n",
    "            'Q': (['time', 'lat', 'lon'], 0.01 * np.ones((1, 96, 144))),\n",
    "            'U': (['time', 'lat', 'lon'], 10 * np.sin(2 * np.radians(lat))[None, :, None]),\n",
    "            'PRECC': (['time', 'lat', 'lon'], 0.001 * np.abs(np.cos(np.radians(lat)))[None, :, None])\n",
    "        }, coords={'time': time, 'lat': lat, 'lon': lon})\n",
    "        ds.attrs = {'title': 'CAM Daily Atmospheric Output', 'model': 'CAM6', 'frequency': 'daily'}\n",
    "    \n",
    "    elif data_type == \"atmosphere_6hourly\":\n",
    "        time_6h = [datetime(2024, 1, 15) + timedelta(hours=6*i) for i in range(4)]\n",
    "        ds = xr.Dataset({\n",
    "            'TS': (['time', 'lat', 'lon'], 285 + 25 * np.cos(np.radians(lat))[None, :, None]),\n",
    "            'PSL': (['time', 'lat', 'lon'], 101325 * np.ones((4, 96, 144)))\n",
    "        }, coords={'time': time_6h, 'lat': lat, 'lon': lon})\n",
    "        ds.attrs = {'title': 'CAM 6-Hourly Surface Output', 'model': 'CAM6', 'frequency': '6hourly'}\n",
    "    \n",
    "    elif data_type == \"ocean\":\n",
    "        depth = np.array([5, 15, 25, 45, 75])  \n",
    "        ds = xr.Dataset({\n",
    "            'TEMP': (['time', 'z_t', 'lat', 'lon'], 290 - 40 * depth[:, None, None] / 1000),\n",
    "            'SALT': (['time', 'z_t', 'lat', 'lon'], 35 * np.ones((1, 5, 96, 144))),\n",
    "            'SSH': (['time', 'lat', 'lon'], 0.1 * np.sin(2 * np.radians(lat))[None, :, None])\n",
    "        }, coords={'time': time, 'z_t': depth, 'lat': lat, 'lon': lon})\n",
    "        ds.attrs = {'title': 'POP Ocean Model Output', 'model': 'POP2', 'frequency': 'monthly'}\n",
    "        \n",
    "    elif data_type == \"land\":\n",
    "        ds = xr.Dataset({\n",
    "            'TSA': (['time', 'lat', 'lon'], 285 + 25 * np.cos(np.radians(lat))[None, :, None]),\n",
    "            'GPP': (['time', 'lat', 'lon'], 0.01 * np.abs(np.cos(np.radians(lat)))[None, :, None]),\n",
    "            'SOILWATER_10CM': (['time', 'lat', 'lon'], 0.3 * np.ones((1, 96, 144)))\n",
    "        }, coords={'time': time, 'lat': lat, 'lon': lon})\n",
    "        ds.attrs = {'title': 'CLM Land Model Output', 'model': 'CLM5', 'frequency': 'daily'}\n",
    "    \n",
    "    elif data_type.startswith(\"seaice\"):\n",
    "        ds = xr.Dataset({\n",
    "            'aice': (['time', 'lat', 'lon'], 0.8 * (np.abs(lat) > 60)[None, :, None]),\n",
    "            'hi': (['time', 'lat', 'lon'], 2.0 * (np.abs(lat) > 70)[None, :, None])\n",
    "        }, coords={'time': time, 'lat': lat, 'lon': lon})\n",
    "        freq = 'monthly' if 'monthly' in data_type else 'daily'\n",
    "        ds.attrs = {'title': 'CICE Sea Ice Output', 'model': 'CICE5', 'frequency': freq}\n",
    "    \n",
    "    elif data_type in [\"restart\", \"forcing\", \"static\"]:\n",
    "        # Simplified restart/forcing files\n",
    "        ds = xr.Dataset({\n",
    "            'DATA': (['lat', 'lon'], 300 * np.ones((96, 144))),\n",
    "        }, coords={'lat': lat, 'lon': lon})\n",
    "        ds.attrs = {'title': f'{data_type.title()} Data', 'purpose': data_type}\n",
    "    \n",
    "    elif data_type == \"timeseries\":\n",
    "        time_monthly = [datetime(2024, m, 15) for m in range(1, 4)]\n",
    "        ds = xr.Dataset({\n",
    "            'index': (['time'], np.random.randn(3)),\n",
    "        }, coords={'time': time_monthly})\n",
    "        ds.attrs = {'title': 'Climate Index Time Series'}\n",
    "    \n",
    "    elif data_type == \"climatology\":\n",
    "        months = np.arange(1, 13)\n",
    "        ds = xr.Dataset({\n",
    "            'climatology': (['month', 'lat', 'lon'], \n",
    "                          288 + 30 * np.cos(np.radians(lat))[None, :, None] * np.ones((12, 96, 144)))\n",
    "        }, coords={'month': months, 'lat': lat, 'lon': lon})\n",
    "        ds.attrs = {'title': 'Climatological Means'}\n",
    "    \n",
    "    else:\n",
    "        # Generic data\n",
    "        ds = xr.Dataset({\n",
    "            'data': (['lat', 'lon'], np.random.randn(96, 144))\n",
    "        }, coords={'lat': lat, 'lon': lon})\n",
    "        ds.attrs = {'title': f'{data_type.title()} Data'}\n",
    "    \n",
    "    # Save file\n",
    "    ds.to_netcdf(filepath, format='NETCDF4_CLASSIC')\n",
    "\n",
    "# Create the complex workflow\n",
    "workflow_dir = create_complex_earth_science_workflow()\n",
    "console.print(f\"\\n[green]‚úÖ Complex Earth Science workflow created: {workflow_dir.name}[/green]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding File Classification\n",
    "\n",
    "Now let's examine what we created and see how Tellus would automatically classify these files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's analyze our workflow directory and classify files\n",
    "def analyze_earth_science_files(directory):\n",
    "    \"\"\"\n",
    "    Analyze files in Earth Science workflow and classify them.\n",
    "    This simulates what Tellus does automatically.\n",
    "    \"\"\"\n",
    "    \n",
    "    classified_files = {\n",
    "        'INPUT': [],\n",
    "        'OUTPUT': [],\n",
    "        'RESTART': [],\n",
    "        'DIAGNOSTIC': [],\n",
    "        'LOG': [],\n",
    "        'SCRIPT': [],\n",
    "        'TEMPORARY': [],\n",
    "        'METADATA': [],\n",
    "        'INTERMEDIATE': []\n",
    "    }\n",
    "    \n",
    "    importance_levels = {\n",
    "        'CRITICAL': [],\n",
    "        'IMPORTANT': [],\n",
    "        'OPTIONAL': [],\n",
    "        'TEMPORARY': []\n",
    "    }\n",
    "    \n",
    "    total_size = 0\n",
    "    \n",
    "    for file_path in directory.rglob('*'):\n",
    "        if not file_path.is_file():\n",
    "            continue\n",
    "            \n",
    "        rel_path = file_path.relative_to(directory)\n",
    "        size = file_path.stat().st_size\n",
    "        total_size += size\n",
    "        \n",
    "        # Classify by content type (what Tellus does automatically)\n",
    "        content_type, importance = classify_earth_science_file(rel_path)\n",
    "        \n",
    "        file_info = {\n",
    "            'path': str(rel_path),\n",
    "            'size': size,\n",
    "            'content_type': content_type,\n",
    "            'importance': importance\n",
    "        }\n",
    "        \n",
    "        classified_files[content_type].append(file_info)\n",
    "        importance_levels[importance].append(file_info)\n",
    "    \n",
    "    return classified_files, importance_levels, total_size\n",
    "\n",
    "def classify_earth_science_file(file_path):\n",
    "    \"\"\"\n",
    "    Classify a single Earth Science file.\n",
    "    This is a simplified version of Tellus's classification logic.\n",
    "    \"\"\"\n",
    "    \n",
    "    path_str = str(file_path).lower()\n",
    "    name = file_path.name.lower()\n",
    "    \n",
    "    # INPUT FILES - Model configuration and boundary conditions\n",
    "    if (name.startswith('user_nl_') or \n",
    "        'forcing' in name or \n",
    "        'topography' in name or\n",
    "        'land_surface' in name or\n",
    "        'initial' in name):\n",
    "        return 'INPUT', 'CRITICAL'\n",
    "    \n",
    "    # RESTART FILES - For continuing simulations\n",
    "    if (('.r.' in name and '.nc' in name) or \n",
    "        name.startswith('rpointer')):\n",
    "        return 'RESTART', 'CRITICAL'\n",
    "    \n",
    "    # OUTPUT FILES - Primary model results\n",
    "    if ('output' in path_str and '.nc' in name and \n",
    "        any(model in name for model in ['cam.h', 'pop.h', 'clm.h', 'cice.h'])):\n",
    "        return 'OUTPUT', 'IMPORTANT'\n",
    "    \n",
    "    # DIAGNOSTIC FILES - Derived analysis products\n",
    "    if ('diagnostic' in path_str or \n",
    "        any(term in name for term in ['index', 'climatology', 'budget', 'transport'])):\n",
    "        return 'DIAGNOSTIC', 'IMPORTANT'\n",
    "    \n",
    "    # LOG FILES - Model run information\n",
    "    if (name.endswith('.log') or \n",
    "        'timing' in name or\n",
    "        path_str.startswith('logs/')):\n",
    "        return 'LOG', 'OPTIONAL'\n",
    "    \n",
    "    # SCRIPT FILES - Analysis and processing code\n",
    "    if (name.endswith(('.py', '.sh', '.ncl', '.m')) or\n",
    "        'script' in path_str):\n",
    "        return 'SCRIPT', 'IMPORTANT'\n",
    "    \n",
    "    # TEMPORARY FILES - Should not be archived\n",
    "    if ('temp' in path_str or \n",
    "        name.startswith(('temp_', 'scratch_', 'work_in_progress')) or\n",
    "        'debug' in name):\n",
    "        return 'TEMPORARY', 'TEMPORARY'\n",
    "    \n",
    "    # METADATA FILES - Documentation and catalogs\n",
    "    if (name.endswith(('.md', '.txt', '.json', '.yaml', '.yml')) or\n",
    "        'docs' in path_str):\n",
    "        return 'METADATA', 'OPTIONAL'\n",
    "    \n",
    "    # INTERMEDIATE FILES - Processing intermediates\n",
    "    if name.endswith(('.dat', '.tmp', '.cache')):\n",
    "        return 'INTERMEDIATE', 'OPTIONAL'\n",
    "    \n",
    "    # Default classification\n",
    "    return 'METADATA', 'OPTIONAL'\n",
    "\n",
    "# Analyze our workflow\n",
    "classified_files, importance_levels, total_size = analyze_earth_science_files(workflow_dir)\n",
    "\n",
    "# Display classification results\n",
    "console.print(\"\\n[bold blue]üîç Automatic File Classification Results[/bold blue]\")\n",
    "console.print(\"=\" * 60)\n",
    "\n",
    "# Create summary table\n",
    "table = Table(title=\"File Classification Summary\")\n",
    "table.add_column(\"Content Type\", style=\"cyan\")\n",
    "table.add_column(\"Files\", justify=\"right\", style=\"green\")\n",
    "table.add_column(\"Size (MB)\", justify=\"right\", style=\"yellow\")\n",
    "table.add_column(\"Purpose\", style=\"dim\")\n",
    "\n",
    "# Add rows for each content type\n",
    "content_descriptions = {\n",
    "    'INPUT': 'Model configuration and forcing data',\n",
    "    'OUTPUT': 'Primary scientific results',\n",
    "    'RESTART': 'Files for continuing simulations',\n",
    "    'DIAGNOSTIC': 'Derived analysis products',\n",
    "    'LOG': 'Model run information and diagnostics',\n",
    "    'SCRIPT': 'Analysis and processing workflows',\n",
    "    'TEMPORARY': 'Temporary files (exclude from archive)',\n",
    "    'METADATA': 'Documentation and catalogs',\n",
    "    'INTERMEDIATE': 'Processing intermediate files'\n",
    "}\n",
    "\n",
    "for content_type, files in classified_files.items():\n",
    "    if files:  # Only show types with files\n",
    "        file_count = len(files)\n",
    "        type_size = sum(f['size'] for f in files) / (1024 * 1024)\n",
    "        description = content_descriptions[content_type]\n",
    "        \n",
    "        table.add_row(content_type, str(file_count), f\"{type_size:.1f}\", description)\n",
    "\n",
    "console.print(table)\n",
    "\n",
    "# Show importance breakdown\n",
    "console.print(\"\\n[bold blue]‚öñÔ∏è File Importance Levels[/bold blue]\")\n",
    "importance_table = Table()\n",
    "importance_table.add_column(\"Importance\", style=\"cyan\")\n",
    "importance_table.add_column(\"Files\", justify=\"right\", style=\"green\")\n",
    "importance_table.add_column(\"Size (MB)\", justify=\"right\", style=\"yellow\")\n",
    "importance_table.add_column(\"Archive Strategy\", style=\"dim\")\n",
    "\n",
    "importance_strategies = {\n",
    "    'CRITICAL': 'Always archive (needed for reproduction)',\n",
    "    'IMPORTANT': 'Usually archive (valuable for analysis)',\n",
    "    'OPTIONAL': 'Archive selectively (useful but not essential)',\n",
    "    'TEMPORARY': 'Never archive (can be regenerated)'\n",
    "}\n",
    "\n",
    "for importance, files in importance_levels.items():\n",
    "    if files:\n",
    "        file_count = len(files)\n",
    "        imp_size = sum(f['size'] for f in files) / (1024 * 1024)\n",
    "        strategy = importance_strategies[importance]\n",
    "        \n",
    "        importance_table.add_row(importance, str(file_count), f\"{imp_size:.1f}\", strategy)\n",
    "\n",
    "console.print(importance_table)\n",
    "\n",
    "console.print(f\"\\n[green]Total files analyzed: {sum(len(files) for files in classified_files.values())}[/green]\")\n",
    "console.print(f\"[green]Total size: {total_size / (1024 * 1024):.1f} MB[/green]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Creating Selective Archives\n",
    "\n",
    "Now let's create different types of archives based on our file classification. This shows the power of intelligent archiving:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def create_selective_archive(source_dir, archive_path, content_types=None, importance_levels=None, exclude_patterns=None):\n",
    "    \"\"\"\n",
    "    Create a selective archive based on file classification.\n",
    "    This demonstrates the core concept behind Tellus selective archiving.\n",
    "    \"\"\"\n",
    "    \n",
    "    files_to_archive = []\n",
    "    metadata_files = []\n",
    "    excluded_files = []\n",
    "    \n",
    "    for file_path in source_dir.rglob('*'):\n",
    "        if not file_path.is_file():\n",
    "            continue\n",
    "            \n",
    "        rel_path = file_path.relative_to(source_dir)\n",
    "        content_type, importance = classify_earth_science_file(rel_path)\n",
    "        \n",
    "        # Check exclusion patterns\n",
    "        if exclude_patterns:\n",
    "            if any(pattern in str(rel_path).lower() for pattern in exclude_patterns):\n",
    "                excluded_files.append({\n",
    "                    'path': str(rel_path),\n",
    "                    'reason': 'excluded_pattern',\n",
    "                    'content_type': content_type,\n",
    "                    'importance': importance\n",
    "                })\n",
    "                continue\n",
    "        \n",
    "        # Check content type filter\n",
    "        include_file = True\n",
    "        if content_types and content_type not in content_types:\n",
    "            include_file = False\n",
    "        \n",
    "        # Check importance level filter\n",
    "        if importance_levels and importance not in importance_levels:\n",
    "            include_file = False\n",
    "        \n",
    "        if include_file:\n",
    "            file_info = {\n",
    "                'path': str(rel_path),\n",
    "                'size': file_path.stat().st_size,\n",
    "                'content_type': content_type,\n",
    "                'importance': importance,\n",
    "                'modified': file_path.stat().st_mtime\n",
    "            }\n",
    "            files_to_archive.append((file_path, file_info))\n",
    "            metadata_files.append(file_info)\n",
    "        else:\n",
    "            excluded_files.append({\n",
    "                'path': str(rel_path),\n",
    "                'reason': 'filtered_out',\n",
    "                'content_type': content_type,\n",
    "                'importance': importance\n",
    "            })\n",
    "    \n",
    "    # Create the archive\n",
    "    console.print(f\"[blue]Creating selective archive with {len(files_to_archive)} files...[/blue]\")\n",
    "    \n",
    "    with tarfile.open(archive_path, \"w:gz\") as tar:\n",
    "        for file_path, file_info in files_to_archive:\n",
    "            rel_path = Path(file_info['path'])\n",
    "            tar.add(file_path, arcname=rel_path)\n",
    "            console.print(f\"  Added: {rel_path} [{file_info['content_type']}, {file_info['importance']}]\")\n",
    "    \n",
    "    # Create metadata\n",
    "    archive_metadata = {\n",
    "        'metadata_version': '1.0',\n",
    "        'created_at': datetime.now().isoformat(),\n",
    "        'selection_criteria': {\n",
    "            'content_types': content_types,\n",
    "            'importance_levels': importance_levels,\n",
    "            'exclude_patterns': exclude_patterns\n",
    "        },\n",
    "        'archive_stats': {\n",
    "            'files_included': len(files_to_archive),\n",
    "            'files_excluded': len(excluded_files),\n",
    "            'total_size': sum(f[1]['size'] for f in files_to_archive),\n",
    "            'compression_type': 'gzip'\n",
    "        },\n",
    "        'included_files': metadata_files,\n",
    "        'excluded_files': excluded_files\n",
    "    }\n",
    "    \n",
    "    metadata_path = archive_path.with_suffix('.metadata.json')\n",
    "    metadata_path.write_text(json.dumps(archive_metadata, indent=2))\n",
    "    \n",
    "    return archive_path, metadata_path, len(files_to_archive), len(excluded_files)\n",
    "\n",
    "# Create different types of selective archives\n",
    "archive_dir = tutorial_dir / \"selective_archives\"\n",
    "archive_dir.mkdir(exist_ok=True)\n",
    "\n",
    "console.print(\"\\n[bold blue]üéØ Creating Selective Archives[/bold blue]\")\n",
    "console.print(\"=\" * 50)\n",
    "\n",
    "# Archive 1: Critical files only (for backup/restart)\n",
    "console.print(\"\\n[cyan]1. Creating 'Critical Files Only' archive...[/cyan]\")\n",
    "console.print(\"[dim]Purpose: Backup essential files needed to restart simulation[/dim]\")\n",
    "\n",
    "critical_archive, critical_metadata, included_critical, excluded_critical = create_selective_archive(\n",
    "    workflow_dir,\n",
    "    archive_dir / \"critical_files_only.tar.gz\",\n",
    "    importance_levels=['CRITICAL']\n",
    ")\n",
    "\n",
    "console.print(f\"  ‚úÖ Created: {critical_archive.name}\")\n",
    "console.print(f\"  üìä Files included: {included_critical}, excluded: {excluded_critical}\")\n",
    "console.print(f\"  üíæ Size: {critical_archive.stat().st_size / (1024*1024):.1f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Archive 2: Scientific output only (for analysis/sharing)\n",
    "console.print(\"\\n[cyan]2. Creating 'Scientific Output Only' archive...[/cyan]\")\n",
    "console.print(\"[dim]Purpose: Share primary results with collaborators[/dim]\")\n",
    "\n",
    "output_archive, output_metadata, included_output, excluded_output = create_selective_archive(\n",
    "    workflow_dir,\n",
    "    archive_dir / \"scientific_output.tar.gz\",\n",
    "    content_types=['OUTPUT', 'DIAGNOSTIC'],\n",
    "    importance_levels=['IMPORTANT', 'CRITICAL']\n",
    ")\n",
    "\n",
    "console.print(f\"  ‚úÖ Created: {output_archive.name}\")\n",
    "console.print(f\"  üìä Files included: {included_output}, excluded: {excluded_output}\")\n",
    "console.print(f\"  üíæ Size: {output_archive.stat().st_size / (1024*1024):.1f} MB\")\n",
    "\n",
    "# Archive 3: Analysis workflow (code + documentation)\n",
    "console.print(\"\\n[cyan]3. Creating 'Analysis Workflow' archive...[/cyan]\")\n",
    "console.print(\"[dim]Purpose: Preserve analysis methods and reproducibility[/dim]\")\n",
    "\n",
    "workflow_archive, workflow_metadata, included_workflow, excluded_workflow = create_selective_archive(\n",
    "    workflow_dir,\n",
    "    archive_dir / \"analysis_workflow.tar.gz\",\n",
    "    content_types=['SCRIPT', 'METADATA'],\n",
    "    exclude_patterns=['temp', 'debug', 'log']\n",
    ")\n",
    "\n",
    "console.print(f\"  ‚úÖ Created: {workflow_archive.name}\")\n",
    "console.print(f\"  üìä Files included: {included_workflow}, excluded: {excluded_workflow}\")\n",
    "console.print(f\"  üíæ Size: {workflow_archive.stat().st_size / (1024*1024):.1f} MB\")\n",
    "\n",
    "# Archive 4: Clean production archive (exclude temporary and logs)\n",
    "console.print(\"\\n[cyan]4. Creating 'Clean Production' archive...[/cyan]\")\n",
    "console.print(\"[dim]Purpose: Production-ready archive without clutter[/dim]\")\n",
    "\n",
    "production_archive, production_metadata, included_production, excluded_production = create_selective_archive(\n",
    "    workflow_dir,\n",
    "    archive_dir / \"production_clean.tar.gz\",\n",
    "    importance_levels=['CRITICAL', 'IMPORTANT'],\n",
    "    exclude_patterns=['temp', 'debug', 'log', 'timing']\n",
    ")\n",
    "\n",
    "console.print(f\"  ‚úÖ Created: {production_archive.name}\")\n",
    "console.print(f\"  üìä Files included: {included_production}, excluded: {excluded_production}\")\n",
    "console.print(f\"  üíæ Size: {production_archive.stat().st_size / (1024*1024):.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Comparing Archive Strategies\n",
    "\n",
    "Let's compare our different archiving strategies to understand when to use each approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all archives\n",
    "def compare_archives(archive_info_list):\n",
    "    \"\"\"\n",
    "    Create a comparison table for different archive strategies.\n",
    "    \"\"\"\n",
    "    \n",
    "    comparison_table = Table(title=\"Archive Strategy Comparison\")\n",
    "    comparison_table.add_column(\"Archive Name\", style=\"cyan\")\n",
    "    comparison_table.add_column(\"Purpose\", style=\"green\")\n",
    "    comparison_table.add_column(\"Files\", justify=\"right\", style=\"yellow\")\n",
    "    comparison_table.add_column(\"Size (MB)\", justify=\"right\", style=\"magenta\")\n",
    "    comparison_table.add_column(\"Best For\", style=\"dim\")\n",
    "    \n",
    "    for info in archive_info_list:\n",
    "        comparison_table.add_row(\n",
    "            info['name'],\n",
    "            info['purpose'],\n",
    "            str(info['files']),\n",
    "            f\"{info['size']:.1f}\",\n",
    "            info['best_for']\n",
    "        )\n",
    "    \n",
    "    return comparison_table\n",
    "\n",
    "# Gather archive information\n",
    "archives_info = [\n",
    "    {\n",
    "        'name': 'Critical Files Only',\n",
    "        'purpose': 'Essential restart files',\n",
    "        'files': included_critical,\n",
    "        'size': critical_archive.stat().st_size / (1024*1024),\n",
    "        'best_for': 'Quick backup, minimal storage'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Scientific Output',\n",
    "        'purpose': 'Analysis data sharing',\n",
    "        'files': included_output,\n",
    "        'size': output_archive.stat().st_size / (1024*1024),\n",
    "        'best_for': 'Collaborator sharing, analysis'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Analysis Workflow',\n",
    "        'purpose': 'Reproducible methods',\n",
    "        'files': included_workflow,\n",
    "        'size': workflow_archive.stat().st_size / (1024*1024),\n",
    "        'best_for': 'Code preservation, methods'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Production Clean',\n",
    "        'purpose': 'Professional archiving',\n",
    "        'files': included_production,\n",
    "        'size': production_archive.stat().st_size / (1024*1024),\n",
    "        'best_for': 'Long-term storage, publications'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Display comparison\n",
    "console.print(\"\\n[bold blue]üìä Archive Strategy Comparison[/bold blue]\")\n",
    "comparison_table = compare_archives(archives_info)\n",
    "console.print(comparison_table)\n",
    "\n",
    "# Calculate space savings\n",
    "original_size = sum(f.stat().st_size for f in workflow_dir.rglob('*') if f.is_file()) / (1024*1024)\n",
    "total_selective_size = sum(info['size'] for info in archives_info)\n",
    "space_efficiency = (1 - min(info['size'] for info in archives_info) / original_size) * 100\n",
    "\n",
    "console.print(f\"\\n[bold green]üíæ Storage Efficiency[/bold green]\")\n",
    "console.print(f\"Original workflow: {original_size:.1f} MB\")\n",
    "console.print(f\"Smallest selective archive: {min(info['size'] for info in archives_info):.1f} MB\")\n",
    "console.print(f\"Space savings: {space_efficiency:.1f}%\")\n",
    "console.print(f\"All selective archives combined: {total_selective_size:.1f} MB\")\n",
    "\n",
    "savings_explanation = Panel(\n",
    "    \"[green]Key Benefits of Selective Archiving:[/green]\\n\\n\"\n",
    "    \"‚úÖ [cyan]Targeted Storage:[/cyan] Only archive what you need\\n\"\n",
    "    \"‚úÖ [cyan]Faster Transfers:[/cyan] Smaller files move quicker\\n\"\n",
    "    \"‚úÖ [cyan]Organized Access:[/cyan] Find files by purpose\\n\"\n",
    "    \"‚úÖ [cyan]Cost Efficient:[/cyan] Pay less for cloud storage\\n\"\n",
    "    \"‚úÖ [cyan]Clear Intent:[/cyan] Archive purpose is obvious\",\n",
    "    title=\"üéØ Why Use Selective Archiving?\",\n",
    "    border_style=\"green\"\n",
    ")\n",
    "\n",
    "console.print(f\"\\n{savings_explanation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Examining Archive Contents\n",
    "\n",
    "Let's look inside our selective archives to understand what was included and excluded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_archive_contents(metadata_path):\n",
    "    \"\"\"\n",
    "    Examine what's inside a selective archive based on its metadata.\n",
    "    \"\"\"\n",
    "    \n",
    "    metadata = json.loads(metadata_path.read_text())\n",
    "    \n",
    "    console.print(f\"\\n[bold cyan]üìã Archive: {metadata_path.stem.replace('.metadata', '')}[/bold cyan]\")\n",
    "    console.print(\"=\"* 50)\n",
    "    \n",
    "    # Selection criteria\n",
    "    criteria = metadata['selection_criteria']\n",
    "    console.print(f\"[blue]Content Types:[/blue] {criteria.get('content_types', 'All')}\")\n",
    "    console.print(f\"[blue]Importance Levels:[/blue] {criteria.get('importance_levels', 'All')}\")\n",
    "    if criteria.get('exclude_patterns'):\n",
    "        console.print(f\"[blue]Excluded Patterns:[/blue] {criteria['exclude_patterns']}\")\n",
    "    \n",
    "    # Stats\n",
    "    stats = metadata['archive_stats']\n",
    "    console.print(f\"[green]Files Included:[/green] {stats['files_included']}\")\n",
    "    console.print(f\"[yellow]Files Excluded:[/yellow] {stats['files_excluded']}\")\n",
    "    console.print(f\"[magenta]Total Size:[/magenta] {stats['total_size'] / (1024*1024):.1f} MB\")\n",
    "    \n",
    "    # Show some included files by type\n",
    "    included_by_type = {}\n",
    "    for file_info in metadata['included_files']:\n",
    "        content_type = file_info['content_type']\n",
    "        if content_type not in included_by_type:\n",
    "            included_by_type[content_type] = []\n",
    "        included_by_type[content_type].append(file_info)\n",
    "    \n",
    "    if included_by_type:\n",
    "        console.print(\"\\n[bold]üìÑ Included Files by Type:[/bold]\")\n",
    "        for content_type, files in included_by_type.items():\n",
    "            console.print(f\"  [cyan]{content_type}[/cyan] ({len(files)} files):\")\n",
    "            for file_info in files[:3]:  # Show first 3 files\n",
    "                size_str = f\"{file_info['size'] / 1024:.1f} KB\" if file_info['size'] > 1024 else f\"{file_info['size']} B\"\n",
    "                console.print(f\"    ‚Ä¢ {file_info['path']} ({size_str})\")\n",
    "            if len(files) > 3:\n",
    "                console.print(f\"    [dim]... and {len(files) - 3} more[/dim]\")\n",
    "    \n",
    "    # Show some excluded files\n",
    "    excluded_files = metadata.get('excluded_files', [])\n",
    "    if excluded_files:\n",
    "        console.print(f\"\\n[bold]üö´ Sample Excluded Files:[/bold] (showing first 5)\")\n",
    "        for excluded in excluded_files[:5]:\n",
    "            console.print(f\"  ‚Ä¢ {excluded['path']} ({excluded['content_type']}, reason: {excluded['reason']})\")\n",
    "\n",
    "# Examine each archive\n",
    "console.print(\"\\n[bold blue]üîç Detailed Archive Contents[/bold blue]\")\n",
    "\n",
    "examine_archive_contents(critical_metadata)\n",
    "examine_archive_contents(output_metadata)\n",
    "examine_archive_contents(workflow_metadata)\n",
    "examine_archive_contents(production_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-World Decision Making: Which Archive Strategy to Use?\n",
    "\n",
    "Let's create a decision framework for choosing the right archive strategy based on your specific needs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.panel import Panel\n",
    "from rich.columns import Columns\n",
    "\n",
    "# Create decision scenarios\n",
    "decision_scenarios = [\n",
    "    {\n",
    "        'title': 'üöÄ Scenario 1: HPC System Migration',\n",
    "        'situation': 'Need to move simulation to new HPC system and continue run',\n",
    "        'best_choice': 'Critical Files Only',\n",
    "        'reasoning': 'Minimal data transfer, contains everything needed to restart',\n",
    "        'tellus_command': 'tellus archive create migration_package /sim/dir --importance critical'\n",
    "    },\n",
    "    {\n",
    "        'title': 'ü§ù Scenario 2: Collaborator Sharing',\n",
    "        'situation': 'Colleague wants to analyze your CESM output for their paper',\n",
    "        'best_choice': 'Scientific Output',\n",
    "        'reasoning': 'Contains analysis data without unnecessary config/restart files',\n",
    "        'tellus_command': 'tellus archive create shared_results /sim/dir --content-types output,diagnostic'\n",
    "    },\n",
    "    {\n",
    "        'title': 'üìö Scenario 3: Paper Submission',\n",
    "        'situation': 'Need to archive methods and results for journal publication',\n",
    "        'best_choice': 'Production Clean',\n",
    "        'reasoning': 'Professional, complete, excludes temporary/debug files',\n",
    "        'tellus_command': 'tellus archive create paper_submission /sim/dir --importance critical,important --exclude-patterns temp,debug,log'\n",
    "    },\n",
    "    {\n",
    "        'title': 'üî¨ Scenario 4: Methods Preservation',\n",
    "        'situation': 'Want to preserve analysis workflow for future students',\n",
    "        'best_choice': 'Analysis Workflow',\n",
    "        'reasoning': 'Focus on scripts, documentation, reproducibility',\n",
    "        'tellus_command': 'tellus archive create methods_archive /sim/dir --content-types script,metadata'\n",
    "    }\n",
    "]\n",
    "\n",
    "console.print(\"\\n[bold blue]üéØ Decision Framework: Which Archive Strategy?[/bold blue]\")\n",
    "console.print(\"=\" * 70)\n",
    "\n",
    "# Create panels for each scenario\n",
    "panels = []\n",
    "for scenario in decision_scenarios:\n",
    "    panel_content = (\n",
    "        f\"[bold green]Situation:[/bold green]\\n{scenario['situation']}\\n\\n\"\n",
    "        f\"[bold cyan]Best Choice:[/bold cyan] {scenario['best_choice']}\\n\\n\"\n",
    "        f\"[bold yellow]Why:[/bold yellow]\\n{scenario['reasoning']}\\n\\n\"\n",
    "        f\"[bold blue]Command:[/bold blue]\\n[dim]{scenario['tellus_command']}[/dim]\"\n",
    "    )\n",
    "    \n",
    "    panel = Panel(\n",
    "        panel_content,\n",
    "        title=scenario['title'],\n",
    "        border_style=\"blue\",\n",
    "        padding=(1, 1)\n",
    "    )\n",
    "    panels.append(panel)\n",
    "\n",
    "# Display scenarios in columns\n",
    "console.print(Columns(panels[:2], equal=True))\n",
    "console.print(Columns(panels[2:], equal=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Earth Science File Patterns\n",
    "\n",
    "Let's explore how Tellus recognizes different Earth Science model patterns automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create examples of different Earth Science model file patterns\n",
    "earth_science_patterns = {\n",
    "    'CESM': {\n",
    "        'patterns': {\n",
    "            'cam.h0.*.nc': 'CAM atmospheric monthly output',\n",
    "            'cam.h1.*.nc': 'CAM atmospheric daily/hourly output', \n",
    "            'pop.h.*.nc': 'POP ocean monthly output',\n",
    "            'clm.h0.*.nc': 'CLM land monthly output',\n",
    "            'cice.h.*.nc': 'CICE sea ice monthly output',\n",
    "            'cam.r.*.nc': 'CAM restart files',\n",
    "            'pop.r.*.nc': 'POP restart files',\n",
    "            'user_nl_*': 'CESM component namelists',\n",
    "            'rpointer.*': 'Restart pointer files'\n",
    "        },\n",
    "        'description': 'Community Earth System Model - comprehensive climate modeling'\n",
    "    },\n",
    "    'WRF': {\n",
    "        'patterns': {\n",
    "            'wrfout_d*': 'WRF atmospheric output',\n",
    "            'wrfrst_d*': 'WRF restart files',\n",
    "            'wrfbdy_d*': 'WRF boundary condition files',\n",
    "            'namelist.input': 'WRF main namelist',\n",
    "            'namelist.wps': 'WRF preprocessing namelist'\n",
    "        },\n",
    "        'description': 'Weather Research and Forecasting - mesoscale atmospheric modeling'\n",
    "    },\n",
    "    'ICON': {\n",
    "        'patterns': {\n",
    "            '*atm_*': 'ICON atmospheric output',\n",
    "            '*oce_*': 'ICON ocean output', \n",
    "            '*lnd_*': 'ICON land output',\n",
    "            'icon_master.namelist': 'ICON master configuration',\n",
    "            'NAMELIST_*': 'ICON component namelists'\n",
    "        },\n",
    "        'description': 'Icosahedral Nonhydrostatic - next-generation global modeling'\n",
    "    },\n",
    "    'ECHAM': {\n",
    "        'patterns': {\n",
    "            '*BOT*': 'ECHAM surface output',\n",
    "            '*ATM*': 'ECHAM atmospheric output',\n",
    "            'namelist.echam': 'ECHAM namelist',\n",
    "            'rerun_*': 'ECHAM restart files'\n",
    "        },\n",
    "        'description': 'ECHAM - atmospheric general circulation model'\n",
    "    },\n",
    "    'FESOM': {\n",
    "        'patterns': {\n",
    "            '*.fesom.*': 'FESOM ocean output',\n",
    "            'namelist.config': 'FESOM configuration',\n",
    "            'forcing/*': 'FESOM forcing data'\n",
    "        },\n",
    "        'description': 'Finite Element Sea Ice-Ocean Model'\n",
    "    }\n",
    "}\n",
    "\n",
    "console.print(\"\\n[bold blue]üåç Earth Science Model Pattern Recognition[/bold blue]\")\n",
    "console.print(\"=\" * 60)\n",
    "\n",
    "for model, info in earth_science_patterns.items():\n",
    "    console.print(f\"\\n[bold cyan]{model}:[/bold cyan] {info['description']}\")\n",
    "    \n",
    "    patterns_table = Table(show_header=True, header_style=\"bold magenta\")\n",
    "    patterns_table.add_column(\"Pattern\", style=\"yellow\")\n",
    "    patterns_table.add_column(\"Description\", style=\"green\")\n",
    "    \n",
    "    for pattern, description in info['patterns'].items():\n",
    "        patterns_table.add_row(pattern, description)\n",
    "    \n",
    "    console.print(patterns_table)\n",
    "\n",
    "# Show how patterns help with classification\n",
    "classification_help = Panel(\n",
    "    \"[green]How Pattern Recognition Helps:[/green]\\n\\n\"\n",
    "    \"‚úÖ [cyan]Automatic Classification:[/cyan] Files sorted by purpose without manual work\\n\"\n",
    "    \"‚úÖ [cyan]Model-Aware Decisions:[/cyan] Knows that cam.r.*.nc files are critical restarts\\n\"\n",
    "    \"‚úÖ [cyan]Smart Defaults:[/cyan] Suggests appropriate archive strategies per model\\n\"\n",
    "    \"‚úÖ [cyan]Validation:[/cyan] Warns if expected files are missing\\n\"\n",
    "    \"‚úÖ [cyan]Documentation:[/cyan] Automatically documents what each file type contains\",\n",
    "    title=\"üß† Smart Pattern Recognition\",\n",
    "    border_style=\"green\"\n",
    ")\n",
    "\n",
    "console.print(f\"\\n{classification_help}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands-On Exercise: Creating Your Archive Strategy\n",
    "\n",
    "Let's practice with a scenario-based exercise to reinforce the concepts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive exercise setup\n",
    "exercise_scenarios = [\n",
    "    {\n",
    "        'scenario': 'You have a 2-year CESM simulation (500GB) that needs to go to tape storage. You want to continue the run later but tape access is slow and expensive.',\n",
    "        'challenge': 'Balance storage cost with restart capability',\n",
    "        'solution': 'Create two archives: 1) Critical files for restart (small), 2) Complete archive for long-term storage',\n",
    "        'commands': [\n",
    "            'tellus archive create restart_2024 /cesm/run --importance critical --location local_disk',\n",
    "            'tellus archive create complete_2024 /cesm/run --location tape_storage'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'scenario': 'Your WRF hurricane simulation produced 1000+ files. Your collaborator only needs the precipitation and wind fields for their flooding study.',\n",
    "        'challenge': 'Share only relevant data, not everything',\n",
    "        'solution': 'Create selective archive with specific variables/patterns',\n",
    "        'commands': [\n",
    "            'tellus archive create hurricane_winds_precip /wrf/output --patterns \"*RAINNC*,*U10*,*V10*\" --content-types output'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'scenario': 'You developed a new analysis workflow for ICON output. The code worked perfectly and you want to preserve it for future projects.',\n",
    "        'challenge': 'Archive methods for reproducibility',\n",
    "        'solution': 'Focus on scripts, configuration, and documentation',\n",
    "        'commands': [\n",
    "            'tellus archive create icon_analysis_methods /project/dir --content-types script,metadata,input --exclude-patterns output,temp,log'\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "console.print(\"\\n[bold blue]üéì Hands-On Exercise: Archive Strategy Planning[/bold blue]\")\n",
    "console.print(\"=\" * 65)\n",
    "\n",
    "for i, exercise in enumerate(exercise_scenarios, 1):\n",
    "    console.print(f\"\\n[bold yellow]Exercise {i}:[/bold yellow]\")\n",
    "    console.print(f\"[blue]Scenario:[/blue] {exercise['scenario']}\")\n",
    "    console.print(f\"[red]Challenge:[/red] {exercise['challenge']}\")\n",
    "    console.print(f\"[green]Solution Approach:[/green] {exercise['solution']}\")\n",
    "    console.print(f\"[cyan]Recommended Commands:[/cyan]\")\n",
    "    for cmd in exercise['commands']:\n",
    "        console.print(f\"  [dim]{cmd}[/dim]\")\n",
    "    console.print(\"\")\n",
    "\n",
    "# Practice exercise\n",
    "practice_exercise = Panel(\n",
    "    \"[bold green]Your Turn![/bold green]\\n\\n\"\n",
    "    \"[blue]Scenario:[/blue] You have a CESM paleoclimate simulation (Last Glacial Maximum) with:\\n\"\n",
    "    \"‚Ä¢ 50 years of monthly output (200GB)\\n\"\n",
    "    \"‚Ä¢ Restart files for year 50 (5GB)\\n\"\n",
    "    \"‚Ä¢ Analysis scripts and plots (100MB)\\n\"\n",
    "    \"‚Ä¢ Log files and diagnostics (2GB)\\n\\n\"\n",
    "    \"[red]Challenge:[/red] Create THREE different archives for different purposes:\\n\"\n",
    "    \"1. Quick sharing with paleoclimate community\\n\"\n",
    "    \"2. Continuation of simulation to 100 years\\n\"\n",
    "    \"3. Publication-ready archive for journal\\n\\n\"\n",
    "    \"[yellow]Think about:[/yellow]\\n\"\n",
    "    \"‚Ä¢ What content types would you include for each?\\n\"\n",
    "    \"‚Ä¢ What importance levels are relevant?\\n\"\n",
    "    \"‚Ä¢ Any patterns to exclude?\\n\"\n",
    "    \"‚Ä¢ Which storage location for each archive?\",\n",
    "    title=\"üß† Practice Exercise\",\n",
    "    border_style=\"green\"\n",
    ")\n",
    "\n",
    "console.print(practice_exercise)\n",
    "\n",
    "# Show solution after thinking time\n",
    "solution_panel = Panel(\n",
    "    \"[bold cyan]Suggested Solutions:[/bold cyan]\\n\\n\"\n",
    "    \"[green]1. Community Sharing:[/green]\\n\"\n",
    "    \"tellus archive create lgm_community_data /simulation \\\\\\n\"\n",
    "    \"  --content-types output,diagnostic --location fast_cloud\\n\\n\"\n",
    "    \"[green]2. Simulation Continuation:[/green]\\n\"\n",
    "    \"tellus archive create lgm_restart_y50 /simulation \\\\\\n\"\n",
    "    \"  --importance critical --location local_backup\\n\\n\"\n",
    "    \"[green]3. Publication Archive:[/green]\\n\"\n",
    "    \"tellus archive create lgm_publication /simulation \\\\\\n\"\n",
    "    \"  --importance critical,important \\\\\\n\"\n",
    "    \"  --exclude-patterns log,temp,debug \\\\\\n\"\n",
    "    \"  --location long_term_storage\",\n",
    "    title=\"üí° Solution\",\n",
    "    border_style=\"cyan\"\n",
    ")\n",
    "\n",
    "console.print(f\"\\n{solution_panel}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Mistakes and How to Avoid Them\n",
    "\n",
    "Let's look at typical beginner mistakes in selective archiving:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common mistakes and solutions\n",
    "common_mistakes = [\n",
    "    {\n",
    "        'mistake': '‚ùå Over-filtering',\n",
    "        'example': 'Creating \"output only\" archive but excluding diagnostic files',\n",
    "        'problem': 'Diagnostic files often contain derived quantities needed for analysis',\n",
    "        'solution': 'Include both OUTPUT and DIAGNOSTIC content types for analysis archives',\n",
    "        'better_command': 'tellus archive create analysis_data /sim --content-types output,diagnostic'\n",
    "    },\n",
    "    {\n",
    "        'mistake': '‚ùå Including temporary files',\n",
    "        'example': 'Not excluding temp/debug files from production archives',\n",
    "        'problem': 'Archives become bloated with unnecessary temporary data',\n",
    "        'solution': 'Always exclude temporary patterns in production archives',\n",
    "        'better_command': 'tellus archive create production /sim --exclude-patterns temp,debug,scratch,work_in_progress'\n",
    "    },\n",
    "    {\n",
    "        'mistake': '‚ùå Wrong restart strategy',\n",
    "        'example': 'Including only .r. files without pointer files or namelists',\n",
    "        'problem': 'Cannot actually restart simulation without complete configuration',\n",
    "        'solution': 'Use importance:critical which includes all restart-essential files',\n",
    "        'better_command': 'tellus archive create restart_package /sim --importance critical'\n",
    "    },\n",
    "    {\n",
    "        'mistake': '‚ùå Ignoring file relationships',\n",
    "        'example': 'Archiving analysis scripts without the input data they process',\n",
    "        'problem': 'Scripts become useless without their input data context',\n",
    "        'solution': 'Consider workflow dependencies when selecting content types',\n",
    "        'better_command': 'tellus archive create complete_workflow /sim --content-types script,output,metadata'\n",
    "    },\n",
    "    {\n",
    "        'mistake': '‚ùå Poor naming conventions',\n",
    "        'example': 'Using generic names like \"archive1\", \"data_backup\"',\n",
    "        'problem': 'Cannot identify purpose or contents later',\n",
    "        'solution': 'Use descriptive names that indicate content and purpose',\n",
    "        'better_command': 'tellus archive create cesm_lgm_outputs_2024q1 /sim --content-types output'\n",
    "    }\n",
    "]\n",
    "\n",
    "console.print(\"\\n[bold blue]‚ö†Ô∏è  Common Mistakes in Selective Archiving[/bold blue]\")\n",
    "console.print(\"=\" * 60)\n",
    "\n",
    "for i, mistake in enumerate(common_mistakes, 1):\n",
    "    console.print(f\"\\n[bold red]{i}. {mistake['mistake']}[/bold red]\")\n",
    "    console.print(f\"[yellow]Example:[/yellow] {mistake['example']}\")\n",
    "    console.print(f\"[red]Problem:[/red] {mistake['problem']}\")\n",
    "    console.print(f\"[green]Solution:[/green] {mistake['solution']}\")\n",
    "    console.print(f\"[cyan]Better Command:[/cyan] [dim]{mistake['better_command']}[/dim]\")\n",
    "\n",
    "# Best practices summary\n",
    "best_practices = Panel(\n",
    "    \"[bold green]Best Practices for Selective Archiving:[/bold green]\\n\\n\"\n",
    "    \"‚úÖ [cyan]Test First:[/cyan] Create small test archives to verify selection criteria\\n\"\n",
    "    \"‚úÖ [cyan]Document Purpose:[/cyan] Use descriptive archive names and metadata\\n\"\n",
    "    \"‚úÖ [cyan]Consider Relationships:[/cyan] Think about file dependencies in your workflow\\n\"\n",
    "    \"‚úÖ [cyan]Validate Contents:[/cyan] Always check archive metadata before long-term storage\\n\"\n",
    "    \"‚úÖ [cyan]Plan for Future:[/cyan] Consider how you'll use the archive later\\n\"\n",
    "    \"‚úÖ [cyan]Use Patterns Wisely:[/cyan] Leverage model-specific patterns for better classification\\n\"\n",
    "    \"‚úÖ [cyan]Multiple Strategies:[/cyan] Create different archives for different purposes\",\n",
    "    title=\"üìã Best Practices\",\n",
    "    border_style=\"green\"\n",
    ")\n",
    "\n",
    "console.print(f\"\\n{best_practices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup tutorial files\n",
    "import shutil\n",
    "\n",
    "console.print(\"\\n[bold blue]üßπ Cleaning up tutorial files...[/bold blue]\")\n",
    "shutil.rmtree(tutorial_dir)\n",
    "console.print(f\"[green]‚úÖ Cleaned up: {tutorial_dir}[/green]\")\n",
    "\n",
    "# Tutorial summary\n",
    "summary = Panel(\n",
    "    \"[bold green]üéì Tutorial 2 Complete - You've Mastered Selective Archiving![/bold green]\\n\\n\"\n",
    "    \"[cyan]Key Skills Learned:[/cyan]\\n\"\n",
    "    \"‚úÖ Understanding automatic file classification\\n\"\n",
    "    \"‚úÖ Creating targeted archives for specific purposes\\n\"\n",
    "    \"‚úÖ Recognizing Earth Science model patterns\\n\"\n",
    "    \"‚úÖ Making strategic decisions about what to archive\\n\"\n",
    "    \"‚úÖ Avoiding common archiving mistakes\\n\\n\"\n",
    "    \"[yellow]Real-World Impact:[/yellow]\\n\"\n",
    "    \"‚Ä¢ Reduce storage costs by 60-90%\\n\"\n",
    "    \"‚Ä¢ Faster data transfers and access\\n\"\n",
    "    \"‚Ä¢ Better organized, purposeful archives\\n\"\n",
    "    \"‚Ä¢ Improved collaboration and sharing\\n\\n\"\n",
    "    \"[blue]Next: Tutorial 3 - DateTime-Based Extraction[/blue]\",\n",
    "    title=\"üéâ Tutorial Summary\",\n",
    "    border_style=\"green\"\n",
    ")\n",
    "\n",
    "console.print(summary)\n",
    "\n",
    "console.print(\"\\n[bold blue]üìö Ready for Next Tutorial?[/bold blue]\")\n",
    "console.print(\"Tutorial 3 will teach you how to extract specific time periods from your archives - perfect for seasonal analysis, event studies, and temporal workflows.\")\n",
    "console.print(\"\\n[dim]Continue to: archive-tutorial-03-datetime-filtering.ipynb[/dim]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
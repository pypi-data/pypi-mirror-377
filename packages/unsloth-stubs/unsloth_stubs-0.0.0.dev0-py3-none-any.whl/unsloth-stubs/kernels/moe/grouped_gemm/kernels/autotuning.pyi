import torch
import triton
from _typeshed import Incomplete

logger: Incomplete
DEFAULT_M_BLOCK_SIZES: Incomplete
DEFAULT_N_BLOCK_SIZES: Incomplete
DEFAULT_K_BLOCK_SIZES: Incomplete
DEFAULT_NUM_CTAS: int
DEFAULT_NUM_WARPS: Incomplete
DEFAULT_NUM_STAGES: Incomplete
BOOLS: Incomplete

def val_to_list(val): ...
def convert_args_to_list(args): ...
def get_forward_configs(BLOCK_M=..., BLOCK_N=..., BLOCK_K=..., TMA_LOAD_X: bool = True, TMA_LOAD_W: bool = True, TMA_STORE: bool = False, num_warps=..., num_stages=..., num_ctas=...): ...
def get_dX_kernel_configs(BLOCK_M=..., BLOCK_N=..., BLOCK_K=..., TMA_LOAD_dY: bool = True, TMA_LOAD_W: bool = True, TMA_STORE: bool = False, num_warps=..., num_stages=..., num_ctas=...): ...
def get_dW_kernel_configs(BLOCK_M=..., BLOCK_N=..., BLOCK_K=..., num_warps=..., num_stages=..., num_ctas=..., TMA_LOAD_dY: bool = True, TMA_LOAD_X: bool = True, TMA_STORE: bool = False): ...
def estimate_smem_reqs(num_stages: int, BLOCK_SIZE_M: int, BLOCK_SIZE_N: int, BLOCK_SIZE_K: int, dtype: torch.dtype): ...
def exceeds_smem_capacity(num_stages: int, BLOCK_SIZE_M: int, BLOCK_SIZE_N: int, BLOCK_SIZE_K: int, dtype: torch.dtype, smem_size: int, slack: float = 50000): ...
def common_prune_criteria(config: triton.Config, kwargs: dict, dtype): ...
def maybe_disable_tma(config: triton.Config): ...
def prune_kernel_configs_fwd(configs: list[triton.Config], args, **kwargs): ...
def prune_dX_configs(configs: list[triton.Config], args, **kwargs): ...
def prune_kernel_configs_backward_dW(configs: list[triton.Config], args, **kwargs): ...

import os
import torch
from typing import Callable

__all__ = ['print_quantization_methods', 'unsloth_save_model', 'save_to_gguf', 'patch_saving_functions', 'create_huggingface_repo']

def print_quantization_methods() -> None: ...
@torch.inference_mode
def unsloth_save_model(model, tokenizer, save_directory: str | os.PathLike, save_method: str = 'lora', push_to_hub: bool = False, token: str | bool | None = None, is_main_process: bool = True, state_dict: dict | None = None, save_function: Callable = ..., max_shard_size: int | str = '5GB', safe_serialization: bool = True, variant: str | None = None, save_peft_format: bool = True, use_temp_dir: bool | None = None, commit_message: str | None = 'Trained with Unsloth', private: bool | None = None, create_pr: bool = False, revision: str = None, commit_description: str = 'Upload model trained with Unsloth 2x faster', tags: list[str] = None, temporary_location: str = '_unsloth_temporary_saved_buffers', maximum_memory_usage: float = 0.9): ...
def save_to_gguf(model_type: str, model_dtype: str, is_sentencepiece: bool = False, model_directory: str = 'unsloth_finetuned_model', quantization_method: str = 'fast_quantized', first_conversion: str = None, _run_installer=None): ...
def create_huggingface_repo(model, save_directory, token=None, private: bool = False): ...
def patch_saving_functions(model, vision: bool = False): ...

import torch
import triton
from _typeshed import Incomplete
from dataclasses import asdict as asdict
from grouped_gemm.kernels.tuning import KernelConfigBackward_dW as KernelConfigBackward_dW, KernelConfigBackward_dX as KernelConfigBackward_dX, KernelConfigForward as KernelConfigForward

logger: Incomplete
formatter: Incomplete
ch: Incomplete

def supports_tma(): ...
def get_per_device_per_stream_alloc_fn(device): ...
def log_kernel_info(compiled_kernel: triton.compiler.CompiledKernel, best_config: triton.Config = None): ...
def grouped_gemm_forward(X: torch.Tensor, W: torch.Tensor, topk: int, m_sizes: torch.Tensor, gather_indices: torch.Tensor = None, topk_weights: torch.Tensor = None, permute_x: bool = False, permute_y: bool = False, fuse_mul_post: bool = False, autotune: bool = False, BLOCK_SIZE_M: int = 32, BLOCK_SIZE_N: int = 32, BLOCK_SIZE_K: int = 32, num_warps: int = 4, num_stages: int = 2, use_tma_load_w: bool = False, use_tma_load_x: bool = False, use_tma_store: bool = False, flatten: bool = True, debug: bool = False) -> torch.Tensor: ...
def grouped_gemm_dX(dY: torch.Tensor, W: torch.Tensor, gather_indices: torch.Tensor, m_sizes: torch.Tensor, topk: int, BLOCK_SIZE_M: int = 32, BLOCK_SIZE_N: int = 32, BLOCK_SIZE_K: int = 32, debug: bool = False, permute_x: bool = False, permute_y: bool = False, use_tma_load_w: bool = False, use_tma_load_dy: bool = False, use_tma_store: bool = False, num_warps: int = 4, num_stages: int = 2, flatten: bool = True, fuse_mul_pre: bool = False, fuse_mul_post: bool = False, autotune: bool = False) -> torch.Tensor: ...
def grouped_gemm_dW(X: torch.Tensor, dY: torch.Tensor, m_sizes: torch.Tensor, gather_indices: torch.Tensor, topk: int, BLOCK_SIZE_M: int = 32, BLOCK_SIZE_N: int = 32, BLOCK_SIZE_K: int = 32, permute_x: bool = False, permute_y: bool = False, use_tma_load_dy: bool = False, use_tma_load_x: bool = False, use_tma_store: bool = False, fuse_mul_pre: bool = False, fuse_mul_post: bool = False, num_warps: int = 4, num_stages: int = 2, flatten: bool = True, autotune: bool = False, debug: bool = False) -> torch.Tensor: ...

class GroupedGemm(torch.autograd.Function):
    @staticmethod
    def forward(ctx, X, W, m_sizes, topk, gather_indices, permute_x, permute_y, topk_weights, fuse_mul_post, kernel_config_fwd, kernel_config_bwd_dX, kernel_config_bwd_dW, autotune, dX_only, dW_only): ...
    @staticmethod
    def backward(ctx, dY): ...

def check_valid_config_fwd(permute_x, permute_y, use_tma_load_x, use_tma_load_w, use_tma_store, fuse_mul_post, is_first_gemm) -> None: ...
def check_valid_config_bwd_dW(permute_x, permute_y, use_tma_load_dY, use_tma_load_x, use_tma_store, fuse_mul_post, is_first_gemm) -> None: ...
def check_valid_config_bwd_dX(permute_x, permute_y, use_tma_load_dY, use_tma_load_w, use_tma_store, fuse_mul_post, is_first_gemm) -> None: ...
def grouped_gemm(X: torch.Tensor, W: torch.Tensor, m_sizes: torch.Tensor, topk: int, gather_indices: torch.Tensor = None, permute_x: bool = False, permute_y: bool = False, topk_weights=None, fuse_mul_post: bool = False, kernel_config_fwd: KernelConfigForward = None, kernel_config_bwd_dX: KernelConfigBackward_dX = None, kernel_config_bwd_dW: KernelConfigBackward_dW = None, autotune: bool = False, is_first_gemm: bool = True, dX_only: bool = False, dW_only: bool = False): ...

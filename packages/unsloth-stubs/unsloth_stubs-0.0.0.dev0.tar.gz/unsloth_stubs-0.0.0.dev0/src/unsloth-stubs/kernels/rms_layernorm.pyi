import torch
from .utils import calculate_settings as calculate_settings, torch_gpu_device as torch_gpu_device
from transformers.models.llama.modeling_llama import LlamaRMSNorm
from transformers.models.mllama.modeling_mllama import MllamaTextRMSNorm

class Fast_RMS_Layernorm(torch.autograd.Function):
    @staticmethod
    def forward(ctx, X: torch.Tensor, W: torch.Tensor, eps: float, gemma: bool = False): ...
    @staticmethod
    def backward(ctx, dY: torch.Tensor): ...

@torch.compiler.disable
def fast_rms_layernorm(layernorm, X: torch.Tensor, gemma: bool = False): ...

class Unsloth_LlamaRMSNorm(LlamaRMSNorm):
    def forward(self, X): ...

class Unsloth_MllamaTextRMSNorm(MllamaTextRMSNorm):
    def forward(self, X): ...

def patch_rms_layernorm() -> None: ...
def unpatch_rms_layernorm() -> None: ...
def test_rms_layernorm(dim: int = 1024, eps: float = 1e-05, dtype=..., bsz: int = 21, random_state: int = 3407, seqlen: int = 3341) -> None: ...
def testing_suite_layernorm() -> None: ...

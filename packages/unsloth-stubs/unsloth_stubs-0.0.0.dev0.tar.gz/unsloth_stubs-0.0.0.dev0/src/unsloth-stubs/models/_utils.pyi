import logging
import xformers.ops.fmha as xformers
from _typeshed import Incomplete
from importlib.metadata import version as importlib_version
from peft.utils.integrations import dequantize_module_weight as dequantize_module_weight
from platform import system as platform_system
from typing import Any
from unsloth_zoo.gradient_checkpointing import Unsloth_Offloaded_Gradient_Checkpointer as Unsloth_Offloaded_Gradient_Checkpointer, patch_gradient_checkpointing as patch_gradient_checkpointing, patch_unsloth_gradient_checkpointing as patch_unsloth_gradient_checkpointing, patch_unsloth_smart_gradient_checkpointing as patch_unsloth_smart_gradient_checkpointing, unpatch_gradient_checkpointing as unpatch_gradient_checkpointing, unpatch_unsloth_gradient_checkpointing as unpatch_unsloth_gradient_checkpointing, unpatch_unsloth_smart_gradient_checkpointing as unpatch_unsloth_smart_gradient_checkpointing, unsloth_offloaded_gradient_checkpoint as unsloth_offloaded_gradient_checkpoint
from unsloth_zoo.loss_utils import HAS_CUT_CROSS_ENTROPY as HAS_CUT_CROSS_ENTROPY, fused_linear_cross_entropy as fused_linear_cross_entropy, unsloth_fused_ce_loss as unsloth_fused_ce_loss
from unsloth_zoo.patching_utils import patch_compiled_autograd as patch_compiled_autograd, patch_compiling_bitsandbytes as patch_compiling_bitsandbytes, patch_layernorm as patch_layernorm, patch_model_and_tokenizer as patch_model_and_tokenizer, patch_torch_compile as patch_torch_compile
from unsloth_zoo.vision_utils import process_vision_info as process_vision_info
from xformers import __version__ as xformers_version

__all__ = ['SUPPORTS_BFLOAT16', 'is_bfloat16_supported', 'is_vLLM_available', 'prepare_model_for_kbit_training', 'xformers', 'xformers_attention', 'xformers_version', '__version__', 'importlib_version', 'HAS_FLASH_ATTENTION', 'HAS_FLASH_ATTENTION_SOFTCAPPING', 'USE_MODELSCOPE', 'platform_system', 'patch_tokenizer', 'get_statistics', 'Unsloth_Offloaded_Gradient_Checkpointer', 'offload_to_disk', 'offload_input_embeddings', 'offload_output_embeddings', 'unsloth_offloaded_gradient_checkpoint', 'torch_compile_options', 'patch_linear_scaling', 'patch_llama_rope_scaling', 'create_boolean_mask', 'torch_amp_custom_fwd', 'torch_amp_custom_bwd', 'patch_gradient_accumulation_fix', 'patch_compiling_bitsandbytes', 'patch_regional_compilation', 'patch_layernorm', 'patch_torch_compile', 'patch_model_and_tokenizer', 'patch_unsloth_gradient_checkpointing', 'unpatch_unsloth_gradient_checkpointing', 'patch_gradient_checkpointing', 'unpatch_gradient_checkpointing', 'HAS_CUT_CROSS_ENTROPY', 'EMPTY_LOGITS', 'fused_linear_cross_entropy', 'unsloth_fused_ce_loss', 'patch_unsloth_smart_gradient_checkpointing', 'unpatch_unsloth_smart_gradient_checkpointing', 'patch_compiled_autograd', 'process_vision_info', 'unsloth_compile_transformers', 'patch_fast_lora', 'validate_loftq_config', 'RaiseUninitialized', 'dequantize_module_weight']

__version__: str

class HideLoggingMessage(logging.Filter):
    text: Incomplete
    def __init__(self, text) -> None: ...
    def filter(self, x): ...

class _RaiseUninitialized(logging.Handler):
    def __init__(self) -> None: ...
    def emit(self, record) -> None: ...

class RaiseUninitialized:
    error_handler: Incomplete
    def __init__(self) -> None: ...
    def remove(self) -> None: ...

torch_amp_custom_fwd: Incomplete
torch_amp_custom_bwd: Incomplete
SUPPORTS_BFLOAT16: bool
HAS_FLASH_ATTENTION: bool
HAS_FLASH_ATTENTION_SOFTCAPPING: bool
xformers_attention: Incomplete
torch_compile_options: Incomplete

def patch_regional_compilation(): ...
def prepare_model_for_kbit_training(model: Any, use_gradient_checkpointing: Incomplete = True, use_reentrant: bool | None = True) -> Any: ...
def get_statistics() -> None: ...
def offload_to_disk(W, model, name, temporary_location: str = '_unsloth_temporary_saved_buffers'): ...
def offload_input_embeddings(model, temporary_location: str = '_unsloth_temporary_saved_buffers'): ...
def offload_output_embeddings(model, temporary_location: str = '_unsloth_temporary_saved_buffers'): ...
def is_bfloat16_supported(): ...
def is_vLLM_available(): ...
def patch_linear_scaling(model_name: str = 'gemma2', rope_module=None, scaled_rope_module=None, attention_module=None): ...
def patch_llama_rope_scaling(model_name: str = 'llama', rope_module=None, scaled_rope_module=None, extended_rope_module=None, attention_module=None, longrope_module=None): ...
def create_boolean_mask(n: int = 4096, sliding_window: int = 2048): ...
def patch_gradient_accumulation_fix(Trainer) -> None: ...
def patch_tokenizer(model, tokenizer): ...
def patch_fast_lora() -> None: ...
def unsloth_compile_transformers(dtype, model_name, model_types, token=None, revision=None, trust_remote_code: bool = False, sdpa_dynamic_mask: bool = True, sdpa_bool_masks: bool = True, sdpa_gqa_replace: bool = True, sdpa_dynamic_compile: bool = True, compile_attention: bool = True, disable_causal_masks: bool = True, compile_torch_modules: bool = True, compile_custom_modules: bool = True, compile_function_calls: bool = True, fuse_lm_head: bool = True, gradient_checkpointing: bool = True, manual_replacements: bool = True, fast_lora_forwards: bool = True, fast_residual_stream: bool = True, accurate_accumulation: bool = True, epilogue_fusion: bool = True, max_autotune: bool = False, shape_padding: bool = True, cudagraphs: bool = False, debug: bool = False, fullgraph: bool = True, import_from_cache: bool = False, disable: bool = False, return_logits: bool = False, unsloth_force_compile: bool = False): ...

class EmptyLogits:
    def __init__(self) -> None: ...
    def raise_getattr_error(self, attr): ...
    __getitem__ = raise_logits_error
    __getattr__ = raise_getattr_error

EMPTY_LOGITS: Incomplete
USE_MODELSCOPE: Incomplete

def validate_loftq_config(loftq_config, lora_dropout, bias, init_lora_weights, model): ...

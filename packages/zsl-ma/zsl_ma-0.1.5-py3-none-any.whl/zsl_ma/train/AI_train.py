import argparse
import os
import time  # å¯¼å…¥æ—¶é—´è®¡ç®—æ¨¡å—
from dataclasses import dataclass
from typing import Optional, List

from torchvision import transforms

from zsl_ma.tools.predict_untils import euclidean_predict
from zsl_ma.tools.tool import get_device, setup_save_dirs, setup_logger
from zsl_ma.train import train_disent
from zsl_ma.train.train_att_proj import train_proj, TrainAttprojConfig
from zsl_ma.train.train_cls import TrainClsConfig, train_cls
from zsl_ma.train.train_disent import TrainDisentConfig
from zsl_ma.train.train_fea_proj import TrainFeaProjConfig, train_fea_proj


def run(configs):
    # -------------------------- åˆå§‹åŒ–:æ•´ä½“æ—¶é—´è®°å½•+ä¿å­˜ç›®å½•+æ—¥å¿— --------------------------
    start_total_time = time.perf_counter()  # æ•´ä½“è®­ç»ƒå¼€å§‹æ—¶é—´(é«˜ç²¾åº¦)
    save_dir = setup_save_dirs(configs.save_dir, 'exp')
    logger = setup_logger(save_dir)
    logger.info("=" * 60)
    logger.info("å¼€å§‹é›¶æ ·æœ¬å­¦ä¹ å®Œæ•´è®­ç»ƒæµç¨‹")
    logger.info(f"é…ç½®ä¿¡æ¯:{vars(configs)}")
    logger.info(f"ç»“æœä¿å­˜ç›®å½•:{save_dir}")
    logger.info("=" * 60)

    # è®¾å¤‡åˆå§‹åŒ–
    device = get_device()
    logger.info(f"ä½¿ç”¨è®¡ç®—è®¾å¤‡:{device.type}(è®¾å¤‡ID:{device.index if device.index is not None else 'æ— '})")

    # ç»Ÿä¸€å›¾åƒå˜æ¢é…ç½®
    transform = transforms.Compose([
        transforms.Resize((64, 64)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    logger.info("å›¾åƒé¢„å¤„ç†ç®¡é“åˆå§‹åŒ–å®Œæˆ:Resize(64,64) â†’ ToTensor â†’ Normalize")

    # -------------------------- 1. ç‰¹å¾è§£è€¦è®­ç»ƒ(å«è€—æ—¶è®¡ç®—) --------------------------
    logger.info("\n" + "=" * 50)
    logger.info("é˜¶æ®µ1/4:ç‰¹å¾è§£è€¦æ¨¡å‹è®­ç»ƒ")
    logger.info("=" * 50)

    # é…ç½®ç‰¹å¾è§£è€¦å‚æ•°
    disent_config = TrainDisentConfig(
        device=device,
        transform=transform,
        class_dims=configs.class_dims,
        attribute_dim=configs.attribute_dim,
        save_dir=save_dir,
        data_dir=configs.data_dir,
        train_class=configs.train_class,
        factor_index_map_path=configs.factor_index_map_path,
        batch_size=configs.disent_batch_size,
        epochs=configs.disent_epochs,
        lr=configs.disent_lr,
        patience=configs.disent_patience,
        num_workers=configs.num_workers,
    )
    logger.info(f"ç‰¹å¾è§£è€¦è®­ç»ƒé…ç½®:{disent_config}")

    # è®°å½•é˜¶æ®µå¼€å§‹æ—¶é—´
    start_disent = time.perf_counter()
    # è°ƒç”¨è®­ç»ƒå‡½æ•°(éœ€ç¡®ä¿train_disentå†…éƒ¨ç”¨loggerè¾“å‡ºè¯¦ç»†æ—¥å¿—)
    train_disent.train_disent(disent_config)  # ä¼ å…¥logger,è®©è®­ç»ƒç»†èŠ‚å†™å…¥æ—¥å¿—
    # è®¡ç®—é˜¶æ®µè€—æ—¶
    end_disent = time.perf_counter()
    disent_time = end_disent - start_disent
    logger.info(f"é˜¶æ®µ1å®Œæˆ:ç‰¹å¾è§£è€¦è®­ç»ƒè€—æ—¶ â†’ {disent_time:.2f}ç§’({disent_time / 60:.2f}åˆ†é’Ÿ)")

    # -------------------------- 2. å±æ€§æŠ•å½±è®­ç»ƒ(å«è€—æ—¶è®¡ç®—) --------------------------
    logger.info("\n" + "=" * 50)
    logger.info("é˜¶æ®µ2/4:å±æ€§æŠ•å½±æ¨¡å‹è®­ç»ƒ")
    logger.info("=" * 50)

    # é…ç½®å±æ€§æŠ•å½±å‚æ•°
    att_proj_opts = TrainAttprojConfig(
        device=device,
        save_dir=save_dir,
        embed_dim=configs.embed_dim,
        attribute_dims=configs.attribute_dim,
        lr=configs.att_proj_lr,
        epochs=configs.att_proj_epochs,
        batch_size=configs.att_proj_batch_size,
        patience=configs.att_proj_patience,
        num_workers=configs.num_workers,
    )
    logger.info(f"å±æ€§æŠ•å½±è®­ç»ƒé…ç½®:{att_proj_opts}")

    # è®°å½•é˜¶æ®µå¼€å§‹æ—¶é—´
    start_att_proj = time.perf_counter()
    # è°ƒç”¨è®­ç»ƒå‡½æ•°(ä¼ å…¥logger)
    train_proj(att_proj_opts)  # ç¡®ä¿train_projå†…éƒ¨ç”¨loggerè¾“å‡º
    # è®¡ç®—é˜¶æ®µè€—æ—¶
    end_att_proj = time.perf_counter()
    att_proj_time = end_att_proj - start_att_proj
    logger.info(f"é˜¶æ®µ2å®Œæˆ:å±æ€§æŠ•å½±è®­ç»ƒè€—æ—¶ â†’ {att_proj_time:.2f}ç§’({att_proj_time / 60:.2f}åˆ†é’Ÿ)")

    # -------------------------- 3. åˆ†ç±»å™¨è®­ç»ƒ(å«è€—æ—¶è®¡ç®—) --------------------------
    logger.info("\n" + "=" * 50)
    logger.info("é˜¶æ®µ3/4:åˆ†ç±»å™¨æ¨¡å‹è®­ç»ƒ")
    logger.info("=" * 50)

    # é…ç½®åˆ†ç±»å™¨å‚æ•°
    cls_config = TrainClsConfig(
        device=device,
        transform=transform,
        data_dir=configs.data_dir,
        save_dir=save_dir,
        train_class=configs.train_class,
        num_workers=configs.num_workers,
        epochs=configs.cls_epochs,
        batch_size=configs.cls_batch_size,
        lr=configs.cls_lr,
        patience=configs.cls_patience,
    )
    logger.info(f"åˆ†ç±»å™¨è®­ç»ƒé…ç½®:{cls_config}")

    # è®°å½•é˜¶æ®µå¼€å§‹æ—¶é—´
    start_cls = time.perf_counter()
    # è°ƒç”¨è®­ç»ƒå‡½æ•°(ä¼ å…¥logger)
    train_cls(cls_config)  # ç¡®ä¿train_clså†…éƒ¨ç”¨loggerè¾“å‡º
    # è®¡ç®—é˜¶æ®µè€—æ—¶
    end_cls = time.perf_counter()
    cls_time = end_cls - start_cls
    logger.info(f"é˜¶æ®µ3å®Œæˆ:åˆ†ç±»å™¨è®­ç»ƒè€—æ—¶ â†’ {cls_time:.2f}ç§’({cls_time / 60:.2f}åˆ†é’Ÿ)")

    # -------------------------- 4. ç‰¹å¾æŠ•å½±è®­ç»ƒ(å«è€—æ—¶è®¡ç®—) --------------------------
    logger.info("\n" + "=" * 50)
    logger.info("é˜¶æ®µ4/4:ç‰¹å¾æŠ•å½±æ¨¡å‹è®­ç»ƒ")
    logger.info("=" * 50)

    # é…ç½®ç‰¹å¾æŠ•å½±å‚æ•°
    fea_proj_opts = TrainFeaProjConfig(
        device=device,
        transform=transform,
        data_dir=configs.data_dir,
        save_dir=save_dir,
        train_class=configs.train_class,
        factor_index_map_path=configs.factor_index_map_path,
        num_workers=configs.num_workers,
        batch_size=configs.fea_proj_batch_size,
        epochs=configs.fea_proj_epochs,
        lr=configs.fea_proj_lr,
        patience=configs.fea_proj_patience,
    )
    logger.info(f"ç‰¹å¾æŠ•å½±è®­ç»ƒé…ç½®:{fea_proj_opts}")

    # è®°å½•é˜¶æ®µå¼€å§‹æ—¶é—´
    start_fea_proj = time.perf_counter()
    # è°ƒç”¨è®­ç»ƒå‡½æ•°(ä¼ å…¥logger)
    fea_proj_model = train_fea_proj(fea_proj_opts)  # ç¡®ä¿train_fea_projå†…éƒ¨ç”¨loggerè¾“å‡º
    # è®¡ç®—é˜¶æ®µè€—æ—¶
    end_fea_proj = time.perf_counter()
    fea_proj_time = end_fea_proj - start_fea_proj
    logger.info(f"é˜¶æ®µ4å®Œæˆ:ç‰¹å¾æŠ•å½±è®­ç»ƒè€—æ—¶ â†’ {fea_proj_time:.2f}ç§’({fea_proj_time / 60:.2f}åˆ†é’Ÿ)")

    # -------------------------- 5. é¢„æµ‹è¯„ä¼°(å«è€—æ—¶è®¡ç®—) --------------------------
    logger.info("\n" + "=" * 50)
    logger.info("é¢å¤–é˜¶æ®µ:æµ‹è¯•é›†é¢„æµ‹è¯„ä¼°")
    logger.info("=" * 50)

    test_list = configs.test_list
    logger.info(f"å¾…æµ‹è¯•é›†åˆ—è¡¨:{test_list},é¢„æµ‹æ‰¹æ¬¡å¤§å°:{configs.predict_batch_size}")

    # è®°å½•é¢„æµ‹æ€»æ—¶é—´
    total_predict_time = 0.0

    for test_img in test_list:
        try:
            # å•æµ‹è¯•é›†è€—æ—¶è®°å½•
            start_single_pred = time.perf_counter()
            # æ‰§è¡Œé¢„æµ‹
            df, metrics = euclidean_predict(
                fea_proj_model,
                configs.data_dir,
                os.path.join(save_dir, 'attributes', 'semantic_embed'),
                os.path.join(configs.data_dir, f'{test_img}.txt'),
                configs.factor_index_map_path,
                device,
                transform,
                ignore_factors=['Operating Condition'],
                batch_size=configs.predict_batch_size
            )
            # å•æµ‹è¯•é›†è€—æ—¶
            end_single_pred = time.perf_counter()
            single_pred_time = end_single_pred - start_single_pred
            total_predict_time += single_pred_time

            # ä¿å­˜é¢„æµ‹ç»“æœ(æ—¥å¿—å†™å…¥)
            result_path = os.path.join(save_dir, f'{test_img}-é¢„æµ‹ç»“æœ.csv')
            df.to_csv(result_path, index=False, encoding='utf-8-sig')
            logger.info(f"âœ… {test_img} é¢„æµ‹å®Œæˆ:")
            logger.info(f"   - ç»“æœæ–‡ä»¶:{result_path}")
            logger.info("   - é¢„æµ‹æŒ‡æ ‡:")
            logger.info('\n' + metrics)
            logger.info(f"   - å•æµ‹è¯•é›†è€—æ—¶:{single_pred_time:.2f}ç§’")

        except Exception as e:
            logger.error(f"âŒ {test_img} é¢„æµ‹å¤±è´¥:{e}", exc_info=True)  # exc_info=Trueè®°å½•å †æ ˆä¿¡æ¯

    # æ€»é¢„æµ‹è€—æ—¶
    logger.info(f"é¢å¤–é˜¶æ®µå®Œæˆ:æ‰€æœ‰æµ‹è¯•é›†é¢„æµ‹æ€»è€—æ—¶ â†’ {total_predict_time:.2f}ç§’({total_predict_time / 60:.2f}åˆ†é’Ÿ)")

    # -------------------------- 6. æ•´ä½“è®­ç»ƒæµç¨‹æ€»ç»“ --------------------------
    logger.info("\n" + "=" * 60)
    logger.info("å®Œæ•´è®­ç»ƒæµç¨‹æ€»ç»“")
    logger.info("=" * 60)

    # å„é˜¶æ®µè€—æ—¶æ±‡æ€»
    logger.info("å„é˜¶æ®µè€—æ—¶æ˜ç»†:")
    logger.info(f"1. ç‰¹å¾è§£è€¦è®­ç»ƒ:{disent_time:.2f}ç§’({disent_time / 60:.2f}åˆ†é’Ÿ)")
    logger.info(f"2. å±æ€§æŠ•å½±è®­ç»ƒ:{att_proj_time:.2f}ç§’({att_proj_time / 60:.2f}åˆ†é’Ÿ)")
    logger.info(f"3. åˆ†ç±»å™¨è®­ç»ƒ:{cls_time:.2f}ç§’({cls_time / 60:.2f}åˆ†é’Ÿ)")
    logger.info(f"4. ç‰¹å¾æŠ•å½±è®­ç»ƒ:{fea_proj_time:.2f}ç§’({fea_proj_time / 60:.2f}åˆ†é’Ÿ)")
    logger.info(f"5. æµ‹è¯•é›†é¢„æµ‹:{total_predict_time:.2f}ç§’({total_predict_time / 60:.2f}åˆ†é’Ÿ)")

    # æ•´ä½“æ€»è€—æ—¶
    end_total_time = time.perf_counter()
    total_time = end_total_time - start_total_time
    logger.info(
        f"ğŸ“Š æ•´ä½“è®­ç»ƒä¸é¢„æµ‹æµç¨‹æ€»è€—æ—¶ â†’ {total_time:.2f}ç§’({total_time / 60:.2f}åˆ†é’Ÿ / {total_time / 3600:.2f}å°æ—¶)")
    logger.info(f"ğŸ‰ æ‰€æœ‰æµç¨‹å®Œæˆï¼ç»“æœç»Ÿä¸€ä¿å­˜äº:{save_dir}")
    logger.info("=" * 60)


def get_train_args(args: Optional[list] = None) -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="é›¶æ ·æœ¬å­¦ä¹ å®Œæ•´è®­ç»ƒæµç¨‹é…ç½®(å«æ—¥å¿—ä¸è€—æ—¶ç»Ÿè®¡)")

    # åŸºç¡€è·¯å¾„é…ç½®
    parser.add_argument('--data_dir', type=str,
                        default=r'D:\Code\2-ZSL\0-data\CWRU\dataset',
                        help="æ•°æ®é›†æ ¹ç›®å½•(å«è®­ç»ƒ/æµ‹è¯•æ•°æ®)")
    parser.add_argument('--save_dir', type=str,
                        default=r'D:\Code\2-ZSL\1-output\ç‰¹å¾è§£è€¦ç»“æœ',
                        help="æ¨¡å‹ã€æ—¥å¿—ã€ç»“æœæ–‡ä»¶ä¿å­˜æ ¹ç›®å½•")
    parser.add_argument('--train_class', type=str,
                        default=r'D:\Code\2-ZSL\0-data\CWRU\dataset\seen_classes.txt',
                        help="è®­ç»ƒç±»åˆ«åˆ—è¡¨æ–‡ä»¶è·¯å¾„(æ¯è¡Œä¸€ä¸ªç±»åˆ«)")
    parser.add_argument('--factor_index_map_path', type=str,
                        default=r'D:\Code\2-ZSL\0-data\CWRU\dataset\factor_index_map.txt',
                        help="å› å­-ç´¢å¼•æ˜ å°„æ–‡ä»¶è·¯å¾„(ç”¨äºè§£è€¦æ¨¡å‹)")

    # æµ‹è¯•é…ç½®
    parser.add_argument('--test_list', type=str, nargs='+',
                        default=['0HP', '1HP', '2HP', '3HP'],
                        help="æµ‹è¯•é›†å‰ç¼€åˆ—è¡¨(å¦‚['0HP','1HP'],å¯¹åº”æµ‹è¯•é›†æ–‡ä»¶{å‰ç¼€}.txt)")
    parser.add_argument('--predict_batch_size', type=int, default=1000,
                        help="é¢„æµ‹é˜¶æ®µçš„æ‰¹æ¬¡å¤§å°(æ ¹æ®æ˜¾å­˜è°ƒæ•´)")

    # é€šç”¨é…ç½®
    parser.add_argument('--num_workers', type=int, default=0,
                        help="æ•°æ®åŠ è½½è¿›ç¨‹æ•°(Windowså»ºè®®0,Linuxå»ºè®®CPUæ ¸å¿ƒæ•°-1)")
    parser.add_argument('--attribute_dim', type=int, default=64,
                        help="å±æ€§å‘é‡ç»´åº¦(è§£è€¦æ¨¡å‹ä¸å±æ€§æŠ•å½±æ¨¡å‹å…±ç”¨)")
    parser.add_argument('--embed_dim', type=int, default=512,
                        help="åµŒå…¥å‘é‡ç»´åº¦(å±æ€§æŠ•å½±æ¨¡å‹ç”¨)")

    # ç‰¹å¾è§£è€¦è®­ç»ƒé…ç½®
    parser.add_argument('--class_dims', type=int, nargs='+', default=[3, 4, 4],
                        help="è§£è€¦æ¨¡å‹çš„ç±»åˆ«ç»´åº¦åˆ—è¡¨(å¦‚[æ•…éšœç±»å‹æ•°, è´Ÿè½½æ•°, å…¶ä»–å› å­æ•°])")
    parser.add_argument('--disent_epochs', type=int, default=1,
                        help="ç‰¹å¾è§£è€¦æ¨¡å‹è®­ç»ƒè½®æ•°(è°ƒè¯•ç”¨1,æ­£å¼è®­ç»ƒå»ºè®®100+)")
    parser.add_argument('--disent_batch_size', type=int, default=20,
                        help="ç‰¹å¾è§£è€¦è®­ç»ƒæ‰¹æ¬¡å¤§å°(æ ¹æ®æ˜¾å­˜è°ƒæ•´)")
    parser.add_argument('--disent_lr', type=float, default=1e-2,
                        help="ç‰¹å¾è§£è€¦æ¨¡å‹å­¦ä¹ ç‡(åˆå§‹å»ºè®®1e-2,åæœŸå¯è¡°å‡)")
    parser.add_argument('--disent_patience', type=int, default=10,
                        help="ç‰¹å¾è§£è€¦æ—©åœè€å¿ƒå€¼(è¿ç»­10è½®æ— æå‡åˆ™åœæ­¢)")

    # å±æ€§æŠ•å½±è®­ç»ƒé…ç½®
    parser.add_argument('--att_proj_epochs', type=int, default=1,
                        help="å±æ€§æŠ•å½±æ¨¡å‹è®­ç»ƒè½®æ•°(è°ƒè¯•ç”¨1,æ­£å¼è®­ç»ƒå»ºè®®50+)")
    parser.add_argument('--att_proj_batch_size', type=int, default=100,
                        help="å±æ€§æŠ•å½±è®­ç»ƒæ‰¹æ¬¡å¤§å°")
    parser.add_argument('--att_proj_lr', type=float, default=1e-3,
                        help="å±æ€§æŠ•å½±æ¨¡å‹å­¦ä¹ ç‡")
    parser.add_argument('--att_proj_patience', type=int, default=10,
                        help="å±æ€§æŠ•å½±æ—©åœè€å¿ƒå€¼")

    # åˆ†ç±»å™¨è®­ç»ƒé…ç½®
    parser.add_argument('--cls_epochs', type=int, default=1,
                        help="åˆ†ç±»å™¨æ¨¡å‹è®­ç»ƒè½®æ•°(è°ƒè¯•ç”¨1,æ­£å¼è®­ç»ƒå»ºè®®100+)")
    parser.add_argument('--cls_batch_size', type=int, default=40,
                        help="åˆ†ç±»å™¨è®­ç»ƒæ‰¹æ¬¡å¤§å°")
    parser.add_argument('--cls_lr', type=float, default=1e-3,
                        help="åˆ†ç±»å™¨æ¨¡å‹å­¦ä¹ ç‡")
    parser.add_argument('--cls_patience', type=int, default=10,
                        help="åˆ†ç±»å™¨æ—©åœè€å¿ƒå€¼")

    # ç‰¹å¾æŠ•å½±è®­ç»ƒé…ç½®
    parser.add_argument('--fea_proj_epochs', type=int, default=2,
                        help="ç‰¹å¾æŠ•å½±æ¨¡å‹è®­ç»ƒè½®æ•°(è°ƒè¯•ç”¨2,æ­£å¼è®­ç»ƒå»ºè®®150+)")
    parser.add_argument('--fea_proj_batch_size', type=int, default=300,
                        help="ç‰¹å¾æŠ•å½±è®­ç»ƒæ‰¹æ¬¡å¤§å°")
    parser.add_argument('--fea_proj_lr', type=float, default=1e-3,
                        help="ç‰¹å¾æŠ•å½±æ¨¡å‹å­¦ä¹ ç‡")
    parser.add_argument('--fea_proj_patience', type=int, default=10,
                        help="ç‰¹å¾æŠ•å½±æ—©åœè€å¿ƒå€¼")

    return parser.parse_args(args) if args is not None else parser.parse_args()


@dataclass
class TrainConfig:
    # ç‰¹å¾è§£è€¦è®­ç»ƒé…ç½®
    class_dims: List[int]
    disent_epochs: int = 150
    disent_batch_size: int = 500
    disent_lr: float = 1e-2
    disent_patience: int = 10
    # åŸºç¡€è·¯å¾„é…ç½®
    data_dir: str = r'D:\Code\2-ZSL\0-data\CWRU\dataset'
    save_dir: str = r'D:\Code\2-ZSL\1-output\ç‰¹å¾è§£è€¦ç»“æœ'
    train_class: str = r'D:\Code\2-ZSL\0-data\CWRU\dataset\seen_classes.txt'
    factor_index_map_path: str = r'D:\Code\2-ZSL\0-data\CWRU\dataset\factor_index_map.txt'

    # æµ‹è¯•é…ç½®
    test_list: List[str] = None  # ç”¨Noneåˆå§‹åŒ–ï¼Œåç»­èµ‹å€¼é»˜è®¤åˆ—è¡¨
    predict_batch_size: int = 1000

    # é€šç”¨é…ç½®
    num_workers: int = 0
    attribute_dim: int = 64
    embed_dim: int = 512

    # å±æ€§æŠ•å½±è®­ç»ƒé…ç½®
    att_proj_epochs: int = 600
    att_proj_batch_size: int = 500
    att_proj_lr: float = 1e-3
    att_proj_patience: int = 10

    # åˆ†ç±»å™¨è®­ç»ƒé…ç½®
    cls_epochs: int = 100
    cls_batch_size: int = 500
    cls_lr: float = 1e-3
    cls_patience: int = 10

    # ç‰¹å¾æŠ•å½±è®­ç»ƒé…ç½®
    fea_proj_epochs: int = 150
    fea_proj_batch_size: int = 500
    fea_proj_lr: float = 1e-3
    fea_proj_patience: int = 10

    def __post_init__(self):
        if self.test_list is None:
            self.test_list = ['0HP', '1HP', '2HP', '3HP']


if __name__ == '__main__':
    # opts = get_train_args()
    opts = TrainConfig(class_dims=[3, 4, 4],
                       disent_batch_size=20,
                       att_proj_batch_size=100,
                       cls_batch_size=40,
                       fea_proj_batch_size=100,
                       disent_epochs=2,
                       cls_epochs=2,
                       att_proj_epochs=2,
                       fea_proj_epochs=2,
                       )
    # æ‰“å°é…ç½®(æ§åˆ¶å°å¿«é€ŸæŸ¥çœ‹,æ—¥å¿—ä¼šé‡å¤è®°å½•)
    print("=" * 50)
    print("å½“å‰è®­ç»ƒé…ç½®(å‘½ä»¤è¡Œå‚æ•°)")
    print("=" * 50)
    for arg, value in sorted(vars(opts).items()):
        print(f"  --{arg}: {value}")
    print("=" * 50)
    # å¯åŠ¨è®­ç»ƒæµç¨‹
    run(opts)
